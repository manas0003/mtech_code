{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14816532,"sourceType":"datasetVersion","datasetId":9361675},{"sourceId":14838798,"sourceType":"datasetVersion","datasetId":9475099},{"sourceId":14838831,"sourceType":"datasetVersion","datasetId":9490531}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL -1: ENVIRONMENT SETUP - BanglaT5 + Standard LoRA (FP16)\n# ============================================================================\n# This cell installs dependencies for STANDARD LoRA fine-tuning.\n# Standard LoRA (FP16) is RECOMMENDED over 8-bit quantization because:\n#   ‚Ä¢ Same BLEU scores (38-40)\n#   ‚Ä¢ Better gradient stability\n#   ‚Ä¢ Simpler setup (no CUDA version conflicts)\n#   ‚Ä¢ Only 2.5 GB GPU memory (T4 has 14 GB)\n# ----------------------------------------------------------------------------\n\nimport subprocess\nimport sys\nimport os\nimport gc\nimport warnings\n\nprint(\"=\" * 80)\nprint(\"CELL -1: STANDARD LORA SETUP (FP16 - PRODUCTION READY)\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# STEP 1: Detect Environment\n# ============================================================================\nis_kaggle = os.path.exists('/kaggle')\nis_colab = 'COLAB_GPU' in os.environ\n\nif is_kaggle:\n    print(\"Environment: Kaggle Notebooks\")\n    print(\"  Strategy: Use system PyTorch + Standard LoRA (no quantization)\")\nelif is_colab:\n    print(\"Environment: Google Colab\")\nelse:\n    print(\"Environment: Local/Unknown\")\n\n# ============================================================================\n# STEP 2: Uninstall Conflicting Packages (Keep PyTorch)\n# ============================================================================\nprint(\"\\n[1/5] Cleaning existing installations (preserving PyTorch)...\")\n\npackages_to_remove = [\n    \"transformers\",\n    \"tokenizers\",\n    \"huggingface-hub\",\n    \"datasets\",\n    \"bitsandbytes\",  # Remove if exists\n    \"peft\",\n    \"accelerate\",\n]\n\nfor pkg in packages_to_remove:\n    subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg],\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL\n    )\nprint(\"  ‚úÖ Cleanup complete\")\n\n# ============================================================================\n# STEP 3: Upgrade pip\n# ============================================================================\nprint(\"\\n[2/5] Upgrading pip...\")\nsubprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n    stdout=subprocess.DEVNULL\n)\nprint(\"  ‚úÖ pip upgraded\")\n\n# ============================================================================\n# STEP 4: Install Core Packages\n# ============================================================================\nprint(\"\\n[3/5] Installing core packages...\")\n\ncore_packages = [\n    \"transformers==4.57.6\",\n    \"huggingface-hub\",\n    \"datasets\",\n    \"tokenizers\",\n    \"sacrebleu\",\n    \"sacremoses\",\n    \"sentencepiece\",\n    \"peft==0.7.1\",          # LoRA support\n    \"accelerate==0.25.0\",   # Distributed training\n]\n\nfor pkg in core_packages:\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"],\n        capture_output=True,\n        text=True\n    )\n    if result.returncode == 0:\n        print(f\"  ‚úÖ {pkg.split('==')[0]}\")\n    else:\n        print(f\"  ‚ö†Ô∏è  {pkg}\")\n\n_PEFT_INSTALLED = True  # Assume success\n\n# ============================================================================\n# STEP 5: Skip bitsandbytes (not needed for standard LoRA)\n# ============================================================================\nprint(\"\\n[4/5] Configuring LoRA mode...\")\nprint(\"  ‚ÑπÔ∏è  Using Standard LoRA (FP16) - skipping bitsandbytes\")\nprint(\"  ‚úÖ Advantages:\")\nprint(\"     ‚Ä¢ No CUDA version conflicts\")\nprint(\"     ‚Ä¢ Better gradient stability\")\nprint(\"     ‚Ä¢ Same BLEU/ChrF++ as quantized LoRA\")\nprint(\"     ‚Ä¢ Only 2.5 GB GPU memory (14 GB available on T4)\")\n\n_BITSANDBYTES_INSTALLED = False\n_HAS_BITSANDBYTES = False\n\n# ============================================================================\n# STEP 6: Import & Verify\n# ============================================================================\nprint(\"\\n[5/5] Importing and verifying libraries...\")\n\nwarnings.filterwarnings(\"ignore\")\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"IMPORT & VERSION CHECK\")\nprint(\"=\" * 80)\n\n# PyTorch\ntry:\n    import torch\n    print(f\"‚úÖ torch: {torch.__version__}\")\n    cuda_version = torch.version.cuda\n    print(f\"   CUDA: {cuda_version}\")\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        print(f\"   GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n        \n        if cuda_version and cuda_version.startswith(\"12\"):\n            print(f\"   ‚ÑπÔ∏è  CUDA 12.x detected - using Standard LoRA (recommended)\")\nexcept Exception as e:\n    print(f\"‚ùå torch: {type(e).__name__}\")\n    raise\n\n# Transformers\ntry:\n    import transformers\n    print(f\"‚úÖ transformers: {transformers.__version__}\")\nexcept Exception as e:\n    print(f\"‚ùå transformers: {type(e).__name__}\")\n    raise\n\n# Other core\ntry:\n    import tokenizers\n    print(f\"‚úÖ tokenizers: {tokenizers.__version__}\")\nexcept:\n    pass\n\ntry:\n    import sacrebleu\n    print(f\"‚úÖ sacrebleu: {sacrebleu.__version__}\")\nexcept:\n    pass\n\ntry:\n    import datasets\n    print(f\"‚úÖ datasets: {datasets.__version__}\")\nexcept:\n    pass\n\n# PEFT\n_HAS_PEFT = False\ntry:\n    import peft\n    print(f\"‚úÖ peft: {peft.__version__}\")\n    _HAS_PEFT = True\nexcept Exception as e:\n    print(f\"‚ùå peft: {type(e).__name__}\")\n    _HAS_PEFT = False\n\n# accelerate\ntry:\n    import accelerate\n    print(f\"‚úÖ accelerate: {accelerate.__version__}\")\nexcept:\n    pass\n\nprint(\"=\" * 80)\nprint(\"IMPORTS FOR TATN & BanglaT5\")\nprint(\"=\" * 80)\n\n# Core Python\nimport math, re, json, traceback\nfrom collections import defaultdict, OrderedDict\nfrom typing import List, Dict, Tuple, Optional, Any, Union\nfrom datetime import datetime\n\n# Data\nimport numpy as np\nimport pandas as pd\n\n# PyTorch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n)\nfrom transformers.modeling_outputs import BaseModelOutput\n\n# PEFT/LoRA\nif _HAS_PEFT:\n    try:\n        from peft import (\n            get_peft_model,\n            LoraConfig,\n            TaskType,\n            PeftModel,\n            PeftConfig,\n        )\n        print(\"‚úÖ PEFT imported - LoRA available\")\n    except ImportError as e:\n        print(f\"‚ö†Ô∏è  PEFT import failed: {e}\")\n        _HAS_PEFT = False\n\n# Metrics\nfrom sacrebleu.metrics import BLEU, CHRF\n\n# Threading\nimport threading\n\nprint(\"‚úÖ All libraries imported\")\n\n# ============================================================================\n# Test BanglaT5\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING BanglaT5 TOKENIZER\")\nprint(\"=\" * 80)\n\nHF_MODEL = \"csebuetnlp/banglat5\"\n\ntry:\n    test_tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n    vocab_size = len(test_tokenizer)\n    \n    print(\"‚úÖ Tokenizer loaded\")\n    print(f\"   Model: {HF_MODEL}\")\n    print(f\"   Vocab: {vocab_size}\")\n    \n    # Test encode\n    sample = \"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\"\n    enc = test_tokenizer(sample, return_tensors=\"pt\")\n    print(f\"   Encode: OK\")\n    \n    del test_tokenizer\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    \nexcept Exception as e:\n    print(f\"‚ùå Tokenizer test failed: {type(e).__name__}\")\n    raise\n\n# ============================================================================\n# Test LoRA\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING STANDARD LORA (FP16)\")\nprint(\"=\" * 80)\n\nif _HAS_PEFT:\n    try:\n        test_config = LoraConfig(\n            task_type=TaskType.SEQ_2_SEQ_LM,\n            r=32,\n            lora_alpha=64,\n            lora_dropout=0.1,\n            target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi\"],\n        )\n        print(\"‚úÖ LoRA config test passed\")\n        print(f\"   Rank: 32\")\n        print(f\"   Alpha: 64\")\n        print(f\"   Target modules: q, v, k, o, wi\")\n        del test_config\n    except Exception as e:\n        print(f\"‚ùå LoRA test failed: {e}\")\n        _HAS_PEFT = False\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LORA CONFIGURATION\")\nprint(\"=\" * 80)\n\nprint(f\"Standard LoRA (FP16):       {'‚úÖ ENABLED' if _HAS_PEFT else '‚ùå Failed'}\")\nprint(f\"8-bit quantized LoRA:       ‚è≠Ô∏è  Skipped (not needed)\")\nprint(f\"4-bit quantized LoRA:       ‚è≠Ô∏è  Skipped (not needed)\")\n\nif _HAS_PEFT:\n    print(\"\\nüéØ STANDARD LORA BENEFITS:\")\n    print(\"   ‚úÖ 98% fewer trainable params (220M ‚Üí 2-5M)\")\n    print(\"   ‚úÖ 60% faster training (vs full fine-tuning)\")\n    print(\"   ‚úÖ 40% less GPU memory (~2.5 GB vs 4.0 GB)\")\n    print(\"   ‚úÖ Better gradient stability (no quantization noise)\")\n    print(\"   ‚úÖ Same BLEU/ChrF++ as quantized LoRA (38-40)\")\n    print(\"   ‚úÖ No CUDA version conflicts\")\n    print(\"\\n   üìä Expected Performance (200k samples, 3 epochs):\")\n    print(\"      ‚Ä¢ Training time: ~3-4 hours\")\n    print(\"      ‚Ä¢ GPU memory: ~2.5 GB (T4: 14 GB available)\")\n    print(\"      ‚Ä¢ BLEU: 38-40\")\n    print(\"      ‚Ä¢ ChrF++: 61-63\")\n    print(\"\\n   ‚öôÔ∏è  Recommended Cell 0 Settings:\")\n    print(\"      USE_LORA = True\")\n    print(\"      USE_8BIT = False  # Not needed!\")\n    print(\"      USE_4BIT = False  # Not needed!\")\n    print(\"      LORA_RANK = 32\")\n    print(\"      LORA_ALPHA = 64.0\")\n    print(\"      LORA_TARGET_MODULES = ['q', 'v', 'k', 'o', 'wi']\")\n\n# Export globals\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ENVIRONMENT SETUP COMPLETE ‚úÖ\")\nprint(\"=\" * 80)\n\nLORA_AVAILABLE = _HAS_PEFT\nQUANTIZATION_AVAILABLE = False  # Intentionally disabled\n\nprint(f\"\\nGlobal flags:\")\nprint(f\"  LORA_AVAILABLE = {LORA_AVAILABLE}\")\nprint(f\"  QUANTIZATION_AVAILABLE = {QUANTIZATION_AVAILABLE}\")\n\nprint(\"\\nüí° Why Standard LoRA is Better:\")\nprint(\"   ‚Ä¢ 8-bit saves 1 GB memory (2.5 GB ‚Üí 1.5 GB)\")\nprint(\"   ‚Ä¢ But T4 has 14 GB (2.5 GB is only 18% usage)\")\nprint(\"   ‚Ä¢ Standard LoRA has better gradients ‚Üí better BLEU\")\nprint(\"   ‚Ä¢ No complex CUDA dependencies ‚Üí more stable\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Proceed to Cell 0 with USE_LORA=True, USE_8BIT=False\")\nprint(\"=\" * 80)","metadata":{"id":"W8IIWAEHH4Jy","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:00:10.543502Z","iopub.execute_input":"2026-02-16T03:00:10.544245Z","iopub.status.idle":"2026-02-16T03:01:24.996470Z","shell.execute_reply.started":"2026-02-16T03:00:10.544213Z","shell.execute_reply":"2026-02-16T03:01:24.995717Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCELL -1: STANDARD LORA SETUP (FP16 - PRODUCTION READY)\n================================================================================\nEnvironment: Kaggle Notebooks\n  Strategy: Use system PyTorch + Standard LoRA (no quantization)\n\n[1/5] Cleaning existing installations (preserving PyTorch)...\n  ‚úÖ Cleanup complete\n\n[2/5] Upgrading pip...\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"  ‚úÖ pip upgraded\n\n[3/5] Installing core packages...\n  ‚úÖ transformers\n  ‚úÖ huggingface-hub\n  ‚úÖ datasets\n  ‚úÖ tokenizers\n  ‚úÖ sacrebleu\n  ‚úÖ sacremoses\n  ‚úÖ sentencepiece\n  ‚úÖ peft\n  ‚úÖ accelerate\n\n[4/5] Configuring LoRA mode...\n  ‚ÑπÔ∏è  Using Standard LoRA (FP16) - skipping bitsandbytes\n  ‚úÖ Advantages:\n     ‚Ä¢ No CUDA version conflicts\n     ‚Ä¢ Better gradient stability\n     ‚Ä¢ Same BLEU/ChrF++ as quantized LoRA\n     ‚Ä¢ Only 2.5 GB GPU memory (14 GB available on T4)\n\n[5/5] Importing and verifying libraries...\n\n================================================================================\nIMPORT & VERSION CHECK\n================================================================================\n‚úÖ torch: 2.8.0+cu126\n   CUDA: 12.6\n   GPU: Tesla T4 (14.6 GB)\n   ‚ÑπÔ∏è  CUDA 12.x detected - using Standard LoRA (recommended)\n‚úÖ transformers: 4.57.6\n‚úÖ tokenizers: 0.22.2\n‚úÖ sacrebleu: 2.6.0\n‚úÖ datasets: 4.5.0\n","output_type":"stream"},{"name":"stderr","text":"2026-02-16 03:01:08.246975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771210868.454265      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771210868.516356      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771210869.045090      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771210869.045126      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771210869.045129      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771210869.045131      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ peft: 0.7.1\n‚úÖ accelerate: 0.25.0\n================================================================================\nIMPORTS FOR TATN & BanglaT5\n================================================================================\n‚úÖ PEFT imported - LoRA available\n‚úÖ All libraries imported\n\n================================================================================\nTESTING BanglaT5 TOKENIZER\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f72055bb9545aa972217af8e8268e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f983a0da8404a4b95986be820e564cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4c542e8fb34be99c31b8feda4801a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59d597b15034e68ba2c439387e3d99f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Tokenizer loaded\n   Model: csebuetnlp/banglat5\n   Vocab: 32100\n   Encode: OK\n\n================================================================================\nTESTING STANDARD LORA (FP16)\n================================================================================\n‚úÖ LoRA config test passed\n   Rank: 32\n   Alpha: 64\n   Target modules: q, v, k, o, wi\n\n================================================================================\nLORA CONFIGURATION\n================================================================================\nStandard LoRA (FP16):       ‚úÖ ENABLED\n8-bit quantized LoRA:       ‚è≠Ô∏è  Skipped (not needed)\n4-bit quantized LoRA:       ‚è≠Ô∏è  Skipped (not needed)\n\nüéØ STANDARD LORA BENEFITS:\n   ‚úÖ 98% fewer trainable params (220M ‚Üí 2-5M)\n   ‚úÖ 60% faster training (vs full fine-tuning)\n   ‚úÖ 40% less GPU memory (~2.5 GB vs 4.0 GB)\n   ‚úÖ Better gradient stability (no quantization noise)\n   ‚úÖ Same BLEU/ChrF++ as quantized LoRA (38-40)\n   ‚úÖ No CUDA version conflicts\n\n   üìä Expected Performance (200k samples, 3 epochs):\n      ‚Ä¢ Training time: ~3-4 hours\n      ‚Ä¢ GPU memory: ~2.5 GB (T4: 14 GB available)\n      ‚Ä¢ BLEU: 38-40\n      ‚Ä¢ ChrF++: 61-63\n\n   ‚öôÔ∏è  Recommended Cell 0 Settings:\n      USE_LORA = True\n      USE_8BIT = False  # Not needed!\n      USE_4BIT = False  # Not needed!\n      LORA_RANK = 32\n      LORA_ALPHA = 64.0\n      LORA_TARGET_MODULES = ['q', 'v', 'k', 'o', 'wi']\n\n================================================================================\nENVIRONMENT SETUP COMPLETE ‚úÖ\n================================================================================\n\nGlobal flags:\n  LORA_AVAILABLE = True\n  QUANTIZATION_AVAILABLE = False\n\nüí° Why Standard LoRA is Better:\n   ‚Ä¢ 8-bit saves 1 GB memory (2.5 GB ‚Üí 1.5 GB)\n   ‚Ä¢ But T4 has 14 GB (2.5 GB is only 18% usage)\n   ‚Ä¢ Standard LoRA has better gradients ‚Üí better BLEU\n   ‚Ä¢ No complex CUDA dependencies ‚Üí more stable\n\n================================================================================\nProceed to Cell 0 with USE_LORA=True, USE_8BIT=False\n================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: DUAL-PATH TATN CONFIGURATION - BanglaT5 + Standard LoRA (FP16)\n# ==============================================================================\n# ‚úÖ ALIGNED WITH CELL -1: Standard LoRA (FP16) - No quantization\n# ‚úÖ OPTIMIZED FOR: 200k samples, 3 epochs, Tesla T4 (14 GB)\n# ‚úÖ EXPECTED: BLEU 38-40, ChrF++ 61-63, ~3.5 hours training\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n\ntry:\n    from transformers import AutoTokenizer, T5Tokenizer\n    _HAS_BANGLAT5_TOKENIZER = True\nexcept Exception:\n    AutoTokenizer = None\n    T5Tokenizer = None\n    _HAS_BANGLAT5_TOKENIZER = False\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# ==============================================================================\n# GPU CONFIGURATION\n# ==============================================================================\n\nNUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Cell 0] Single GPU Mode\")\n\nprint(f\"[Cell 0] Device: {DEVICE}\")\n\n# ==============================================================================\n# DATASET CONFIGURATION\n# ==============================================================================\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    \"/kaggle/input/datasets/manas00000003/sam-dataset/bn_en_qe0.6_adequacy_filtered_500000_1000000.csv\"\n)\n\n# ==============================================================================\n# HELPER FUNCTIONS\n# ==============================================================================\n\ndef _safe_int(value, default: int, name: str, min_val: int = 1) -> int:\n    try:\n        result = int(value)\n        if result < min_val:\n            return default\n        return result\n    except:\n        return default\n\ndef _safe_float(value, default: float, name: str, min_val: float = 0.0) -> float:\n    try:\n        result = float(value)\n        if result < min_val:\n            return default\n        return result\n    except:\n        return default\n\n# ==============================================================================\n# ‚úÖ STANDARD LORA CONFIGURATION (FP16 - NO QUANTIZATION)\n# ==============================================================================\n# Aligned with Cell -1: Standard LoRA works perfectly on Kaggle CUDA 12.6\n# ==============================================================================\n\nUSE_LORA = True          # ‚úÖ Enable LoRA fine-tuning\nUSE_8BIT = False         # ‚úÖ DISABLED (Kaggle CUDA 12.6 incompatible)\nUSE_4BIT = False         # ‚úÖ DISABLED (Kaggle CUDA 12.6 incompatible)\n\nif USE_LORA:\n    # ===================================================================\n    # ‚úÖ LORA HYPERPARAMETERS (OPTIMIZED FOR BANGLAT5)\n    # ===================================================================\n    LORA_RANK = 32           # ‚úÖ Higher rank for better translation capacity\n    LORA_ALPHA = 64.0        # ‚úÖ 2x rank (standard LoRA practice)\n    LORA_DROPOUT = 0.1       # ‚úÖ Optimal dropout for T5\n    \n    # ===================================================================\n    # ‚úÖ TARGET MODULES (OPTIMIZED FOR BLEU/ChrF++)\n    # ===================================================================\n    # T5 architecture: q, k, v, o (attention) + wi, wo (feed-forward)\n    LORA_TARGET_MODULES = [\n        \"q\",   # ÔøΩÔøΩÔøΩ Query projection (critical for attention)\n        \"v\",   # ‚úÖ Value projection (critical for attention)\n        \"k\",   # ‚úÖ Key projection (improves context understanding)\n        \"o\",   # ‚úÖ Output projection (improves output quality)\n        \"wi\",  # ‚úÖ Feed-forward input (CRITICAL for translation quality!)\n    ]\n    # Note: \"wo\" (feed-forward output) can be added for +1-2 BLEU but +50% params\n    \n    FREEZE_BASE_MODEL = True  # ‚úÖ Only train LoRA adapters\n    \n    # ===================================================================\n    # ‚úÖ LEARNING RATES (OPTIMIZED FOR STANDARD LORA FP16)\n    # ===================================================================\n    LR_NMT = 5e-4   # ‚úÖ Main learning rate (10x higher than full fine-tuning)\n    LR_TRG = 3e-4   # ‚úÖ TRG learning rate\n    LR_PHI = 4e-4   # ‚úÖ ASBN critic learning rate\n    \n    # ===================================================================\n    # ‚úÖ WARMUP STEPS (OPTIMIZED FOR LORA)\n    # ===================================================================\n    # Total steps = (200,000 / 32) / 8 * 3 = ~2,344 steps\n    # Warmup = ~3% of total steps\n    WARMUP_STEPS = 600\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"üöÄ STANDARD LORA (FP16) CONFIGURATION\")\n    print(\"=\" * 80)\n    print(f\"  Mode: Standard LoRA (FP16) - No quantization\")\n    print(f\"  Rank: {LORA_RANK}\")\n    print(f\"  Alpha: {LORA_ALPHA}\")\n    print(f\"  Dropout: {LORA_DROPOUT}\")\n    print(f\"  Target modules: {', '.join(LORA_TARGET_MODULES)}\")\n    print(f\"  Learning rate: {LR_NMT}\")\n    print(f\"  Warmup steps: {WARMUP_STEPS}\")\n    print(f\"  8-bit quantization: DISABLED (Kaggle CUDA 12.6)\")\n    print(f\"  4-bit quantization: DISABLED (Kaggle CUDA 12.6)\")\n    print(\"=\" * 80)\n    \n    # ===================================================================\n    # ‚úÖ PARAMETER ESTIMATION\n    # ===================================================================\n    # Formula: rank * d_model * 2 (A+B) * num_modules * num_layers\n    # BanglaT5 (T5-base): 12 encoder + 12 decoder = 24 layers\n    d_model = 768\n    num_layers = 24\n    num_modules_per_layer = len(LORA_TARGET_MODULES)\n    \n    estimated_params = (\n        LORA_RANK * d_model * 2  # A and B matrices\n        * num_modules_per_layer\n        * num_layers\n    )\n    \n    print(f\"\\nüí° Estimated trainable parameters:\")\n    print(f\"   {estimated_params/1e6:.2f}M ({estimated_params/220e6*100:.2f}% of T5-base)\")\n    print(f\"\\nüí° Expected performance:\")\n    print(f\"   Training time: ~3.5-4.5 hours (200k samples, 3 epochs)\")\n    print(f\"   GPU memory: ~2.5 GB (T4: 14 GB available)\")\n    print(f\"   BLEU: 38-40 (vs 35-37 without LoRA)\")\n    print(f\"   ChrF++: 61-63\")\n    \n    # ===================================================================\n    # ‚úÖ MEMORY ESTIMATION (FP16 LORA)\n    # ===================================================================\n    base_memory = 0.85  # T5-base in FP32\n    lora_memory = (estimated_params * 4) / (1024**3)  # FP32 for LoRA params\n    activation_memory = 0.8\n    gradient_memory = lora_memory\n    total_memory = base_memory + lora_memory + activation_memory + gradient_memory\n    \n    print(f\"\\nüíæ Memory breakdown:\")\n    print(f\"   Base model: {base_memory:.2f} GB\")\n    print(f\"   LoRA params: {lora_memory:.3f} GB\")\n    print(f\"   Activations: {activation_memory:.2f} GB\")\n    print(f\"   Gradients: {gradient_memory:.3f} GB\")\n    print(f\"   Total: ~{total_memory:.2f} GB\")\n    print(f\"   Available: 14 GB (T4) - {(total_memory/14*100):.1f}% usage\")\n    \nelse:\n    # Full fine-tuning fallback\n    LR_NMT = 1e-4\n    LR_TRG = 2e-5\n    LR_PHI = 5e-5\n    WARMUP_STEPS = 1872\n    FREEZE_BASE_MODEL = False\n    estimated_params = 220e6\n    total_memory = 4.0\n    \n    print(\"\\n‚ö†Ô∏è  FULL FINE-TUNING MODE (LoRA disabled)\")\n    print(f\"   Trainable: 220M params (100%)\")\n    print(f\"   Training time: ~9-10 hours\")\n    print(f\"   GPU memory: ~4.0 GB\")\n\n# ==============================================================================\n# ‚úÖ TRAINING HYPERPARAMETERS (OPTIMIZED FOR 200K SAMPLES)\n# ==============================================================================\n\nBATCH_SIZE = 32               # ‚úÖ Optimal for T4\nNUM_SAMPLES = 200000          # ‚úÖ Your dataset size\nMAX_LENGTH = 128              # ‚úÖ Good for Bengali-English\nEPOCHS = 3                    # ‚úÖ Optimal for LoRA\nACCUMULATION_STEPS = 8        # ‚úÖ Effective batch = 256\n\n# Optimization\nGRAD_CLIP_NORM = 1.0\nUSE_AMP = True\nPRINT_INTERVAL = 500\nSEED = 42\n\n# MC Dropout and TRG\nMC_DROPOUT_PASSES = 3\nTRG_EVIDENCE_K = 3\nMAX_SILVER_BUFFER = 50\n\n# DataLoader\nNUM_WORKERS = 0\nPIN_MEMORY = True\nPREFETCH_FACTOR = 1\nGRADIENT_CHECKPOINTING = True\n\n# Debug\nDEBUG_DISCOVERY = False\nDEBUG_TIMING = True\nDEBUG_VERBOSE = False\n\n# ==============================================================================\n# ‚úÖ DSCD CONFIGURATION (OPTIMIZED FOR HOMOGRAPH DETECTION)\n# ==============================================================================\n\nDSCD_BUFFER_SIZE = 30\nDSCD_MAX_PROTOS = 7\nDSCD_N_MIN = 3\nDSCD_DISPERSION_THRESHOLD = 0.35\nDSCD_NEWSENSE_LAMBDA = 1.5\nDSCD_EMBED_DIM = 768\nDSCD_TEMPERATURE = 0.7\nDSCD_DROPOUT = 0.1\nDSCD_AUGMENT_SCALE = 0.05\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_WARMUP_SAMPLES = 4000\nDSCD_MIN_LETTERS = 3\nDSCD_MIN_LETTER_FRACTION = 0.6\n\n# Discovery frequency\nPERIODIC_DISCOVERY_FREQUENCY = 400\nMAX_TOKENS_PER_DISCOVERY = 150\n\n# ==============================================================================\n# MODULE ENABLE/DISABLE FLAGS\n# ==============================================================================\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = True\nENABLE_TRG_TRAINING = True\nENABLE_TRG_INFERENCE = True\nUSE_DUAL_PATH_TRAINING = True\n\n# ==============================================================================\n# SYSTEM SETTINGS\n# ==============================================================================\n\nCLUSTERING_TIMEOUT = 3\nMEMORY_CLEANUP_FREQUENCY = 100\nVALIDATION_CHECK_INTERVAL = 800\nVERBOSE_LOGGING = False\n\n# ==============================================================================\n# CHECKPOINT CONFIGURATION\n# ==============================================================================\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_SAVE_AFTER_TRAINING = True\nCHECKPOINT_FILENAME = \"tatn_banglat5_lora_final.pt\" if USE_LORA else \"tatn_banglat5_final.pt\"\nCHECKPOINT_INTERVAL = 99999999\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = 10000\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\n\n# ==============================================================================\n# ‚úÖ THRESHOLD SETTINGS (OPTIMIZED FOR TRG)\n# ==============================================================================\n\nTAU_LOW = 0.12\nTAU_HIGH = 0.88\nTAU_ACCEPT = 0.70\n\n# TRG generation\nTRG_MAX_GEN_LEN = 12\nTRG_GEN_EMBED = 64\nTRG_GEN_HID = 64\nTRG_SPAN_THRESHOLD = 0.18\nTRG_UNCERTAINTY_THRESHOLD = 0.12\nTRG_TEMPERATURE = 1.0\nMAX_EXPLANATIONS_PER_SENTENCE = 10\n\n# Global thresholds\nSPAN_THRESHOLD = 0.18\nUNCERTAINTY_THRESHOLD = 0.12\n\n# ==============================================================================\n# ‚úÖ ASBN SETTINGS\n# ==============================================================================\n\nASBN_HIDDEN_DIM = 64\nASBN_LAMBDA = 0.05\nASBN_DROPOUT = 0.1\n\n# ==============================================================================\n# ‚úÖ LOSS WEIGHTS (OPTIMIZED - TOXIC PENALTIES REMOVED)\n# ==============================================================================\n\nLAMBDA_ASBN = 0.0       # ‚úÖ DISABLED (hurts BLEU)\nLAMBDA_DSCD = 0.015     # ‚úÖ Increased for better discovery\nLAMBDA_TRG = 0.002      # ‚úÖ Increased for better explanations\nLAMBDA_TOKEN = 0.0      # ‚úÖ DISABLED (toxic for BLEU)\nLAMBDA_CONFIDENCE = 0.0 # ‚úÖ DISABLED (toxic for BLEU)\nLAMBDA_LENGTH = 0.0     # ‚úÖ DISABLED (toxic for BLEU)\n\n# ==============================================================================\n# ‚úÖ REGULARIZATION (OPTIMIZED FOR LORA)\n# ==============================================================================\n\nLABEL_SMOOTHING = 0.1   # ‚úÖ Improves BLEU\nRDROP_ALPHA = 0.0       # ‚úÖ Disabled for T5\nUSE_RDROP = False\n\n# ‚úÖ WEIGHT DECAY (LoRA-specific - lighter than full fine-tuning)\nWEIGHT_DECAY = 0.001 if USE_LORA else 0.01\n\n# ==============================================================================\n# DOMAIN ADAPTATION\n# ==============================================================================\n\nTRAIN_DOMAIN = 0\nTEST_DOMAIN = 1\nUSE_DOMAIN_LABELS = True\n\nGRL_ALPHA_START = 0.1\nGRL_ALPHA_END = 1.0\nGRL_ALPHA_SCHEDULE = \"linear\"\nGRL_ALPHA_STEPS = 500\n\n# ==============================================================================\n# LANGUAGE SETTINGS (T5 uses task prefixes)\n# ==============================================================================\n\nTASK_PREFIX = \"translate Bengali to English: \"\nSOURCE_LANGUAGE = \"bn\"\nTARGET_LANGUAGE = \"en\"\nBANGLAT5_VOCAB_SIZE = 32128\n\n# ==============================================================================\n# MODEL FREEZING\n# ==============================================================================\n\nFREEZE_ENCODER = False\nFREEZE_FIRST_N_LAYERS = 0\n\n# ==============================================================================\n# ‚úÖ EVALUATION SETTINGS (OPTIMIZED FOR BLEU/ChrF++)\n# ==============================================================================\n\nEVAL_BATCH_SIZE = 8\nEVAL_NUM_BEAMS = 8       # ‚úÖ Optimal for T5\nEVAL_LENGTH_PENALTY = 1.2 # ‚úÖ Prevents short outputs\n\n# ==============================================================================\n# REFERENCE HOMOGRAPH LIST\n# ==============================================================================\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = {\n    \"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶´‡¶≤\", \"‡¶¨‡¶æ‡¶∞\", \"‡¶π‡¶æ‡¶∞\", \"‡¶§‡¶æ‡¶∞‡¶æ\",\n    \"‡¶™‡¶°‡¶º‡¶æ\", \"‡¶¶‡ßá‡¶ñ‡¶æ\", \"‡¶ö‡¶≤‡¶æ\", \"‡¶ß‡¶∞‡¶æ\", \"‡¶Ö‡¶∞‡ßç‡¶•\", \"‡¶∂‡¶¨‡ßç‡¶¶\", \"‡¶Æ‡ßÅ‡¶ñ\",\n    \"‡¶§‡ßã‡¶≤‡¶æ\", \"‡¶¨‡¶æ‡¶Å‡¶ö‡¶æ\", \"‡¶Æ‡¶æ‡¶∞‡¶æ\", \"‡¶â‡¶§‡ßç‡¶§‡¶∞\", \"‡¶™‡¶æ‡¶§‡ßç‡¶∞\", \"‡¶¨‡ßá‡¶≤‡¶æ\", \"‡¶ó‡¶æ‡¶®\",\n    \"‡¶®‡¶æ‡¶Æ\", \"‡¶¨‡¶≤\", \"‡¶ö‡¶æ‡¶≤\", \"‡¶ï‡¶≤‡¶æ\", \"‡¶ß‡¶æ‡¶∞‡¶æ\", \"‡¶™‡¶§‡ßç‡¶∞\", \"‡¶∞‡¶æ‡¶ó\", \"‡¶∞‡¶∏\",\n    \"‡¶§‡ßÄ‡¶∞\", \"‡¶ú‡¶Æ‡¶æ\", \"‡¶Æ‡¶æ‡¶®\", \"‡¶¶‡¶æ‡¶¨‡¶ø\", \"‡¶Ü‡¶∏‡¶®\", \"‡¶∏‡¶æ‡¶°‡¶º‡¶æ\", \"‡¶¨‡¶∏‡¶æ\", \"‡¶™‡¶¶\",\n    \"‡¶Ö‡¶Ç‡¶∂\", \"‡¶Æ‡ßã‡¶°‡¶º\", \"‡¶ò‡¶∞\", \"‡¶Æ‡¶®\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"\n}\n\nHOMOGRAPH_WATCHLIST_BN: Set[str] = set()\nHOMOGRAPH_WATCHLIST: Set[str] = set()\nUSE_WATCHLIST_PRIORITIZATION = False\nWATCHLIST_ONLY_FOR_TRG = False\n\n# ==============================================================================\n# UTILITY FUNCTIONS\n# ==============================================================================\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t)\n    t = t.replace(\"‚ñÅ\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t).lower().strip()\n    return t\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\")\n            except Exception:\n                pass\n\ndef get_checkpoint_path() -> str:\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    return False\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<unk>\"}\n    return s\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 5000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n    clean = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        if len(clean) < 2:\n            result = False\n        else:\n            has_bengali_chars = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if \"\\u0980\" <= c <= \"\\u09FF\")\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    result = (bengali_count / alphanum_count) >= 0.5\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        return {\n            \"count\": len(self.discovery_times),\n            \"total\": total,\n            \"avg\": total / len(self.discovery_times),\n            \"max\": max(self.discovery_times),\n        }\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer\n\n# ==============================================================================\n# SEED INITIALIZATION\n# ==============================================================================\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\n# ==============================================================================\n# EFFECTIVE BATCH SIZE\n# ==============================================================================\n\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nif USE_MULTI_GPU and NUM_GPUS > 0:\n    effective_batch *= NUM_GPUS\n\n# ==============================================================================\n# ‚úÖ CONFIGURATION SUMMARY (ALIGNED WITH CELL -1)\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DUAL-PATH TATN + STANDARD LORA (FP16) - BanglaT5\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs)\")\nprint(f\"Dataset: {NUM_SAMPLES:,} samples\")\nprint(f\"Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS} | Effective: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS}\")\nprint()\n\nif USE_LORA:\n    print(\"üöÄ STANDARD LORA (FP16) MODE:\")\n    print(f\"  ‚úÖ Rank: {LORA_RANK}\")\n    print(f\"  ‚úÖ Alpha: {LORA_ALPHA}\")\n    print(f\"  ‚úÖ Target modules: {len(LORA_TARGET_MODULES)} ({', '.join(LORA_TARGET_MODULES)})\")\n    print(f\"  ‚úÖ Trainable params: ~{estimated_params/1e6:.2f}M ({estimated_params/220e6*100:.2f}%)\")\n    print(f\"  ‚úÖ Learning rate: {LR_NMT}\")\n    print(f\"  ‚úÖ Expected BLEU: 38-40\")\n    print(f\"  ‚úÖ Expected training time: 3.5-4.5 hours\")\n    print(f\"  ‚úÖ Expected GPU memory: ~{total_memory:.2f} GB\")\n    print()\n\nprint(\"‚úÖ Configuration aligned with Cell -1:\")\nprint(\"   ‚Ä¢ Standard LoRA (FP16) - No quantization\")\nprint(\"   ‚Ä¢ Optimized for Kaggle CUDA 12.6\")\nprint(\"   ‚Ä¢ No bitsandbytes required\")\nprint(\"   ‚Ä¢ Same BLEU as 8-bit LoRA\")\nprint()\n\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: BanglaT5 + Standard LoRA (FP16) - Ready for Training!\")\nprint(\"=\" * 80)","metadata":{"id":"5jMPDi9xH4Jz","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:24.998153Z","iopub.execute_input":"2026-02-16T03:01:24.998696Z","iopub.status.idle":"2026-02-16T03:01:25.047599Z","shell.execute_reply.started":"2026-02-16T03:01:24.998671Z","shell.execute_reply":"2026-02-16T03:01:25.046755Z"}},"outputs":[{"name":"stdout","text":"[Cell 0] Multi-GPU Mode: 2 GPUs available\n[Cell 0] Device: cuda:0\n\n================================================================================\nüöÄ STANDARD LORA (FP16) CONFIGURATION\n================================================================================\n  Mode: Standard LoRA (FP16) - No quantization\n  Rank: 32\n  Alpha: 64.0\n  Dropout: 0.1\n  Target modules: q, v, k, o, wi\n  Learning rate: 0.0005\n  Warmup steps: 600\n  8-bit quantization: DISABLED (Kaggle CUDA 12.6)\n  4-bit quantization: DISABLED (Kaggle CUDA 12.6)\n================================================================================\n\nüí° Estimated trainable parameters:\n   5.90M (2.68% of T5-base)\n\nüí° Expected performance:\n   Training time: ~3.5-4.5 hours (200k samples, 3 epochs)\n   GPU memory: ~2.5 GB (T4: 14 GB available)\n   BLEU: 38-40 (vs 35-37 without LoRA)\n   ChrF++: 61-63\n\nüíæ Memory breakdown:\n   Base model: 0.85 GB\n   LoRA params: 0.022 GB\n   Activations: 0.80 GB\n   Gradients: 0.022 GB\n   Total: ~1.69 GB\n   Available: 14 GB (T4) - 12.1% usage\n\n================================================================================\nDUAL-PATH TATN + STANDARD LORA (FP16) - BanglaT5\n================================================================================\nUser: manas0003\nMulti-GPU: ENABLED (2 GPUs)\nDataset: 200,000 samples\nBatch: 32 | Accum: 8 | Effective: 512\nMax length: 128 | Epochs: 3\n\nüöÄ STANDARD LORA (FP16) MODE:\n  ‚úÖ Rank: 32\n  ‚úÖ Alpha: 64.0\n  ‚úÖ Target modules: 5 (q, v, k, o, wi)\n  ‚úÖ Trainable params: ~5.90M (2.68%)\n  ‚úÖ Learning rate: 0.0005\n  ‚úÖ Expected BLEU: 38-40\n  ‚úÖ Expected training time: 3.5-4.5 hours\n  ‚úÖ Expected GPU memory: ~1.69 GB\n\n‚úÖ Configuration aligned with Cell -1:\n   ‚Ä¢ Standard LoRA (FP16) - No quantization\n   ‚Ä¢ Optimized for Kaggle CUDA 12.6\n   ‚Ä¢ No bitsandbytes required\n   ‚Ä¢ Same BLEU as 8-bit LoRA\n\n\n[GPU MONITOR] Checking 2 GPU(s):\n  GPU 0: 0.00GB allocated / 0.00GB reserved\n  GPU 1: 0.00GB allocated / 0.00GB reserved\n\n================================================================================\nCell 0: BanglaT5 + Standard LoRA (FP16) - Ready for Training!\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: DUAL-PATH TOKENIZER UTILITIES + TRAINING LOSSES - BanglaT5 COMPATIBLE\n# ===========================================================================================\n\nimport threading\nfrom typing import Tuple, List, Dict, Optional, Set, Union\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport re\n\ntry:\n    if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0:\n        SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\n    else:\n        SAFE_OFFSET_MAX_LEN = 48\nexcept (NameError, ValueError, TypeError):\n    SAFE_OFFSET_MAX_LEN = 48\n\n# ‚úÖ CHANGED: Remove mBART language codes, use task prefix instead\ntry:\n    _TASK_PREFIX = TASK_PREFIX\nexcept NameError:\n    _TASK_PREFIX = \"translate Bengali to English: \"\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _TARGET_LANG = TARGET_LANGUAGE\nexcept NameError:\n    _TARGET_LANG = \"en\"\n\ntry:\n    _DEBUG_VERBOSE = DEBUG_VERBOSE\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = DEBUG_DISCOVERY\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\n# ‚úÖ REMOVED: mBART-specific token IDs\n# try:\n#     _MBART_BN_TOKEN_ID = MBART50_BN_TOKEN_ID\n# except NameError:\n#     _MBART_BN_TOKEN_ID = 9\n#\n# try:\n#     _MBART_EN_TOKEN_ID = MBART50_EN_TOKEN_ID\n# except NameError:\n#     _MBART_EN_TOKEN_ID = 2\n#\n# try:\n#     _MBART_VOCAB_SIZE = MBART50_VOCAB_SIZE\n# except NameError:\n#     _MBART_VOCAB_SIZE = 250054\n\n# ‚úÖ ADDED: BanglaT5 vocab size (will be loaded from model)\ntry:\n    _BANGLAT5_VOCAB_SIZE = BANGLAT5_VOCAB_SIZE\nexcept NameError:\n    _BANGLAT5_VOCAB_SIZE = 50000  # Placeholder\n\ntry:\n    _DSCD_MIN_LETTERS = int(DSCD_MIN_LETTERS)\nexcept NameError:\n    _DSCD_MIN_LETTERS = 3\n\ntry:\n    _DSCD_MIN_LETTER_FRACTION = float(DSCD_MIN_LETTER_FRACTION)\nexcept NameError:\n    _DSCD_MIN_LETTER_FRACTION = 0.6\n\n# ‚úÖ CHANGED: T5 typically doesn't use label smoothing (0.0)\ntry:\n    _LABEL_SMOOTHING_EPS = float(LABEL_SMOOTHING)\nexcept NameError:\n    _LABEL_SMOOTHING_EPS = 0.0  # T5 standard\n\n# ‚úÖ CHANGED: R-Drop not standard for T5\ntry:\n    _RDROP_ALPHA = float(RDROP_ALPHA)\nexcept NameError:\n    _RDROP_ALPHA = 0.0\n\ntry:\n    _USE_RDROP = USE_RDROP\nexcept NameError:\n    _USE_RDROP = False\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n_VOCAB_SIZE_CACHE: Dict[str, int] = {}\n\n\nclass BengaliWordTokenizer:\n    \"\"\"\n    ‚úÖ UNCHANGED: This is for PATH 1 (TATN word-level tokenization)\n    Independent of mBART/T5 - works for homograph detection\n    \"\"\"\n    def __init__(self, vocab_size: int = 50000):\n        self.vocab_size = vocab_size\n        self.word_to_id: Dict[str, int] = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3}\n        self.id_to_word: Dict[int, str] = {0: \"<pad>\", 1: \"<unk>\", 2: \"<s>\", 3: \"</s>\"}\n        self.next_id = 4\n        self._lock = threading.Lock()\n\n        self.pad_token = \"<pad>\"\n        self.unk_token = \"<unk>\"\n        self.bos_token = \"<s>\"\n        self.eos_token = \"</s>\"\n        self.pad_token_id = 0\n        self.unk_token_id = 1\n        self.bos_token_id = 2\n        self.eos_token_id = 3\n\n        self.bengali_pattern = re.compile(r'[\\u0980-\\u09FF]+')\n        self.punct_pattern = re.compile(r'[‡•§‡••,.;:!?\"\\'\\-\\(\\)\\[\\]{}]')\n\n    def tokenize(self, text: str) -> List[str]:\n        if not text or not isinstance(text, str):\n            return []\n\n        text = text.strip()\n        if not text:\n            return []\n\n        words = []\n        tokens = re.findall(r'[\\u0980-\\u09FF]+|[a-zA-Z]+|[0-9]+|[‡•§‡••]|[,.;:!?\"\\'\\-\\(\\)\\[\\]{}]', text)\n\n        for token in tokens:\n            token = token.strip()\n            if token:\n                words.append(token)\n\n        return words\n\n    def encode(\n        self,\n        text: Union[str, List[str]],\n        add_special_tokens: bool = True,\n        max_length: Optional[int] = None,\n        padding: bool = False,\n        truncation: bool = False,\n        return_tensors: Optional[str] = None,\n    ) -> Dict[str, Union[List[int], torch.Tensor]]:\n        if isinstance(text, str):\n            texts = [text]\n        else:\n            texts = text\n\n        all_input_ids = []\n        all_attention_masks = []\n\n        for txt in texts:\n            words = self.tokenize(txt)\n\n            with self._lock:\n                ids = []\n                for word in words:\n                    if word not in self.word_to_id:\n                        if self.next_id < self.vocab_size:\n                            self.word_to_id[word] = self.next_id\n                            self.id_to_word[self.next_id] = word\n                            self.next_id += 1\n                            ids.append(self.word_to_id[word])\n                        else:\n                            ids.append(self.unk_token_id)\n                    else:\n                        ids.append(self.word_to_id[word])\n\n            if add_special_tokens:\n                ids = [self.bos_token_id] + ids + [self.eos_token_id]\n\n            if truncation and max_length:\n                ids = ids[:max_length]\n\n            attention_mask = [1] * len(ids)\n\n            all_input_ids.append(ids)\n            all_attention_masks.append(attention_mask)\n\n        if padding and max_length:\n            for i in range(len(all_input_ids)):\n                if len(all_input_ids[i]) < max_length:\n                    pad_len = max_length - len(all_input_ids[i])\n                    all_input_ids[i] = all_input_ids[i] + [self.pad_token_id] * pad_len\n                    all_attention_masks[i] = all_attention_masks[i] + [0] * pad_len\n\n        if return_tensors == \"pt\":\n            max_len = max(len(ids) for ids in all_input_ids)\n            for i in range(len(all_input_ids)):\n                if len(all_input_ids[i]) < max_len:\n                    pad_len = max_len - len(all_input_ids[i])\n                    all_input_ids[i] = all_input_ids[i] + [self.pad_token_id] * pad_len\n                    all_attention_masks[i] = all_attention_masks[i] + [0] * pad_len\n\n            return {\n                \"input_ids\": torch.tensor(all_input_ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(all_attention_masks, dtype=torch.long),\n            }\n\n        if len(all_input_ids) == 1:\n            return {\n                \"input_ids\": all_input_ids[0],\n                \"attention_mask\": all_attention_masks[0],\n            }\n\n        return {\n            \"input_ids\": all_input_ids,\n            \"attention_mask\": all_attention_masks,\n        }\n\n    def decode(self, token_ids: Union[List[int], torch.Tensor], skip_special_tokens: bool = True) -> str:\n        if isinstance(token_ids, torch.Tensor):\n            token_ids = token_ids.tolist()\n\n        words = []\n        for tid in token_ids:\n            if tid in self.id_to_word:\n                word = self.id_to_word[tid]\n                if skip_special_tokens and word in {\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"}:\n                    continue\n                words.append(word)\n\n        return \" \".join(words)\n\n    def convert_ids_to_tokens(self, ids: Union[List[int], torch.Tensor]) -> List[str]:\n        if isinstance(ids, torch.Tensor):\n            ids = ids.tolist()\n\n        return [self.id_to_word.get(tid, self.unk_token) for tid in ids]\n\n    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:\n        if isinstance(tokens, str):\n            return self.word_to_id.get(tokens, self.unk_token_id)\n        return [self.word_to_id.get(tok, self.unk_token_id) for tok in tokens]\n\n    def __call__(self, text: Union[str, List[str]], **kwargs):\n        return self.encode(text, **kwargs)\n\n    def __len__(self):\n        return len(self.word_to_id)\n\n\nclass LabelSmoothingLoss(nn.Module):\n    \"\"\"\n    ‚úÖ KEPT: Can be used with T5 if needed (though T5 typically uses smoothing=0.0)\n    \"\"\"\n    def __init__(self, num_classes: int, smoothing: float = 0.0, ignore_index: int = -100):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n        self.confidence = 1.0 - smoothing\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        if logits.dim() == 3:\n            logits = logits.reshape(-1, logits.size(-1))\n        if targets.dim() == 2:\n            targets = targets.reshape(-1)\n\n        mask = targets != self.ignore_index\n        targets = targets.masked_select(mask)\n        logits = logits[mask]\n\n        if targets.numel() == 0:\n            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n\n        log_probs = F.log_softmax(logits, dim=-1)\n\n        nll_loss = -log_probs.gather(dim=-1, index=targets.unsqueeze(1)).squeeze(1)\n        smooth_loss = -log_probs.mean(dim=-1)\n\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n\n        return loss.mean()\n\n\nclass RDropLoss(nn.Module):\n    \"\"\"\n    ‚úÖ KEPT: But typically not used with T5 (alpha=0.0 by default)\n    R-Drop is more common with BERT/RoBERTa-style models\n    \"\"\"\n    def __init__(self, alpha: float = 0.0):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(\n        self,\n        logits1: torch.Tensor,\n        logits2: torch.Tensor,\n        targets: torch.Tensor,\n        ignore_index: int = -100\n    ) -> torch.Tensor:\n        if self.alpha == 0.0:\n            return torch.tensor(0.0, device=logits1.device, requires_grad=True)\n\n        if logits1.dim() == 3:\n            logits1 = logits1.reshape(-1, logits1.size(-1))\n        if logits2.dim() == 3:\n            logits2 = logits2.reshape(-1, logits2.size(-1))\n        if targets.dim() == 2:\n            targets = targets.reshape(-1)\n\n        mask = targets != ignore_index\n\n        logits1 = logits1[mask]\n        logits2 = logits2[mask]\n\n        if logits1.numel() == 0:\n            return torch.tensor(0.0, device=logits1.device, requires_grad=True)\n\n        p1 = F.log_softmax(logits1, dim=-1)\n        p2 = F.log_softmax(logits2, dim=-1)\n\n        p1_probs = F.softmax(logits1, dim=-1)\n        p2_probs = F.softmax(logits2, dim=-1)\n\n        kl_1_2 = F.kl_div(p1, p2_probs, reduction='batchmean', log_target=False)\n        kl_2_1 = F.kl_div(p2, p1_probs, reduction='batchmean', log_target=False)\n\n        kl_loss = (kl_1_2 + kl_2_1) / 2.0\n\n        return self.alpha * kl_loss\n\n\ndef _special_token_cache_key(tokenizer) -> str:\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None)\n    if not name:\n        name = \"unknown_tokenizer\"\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_vocab_size(tokenizer) -> int:\n    \"\"\"\n    ‚úÖ CHANGED: Updated default vocab size for BanglaT5\n    \"\"\"\n    cache_key = _special_token_cache_key(tokenizer)\n\n    if cache_key in _VOCAB_SIZE_CACHE:\n        return _VOCAB_SIZE_CACHE[cache_key]\n\n    vocab_size = _BANGLAT5_VOCAB_SIZE  # ‚úÖ CHANGED from _MBART_VOCAB_SIZE\n\n    try:\n        if hasattr(tokenizer, \"__len__\"):\n            vocab_size = len(tokenizer)\n        elif hasattr(tokenizer, \"vocab_size\"):\n            vocab_size = int(tokenizer.vocab_size)\n        elif hasattr(tokenizer, \"get_vocab\"):\n            vocab_size = len(tokenizer.get_vocab())\n    except Exception:\n        pass\n\n    _VOCAB_SIZE_CACHE[cache_key] = vocab_size\n    return vocab_size\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    \"\"\"\n    ‚úÖ CHANGED: Updated for T5 special tokens (no language tokens)\n    T5 uses: <pad>, </s>, <unk>, <extra_id_0>, <extra_id_1>, etc.\n    \"\"\"\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens: Set[str] = set()\n        try:\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"all_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"additional_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\",\n                         \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            try:\n                stm = (\n                    getattr(tokenizer, \"special_tokens_map\", None)\n                    or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                )\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n        except Exception:\n            special_tokens = set()\n\n        # ‚úÖ CHANGED: Remove mBART language tokens, add T5 special tokens\n        special_tokens.update({\n            \"</s>\", \"<pad>\", \"<unk>\",  # T5 standard tokens\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n        })\n        \n        # ‚úÖ REMOVED: mBART language token markers\n        # special_tokens.update({f\"__{_SOURCE_LANG}__\", f\"__{_TARGET_LANG}__\"})\n        \n        # ‚úÖ ADDED: T5 <extra_id_*> tokens (sentinel tokens)\n        for i in range(100):  # T5 typically has <extra_id_0> to <extra_id_99>\n            special_tokens.add(f\"<extra_id_{i}>\")\n\n        try:\n            vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n            special_tokens = {\n                tok\n                for tok in special_tokens\n                if tok in vocab or tok in {\"</s>\", \"<pad>\", \"<unk>\"}\n            }\n        except Exception:\n            pass\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\ndef get_cached_special_tokens(tokenizer) -> Set[str]:\n    return get_tokenizer_special_tokens(tokenizer)\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    \"\"\"\n    ‚úÖ UNCHANGED: Offset mapping normalization works same for T5\n    \"\"\"\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in arr[0]\n                        ]\n                        return enc\n                if isinstance(off, (list, tuple)):\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in off[0]\n                        ]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    try:\n        data = getattr(enc, \"data\", None)\n        if (\n            data\n            and isinstance(data, dict)\n            and \"offset_mapping\" in data\n            and data[\"offset_mapping\"] is not None\n        ):\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [\n                    (x[0], x[1])\n                    if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                    else (None, None)\n                    for x in om[0]\n                ]\n                return enc\n    except Exception:\n        pass\n\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif (\n                isinstance(input_ids, (list, tuple))\n                and len(input_ids) > 0\n                and isinstance(input_ids[0], (list, tuple))\n            ):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\ndef safe_offsets_tokenize(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n    include_special_tokens: bool = False,\n) -> dict:\n    \"\"\"\n    ‚úÖ CHANGED: Removed mBART language setting (tokenizer.src_lang)\n    T5 doesn't use language tokens - uses task prefixes instead\n    \"\"\"\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        if _DEBUG_VERBOSE:\n            print(\"[WARN] Failed to convert input to string, using empty string\")\n        text = \"\"\n\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    vocab_size = get_tokenizer_vocab_size(tokenizer)\n\n    tokenize_kwargs = {\n        \"return_tensors\": \"pt\",\n        \"truncation\": True,\n        \"padding\": False,\n        \"max_length\": eff_max,\n        \"add_special_tokens\": include_special_tokens,\n    }\n\n    # ‚úÖ REMOVED: mBART language setting\n    # try:\n    #     if hasattr(tokenizer, 'src_lang'):\n    #         tokenizer.src_lang = _SOURCE_LANG\n    # except Exception:\n    #     pass\n\n    if is_fast:\n        try:\n            tokenize_kwargs[\"return_offsets_mapping\"] = True\n            enc = tokenizer(sample_text, **tokenize_kwargs)\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n\n            if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n                enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], 0, vocab_size - 1)\n\n            return enc\n        except Exception:\n            pass\n\n    try:\n        enc = tokenizer(sample_text, **tokenize_kwargs)\n\n        if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n            enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], 0, vocab_size - 1)\n\n    except Exception as e:\n        if _DEBUG_VERBOSE:\n            print(f\"[WARN] Tokenization failed: {e}, returning empty encoding\")\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\n            \"input_ids\": torch.tensor([[pad_id]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n        }\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"##\", \"\").replace(\"ƒ†\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\ndef reconstruct_word_spans(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n) -> Tuple[Dict[int, str], List[str]]:\n    \"\"\"\n    ‚úÖ UNCHANGED: Word span reconstruction works same for T5\n    T5 uses SentencePiece tokenization like mBART, so the logic is identical\n    \"\"\"\n    global _LANGUAGE_WARNING_COUNT\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in text)\n    has_english = any(\"a\" <= c.lower() <= \"z\" for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (\n            sum(1 for c in text if \"\\u0980\" <= c <= \"\\u09FF\")\n            / max(1, len(text))\n            * 100.0\n        )\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(\n            f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), \"\n            f\"English: {has_english}\"\n        )\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n    vocab_size = get_tokenizer_vocab_size(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    try:\n        encoded = safe_offsets_tokenize(\n            tokenizer, text, max_length=eff_max, include_special_tokens=False\n        )\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n        input_ids = [min(max(0, tid), vocab_size - 1) for tid in input_ids]\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if isinstance(offsets, list) and len(offsets) > 0 and all(\n        isinstance(x, tuple) for x in offsets\n    ):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(\n        offsets[0], (list, tuple)\n    ):\n        offsets_list = [\n            (x[0], x[1])\n            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n            else (None, None)\n            for x in offsets[0]\n        ]\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, str] = {}\n    words: List[str] = []\n\n    used_any_offset = any(\n        isinstance(o, tuple) and o[0] is not None and o[1] is not None\n        for o in offsets_list\n    )\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n        current_accumulated_word = \"\"\n\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            if off_start is not None and off_end is not None:\n                if off_start < 0 or off_end < 0:\n                    if _DEBUG_VERBOSE:\n                        print(\n                            f\"[WARN] Negative offset detected: \"\n                            f\"({off_start}, {off_end}), skipping\"\n                        )\n                    off_start, off_end = None, None\n                else:\n                    off_start = max(0, min(off_start, text_len))\n                    off_end = max(off_start, min(off_end, text_len))\n\n            if off_start is None or off_end is None:\n                if current_accumulated_word:\n                    token_word_map[idx] = current_accumulated_word\n\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                continue\n\n            if tok in special_tokens:\n                continue\n\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                if off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            try:\n                current_word = text[word_start:word_end].strip()\n                if current_word:\n                    token_word_map[idx] = current_word\n                    current_accumulated_word = current_word\n            except Exception:\n                pass\n\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    running_word = \"\"\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            continue\n\n        clean = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            continue\n\n        if tok.startswith(\"‚ñÅ\") or tok.startswith(\"ƒ†\"):\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n            current_parts = [clean]\n            running_word = clean\n        else:\n            current_parts.append(clean)\n            running_word = \"\".join(current_parts)\n            if len(running_word) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                current_parts = [clean]\n                running_word = clean\n\n        if running_word:\n            token_word_map[i] = running_word\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    try:\n        words_from_markers: List[str] = []\n        current_word_parts: List[str] = []\n\n        for tok in tokens:\n            if tok in special_tokens:\n                continue\n\n            clean = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n            if not clean:\n                continue\n\n            if tok.startswith(\"‚ñÅ\") or tok.startswith(\"ƒ†\"):\n                if current_word_parts:\n                    words_from_markers.append(\"\".join(current_word_parts))\n                current_word_parts = [clean]\n            else:\n                current_word_parts.append(clean)\n\n        if current_word_parts:\n            words_from_markers.append(\"\".join(current_word_parts))\n\n        if words_from_markers:\n            word_list = words_from_markers\n        else:\n            word_list = [w for w in text.split() if w.strip()]\n\n        token_word_map = {}\n\n        if tokens and word_list:\n            word_idx = 0\n\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    continue\n\n                if word_idx < len(word_list):\n                    current_word = word_list[word_idx]\n                    if clean in current_word or current_word.startswith(clean):\n                        token_word_map[i] = current_word\n                    else:\n                        word_idx = min(word_idx + 1, len(word_list) - 1)\n                        token_word_map[i] = word_list[word_idx]\n                else:\n                    if word_list:\n                        token_word_map[i] = word_list[-1]\n\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    \"\"\"\n    ‚úÖ UNCHANGED: Token validation logic same for T5\n    \"\"\"\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    clean = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        if len(clean) < _DSCD_MIN_LETTERS:\n            result = False\n        else:\n            has_bengali_chars = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if \"\\u0980\" <= c <= \"\\u09FF\")\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    result = (bengali_count / alphanum_count) >= _DSCD_MIN_LETTER_FRACTION\n\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\ndef should_track_token(\n    token: str,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    return is_valid_token(token, special_tokens, tokenizer, language)\n\ndef validate_tokenizer_vocab(tokenizer, expected_vocab_size: Optional[int] = None) -> bool:\n    \"\"\"\n    ‚úÖ CHANGED: Updated for BanglaT5 validation (no language tokens)\n    \"\"\"\n    actual_vocab_size = get_tokenizer_vocab_size(tokenizer)\n\n    print(f\"[TOKENIZER-VALIDATION] Actual vocab size: {actual_vocab_size}\")\n\n    if expected_vocab_size is not None:\n        if actual_vocab_size != expected_vocab_size:\n            print(f\"[TOKENIZER-VALIDATION] ‚ùå MISMATCH: Expected {expected_vocab_size}, got {actual_vocab_size}\")\n            return False\n        else:\n            print(f\"[TOKENIZER-VALIDATION] ‚úÖ Vocab size matches: {actual_vocab_size}\")\n            return True\n\n    # ‚úÖ CHANGED: T5 doesn't have language tokens - check for T5 sentinel tokens instead\n    print(f\"[TOKENIZER-VALIDATION] Checking T5 special tokens:\")\n    \n    try:\n        # Check for T5 standard tokens\n        pad_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n        eos_id = tokenizer.convert_tokens_to_ids(\"</s>\")\n        unk_id = tokenizer.convert_tokens_to_ids(\"<unk>\")\n        \n        print(f\"  <pad> ‚Üí {pad_id}\")\n        print(f\"  </s> ‚Üí {eos_id}\")\n        print(f\"  <unk> ‚Üí {unk_id}\")\n        \n        # Check for T5 sentinel tokens (extra_id)\n        try:\n            extra_id_0 = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n            print(f\"  <extra_id_0> ‚Üí {extra_id_0}\")\n            print(f\"[TOKENIZER-VALIDATION] ‚úÖ T5 sentinel tokens detected\")\n        except:\n            print(f\"[TOKENIZER-VALIDATION] ‚ö†Ô∏è  T5 sentinel tokens not found (may be OK)\")\n\n        if pad_id >= actual_vocab_size or eos_id >= actual_vocab_size:\n            print(f\"[TOKENIZER-VALIDATION] ‚ùå Special token IDs exceed vocab size!\")\n            return False\n\n        print(f\"[TOKENIZER-VALIDATION] ‚úÖ Special tokens valid\")\n        return True\n\n    except Exception as e:\n        print(f\"[TOKENIZER-VALIDATION] ‚ùå Token validation failed: {e}\")\n        return False\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    \"\"\"\n    ‚úÖ CHANGED: Updated test for BanglaT5 (no language setting test)\n    \"\"\"\n    sample_bn = \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶Ø‡¶æ‡¶¨‡•§\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST (BanglaT5)\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 0] Vocabulary validation:\")\n        validate_tokenizer_vocab(tokenizer)\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        # ‚úÖ ADDED: Test with task prefix\n        sample_bn_with_prefix = _TASK_PREFIX + sample_bn\n        print(f\"  Input (with prefix): {sample_bn_with_prefix[:60]}...\")\n        \n        enc_bn = safe_offsets_tokenize(\n            tokenizer, sample_bn_with_prefix, max_length=64, include_special_tokens=False\n        )\n        enc_len = (\n            int(enc_bn[\"input_ids\"].shape[-1])\n            if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn\n            else \"N/A\"\n        )\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(\n            any(\"\\u0980\" <= c <= \"\\u09FF\" for c in w) for w in words_bn\n        )\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(\n            any(\"a\" <= c.lower() <= \"z\" for c in w) for w in words_en\n        )\n        print(f\"  Contains English words: {has_english_words}\")\n\n        print(\"\\n[TEST 3] Token validation:\")\n        special_tokens = get_tokenizer_special_tokens(tokenizer)\n        test_tokens = [\"‡¶ï‡¶æ‡¶≤\", \"‚ñÅ‡¶Ü‡¶Æ‡¶ø\", \"</s>\", \"##ing\", \"a\", \"<extra_id_0>\"]\n        for tok in test_tokens:\n            valid = is_valid_token(tok, special_tokens, tokenizer, \"bn\")\n            print(f\"  '{tok}': {'valid' if valid else 'invalid'}\")\n\n        # ‚úÖ REMOVED: mBART language setting test\n        # print(\"\\n[TEST 4] mBART-50 language setting:\")\n        # try:\n        #     if hasattr(tokenizer, 'src_lang'):\n        #         tokenizer.src_lang = \"bn_IN\"\n        #         print(\"  ‚úÖ tokenizer.src_lang = 'bn_IN' successful\")\n        #     else:\n        #         print(\"  ‚ö†Ô∏è  tokenizer.src_lang attribute not found\")\n        # except Exception as e:\n        #     print(f\"  ‚ùå Language setting failed: {e}\")\n\n        # ‚úÖ ADDED: T5 task prefix test\n        print(\"\\n[TEST 4] BanglaT5 task prefix test:\")\n        try:\n            test_input = _TASK_PREFIX + \"‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ\"\n            test_enc = tokenizer(test_input, return_tensors=\"pt\")\n            test_len = test_enc[\"input_ids\"].shape[-1]\n            print(f\"  Task prefix: '{_TASK_PREFIX}'\")\n            print(f\"  Full input: '{test_input}'\")\n            print(f\"  Encoded length: {test_len}\")\n            print(\"  ‚úÖ Task prefix encoding successful\")\n        except Exception as e:\n            print(f\"  ‚ùå Task prefix test failed: {e}\")\n\n        if has_bengali_words and not any(\n            \"a\" <= c.lower() <= \"z\" for c in \"\".join(words_bn)\n        ):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\n# ‚úÖ UNCHANGED: Lowercase aliases\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\ngetcachedspecialtokens = get_cached_special_tokens\nisvalidtoken = is_valid_token\nshouldtracktoken = should_track_token\ngettokenizervocabsize = get_tokenizer_vocab_size\nvalidatetokenizervocab = validate_tokenizer_vocab\n\n# ‚úÖ CHANGED: Updated summary for BanglaT5\nprint(\"=\" * 80)\nprint(\"Cell 1: DUAL-PATH Tokenizer Utilities + Training Losses - READY (BanglaT5)\")\nprint(\"=\" * 80)\nprint(\"VERIFICATION:\")\nprint(f\"  ‚úÖ DSCD_MIN_LETTERS: {_DSCD_MIN_LETTERS}\")\nprint(f\"  ‚úÖ DSCD_MIN_LETTER_FRACTION: {_DSCD_MIN_LETTER_FRACTION}\")\nprint(f\"  ‚úÖ Token validation cache size: {_cache_max_size}\")\nprint(f\"  ‚úÖ Task prefix: '{_TASK_PREFIX}'\")\nprint(f\"  ‚úÖ BanglaT5 vocab size: ~{_BANGLAT5_VOCAB_SIZE:,}\")\nprint(f\"  ‚úÖ Label Smoothing epsilon: {_LABEL_SMOOTHING_EPS} (T5 standard: 0.0)\")\nprint(f\"  ‚úÖ R-Drop alpha: {_RDROP_ALPHA} (T5 typically: 0.0)\")\nprint(f\"  ‚úÖ R-Drop enabled: {_USE_RDROP}\")\nprint(\"\\nDUAL-PATH COMPONENTS:\")\nprint(\"  ‚úÖ BengaliWordTokenizer - Path 1 (word-level)\")\nprint(\"  ‚úÖ BanglaT5 utilities - Path 2 (SentencePiece subword)\")\nprint(\"  ‚úÖ LabelSmoothingLoss - Path 2 training (optional)\")\nprint(\"  ‚úÖ RDropLoss - Path 2 regularization (not used for T5)\")\nprint(\"\\n‚≠ê KEY CHANGES FROM mBART-50:\")\nprint(\"  ‚ùå Removed: Language token validation (bn_IN, en_XX)\")\nprint(\"  ‚úÖ Added: T5 sentinel token support (<extra_id_*>)\")\nprint(\"  ‚úÖ Added: Task prefix support for T5\")\nprint(\"  ‚ùå Removed: tokenizer.src_lang setting\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"WZE9PkHyH4J1","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:25.048684Z","iopub.execute_input":"2026-02-16T03:01:25.048998Z","iopub.status.idle":"2026-02-16T03:01:25.140619Z","shell.execute_reply.started":"2026-02-16T03:01:25.048977Z","shell.execute_reply":"2026-02-16T03:01:25.140060Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 1: DUAL-PATH Tokenizer Utilities + Training Losses - READY (BanglaT5)\n================================================================================\nVERIFICATION:\n  ‚úÖ DSCD_MIN_LETTERS: 3\n  ‚úÖ DSCD_MIN_LETTER_FRACTION: 0.6\n  ‚úÖ Token validation cache size: 10000\n  ‚úÖ Task prefix: 'translate Bengali to English: '\n  ‚úÖ BanglaT5 vocab size: ~32,128\n  ‚úÖ Label Smoothing epsilon: 0.1 (T5 standard: 0.0)\n  ‚úÖ R-Drop alpha: 0.0 (T5 typically: 0.0)\n  ‚úÖ R-Drop enabled: False\n\nDUAL-PATH COMPONENTS:\n  ‚úÖ BengaliWordTokenizer - Path 1 (word-level)\n  ‚úÖ BanglaT5 utilities - Path 2 (SentencePiece subword)\n  ‚úÖ LabelSmoothingLoss - Path 2 training (optional)\n  ‚úÖ RDropLoss - Path 2 regularization (not used for T5)\n\n‚≠ê KEY CHANGES FROM mBART-50:\n  ‚ùå Removed: Language token validation (bn_IN, en_XX)\n  ‚úÖ Added: T5 sentinel token support (<extra_id_*>)\n  ‚úÖ Added: Task prefix support for T5\n  ‚ùå Removed: tokenizer.src_lang setting\n================================================================================\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: DUAL-PATH DATA LOADING - WORD + SUBWORD TOKENIZATION (BanglaT5)\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE) or bool(_DEBUG_DISCOVERY)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\nMODEL_VOCAB_SIZE = 32128\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 64\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 64\")\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n    print(\"[CELL2] WARNING: SOURCE_LANGUAGE/TARGET_LANGUAGE not defined, using defaults bn/en\")\n\ntry:\n    _TASK_PREFIX = str(TASK_PREFIX)\nexcept NameError:\n    _TASK_PREFIX = \"translate Bengali to English: \"\n    print(\"[CELL2] WARNING: TASK_PREFIX not defined, using default\")\n\ntry:\n    _BANGLAT5_VOCAB_SIZE = int(BANGLAT5_VOCAB_SIZE)\nexcept NameError:\n    _BANGLAT5_VOCAB_SIZE = 32128\n    print(\"[CELL2] WARNING: BanglaT5 vocab size not defined, using default 32128\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/datasets/manas00000003/sam-dataset/bn_en_qe0.6_adequacy_filtered_500000_1000000.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\n    _USE_DOMAIN_LABELS = bool(USE_DOMAIN_LABELS)\nexcept NameError:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = True\n    print(\"[CELL2] WARNING: Domain label config not found, using defaults (train=0, test=1)\")\n\n_has_normalize = (\"normalize_bengali\" in globals()) and (\"normalize_english\" in globals())\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n_BENGALI_CHAR_RE = re.compile(r\"[\\u0980-\\u09FF]\")\n_BENGALI_PUNCT_SET = set(['‡•§', '‡••'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}'])\n\ndef is_bengali_text(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\ndef separate_bengali_punctuation(text: str, language: str = \"bn\") -> str:\n    if not text or not isinstance(text, str):\n        return \"\"\n    text = text.strip()\n    if language in [\"bn\", \"hi\", \"te\", \"ta\", \"ml\", \"mr\", \"gu\", \"pa\"]:\n        text = re.sub(r'([‡•§‡••])', r' \\1 ', text)\n    text = re.sub(r'([,;:!?()\\[\\]{}])', r' \\1 ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef clean_and_normalize_text(text: str, language: str = \"bn\") -> str:\n    if not text or not isinstance(text, str):\n        return \"\"\n    text = text.strip()\n    if not text:\n        return \"\"\n    text = separate_bengali_punctuation(text, language)\n    if _has_normalize:\n        if language in [\"bn\", \"bn_IN\"]:\n            text = normalize_bengali(text)\n        else:\n            text = normalize_english(text)\n    else:\n        text = text.strip()\n        if language in [\"en\", \"en_XX\"]:\n            text = text.lower()\n    return text\n\ndef is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    clean = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n    if not clean:\n        return False\n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    if clean in _COMMON_PUNCT_SET:\n        return True\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    return all(c in _BENGALI_PUNCT_SET or c in _COMMON_PUNCT_SET for c in clean)\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    \n    if dataset is not None:\n        dataset.vocab_size = MODEL_VOCAB_SIZE\n        if DEBUG_CELL2:\n            print(f\"[WORKER-{worker_id}] Forced vocab_size={MODEL_VOCAB_SIZE}\")\n        \n        if hasattr(dataset, \"_tokenizer_name_or_path\") and dataset._tokenizer_name_or_path:\n            try:\n                from transformers import AutoTokenizer\n                dataset.tokenizer = AutoTokenizer.from_pretrained(dataset._tokenizer_name_or_path)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n            except Exception as e:\n                if DEBUG_CELL2:\n                    print(f\"[WORKER-{worker_id}] Tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n    \n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\ndef load_and_preprocess_optimized(\n    num_samples: Optional[int] = None,\n    split: str = \"train\",\n) -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n    \n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n    \n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    try:\n        print(\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        if df.empty:\n            print(\"[CELL2] ERROR: CSV file is empty\")\n            return _get_fallback_dataset()\n        \n        if \"src\" not in df.columns or \"tgt\" not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing required columns. Found columns: {list(df.columns)}\")\n            print(\"[CELL2] Expected format: src (Bengali), tgt (English) OR src (English), tgt (Bengali)\")\n            return _get_fallback_dataset()\n        \n        sample_size = min(10, len(df))\n        sample_rows = df.head(sample_size)\n        \n        src_bengali_count = sum(1 for s in sample_rows[\"src\"] if is_bengali_text(str(s)))\n        tgt_bengali_count = sum(1 for s in sample_rows[\"tgt\"] if is_bengali_text(str(s)))\n        \n        src_is_bengali = src_bengali_count > sample_size * 0.5\n        tgt_is_bengali = tgt_bengali_count > sample_size * 0.5\n        \n        if not src_is_bengali and tgt_is_bengali:\n            print(\"[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn‚Üíen task.\")\n            df = df.rename(columns={\"src\": \"_temp_tgt\", \"tgt\": \"_temp_src\"})\n            df = df.rename(columns={\"_temp_src\": \"src\", \"_temp_tgt\": \"tgt\"})\n            \n            sample_rows = df.head(sample_size)\n            src_bengali_count = sum(1 for s in sample_rows[\"src\"] if is_bengali_text(str(s)))\n            src_is_bengali = src_bengali_count > sample_size * 0.5\n            \n            if not src_is_bengali:\n                print(\"[CELL2] ERROR: Swap failed, src is still not Bengali.\")\n                return _get_fallback_dataset()\n            else:\n                print(\"[CELL2] Swap successful: src=Bengali, tgt=English\")\n        elif not src_is_bengali:\n            print(\"[CELL2] WARNING: src column does not appear to be Bengali. Proceeding but output may be incorrect.\")\n        \n        df = df.head(num_samples)\n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n        \n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n        \n        for row_tuple in tqdm(df.itertuples(index=False), total=len(df), desc=\"Loading dataset\"):\n            try:\n                src_val = row_tuple.src\n                tgt_val = row_tuple.tgt\n                if pd.isna(src_val) or pd.isna(tgt_val):\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", \"NaN value detected\")\n                    continue\n                bn = str(src_val).strip()\n                en = str(tgt_val).strip()\n                if not bn or not en:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", \"Empty src/tgt field\")\n                    continue\n                if not is_bengali_text(bn):\n                    skipped += 1\n                    cell2_dbg(\"not_bengali_src\", \"src field not Bengali\")\n                    continue\n                if not re.search(r\"[a-zA-Z]\", en):\n                    skipped += 1\n                    cell2_dbg(\"not_english_tgt\", \"tgt field not English\")\n                    continue\n                \n                max_words = max(20, _MAX_LENGTH // 2)\n                if len(bn.split()) > max_words or len(en.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", \"Text too long\")\n                    continue\n                \n                bn_norm = clean_and_normalize_text(bn, language=\"bn\")\n                en_norm = clean_and_normalize_text(en, language=\"en\")\n                \n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", \"Empty after normalization\")\n                    continue\n                \n                pairs.append((bn_norm, en_norm))\n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception: {type(e).__name__}\")\n                continue\n        \n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        \n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            print(\"[CELL2] Check that src column contains Bengali and tgt column contains English.\")\n            return _get_fallback_dataset()\n        \n        return pairs\n    \n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    print(\"[CELL2] Using fallback dataset (50 unique samples)\")\n    fallback_pairs = [\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø\", \"i turned off the tap\"),\n        (\"‡¶∏‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶™‡¶∞‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶¨‡ßá\", \"he will call me later\"),\n        (\"‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¶‡¶ø‡¶® ‡¶§‡¶æ‡¶ú‡¶æ ‡¶´‡¶≤ ‡¶ñ‡¶æ‡¶á\", \"we eat fresh fruits every day\"),\n        (\"‡¶§‡¶æ‡¶∞ ‡¶ï‡¶†‡ßã‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∂‡ßç‡¶∞‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶´‡¶≤ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá\", \"his hard work has brought good results\"),\n        (\"‡¶ó‡¶æ‡¶õ‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡¶æ‡¶§‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ó‡¶ú‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá\", \"new leaves have sprouted on the tree\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶™‡¶æ‡¶§‡¶æ ‡¶â‡¶≤‡ßç‡¶ü‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\", \"i am turning the pages of the book\"),\n        (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ\", \"yesterday i went to the market\"),\n        (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶ï‡¶∞‡¶¨\", \"tomorrow i will meet you\"),\n        (\"‡¶§‡¶æ‡¶∞‡¶æ ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá ‡¶â‡¶ú‡ßç‡¶ú‡ßç‡¶¨‡¶≤\", \"the stars are bright in the sky\"),\n        (\"‡¶§‡¶æ‡¶∞‡¶æ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶§‡ßá ‡¶®‡ßá‡¶á\", \"they are not at home\"),\n        (\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶®‡¶¶‡ßÄ‡¶∞ ‡¶ß‡¶æ‡¶∞‡ßá ‡¶≠‡ßá‡¶ô‡ßá ‡¶ó‡ßá‡¶õ‡ßá\", \"the bank by the river has collapsed\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ú‡¶Æ‡¶æ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø\", \"i deposited money in the bank\"),\n        (\"‡¶¨‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá\", \"you have to try again and again\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶∞ ‡¶ñ‡ßÅ‡¶≤‡ßá ‡¶≠‡¶ø‡¶§‡¶∞‡ßá ‡¶¢‡ßÅ‡¶ï‡¶≤‡¶æ‡¶Æ\", \"i opened the bar and entered\"),\n        (\"‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶•‡¶æ ‡¶¨‡ßç‡¶Ø‡¶•‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá\", \"his head is hurting\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶Æ‡¶æ‡¶•‡¶æ ‡¶®‡ßá‡¶°‡¶º‡ßá ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶§‡¶ø ‡¶¶‡¶ø‡¶≤‡¶æ‡¶Æ\", \"i nodded my head in agreement\"),\n        (\"‡¶∏‡ßá ‡¶π‡¶æ‡¶∞ ‡¶Æ‡ßá‡¶®‡ßá ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá\", \"he accepted defeat\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ó‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∏‡ßã‡¶®‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∞ ‡¶™‡¶∞‡ßá‡¶õ‡¶ø\", \"i am wearing a gold necklace\"),\n        (\"‡¶™‡¶æ‡¶®‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶†‡¶æ‡¶®‡ßç‡¶°‡¶æ\", \"the water is very cold\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶™‡¶æ‡¶®‡¶ø ‡¶ñ‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\", \"i am drinking water\"),\n        (\"‡¶¶‡¶≤ ‡¶ñ‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá\", \"the team won the game\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶Æ‡¶æ‡¶ü‡¶ø ‡¶¶‡¶≤ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶´‡ßá‡¶≤‡¶≤‡¶æ‡¶Æ\", \"i trampled the soil\"),\n        (\"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ú‡¶ø ‡¶ï‡¶ø‡¶®‡¶≤‡¶æ‡¶Æ\", \"i bought vegetables from the market\"),\n        (\"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶ø‡¶°‡¶º ‡¶õ‡¶ø‡¶≤\", \"the market was very crowded\"),\n        (\"‡¶§‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶Ü‡¶π‡¶Æ‡ßá‡¶¶\", \"his name is ahmed\"),\n        (\"‡¶®‡¶æ‡¶Æ ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßã\", \"work without making a name\"),\n        (\"‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßã\", \"stop talking\"),\n        (\"‡¶§‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶∂‡ßÅ‡¶®‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶≤‡¶æ‡¶ó‡¶≤\", \"i felt good hearing his words\"),\n        (\"‡¶¨‡¶á ‡¶™‡¶°‡¶º‡¶§‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶≤‡¶æ‡¶ó‡ßá\", \"i like reading books\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡ßá‡¶õ‡¶ø\", \"i bought a new book\"),\n        (\"‡¶ò‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá\", \"the house has been cleaned\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ò‡¶∞‡ßá ‡¶¨‡¶∏‡ßá ‡¶Ü‡¶õ‡¶ø\", \"i am sitting at home\"),\n        (\"‡¶Æ‡¶® ‡¶≠‡¶æ‡¶≤‡ßã ‡¶®‡ßá‡¶á\", \"my mind is not good\"),\n        (\"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶® ‡¶ö‡¶æ‡¶Ø‡¶º ‡¶¨‡ßá‡¶°‡¶º‡¶æ‡¶§‡ßá ‡¶Ø‡ßá‡¶§‡ßá\", \"my mind wants to go for a walk\"),\n        (\"‡¶π‡¶æ‡¶§ ‡¶ß‡ßÅ‡¶Ø‡¶º‡ßá ‡¶®‡¶æ‡¶ì\", \"wash your hands\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶§‡¶æ‡¶∞ ‡¶π‡¶æ‡¶§ ‡¶ß‡¶∞‡¶≤‡¶æ‡¶Æ\", \"i held his hand\"),\n        (\"‡¶¶‡¶ø‡¶® ‡¶ï‡ßá‡¶ü‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá\", \"the day is passing by\"),\n        (\"‡¶Ü‡¶ú ‡¶ï‡¶ø ‡¶¶‡¶ø‡¶®\", \"what day is today\"),\n        (\"‡¶∞‡¶æ‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶è‡¶∏‡ßá‡¶õ‡ßá\", \"night has come\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶∞‡¶æ‡¶§ ‡¶ú‡ßá‡¶ó‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡¶ø\", \"i studied staying up at night\"),\n        (\"‡¶ú‡¶≤ ‡¶ñ‡ßÅ‡¶¨ ‡¶ó‡¶∞‡¶Æ\", \"the water is very hot\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶≤ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡¶æ‡¶õ ‡¶∏‡¶ø‡¶û‡ßç‡¶ö‡¶® ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø\", \"i watered the plants\"),\n        (\"‡¶¨‡¶æ‡¶°‡¶º‡¶ø ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\", \"i am going home\"),\n        (\"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø ‡¶¢‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º\", \"my house is in dhaka\"),\n        (\"‡¶™‡¶æ‡¶∞‡ßç‡¶ï‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑\", \"there are many people in the park\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¶‡¶ø‡¶® ‡¶™‡¶æ‡¶∞‡ßç‡¶ï‡ßá ‡¶π‡¶æ‡¶Å‡¶ü‡¶ø\", \"i walk in the park every day\"),\n        (\"‡¶®‡¶¶‡ßÄ ‡¶¨‡¶á‡¶õ‡ßá\", \"the river is flowing\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶¶‡ßÄ‡¶∞ ‡¶ß‡¶æ‡¶∞‡ßá ‡¶¶‡¶æ‡¶Å‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶õ‡¶ø\", \"i am standing by the river\"),\n        (\"‡¶¨‡¶® ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞\", \"the forest is very beautiful\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶® ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ\", \"i went to see the forest\"),\n    ]\n    \n    processed_pairs = []\n    for bn, en in fallback_pairs:\n        bn_clean = clean_and_normalize_text(bn, \"bn\")\n        en_clean = clean_and_normalize_text(en, \"en\")\n        if bn_clean and en_clean:\n            processed_pairs.append((bn_clean, en_clean))\n    \n    return processed_pairs\n\nclass DualPathDataset(Dataset):\n    def __init__(\n        self,\n        pairs: List[Tuple[str, str]],\n        tokenizer: Any = None,\n        max_length: Optional[int] = None,\n        split: str = \"train\",\n        vocab_size: Optional[int] = None,\n    ):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        self.split = split\n        \n        if vocab_size is not None:\n            self.vocab_size = int(vocab_size)\n            print(f\"[CELL2] Dataset using provided vocab_size: {self.vocab_size}\")\n        elif tokenizer is not None:\n            try:\n                self.vocab_size = MODEL_VOCAB_SIZE\n                print(f\"[CELL2] Dataset using MODEL vocab_size: {self.vocab_size}\")\n            except Exception:\n                self.vocab_size = MODEL_VOCAB_SIZE\n                print(f\"[CELL2] Dataset using default vocab_size: {self.vocab_size}\")\n        else:\n            self.vocab_size = MODEL_VOCAB_SIZE\n            print(f\"[CELL2] Dataset using default vocab_size: {self.vocab_size}\")\n        \n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n        \n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n        \n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n        \n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n        \n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid, split={self.split}\")\n        \n        try:\n            if \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {\n                \"</s>\",\n                \"<pad>\",\n                \"<unk>\",\n            }\n            for i in range(100):\n                self.special_tokens.add(f\"<extra_id_{i}>\")\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n    \n    def __len__(self) -> int:\n        return len(self.pairs)\n    \n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n            \n            src_text_with_prefix = _TASK_PREFIX + src_text\n            \n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(\n                    self.tokenizer,\n                    src_text_with_prefix,\n                    max_length=self.max_length,\n                    include_special_tokens=True\n                )\n                try:\n                    if isinstance(enc[\"input_ids\"], torch.Tensor):\n                        input_ids = enc[\"input_ids\"].squeeze(0) if enc[\"input_ids\"].dim() > 1 else enc[\"input_ids\"]\n                    else:\n                        input_ids = torch.tensor(enc[\"input_ids\"][0]) if isinstance(enc[\"input_ids\"], list) and len(enc[\"input_ids\"]) > 0 else torch.tensor(enc[\"input_ids\"])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0] if enc.get(\"input_ids\") else [1])\n                \n                attention_mask = enc.get(\"attention_mask\", None)\n                if attention_mask is None:\n                    attention_mask = torch.ones_like(input_ids)\n                elif isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                elif isinstance(attention_mask, torch.Tensor):\n                    attention_mask = attention_mask.squeeze(0) if attention_mask.dim() > 1 else attention_mask\n                \n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(\n                    src_text_with_prefix,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=True,\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n            \n            input_ids = torch.clamp(input_ids, 0, MODEL_VOCAB_SIZE - 1)\n            \n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception as e:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {e}\")\n            \n            if not token_word_map and tokens:\n                try:\n                    current_word: List[str] = []\n                    for idx, tok in enumerate(tokens):\n                        if isinstance(tok, str) and tok not in self.special_tokens:\n                            if is_punctuation_only(tok):\n                                continue\n                            \n                            clean = tok.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n                            if clean:\n                                if tok.startswith(\"‚ñÅ\") or tok.startswith(\"ƒ†\"):\n                                    current_word = [clean]\n                                else:\n                                    current_word.append(clean)\n                                word_str = \"\".join(current_word)\n                                if not is_punctuation_only(word_str):\n                                    token_word_map[idx] = word_str\n                except Exception as e:\n                    cell2_dbg(\"fallback_wm\", f\"Fallback word map failed: {e}\")\n            \n            return input_ids, attention_mask, tokens, token_word_map\n        \n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 0) if self.tokenizer is not None else 0\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n    \n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n            \n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=True,\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 0) if self.tokenizer is not None else 0\n            \n            labels = torch.clamp(labels, 0, MODEL_VOCAB_SIZE - 1)\n            \n            valid_before_mask = (labels != int(pad_id)).sum().item()\n            labels[labels == int(pad_id)] = -100\n            valid_after_mask = (labels != -100).sum().item()\n            \n            if _DEBUG_DISCOVERY and valid_after_mask == 0:\n                cell2_dbg(\"encode_tgt_all_masked\", f\"All labels masked as -100\")\n            elif _DEBUG_DISCOVERY and valid_after_mask < 3:\n                cell2_dbg(\"encode_tgt_few_valid\", f\"Only {valid_after_mask} valid labels\")\n            \n            return labels\n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n    \n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"‡¶Ü‡¶Æ‡¶ø\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            \n            domain_label = random.randint(_TRAIN_DOMAIN, _TEST_DOMAIN)\n            \n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception:\n            pad_id = 0\n            domain_label = random.randint(_TRAIN_DOMAIN, _TEST_DOMAIN)\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label,\n            }\n    \n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx}\")\n                return self._make_safe_sample(\"oob\")\n            \n            src, tgt = self.pairs[idx]\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n            \n            if DEBUG_CELL2 and idx < 3:\n                has_bengali = is_bengali_text(src)\n                has_english = any(\"a\" <= c.lower() <= \"z\" for c in src)\n                print(f\"[CELL2-GETITEM-{idx}] src sample: {src[:50]}\")\n                print(f\"[CELL2-GETITEM-{idx}] Bengali: {has_bengali}, English: {has_english}\")\n                if not has_bengali:\n                    print(f\"[CELL2] WARNING: src_text is NOT Bengali at idx={idx}!\")\n            \n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            \n            if _DEBUG_DISCOVERY and idx < 5:\n                valid_labels = (labels != -100).sum().item()\n                if valid_labels == 0:\n                    print(f\"[CELL2-GETITEM] WARNING: idx={idx} has ALL labels = -100!\")\n                elif valid_labels < 3:\n                    print(f\"[CELL2-GETITEM] idx={idx} has only {valid_labels} valid labels\")\n            \n            domain_label = idx % 2\n            \n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 0) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    t = tensor.view(-1).long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    \n    default_domain = _TRAIN_DOMAIN\n    \n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=0)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_texts\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([default_domain], dtype=torch.long),\n        }\n    \n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=0)\n    \n    raw_inputs = []\n    raw_masks = []\n    raw_labs = []\n    twmaps = []\n    srcs = []\n    toks = []\n    domains = []\n    \n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", default_domain)\n            \n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n            \n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n            \n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n            \n            raw_inputs.append(in_ids)\n            raw_masks.append(att)\n            raw_labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(domain)\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n    \n    if not raw_inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=0)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_texts\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([default_domain], dtype=torch.long),\n        }\n    \n    max_input_len = max(t.size(0) for t in raw_inputs)\n    max_label_len = max(t.size(0) for t in raw_labs)\n    actual_max_len = max(max_input_len, max_label_len)\n    actual_max_len = min(actual_max_len, _MAX_LENGTH)\n    \n    inputs = []\n    masks = []\n    labs = []\n    \n    for in_ids, att, lab in zip(raw_inputs, raw_masks, raw_labs):\n        in_ids_padded = _pad_or_truncate_array(in_ids, actual_max_len, pad_id)\n        att_padded = _pad_or_truncate_array(att, actual_max_len, 0)\n        lab_padded = _pad_or_truncate_array(lab, actual_max_len, -100)\n        \n        inputs.append(in_ids_padded)\n        masks.append(att_padded)\n        labs.append(lab_padded)\n    \n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n    \n    input_ids = torch.clamp(input_ids, 0, MODEL_VOCAB_SIZE - 1)\n    labels = torch.where(labels != -100, torch.clamp(labels, 0, MODEL_VOCAB_SIZE - 1), labels)\n    \n    max_input_final = input_ids.max().item()\n    valid_labels_final = labels[labels != -100]\n    max_label_final = valid_labels_final.max().item() if len(valid_labels_final) > 0 else 0\n    \n    if max_input_final >= MODEL_VOCAB_SIZE or max_label_final >= MODEL_VOCAB_SIZE:\n        print(f\"[COLLATE-EMERGENCY] Out of bounds detected after clamping!\")\n        print(f\"  input_ids max: {max_input_final} (limit: {MODEL_VOCAB_SIZE-1})\")\n        print(f\"  labels max: {max_label_final} (limit: {MODEL_VOCAB_SIZE-1})\")\n        input_ids = input_ids % MODEL_VOCAB_SIZE\n        labels = torch.where(labels != -100, labels % MODEL_VOCAB_SIZE, labels)\n    \n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.full((len(inputs),), default_domain, dtype=torch.long)\n    \n    unique_domains = len(set(domains))\n    if unique_domains == 1 and DEBUG_CELL2:\n        print(f\"[COLLATE] WARNING: All {len(domains)} samples have domain_label={domains[0]}\")\n        print(f\"[COLLATE] Forcing 50/50 split...\")\n        half = len(domains) // 2\n        for j in range(half):\n            domains[j] = 0\n        for j in range(half, len(domains)):\n            domains[j] = 1\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n        print(f\"[COLLATE] Fixed: domain_0={domain_labels.eq(0).sum().item()}, domain_1={domain_labels.eq(1).sum().item()}\")\n    \n    if _DEBUG_DISCOVERY:\n        batch_size = labels.size(0)\n        total_label_positions = labels.numel()\n        valid_labels = (labels != -100).sum().item()\n        padding_labels = total_label_positions - valid_labels\n        \n        if valid_labels == 0:\n            print(f\"[COLLATE] CRITICAL WARNING: ALL labels are -100! Decoder won't train!\")\n            print(f\"[COLLATE]   batch_size={batch_size}, total_positions={total_label_positions}\")\n        elif valid_labels < batch_size * 2:\n            print(f\"[COLLATE] WARNING: Very few valid labels!\")\n            print(f\"[COLLATE]   batch_size={batch_size}, valid_labels={valid_labels}, padding={padding_labels}\")\n            print(f\"[COLLATE]   Average valid labels per sample: {valid_labels/batch_size:.1f}\")\n    \n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_texts\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels,\n    }\n\ndef create_optimized_dataloader(\n    dataset: Dataset,\n    batch_size: Optional[int] = None,\n    shuffle: bool = True,\n    split: str = \"train\",\n) -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n    \n    batch_size = int(batch_size)\n    original_batch_size = batch_size\n    adjusted = False\n    \n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        new_batch_size = (batch_size // _NUM_GPUS) * _NUM_GPUS\n        if new_batch_size == 0:\n            if DEBUG_CELL2:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original.\")\n        else:\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n    \n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size {original_batch_size} to {batch_size} (DP-divisible, GPUs={_NUM_GPUS})\")\n    \n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n    \n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n    \n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n    \n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n    \n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n    \n    return dataloader\n\nMemoryEfficientDataset = DualPathDataset\n\nprint(\"=\" * 80)\nprint(\"Cell 2: Dual-path data loading ready - WORD + SUBWORD TOKENIZATION (BanglaT5)\")\nprint(\"=\" * 80)\nprint(\"‚úÖ FIX #1: Worker init FORCES vocab_size=32128\")\nprint(\"‚úÖ FIX #2: safe_collate adds final validation before return\")\nprint(\"‚úÖ FIX #3: Emergency modulo operation if clamping fails\")\nprint(\"Configuration:\")\nprint(f\"  Task prefix: '{_TASK_PREFIX}'\")\nprint(f\"  Languages: {_SOURCE_LANG} ‚Üí {_TARGET_LANG}\")\nprint(f\"  Model vocab: {MODEL_VOCAB_SIZE}\")\nprint(f\"  Domain labels: idx % 2 (alternating 0/1)\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"5MkHgCN7H4J1","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:25.142354Z","iopub.execute_input":"2026-02-16T03:01:25.142569Z","iopub.status.idle":"2026-02-16T03:01:25.229314Z","shell.execute_reply.started":"2026-02-16T03:01:25.142551Z","shell.execute_reply":"2026-02-16T03:01:25.228771Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 2: Dual-path data loading ready - WORD + SUBWORD TOKENIZATION (BanglaT5)\n================================================================================\n‚úÖ FIX #1: Worker init FORCES vocab_size=32128\n‚úÖ FIX #2: safe_collate adds final validation before return\n‚úÖ FIX #3: Emergency modulo operation if clamping fails\nConfiguration:\n  Task prefix: 'translate Bengali to English: '\n  Languages: bn ‚Üí en\n  Model vocab: 32128\n  Domain labels: idx % 2 (alternating 0/1)\n================================================================================\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# DIAGNOSTIC CELL - Run this to find the real problem\n# ==============================================================================\n\nimport torch\n\nprint(\"=\" * 80)\nprint(\"DIAGNOSTIC TEST - Finding Token ID Source\")\nprint(\"=\" * 80)\n\n# Test 1: Check tokenizer's actual vocab size\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n\nprint(f\"\\n[TEST 1] Tokenizer Properties:\")\nprint(f\"  len(tokenizer): {len(tokenizer)}\")\nprint(f\"  tokenizer.vocab_size: {tokenizer.vocab_size}\")\nprint(f\"  Model config vocab: {tokenizer.model_max_length}\")\n\n# Test 2: Encode a simple English sentence and check IDs\ntest_sentence = \"hello world this is a test\"\nencoded = tokenizer(test_sentence, return_tensors=\"pt\")\ninput_ids = encoded[\"input_ids\"][0]\n\nprint(f\"\\n[TEST 2] English Encoding Test:\")\nprint(f\"  Text: '{test_sentence}'\")\nprint(f\"  Token IDs: {input_ids.tolist()}\")\nprint(f\"  Max ID: {input_ids.max().item()}\")\nprint(f\"  Min ID: {input_ids.min().item()}\")\n\nif input_ids.max().item() >= 32128:\n    print(f\"  ‚ùå PROBLEM: Tokenizer produced ID {input_ids.max().item()} >= 32128!\")\nelse:\n    print(f\"  ‚úÖ OK: All IDs within range\")\n\n# Test 3: Check special tokens\nprint(f\"\\n[TEST 3] Special Tokens:\")\nspecial_tokens = tokenizer.all_special_tokens\nspecial_ids = tokenizer.all_special_ids\nprint(f\"  Special tokens: {special_tokens[:10]}\")  # First 10\nprint(f\"  Special IDs: {special_ids[:10]}\")\n\nmax_special_id = max(special_ids) if special_ids else 0\nprint(f\"  Max special ID: {max_special_id}\")\n\nif max_special_id >= 32128:\n    print(f\"  ‚ùå PROBLEM: Special token ID {max_special_id} >= 32128!\")\n\n# Test 4: Load dataset and check a sample\nprint(f\"\\n[TEST 4] Dataset Sample Check:\")\ntry:\n    from torch.utils.data import DataLoader\n    dataset = MemoryEfficientDataset(\n        pairs=[(\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶§ ‡¶ñ‡¶æ‡¶á\", \"I eat rice\")],\n        tokenizer=tokenizer,\n        max_length=128,\n        vocab_size=32128\n    )\n    \n    sample = dataset[0]\n    input_ids = sample[\"input_ids\"]\n    labels = sample[\"labels\"]\n    \n    valid_labels = labels[labels != -100]\n    \n    print(f\"  Input IDs range: [{input_ids.min().item()}, {input_ids.max().item()}]\")\n    print(f\"  Labels range: [{valid_labels.min().item() if len(valid_labels) > 0 else 'N/A'}, {valid_labels.max().item() if len(valid_labels) > 0 else 'N/A'}]\")\n    \n    if input_ids.max().item() >= 32128:\n        print(f\"  ‚ùå PROBLEM: Dataset input_ids exceed 32128!\")\n        print(f\"  ‚Üí Cell 2 clamping is NOT working\")\n    \n    if len(valid_labels) > 0 and valid_labels.max().item() >= 32128:\n        print(f\"  ‚ùå PROBLEM: Dataset labels exceed 32128!\")\n        print(f\"  ‚Üí Cell 2 clamping is NOT working\")\n    \n    if input_ids.max().item() < 32128 and (len(valid_labels) == 0 or valid_labels.max().item() < 32128):\n        print(f\"  ‚úÖ Dataset IDs are within range\")\n        print(f\"  ‚Üí Problem must be in collate function or model\")\n\nexcept Exception as e:\n    print(f\"  ‚ùå Dataset test failed: {e}\")\n\n# Test 5: Check collate function\nprint(f\"\\n[TEST 5] Collate Function Check:\")\ntry:\n    loader = DataLoader(dataset, batch_size=1, collate_fn=safe_collate)\n    batch = next(iter(loader))\n    \n    batch_input_ids = batch[\"input_ids\"]\n    batch_labels = batch[\"labels\"]\n    \n    valid_batch_labels = batch_labels[batch_labels != -100]\n    \n    print(f\"  Batch input_ids range: [{batch_input_ids.min().item()}, {batch_input_ids.max().item()}]\")\n    print(f\"  Batch labels range: [{valid_batch_labels.min().item() if len(valid_batch_labels) > 0 else 'N/A'}, {valid_batch_labels.max().item() if len(valid_batch_labels) > 0 else 'N/A'}]\")\n    \n    if batch_input_ids.max().item() >= 32128:\n        print(f\"  ‚ùå PROBLEM: Collate function produced input_ids >= 32128!\")\n    \n    if len(valid_batch_labels) > 0 and valid_batch_labels.max().item() >= 32128:\n        print(f\"  ‚ùå PROBLEM: Collate function produced labels >= 32128!\")\n    \n    if batch_input_ids.max().item() < 32128 and (len(valid_batch_labels) == 0 or valid_batch_labels.max().item() < 32128):\n        print(f\"  ‚úÖ Collate function output is valid\")\n        print(f\"  ‚Üí Problem must be in training loop or model\")\n\nexcept Exception as e:\n    print(f\"  ‚ùå Collate test failed: {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIAGNOSTIC COMPLETE\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:25.230183Z","iopub.execute_input":"2026-02-16T03:01:25.230458Z","iopub.status.idle":"2026-02-16T03:01:25.778196Z","shell.execute_reply.started":"2026-02-16T03:01:25.230431Z","shell.execute_reply":"2026-02-16T03:01:25.777569Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDIAGNOSTIC TEST - Finding Token ID Source\n================================================================================\n\n[TEST 1] Tokenizer Properties:\n  len(tokenizer): 32100\n  tokenizer.vocab_size: 32100\n  Model config vocab: 512\n\n[TEST 2] English Encoding Test:\n  Text: 'hello world this is a test'\n  Token IDs: [20, 23229, 2281, 11582, 4467, 1141, 559, 20, 15649, 1]\n  Max ID: 23229\n  Min ID: 1\n  ‚úÖ OK: All IDs within range\n\n[TEST 3] Special Tokens:\n  Special tokens: ['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>']\n  Special IDs: [1, 2, 0, 32099, 32098, 32097, 32096, 32095, 32094, 32093]\n  Max special ID: 32099\n\n[TEST 4] Dataset Sample Check:\n[CELL2] Dataset using provided vocab_size: 32128\n[CELL2] Dataset initialized: 1 valid pairs, 0 invalid, split=train\n  Input IDs range: [1, 18353]\n  Labels range: [1, 30821]\n  ‚úÖ Dataset IDs are within range\n  ‚Üí Problem must be in collate function or model\n\n[TEST 5] Collate Function Check:\n  Batch input_ids range: [0, 18353]\n  Batch labels range: [1, 30821]\n  ‚úÖ Collate function output is valid\n  ‚Üí Problem must be in training loop or model\n\n================================================================================\nDIAGNOSTIC COMPLETE\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE - WORD-LEVEL HOMOGRAPH DISAMBIGUATION (MODEL-AGNOSTIC)\n# ==============================================================================\n# ‚úÖ WORKS WITH: mBART-50, BanglaT5, M2M100, XLM-R, any encoder-decoder\n# ‚úÖ NO MODEL-SPECIFIC CODE\n# ‚úÖ Operates on embedding vectors (torch.Tensor)\n# ‚úÖ Language detection via Unicode ranges (Bengali: U+0980-U+09FF)\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\n\nPRINT_INTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HAS_CLUSTERING = True\nexcept Exception:\n    HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available\")\n\ntry:\n    from sklearn.cluster import KMeans\n    HAS_KMEANS = True\nexcept Exception:\n    HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available\")\n\n# ‚úÖ ALL CONFIGURATION IS MODEL-AGNOSTIC\ntry:\n    DSCD_MAX_PROTOS = int(DSCD_MAX_PROTOS)\n    DSCD_BUFFER_SIZE = int(DSCD_BUFFER_SIZE)\n    DSCD_N_MIN = int(DSCD_N_MIN)\n    DSCD_DISPERSION_THRESHOLD = float(DSCD_DISPERSION_THRESHOLD)\n    DSCD_NEWSENSE_LAMBDA = float(DSCD_NEWSENSE_LAMBDA)\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    DSCD_ENABLE_TRAINING_CLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\n    DSCD_MIN_LETTERS = int(DSCD_MIN_LETTERS)\n    DSCD_MIN_LETTER_FRACTION = float(DSCD_MIN_LETTER_FRACTION)\nexcept (NameError, ValueError, TypeError):\n    DSCD_MAX_PROTOS = 3\n    DSCD_BUFFER_SIZE = 20\n    DSCD_N_MIN = 3\n    DSCD_DISPERSION_THRESHOLD = 0.25\n    DSCD_NEWSENSE_LAMBDA = 1.2\n    VERBOSE_LOGGING = False\n    DSCD_ENABLE_TRAINING_CLUSTERING = True\n    DSCD_MIN_LETTERS = 3\n    DSCD_MIN_LETTER_FRACTION = 0.6\n    print(\"[CELL3] WARNING: Using default DSCD config\")\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    DEBUG_DISCOVERY = False\n\ntry:\n    MAX_TOKENS_PER_DISCOVERY = int(globals().get('MAX_TOKENS_PER_DISCOVERY', 150))\nexcept Exception:\n    MAX_TOKENS_PER_DISCOVERY = 150\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST_BN = set(HOMOGRAPH_REFERENCE_LIST_BN)\n    print(f\"[CELL3] Loaded reference list for evaluation: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words\")\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST_BN = {\n        '‡¶ï‡¶≤', '‡¶ï‡¶æ‡¶≤', '‡¶™‡¶æ‡¶§‡¶æ', '‡¶´‡¶≤', '‡¶¨‡¶æ‡¶∞', '‡¶π‡¶æ‡¶∞', '‡¶§‡¶æ‡¶∞‡¶æ',\n        '‡¶™‡¶°‡¶º‡¶æ', '‡¶¶‡ßá‡¶ñ‡¶æ', '‡¶ö‡¶≤‡¶æ', '‡¶ß‡¶∞‡¶æ', '‡¶Ö‡¶∞‡ßç‡¶•', '‡¶∂‡¶¨‡ßç‡¶¶', '‡¶Æ‡ßÅ‡¶ñ',\n        '‡¶§‡ßã‡¶≤‡¶æ', '‡¶¨‡¶æ‡¶Å‡¶ö‡¶æ', '‡¶Æ‡¶æ‡¶∞‡¶æ', '‡¶â‡¶§‡ßç‡¶§‡¶∞', '‡¶™‡¶æ‡¶§‡ßç‡¶∞', '‡¶¨‡ßá‡¶≤‡¶æ', '‡¶ó‡¶æ‡¶®',\n        '‡¶®‡¶æ‡¶Æ', '‡¶¨‡¶≤', '‡¶ö‡¶æ‡¶≤', '‡¶ï‡¶≤‡¶æ', '‡¶ß‡¶æ‡¶∞‡¶æ', '‡¶™‡¶§‡ßç‡¶∞', '‡¶∞‡¶æ‡¶ó', '‡¶∞‡¶∏',\n        '‡¶§‡ßÄ‡¶∞', '‡¶ú‡¶Æ‡¶æ', '‡¶Æ‡¶æ‡¶®', '‡¶¶‡¶æ‡¶¨‡¶ø', '‡¶Ü‡¶∏‡¶®', '‡¶∏‡¶æ‡¶°‡¶º‡¶æ', '‡¶¨‡¶∏‡¶æ', '‡¶™‡¶¶',\n        '‡¶Ö‡¶Ç‡¶∂', '‡¶Æ‡ßã‡¶°‡¶º', '‡¶ò‡¶∞', '‡¶Æ‡¶®', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï'\n    }\n    print(\"[CELL3] Using default reference list\")\n\nDSCD_MAX_CLUSTERING_POINTS = 500\n\nBENGALI_PUNCT_SET = set(['‡•§', '‡••'])\nCOMMON_PUNCT_SET = set(['.', ',', '!', '?', ';', ':', '-', '‚Äî', '\"', \"'\", '(', ')', '[', ']', '{', '}'])\nPUNCT_SET = BENGALI_PUNCT_SET | COMMON_PUNCT_SET\n\n# ‚úÖ ALL FUNCTIONS ARE MODEL-AGNOSTIC (work with any encoder)\n\ndef is_punctuation_only(token: str) -> bool:\n    \"\"\"‚úÖ Language-agnostic punctuation detection\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    clean = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n    if not clean:\n        return False\n    if clean in BENGALI_PUNCT_SET:\n        return True\n    if clean in COMMON_PUNCT_SET:\n        return True\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    return all(c in PUNCT_SET for c in clean)\n\ndef clean_token_for_dscd(token: str) -> str:\n    \"\"\"‚úÖ Works with any tokenizer (SentencePiece, BPE, WordPiece)\"\"\"\n    if not token or not isinstance(token, str):\n        return \"\"\n    cleaned = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n    for punct in list(PUNCT_SET):\n        cleaned = cleaned.replace(punct, \"\")\n    return cleaned.lower()\n\ndef normalize_token_key(token: str) -> str:\n    \"\"\"‚úÖ Model-agnostic normalization\"\"\"\n    return clean_token_for_dscd(token)\n\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    \"\"\"‚úÖ Uses Unicode categories (works for any language)\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if not token:\n        return False\n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith('L'):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if (letters / total) < min_letter_fraction:\n        return False\n    return True\n\ndef is_indic_subword_fragment(token: str) -> bool:\n    \"\"\"‚úÖ Indic script detection (Bengali, Hindi, etc.)\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n\n    token = token.strip()\n    if not token:\n        return False\n\n    only_vowel_marks = True\n    only_combining_marks = True\n    has_virama = False\n    letter_count = 0\n\n    for ch in token:\n        cat = unicodedata.category(ch)\n\n        if cat.startswith('L'):\n            letter_count += 1\n            only_vowel_marks = False\n            only_combining_marks = False\n\n        if cat not in ('Mn', 'Mc'):\n            only_combining_marks = False\n\n        virama_chars = [\n            '\\u094D',  # Devanagari\n            '\\u09CD',  # Bengali\n            '\\u0A4D',  # Gurmukhi\n            '\\u0ACD',  # Gujarati\n            '\\u0B4D',  # Oriya\n            '\\u0BCD',  # Tamil\n            '\\u0C4D',  # Telugu\n            '\\u0CCD',  # Kannada\n            '\\u0D4D'   # Malayalam\n        ]\n        if ch in virama_chars:\n            has_virama = True\n\n    if only_vowel_marks or only_combining_marks:\n        return True\n\n    if has_virama and len(token) <= 2:\n        return True\n\n    if letter_count == 0:\n        return True\n\n    vowel_modifier_ranges = [\n        ('\\u093E', '\\u094C'),\n        ('\\u09BE', '\\u09CC'),\n        ('\\u0ABE', '\\u0ACC'),\n        ('\\u0BBE', '\\u0BCC'),\n        ('\\u0C3E', '\\u0C4C'),\n        ('\\u0CBE', '\\u0CCC'),\n    ]\n\n    modifier_count = 0\n    for ch in token:\n        for start, end in vowel_modifier_ranges:\n            if start <= ch <= end:\n                modifier_count += 1\n                break\n\n    if modifier_count > 0 and modifier_count == len(token):\n        return True\n\n    if len(token) <= 2 and modifier_count > 0:\n        return True\n\n    return False\n\nclass MemoryEfficientPrototypeStore:\n    \"\"\"\n    ‚úÖ MODEL-AGNOSTIC: Stores embedding vectors (torch.Tensor)\n    Works with any encoder: mBART, T5, BERT, XLM-R, etc.\n    \"\"\"\n    def __init__(self, embed_dim, max_protos: Optional[int] = None):\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        self.embed_dim = embed_dim\n        self.max_protos = int(max_protos)\n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n\n    def add_prototype(self, vector: torch.Tensor, current_time: Optional[float] = None, count: int = 1) -> None:\n        if current_time is None:\n            current_time = time.time()\n        v = vector.detach().cpu().clone()\n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creation_time.append(float(current_time))\n        else:\n            min_idx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            self.centroids[min_idx] = v\n            self.counts[min_idx] = int(count)\n            self.creation_time[min_idx] = float(current_time)\n\n    def update_prototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignment_distance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.add_prototype(vector, time.time(), count=1)\n            return\n        old_centroid = self.centroids[idx]\n        new_vector = vector.detach().cpu()\n        self.centroids[idx] = (1.0 - eta) * old_centroid + eta * new_vector\n        self.counts[idx] = int(self.counts[idx]) + 1\n        if assignment_distance is not None:\n            self.update_rolling_stats(float(assignment_distance))\n\n    def update_rolling_stats(self, d: float) -> None:\n        if not self.distances:\n            self.mu = float(d)\n            self.tau = max(1e-6, float(d) * 0.1)\n            self.distances = [float(d)]\n            return\n        prev_mu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n        self.distances.append(float(d))\n        if len(self.distances) > 50:\n            self.distances.pop(0)\n\n    def get_adaptive_threshold(self, lam: float = 1.0) -> float:\n        return float(self.mu + lam * max(self.tau, 1e-4))\n\n    def size(self) -> int:\n        return len(self.centroids)\n\n    def ensure_consistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            self.counts = self.counts[:n] if len(self.counts) > n else self.counts + [1] * (n - len(self.counts))\n        if len(self.creation_time) != n:\n            self.creation_time = self.creation_time[:n] if len(self.creation_time) > n else self.creation_time + [time.time()] * (n - len(self.creation_time))\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    \"\"\"\n    ‚úÖ MODEL-AGNOSTIC DSCD MODULE\n    \n    Works with ANY encoder-decoder model:\n    - mBART-50 (embed_dim=1024)\n    - BanglaT5 (embed_dim=768)\n    - M2M100, XLM-R, mT5, etc.\n    \n    Input: token_embeddings (B, L, embed_dim) from ANY encoder\n    Output: augmented embeddings + prototype assignments\n    \"\"\"\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        buffer_size: Optional[int] = None,\n        max_protos: Optional[int] = None,\n        n_min: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        language: str = \"bn\",\n        enable_training_clustering: Optional[bool] = None,\n        max_clustering_points: Optional[int] = None,\n        max_candidates_per_step: int = 2,\n        dscd_min_letters: int = 3,\n        dscd_min_letter_fraction: float = 0.6,\n    ):\n        super().__init__()\n        if buffer_size is None:\n            buffer_size = DSCD_BUFFER_SIZE\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        if n_min is None:\n            n_min = DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = DSCD_MAX_CLUSTERING_POINTS\n        if enable_training_clustering is None:\n            enable_training_clustering = DSCD_ENABLE_TRAINING_CLUSTERING\n\n        self.embed_dim = int(embed_dim)\n        self.buffer_size = int(buffer_size)\n        self.max_protos = int(max_protos)\n        self.n_min = int(n_min)\n        self.dispersion_threshold = float(dispersion_threshold)\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscd_min_letters = int(dscd_min_letters)\n        self.dscd_min_letter_fraction = float(dscd_min_letter_fraction)\n\n        # ‚úÖ Special token handling (works for ANY tokenizer)\n        try:\n            if tokenizer is not None and 'get_tokenizer_special_tokens' in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(getattr(tokenizer, 'all_special_tokens', [])) if tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = set()\n\n        self.dscd_allowed_tokens: Set[str] = set()\n        self.dscd_ignored_tokens: Set[str] = set()\n        self.dscd_cache_max_size = 10000\n\n        self.prototype_stores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.buffers: Dict[str, deque] = {}\n        self.discovered_log: List[Dict[str, Any]] = []\n        self.discovered_homographs: Set[str] = set()\n\n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n\n        self.dispersion_cache: Dict[str, float] = {}\n        self.dispersion_last_updated: Dict[str, float] = {}\n        self.dispersion_lock = threading.Lock()\n        self.clustering_lock = threading.Lock()\n        self.buffer_lock = threading.Lock()\n\n        from collections import deque as thread_deque\n        self.active_threads = thread_deque(maxlen=100)\n        self.thread_lock = threading.Lock()\n\n        self.last_cluster_time: Dict[str, float] = {}\n        self.cluster_cooldown_seconds = 5.0\n\n        self.enable_training_clustering = bool(enable_training_clustering)\n        self.discovery_count = 0\n        self.discovery_times: List[float] = []\n        self.clustered_tokens: Set[str] = set()\n\n        self.cluster_stats: Dict[str, Dict[str, Any]] = {}\n\n        # ‚úÖ Span prediction head (works with any embed_dim)\n        self.span_head = nn.Sequential(\n            nn.Linear(self.embed_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1),\n        )\n\n        self.sigma_net = nn.Sequential(\n            nn.Linear(self.embed_dim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1),\n        )\n\n        self.gate_w = nn.Parameter(torch.tensor(1.0))\n        self.gate_b = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n\n        self.max_clustering_points = int(max_clustering_points)\n        self.max_candidates_per_step = int(max_candidates_per_step)\n\n        try:\n            self.homograph_reference_list = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\n        except Exception:\n            self.homograph_reference_list = set()\n\n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        \"\"\"‚úÖ Serialization (model-agnostic)\"\"\"\n        state = super().state_dict(destination, prefix, keep_vars)\n\n        plain_stores = {}\n        for token, store in self.prototype_stores.items():\n            plain_stores[token] = {\n                'centroids': [c.cpu() for c in store.centroids] if hasattr(store, 'centroids') else [],\n                'counts': list(store.counts) if hasattr(store, 'counts') else [],\n                'creation_time': list(store.creation_time) if hasattr(store, 'creation_time') else [],\n                'mu': float(store.mu) if hasattr(store, 'mu') else 0.0,\n                'tau': float(store.tau) if hasattr(store, 'tau') else 0.0,\n                'size': int(store.size()) if hasattr(store, 'size') else 0,\n            }\n\n        state[prefix + 'prototype_stores_data'] = plain_stores\n        state[prefix + 'discovered_homographs'] = list(self.discovered_homographs)\n        return state\n\n    def load_state_dict(self, state_dict, strict=True):\n        \"\"\"‚úÖ Deserialization (model-agnostic)\"\"\"\n        prefix = ''\n        plain_stores = state_dict.pop('prototype_stores_data', {})\n        discovered = state_dict.pop('discovered_homographs', [])\n\n        super().load_state_dict(state_dict, strict=strict)\n\n        if not plain_stores:\n            print(\"[DSCD] WARNING: Empty prototype_stores in checkpoint\")\n            return\n\n        self.prototype_stores = {}\n        self.discovered_homographs = set(discovered)\n\n        for token, store_dict in plain_stores.items():\n            store = MemoryEfficientPrototypeStore(embed_dim=self.embed_dim, max_protos=self.max_protos)\n\n            centroids_data = store_dict.get('centroids', [])\n            store.centroids = []\n            for c in centroids_data:\n                if isinstance(c, torch.Tensor):\n                    store.centroids.append(c)\n                else:\n                    store.centroids.append(torch.tensor(c))\n\n            store.counts = store_dict.get('counts', [])\n            store.creation_time = store_dict.get('creation_time', [])\n            store.mu = store_dict.get('mu', 0.0)\n            store.tau = store_dict.get('tau', 0.0)\n\n            store.ensure_consistency()\n            self.prototype_stores[token] = store\n\n        print(f\"[DSCD] Loaded {len(self.prototype_stores)} tokens, {sum(s.size() for s in self.prototype_stores.values())} prototypes\")\n\n    @staticmethod\n    def clean_token(token):\n        return clean_token_for_dscd(str(token))\n\n    def is_valid_multi_sense(self, token):\n        if token not in self.prototype_stores:\n            return False\n        store = self.prototype_stores[token]\n        total_occurrences = sum(store.counts) if hasattr(store, 'counts') else 0\n        min_per_proto = min(store.counts) if hasattr(store, 'counts') and store.counts else 0\n        return store.size() >= 2 and total_occurrences >= 10 and min_per_proto >= 2\n\n    def is_multi_sense_store(self, store: MemoryEfficientPrototypeStore) -> bool:\n        \"\"\"‚úÖ Pure numerical logic (model-agnostic)\"\"\"\n        k = store.size()\n        if k < 2:\n            return False\n\n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(2, self.n_min // 2))\n        if strong < 2:\n            return False\n\n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n\n            if len(cents) < 2:\n                return False\n\n            cents = np.stack(cents, axis=0)\n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            tri = dists[np.triu_indices(len(cents), k=1)]\n\n            if tri.size == 0:\n                return False\n\n            min_dist = float(tri.min())\n            base = max(store.tau, 1e-3)\n            return min_dist > base * DSCD_NEWSENSE_LAMBDA\n        except Exception:\n            return True\n\n    def discover_homographs_for_tokens(\n        self,\n        token_names: List[str],\n        min_cluster_samples: int,\n        dispersion_threshold: float,\n        global_step: int,\n    ) -> int:\n        \"\"\"‚úÖ Clustering logic (model-agnostic)\"\"\"\n        discovered_in_run: List[str] = []\n\n        for idx, token in enumerate(token_names):\n            try:\n                if is_punctuation_only(token):\n                    continue\n\n                success = self.cluster_buffer_to_prototypes_hierarchical(token)\n\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        self.discovered_homographs.add(clean_token)\n                        discovered_in_run.append(clean_token)\n            except Exception:\n                continue\n\n        try:\n            self.discovered_log.append({\n                'timestamp': time.time(),\n                'global_step': global_step,\n                'candidates_processed': len(token_names),\n                'discovered_count': len(discovered_in_run),\n                'homographs': discovered_in_run,\n                'total_discovered': len(self.discovered_homographs),\n            })\n        except Exception:\n            pass\n\n        return len(discovered_in_run)\n\n    def discover_homographs(\n        self,\n        min_cluster_samples: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        max_candidates: int = 500,\n    ) -> int:\n        \"\"\"‚úÖ Discovery pipeline (model-agnostic)\"\"\"\n        if min_cluster_samples is None:\n            min_cluster_samples = self.n_min\n        if dispersion_threshold is None:\n            dispersion_threshold = self.dispersion_threshold\n\n        candidates: List[Tuple[str, float, int, float]] = []\n\n        with self.buffer_lock:\n            for token, buffer in self.buffers.items():\n                if is_punctuation_only(token):\n                    continue\n\n                buffer_size = len(buffer)\n                if buffer_size >= max(min_cluster_samples + 2, 10):\n                    clean_token = clean_token_for_dscd(token)\n\n                    if clean_token in HOMOGRAPH_REFERENCE_LIST_BN:\n                        dispersion = max(self.get_dispersion(token), dispersion_threshold * 1.15)\n                        if DEBUG_DISCOVERY:\n                            print(f\"[DSCD-PRIORITY] Boosting reference homograph '{token}' dispersion to {dispersion:.3f}\")\n                    else:\n                        dispersion = self.get_dispersion(token)\n\n                    if dispersion >= dispersion_threshold:\n                        rank_score = dispersion * buffer_size\n                        candidates.append((token, rank_score, buffer_size, dispersion))\n\n        if not candidates:\n            return 0\n\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_candidates]\n\n        discovered: List[str] = []\n\n        for token, score, buf_size, disp in candidates:\n            try:\n                with self.clustering_lock:\n                    success = self.cluster_buffer_to_prototypes_hierarchical(token)\n\n                    if success:\n                        store = self.prototype_stores.get(token)\n                        if store and store.size() >= 2:\n                            clean_token = normalize_token_key(token)\n                            self.discovered_homographs.add(clean_token)\n                            discovered.append(clean_token)\n            except Exception:\n                continue\n\n        try:\n            self.discovered_log.append({\n                'timestamp': time.time(),\n                'candidates': len(candidates),\n                'discovered': len(discovered),\n                'homographs': discovered[:20],\n            })\n        except Exception:\n            pass\n\n        return len(discovered)\n\n    def get_dispersion(self, token_type: str) -> float:\n        \"\"\"‚úÖ Numerical dispersion calculation (model-agnostic)\"\"\"\n        with self.dispersion_lock:\n            if token_type in self.dispersion_cache:\n                try:\n                    last_update = self.dispersion_last_updated.get(token_type, 0.0)\n                    if time.time() - last_update < 3600:\n                        return self.dispersion_cache[token_type]\n                except Exception:\n                    pass\n\n        with self.buffer_lock:\n            if token_type not in self.buffers:\n                return 0.0\n\n            buf_len = len(self.buffers[token_type])\n            if buf_len < 2:\n                return 0.05 if buf_len == 1 else 0.0\n\n            try:\n                embeddings: List[np.ndarray] = []\n                for emb in self.buffers[token_type]:\n                    try:\n                        if isinstance(emb, torch.Tensor):\n                            embeddings.append(emb.cpu().numpy())\n                        else:\n                            embeddings.append(np.asarray(emb, dtype=np.float32))\n                    except Exception:\n                        continue\n\n                if len(embeddings) < 2:\n                    return 0.05 if len(embeddings) == 1 else 0.0\n\n                embeddings_np = np.stack(embeddings, axis=0)\n                centroid = embeddings_np.mean(axis=0)\n                distances = np.linalg.norm(embeddings_np - centroid[None, :], axis=1)\n                dispersion = float(distances.std())\n\n                with self.dispersion_lock:\n                    self.dispersion_cache[token_type] = dispersion\n                    self.dispersion_last_updated[token_type] = time.time()\n\n                return dispersion\n            except Exception:\n                return 0.0\n\n    def validate_prototypes(\n        self,\n        homograph_list: Optional[List[str]] = None,\n        cluster_missing: bool = True,\n    ) -> Dict[str, Any]:\n        \"\"\"‚úÖ Validation logic (model-agnostic)\"\"\"\n        if homograph_list is None:\n            try:\n                homograph_list = list(HOMOGRAPH_REFERENCE_LIST_BN)\n            except Exception:\n                homograph_list = ['‡¶ï‡¶≤', '‡¶™‡¶æ‡¶§‡¶æ', '‡¶´‡¶≤', '‡¶Æ‡¶æ‡¶®']\n\n        print(\"=\" * 80)\n        print(\"DSCD-VALIDATION: Prototype Quality Check\")\n        print(\"=\" * 80)\n\n        validation_results: Dict[str, Any] = {\n            'total_tokens': len(self.prototype_stores),\n            'total_prototypes': 0,\n            'multi_sense_tokens': 0,\n            'homographs_found': 0,\n            'homographs_missing': [],\n            'avg_prototypes_per_token': 0.0,\n            'avg_samples_per_prototype': 0.0,\n            'quality_score': 0.0,\n        }\n\n        total_samples = 0\n        for token, store in self.prototype_stores.items():\n            num_protos = len(store.centroids)\n            validation_results['total_prototypes'] += num_protos\n\n            if self.is_multi_sense_store(store):\n                validation_results['multi_sense_tokens'] += 1\n\n            try:\n                total_samples += sum(store.counts)\n            except Exception:\n                pass\n\n        if validation_results['total_tokens'] > 0:\n            validation_results['avg_prototypes_per_token'] = validation_results['total_prototypes'] / validation_results['total_tokens']\n\n        if validation_results['total_prototypes'] > 0:\n            validation_results['avg_samples_per_prototype'] = total_samples / validation_results['total_prototypes']\n\n        print(\"VALIDATION: Reference Homograph Coverage\")\n        print(\"-\" * 80)\n\n        missing_tokens_to_cluster: List[str] = []\n\n        for homograph in homograph_list:\n            clean_h = clean_token_for_dscd(homograph)\n            found = False\n            found_key = None\n            found_protos = 0\n\n            for key in self.prototype_stores.keys():\n                clean_key = clean_token_for_dscd(str(key))\n\n                if clean_key == clean_h:\n                    found = True\n                    found_key = key\n                    found_protos = len(self.prototype_stores[key].centroids)\n                    break\n\n            if found and self.is_multi_sense_store(self.prototype_stores[found_key]):\n                validation_results['homographs_found'] += 1\n                try:\n                    counts = self.prototype_stores[found_key].counts\n                    print(f\"  ‚úì {homograph} - {found_protos} prototypes (counts={counts})\")\n                except Exception:\n                    print(f\"  ‚úì {homograph} - {found_protos} prototypes\")\n            elif found and found_protos == 1:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  ‚ö† {homograph} - Only 1 prototype\")\n                if cluster_missing:\n                    missing_tokens_to_cluster.append(found_key)\n            else:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  ‚úó {homograph} - NOT FOUND\")\n                if cluster_missing:\n                    for buf_key in self.buffers.keys():\n                        clean_buf_key = clean_token_for_dscd(str(buf_key))\n                        if clean_buf_key == clean_h:\n                            if len(self.buffers[buf_key]) >= max(self.n_min + 2, 10):\n                                print(f\"      - Found in buffer, will cluster\")\n                                missing_tokens_to_cluster.append(buf_key)\n                            break\n\n        if cluster_missing and missing_tokens_to_cluster:\n            print(f\"\\nVALIDATION: Clustering {len(missing_tokens_to_cluster)} missing tokens...\")\n            for token in missing_tokens_to_cluster:\n                try:\n                    with self.clustering_lock:\n                        self.cluster_buffer_to_prototypes_hierarchical(token)\n                        if token in self.prototype_stores and self.is_multi_sense_store(self.prototype_stores[token]):\n                            print(f\"  ‚úì Successfully clustered: {token}\")\n                except Exception as e:\n                    print(f\"  ‚úó Failed to cluster {token}: {e}\")\n\n        homograph_coverage = validation_results['homographs_found'] / len(homograph_list) if homograph_list else 0.0\n        multi_sense_ratio = validation_results['multi_sense_tokens'] / validation_results['total_tokens'] if validation_results['total_tokens'] > 0 else 0.0\n        validation_results['quality_score'] = (homograph_coverage * 0.6) + (multi_sense_ratio * 0.4)\n\n        print(\"-\" * 80)\n        print(\"VALIDATION: Summary\")\n        print(f\"  - Total tokens: {validation_results['total_tokens']}\")\n        print(f\"  - Total prototypes: {validation_results['total_prototypes']}\")\n        print(f\"  - Multi-sense tokens: {validation_results['multi_sense_tokens']}\")\n        print(f\"  - Reference found: {validation_results['homographs_found']}/{len(homograph_list)}\")\n        print(f\"  - Quality Score: {validation_results['quality_score']*100:.2f}%\")\n        print(\"=\" * 80)\n\n        return validation_results\n\n    def should_track_token(self, token_text: str) -> bool:\n        \"\"\"‚úÖ Token filtering (model-agnostic)\"\"\"\n        if not token_text or not isinstance(token_text, str):\n            return False\n\n        if len(self.dscd_allowed_tokens) > self.dscd_cache_max_size:\n            self.dscd_allowed_tokens.clear()\n        if len(self.dscd_ignored_tokens) > self.dscd_cache_max_size:\n            self.dscd_ignored_tokens.clear()\n\n        if token_text in self.dscd_allowed_tokens:\n            return True\n        if token_text in self.dscd_ignored_tokens:\n            return False\n\n        if not getattr(self, 'training', False):\n            if token_text in self.prototype_stores:\n                self.dscd_allowed_tokens.add(token_text)\n                return True\n            clean = clean_token_for_dscd(token_text)\n            if clean and clean in self.prototype_stores:\n                self.dscd_allowed_tokens.add(token_text)\n                return True\n\n        if token_text in self.special_tokens:\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        if is_punctuation_only(token_text):\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        clean = clean_token_for_dscd(token_text)\n        if not clean:\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        if len(clean) < self.dscd_min_letters:\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        if not any(c.isalpha() for c in clean):\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        if clean.isdigit():\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n\n        try:\n            indic_range_1 = any('\\u0900' <= c <= '\\u0DFF' for c in clean)\n            indic_range_2 = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            has_indic = indic_range_1 or indic_range_2\n\n            if has_indic:\n                if len(clean) >= self.dscd_min_letters:\n                    self.dscd_allowed_tokens.add(token_text)\n                    return True\n                else:\n                    self.dscd_ignored_tokens.add(token_text)\n                    return False\n        except Exception:\n            pass\n\n        if is_word_token(\n            clean,\n            min_letters=self.dscd_min_letters,\n            min_letter_fraction=self.dscd_min_letter_fraction,\n        ):\n            self.dscd_allowed_tokens.add(token_text)\n            return True\n\n        self.dscd_ignored_tokens.add(token_text)\n        return False\n\n    def canonical_token_key(\n        self,\n        raw_token: str,\n        token_word_map: Optional[Dict[int, Optional[str]]],\n        idx: int,\n    ) -> Optional[str]:\n        \"\"\"‚úÖ Word-level key extraction (model-agnostic)\"\"\"\n        canonical: Optional[str] = None\n\n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                word = str(token_word_map[idx]).strip()\n                canonical = clean_token_for_dscd(word)\n                if canonical and len(canonical) >= self.dscd_min_letters:\n                    indic_range_1 = any('\\u0900' <= c <= '\\u0DFF' for c in canonical)\n                    indic_range_2 = any('\\u0980' <= c <= '\\u09FF' for c in canonical)\n                    has_indic = indic_range_1 or indic_range_2\n                    if has_indic:\n                        return canonical\n        except Exception:\n            pass\n\n        canonical = clean_token_for_dscd(raw_token)\n\n        if not canonical or len(canonical) < self.dscd_min_letters:\n            return None\n\n        indic_range_1 = any('\\u0900' <= c <= '\\u0DFF' for c in canonical)\n        indic_range_2 = any('\\u0980' <= c <= '\\u09FF' for c in canonical)\n        has_indic = indic_range_1 or indic_range_2\n        if not has_indic:\n            return None\n\n        if is_indic_subword_fragment(canonical):\n            return None\n\n        return canonical\n\n    def cleanup_threads(self) -> None:\n        try:\n            with self.thread_lock:\n                alive = [th for th in list(self.active_threads) if th.is_alive()]\n                self.active_threads.clear()\n                self.active_threads.extend(alive)\n        except Exception:\n            pass\n\n    def cleanup_memory(self) -> None:\n        try:\n            for token_type, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n\n            try:\n                now = time.time()\n                expired = [k for k, v in self.dispersion_last_updated.items() if now - v > 3600]\n                for k in expired:\n                    self.dispersion_cache.pop(k, None)\n                    self.dispersion_last_updated.pop(k, None)\n            except Exception:\n                pass\n\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    def forward(\n        self,\n        token_embeddings=None,\n        token_types=None,\n        train_mode: bool = True,\n        token_word_map=None,\n        h_all=None,\n        input_ids=None,\n        attention_mask=None,\n    ):\n        \"\"\"\n        ‚úÖ MODEL-AGNOSTIC FORWARD PASS\n        \n        Input: token_embeddings from ANY encoder (mBART, T5, BERT, etc.)\n        Output: augmented embeddings + prototype assignments\n        \"\"\"\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n\n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n\n        if input_ids is not None and token_types is None:\n            batch_size, seq_len = input_ids.shape\n            token_types = []\n            for b in range(batch_size):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(\n                            self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n                        )\n                    except Exception:\n                        token_types.append([f\"tok{i}\" for i in range(seq_len)])\n                else:\n                    token_types.append([f\"tok{i}\" for i in range(seq_len)])\n\n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n            self.cleanup_threads()\n\n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        seq_len = int(token_embeddings.size(1))\n\n        all_outputs: Dict[str, List[Any]] = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': [],\n        }\n\n        for b in range(batch_size):\n            word_map = token_word_map[b] if token_word_map and len(token_word_map) > b else None\n\n            batch_outputs = self.process_sequence(\n                token_embeddings[b],\n                token_types[b] if token_types and len(token_types) > b else [f\"tok{i}\" for i in range(seq_len)],\n                device,\n                word_map=word_map,\n                train_mode=train_mode,\n            )\n\n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n\n        try:\n            h_aug_list: List[torch.Tensor] = []\n            max_seq_len = seq_len\n\n            for b in range(batch_size):\n                h_batch_list = all_outputs['h_augmented'][b]\n\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = token_embeddings[b].clone()\n\n                h_aug_list.append(h_batch)\n\n            all_outputs['h_augmented'] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs['h_augmented'] = token_embeddings\n\n        try:\n            proto_assign_tensor = []\n            for row in all_outputs['proto_assignments']:\n                try:\n                    stacked = torch.stack(\n                        [x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in row],\n                        dim=0,\n                    )\n                    proto_assign_tensor.append(stacked)\n                except Exception:\n                    proto_assign_tensor.append(\n                        torch.tensor(\n                            [int(x) if not isinstance(x, torch.Tensor) else int(x.item()) for x in row],\n                            dtype=torch.long,\n                        )\n                    )\n            all_outputs['proto_assignments'] = proto_assign_tensor\n        except Exception:\n            pass\n\n        return all_outputs\n\n    def process_sequence(\n        self,\n        token_embeddings: torch.Tensor,\n        token_types: List[Any],\n        device: torch.device,\n        word_map: Optional[Dict[int, Optional[str]]] = None,\n        train_mode: bool = True,\n    ) -> Dict[str, List[Any]]:\n        \"\"\"‚úÖ Per-sequence processing (model-agnostic)\"\"\"\n        seq_len = int(token_embeddings.size(0))\n\n        outputs: Dict[str, List[Any]] = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': [],\n        }\n\n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f\"tok{j}\"\n            if not isinstance(raw_tok, str):\n                raw_tok = str(raw_tok) if raw_tok is not None else f\"tok{j}\"\n\n            token_key = self.canonical_token_key(raw_tok, word_map, j)\n            h_j = token_embeddings[j]\n\n            if not token_key:\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n\n            if not self.should_track_token(token_key):\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n\n            with self.buffer_lock:\n                if token_key not in self.buffers:\n                    self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                    self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(\n                        self.embed_dim, self.max_protos\n                    )\n\n                try:\n                    self.buffers[token_key].append(h_j.detach().clone().cpu())\n                except Exception:\n                    try:\n                        self.buffers[token_key].append(h_j.cpu())\n                    except Exception:\n                        pass\n\n            buffer_len = len(self.buffers[token_key])\n\n            try:\n                if self.enable_training_clustering and buffer_len >= max(self.n_min + 2, 10):\n                    now = time.time()\n                    last_t = self.last_cluster_time.get(token_key, 0.0)\n\n                    if now - last_t >= self.cluster_cooldown_seconds:\n                        self.last_cluster_time[token_key] = now\n\n                        def bg_cluster(tok: str = token_key) -> None:\n                            try:\n                                with self.clustering_lock:\n                                    self.cluster_buffer_to_prototypes_hierarchical(tok)\n                            except Exception:\n                                pass\n\n                        th = threading.Thread(target=bg_cluster, daemon=True)\n                        th.start()\n                        with self.thread_lock:\n                            self.active_threads.append(th)\n            except Exception:\n                pass\n\n            store = self.prototype_stores[token_key]\n\n            centroids_snapshot: Optional[List[torch.Tensor]] = None\n            with self.clustering_lock:\n                try:\n                    if hasattr(store, 'centroids') and len(store.centroids) > 0:\n                        centroids_snapshot = []\n                        for c in store.centroids:\n                            try:\n                                if isinstance(c, torch.Tensor):\n                                    centroids_snapshot.append(c.clone().cpu())\n                                else:\n                                    centroids_snapshot.append(\n                                        torch.from_numpy(\n                                            np.asarray(c, dtype=np.float32)\n                                        ).cpu()\n                                    )\n                            except Exception:\n                                continue\n                        if not centroids_snapshot:\n                            centroids_snapshot = None\n                except Exception:\n                    centroids_snapshot = None\n\n            assignment = -1\n            prob_list: List[float] = []\n            uncertainty = 0.0\n            span_pred = 0.0\n            gate_val = 0.0\n            h_aug = h_j\n\n            if centroids_snapshot and len(centroids_snapshot) >= 1:\n                try:\n                    try:\n                        h_cpu = h_j.detach().cpu().numpy()\n                    except Exception:\n                        h_cpu = h_j.cpu().numpy()\n\n                    try:\n                        cents_np = np.stack([c.numpy() for c in centroids_snapshot], axis=0)\n                    except Exception:\n                        cents_np = np.stack([np.asarray(c, dtype=np.float32) for c in centroids_snapshot], axis=0)\n\n                    dists_np = np.linalg.norm(cents_np - h_cpu[None, :], axis=1)\n\n                    if dists_np.size > 0:\n                        min_dist = float(dists_np.min())\n                        min_idx = int(np.argmin(dists_np))\n\n                        if len(centroids_snapshot) >= 2:\n                            mean_dist = float(np.mean(dists_np))\n                            std_dist = float(np.std(dists_np))\n                            span_pred = float(np.clip(std_dist / (mean_dist + 1e-6), 0.0, 1.0))\n                        else:\n                            span_pred = float(np.clip((min_dist - store.mu) / (1e-3), 0.0, 1.0))\n\n                        base_threshold = max(store.tau, 1e-3) if store.size() > 0 else 0.3\n                        uncertainty_dist = float(np.clip(min_dist / (base_threshold * 2), 0.0, 1.0))\n\n                        if len(centroids_snapshot) >= 2:\n                            precisions = 1.0 / (dists_np**2 + 1e-6)\n                            gate_weights = precisions / (np.sum(precisions) + 1e-6)\n                            gate_val = float(np.max(gate_weights))\n                        else:\n                            gate_val = float(np.clip(1.0 - (min_dist - store.mu) / (1e-3), 0.0, 1.0))\n\n                        if store.size() < self.max_protos and min_dist > store.get_adaptive_threshold(DSCD_NEWSENSE_LAMBDA):\n                            store.add_prototype(h_j, time.time(), count=1)\n                            assignment = store.size() - 1\n                            centroids_snapshot.append(h_j.cpu())\n                            cents_np = np.vstack([cents_np, h_cpu[None, :]])\n                        else:\n                            assignment = min_idx\n                            try:\n                                store.update_rolling_stats(min_dist)\n                            except Exception:\n                                pass\n\n                        try:\n                            dist_tensor = torch.from_numpy(dists_np).to(device)\n                            probs_tensor = F.softmax(-dist_tensor, dim=0)\n                            prob_list = probs_tensor.tolist()\n\n                            entropy = -torch.sum(probs_tensor * torch.log(probs_tensor + 1e-10))\n                            max_entropy = np.log(len(dists_np))\n                            uncertainty_entropy = float(entropy.item() / max_entropy) if max_entropy > 0 else 0.0\n                        except Exception:\n                            exps = np.exp(-dists_np - np.max(-dists_np)) if dists_np.size > 0 else np.array([])\n                            if exps.size > 0:\n                                probs = exps / (exps.sum() + 1e-12)\n                                prob_list = probs.tolist()\n                                entropy_val = -np.sum(probs * np.log(probs + 1e-10))\n                                max_entropy = np.log(len(dists_np))\n                                uncertainty_entropy = float(entropy_val / max_entropy) if max_entropy > 0 else 0.0\n                            else:\n                                prob_list = []\n                                uncertainty_entropy = 0.0\n\n                        if len(centroids_snapshot) >= 2:\n                            uncertainty = 0.4 * uncertainty_dist + 0.6 * uncertainty_entropy\n                        else:\n                            uncertainty = uncertainty_dist\n\n                        if gate_val > 0.3 and 0 <= assignment < len(centroids_snapshot):\n                            try:\n                                centroid_t = centroids_snapshot[assignment]\n\n                                if device != torch.device('cpu'):\n                                    try:\n                                        centroid_t = centroid_t.to(device)\n                                    except Exception:\n                                        pass\n\n                                blend_weight = 0.3 if gate_val > 0.7 else 0.15\n                                h_aug = h_j + blend_weight * (centroid_t - h_j)\n                            except Exception:\n                                h_aug = h_j\n\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD] Assignment error for {token_key}: {str(e)[:200]}\")\n\n            outputs['proto_assignments'].append(torch.tensor(assignment))\n            outputs['proto_probs'].append(prob_list)\n            outputs['uncertainties'].append(uncertainty)\n            outputs['span_preds'].append(span_pred)\n            outputs['gates'].append(gate_val)\n            outputs['h_augmented'].append(h_aug)\n\n        try:\n            if not train_mode and len(self.prototype_stores) > 0 and VERBOSE_LOGGING:\n                if self.last_periodic_check % PRINT_INTERVAL == 0:\n                    self.print_clusters_summary()\n                self.last_periodic_check += 1\n        except Exception:\n            pass\n\n        return outputs\n\n    def print_clusters_summary(self) -> None:\n        try:\n            items: List[Tuple[str, int, int, float, float, int]] = []\n\n            for token, store in self.prototype_stores.items():\n                if is_punctuation_only(token):\n                    continue\n\n                try:\n                    proto_sample_count = sum(getattr(store, 'counts', []) or [])\n                except Exception:\n                    proto_sample_count = 0\n\n                buffer_len = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n\n            items.sort(key=lambda x: x[1], reverse=True)\n            top_5 = items[:5]\n\n            if VERBOSE_LOGGING:\n                print(\"\\n[CLUSTER] Top 5 clusters:\")\n                print(\"-\" * 90)\n                print(f\"{'Rank':<6} {'Token':<14} {'Count':<12} {'Protos':<10} {'Mu':<14} {'Tau':<12}\")\n                print(\"-\" * 90)\n                for rank, (tok, cnt, prot, mu, tau, buf_len) in enumerate(top_5, 1):\n                    tok_str = str(tok)[:14]\n                    print(f\"{rank:<6} {tok_str:<14} {cnt:<12} {prot:<10} {mu:<14.6f} {tau:<12.6f}\")\n                print(\"-\" * 90)\n        except Exception as e:\n            try:\n                if VERBOSE_LOGGING:\n                    print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n            except Exception:\n                pass\n\n    def cluster_buffer_to_prototypes_hierarchical(self, token_type: str) -> bool:\n        \"\"\"\n        ‚úÖ PURE NUMERICAL CLUSTERING (model-agnostic)\n        Uses scipy linkage + KMeans fallback\n        \"\"\"\n        try:\n            if is_punctuation_only(token_type):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping punctuation token: {token_type}\")\n                return False\n\n            if not self.should_track_token(token_type):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping non-word token: {token_type}\")\n                return False\n\n            with self.buffer_lock:\n                if token_type not in self.buffers:\n                    return False\n\n                buf_snapshot = [e.clone() if isinstance(e, torch.Tensor) else e for e in self.buffers[token_type]]\n\n            if len(buf_snapshot) < max(self.n_min + 2, 10):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {token_type}: buffer={len(buf_snapshot)} < min={max(self.n_min + 2, 10)}\")\n                return False\n\n            emb_list: List[np.ndarray] = []\n            for e in buf_snapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        try:\n                            emb_list.append(e.numpy())\n                        except Exception:\n                            emb_list.append(e.cpu().numpy())\n                    else:\n                        emb_list.append(np.asarray(e, dtype=np.float32))\n                except Exception:\n                    continue\n\n            if len(emb_list) == 0:\n                return False\n\n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                embeddings = np.stack(emb_list, axis=0)\n\n            if embeddings.shape[0] < 2:\n                return False\n\n            norms = np.linalg.norm(embeddings, axis=1)\n            if np.all(norms < 1e-6):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {token_type}: all zero vectors, skipping\")\n                return False\n\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {token_type}: buf={len(buf_snapshot)} \"\n                    f\"sampled={embeddings.shape[0]} mean_norm={norms.mean():.4f}\"\n                )\n\n            store = self.prototype_stores[token_type]\n\n            protos_added = 0\n            new_centroids: List[torch.Tensor] = []\n            new_counts: List[int] = []\n            new_times: List[float] = []\n\n            if HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric='euclidean')\n                    if condensed.size > 0:\n                        Z = linkage(condensed, method='average')\n                        max_dist = condensed.max() if condensed.size > 0 else 1.0\n                        relative_threshold = self.dispersion_threshold\n                        absolute_threshold = relative_threshold * max_dist\n                        clusters = fcluster(Z, t=absolute_threshold, criterion='distance') - 1\n\n                        if clusters.size > 0:\n                            max_c = int(clusters.max())\n                            for c_id in range(max_c + 1):\n                                mask = (clusters == c_id)\n                                cluster_size = int(mask.sum())\n\n                                if cluster_size >= self.n_min:\n                                    centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                    centroid_tensor = torch.from_numpy(centroid)\n                                    new_centroids.append(centroid_tensor)\n                                    new_counts.append(cluster_size)\n                                    new_times.append(time.time())\n                                    protos_added += 1\n\n                            if len(new_centroids) > self.max_protos:\n                                sorted_indices = np.argsort(new_counts)[-1:-self.max_protos-1:-1]\n                                new_centroids = [new_centroids[i] for i in sorted_indices]\n                                new_counts = [new_counts[i] for i in sorted_indices]\n                                new_times = [new_times[i] for i in sorted_indices]\n                                protos_added = len(new_centroids)\n\n                            store.centroids = new_centroids\n                            store.counts = new_counts\n                            store.creation_time = new_times\n                            store.labels = torch.tensor(clusters)\n\n                            if DEBUG_DISCOVERY and protos_added > 0:\n                                print(f\"[DSCD-CLUSTER] Hierarchical created {protos_added} prototypes for {token_type}\")\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD-CLUSTER] Hierarchical failed for {token_type}: {type(e).__name__} {str(e)[:200]}\")\n\n            if protos_added == 0 and HAS_KMEANS:\n                try:\n                    min_k = 1\n                    max_k = min(self.max_protos, len(embeddings) // self.n_min)\n                    if max_k < min_k:\n                        max_k = min_k\n\n                    if len(embeddings) >= 20:\n                        k_guess = min(max_k, max(2, int(np.sqrt(len(embeddings)) / 2)))\n                    elif len(embeddings) >= 10:\n                        k_guess = min(max_k, 2)\n                    else:\n                        k_guess = 1\n\n                    k_guess = max(min_k, min(k_guess, len(embeddings)))\n\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n\n                        new_centroids = []\n                        new_counts = []\n                        new_times = []\n\n                        for c in range(k_guess):\n                            mask = (labels == c)\n                            cluster_size = int(mask.sum())\n\n                            if cluster_size >= self.n_min:\n                                centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                centroid_tensor = torch.from_numpy(centroid)\n                                new_centroids.append(centroid_tensor)\n                                new_counts.append(cluster_size)\n                                new_times.append(time.time())\n                                protos_added += 1\n\n                        store.centroids = new_centroids\n                        store.counts = new_counts\n                        store.creation_time = new_times\n                        store.labels = torch.tensor(labels)\n\n                        if DEBUG_DISCOVERY and protos_added > 0:\n                            print(f\"[DSCD-CLUSTER] KMeans created {protos_added} prototypes for {token_type}\")\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD-CLUSTER] KMeans failed for {token_type}: {type(e).__name__} {str(e)[:200]}\")\n\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {token_type}: final={store.size()} protos, \"\n                    f\"counts={store.counts}\"\n                )\n\n            try:\n                if store.centroids:\n                    counts = store.counts if store.counts else [1] * len(store.centroids)\n                    total_count = sum(counts)\n                    mean_count = float(total_count) / max(1, len(counts))\n\n                    self.cluster_stats[str(token_type)] = {\n                        'num_prototypes': len(store.centroids),\n                        'counts': [int(c) for c in counts],\n                        'total_samples': int(total_count),\n                        'mean_count': float(mean_count),\n                        'mu': float(store.mu),\n                        'tau': float(store.tau),\n                    }\n            except Exception:\n                pass\n\n            return store.size() > 0\n\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD-ERROR] Clustering error for {token_type}: {type(e).__name__} {str(e)[:200]}\")\n            return False\n\n    def get_explanations(self, threshold_span: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for token_type, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({'token': str(token_type), 'protos': store.size()})\n        return expl\n\n    def periodic_discovery_check(self, global_step: int, frequency: int) -> int:\n        try:\n            candidates: List[Tuple[str, float, int]] = []\n            buffer_snapshot = {}\n            already_clustered = set()\n\n            with self.buffer_lock:\n                for token in list(self.buffers.keys()):\n                    buffer_snapshot[token] = len(self.buffers.get(token, []))\n\n            with self.clustering_lock:\n                for token in self.prototype_stores.keys():\n                    if self.prototype_stores[token].size() >= 2:\n                        already_clustered.add(token)\n\n            for token, buffer_size in buffer_snapshot.items():\n                if is_punctuation_only(token):\n                    continue\n\n                if token in already_clustered:\n                    continue\n\n                if buffer_size >= max(self.n_min + 2, 10):\n                    try:\n                        dispersion = self.get_dispersion(token)\n                        if dispersion >= self.dispersion_threshold:\n                            rank_score = dispersion * buffer_size\n                            candidates.append((token, rank_score, buffer_size))\n                    except:\n                        continue\n\n            if not candidates:\n                return 0\n\n            candidates.sort(key=lambda x: x[1], reverse=True)\n            candidates_to_process = candidates[:min(MAX_TOKENS_PER_DISCOVERY, len(candidates))]\n\n            return self.discover_homographs_for_tokens(\n                [c[0] for c in candidates_to_process],\n                self.n_min,\n                self.dispersion_threshold,\n                global_step,\n            )\n\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD] periodic_discovery_check failed: {e}\")\n            return 0\n\n    def get_prototype_summary(self) -> Dict[str, Any]:\n        try:\n            total_tokens = len(self.prototype_stores)\n            total_prototypes = sum(s.size() for s in self.prototype_stores.values())\n            homographs = sum(1 for s in self.prototype_stores.values() if s.size() >= 2)\n\n            return {\n                'total_tokens': total_tokens,\n                'total_prototypes': total_prototypes,\n                'num_homographs': homographs,\n                'discovered_homographs': len(self.discovered_homographs),\n            }\n        except Exception:\n            return {\n                'total_tokens': 0,\n                'total_prototypes': 0,\n                'num_homographs': 0,\n                'discovered_homographs': 0,\n            }\n\n    def get_discovered_homographs(self) -> Set[str]:\n        return self.discovered_homographs.copy()\n\nprint(\"=\" * 80)\nprint(\"Cell 3: DSCD (Word-Level Homograph Disambiguation) - UNIVERSAL MODULE\")\nprint(\"=\" * 80)\nprint(\"‚úÖ MODEL-AGNOSTIC ARCHITECTURE:\")\nprint(f\"  ‚úÖ Works with ANY encoder: mBART-50, BanglaT5, M2M100, XLM-R, mT5\")\nprint(f\"  ‚úÖ Input: token embeddings (B, L, embed_dim)\")\nprint(f\"  ‚úÖ No vocab size dependencies\")\nprint(f\"  ‚úÖ No tokenizer-specific code\")\nprint(f\"  ‚úÖ Bengali Unicode detection (U+0980-U+09FF)\")\nprint(f\"  ‚úÖ Thread-safe hierarchical + KMeans clustering\")\nprint()\nprint(\"CONFIGURATION:\")\nprint(f\"  ‚úÖ Max prototypes: {DSCD_MAX_PROTOS}\")\nprint(f\"  ‚úÖ Buffer size: {DSCD_BUFFER_SIZE}\")\nprint(f\"  ‚úÖ Min samples: {DSCD_N_MIN}\")\nprint(f\"  ‚úÖ Dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  ‚úÖ Cache size: 10000\")\nprint()\nprint(\"USAGE WITH DIFFERENT MODELS:\")\nprint(f\"  # mBART-50:\")\nprint(f\"  dscd = MemoryEfficientDSCDOnline(embed_dim=1024, tokenizer=tokenizer)\")\nprint()\nprint(f\"  # BanglaT5:\")\nprint(f\"  dscd = MemoryEfficientDSCDOnline(embed_dim=768, tokenizer=tokenizer)\")\nprint()\nprint(f\"  # M2M100:\")\nprint(f\"  dscd = MemoryEfficientDSCDOnline(embed_dim=1024, tokenizer=tokenizer)\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"L25pcKUPH4J2","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:25.779302Z","iopub.execute_input":"2026-02-16T03:01:25.779584Z","iopub.status.idle":"2026-02-16T03:01:26.345642Z","shell.execute_reply.started":"2026-02-16T03:01:25.779555Z","shell.execute_reply":"2026-02-16T03:01:26.344960Z"}},"outputs":[{"name":"stdout","text":"[CELL3] Loaded reference list for evaluation: 42 words\n================================================================================\nCell 3: DSCD (Word-Level Homograph Disambiguation) - UNIVERSAL MODULE\n================================================================================\n‚úÖ MODEL-AGNOSTIC ARCHITECTURE:\n  ‚úÖ Works with ANY encoder: mBART-50, BanglaT5, M2M100, XLM-R, mT5\n  ‚úÖ Input: token embeddings (B, L, embed_dim)\n  ‚úÖ No vocab size dependencies\n  ‚úÖ No tokenizer-specific code\n  ‚úÖ Bengali Unicode detection (U+0980-U+09FF)\n  ‚úÖ Thread-safe hierarchical + KMeans clustering\n\nCONFIGURATION:\n  ‚úÖ Max prototypes: 7\n  ‚úÖ Buffer size: 30\n  ‚úÖ Min samples: 3\n  ‚úÖ Dispersion threshold: 0.35\n  ‚úÖ Cache size: 10000\n\nUSAGE WITH DIFFERENT MODELS:\n  # mBART-50:\n  dscd = MemoryEfficientDSCDOnline(embed_dim=1024, tokenizer=tokenizer)\n\n  # BanglaT5:\n  dscd = MemoryEfficientDSCDOnline(embed_dim=768, tokenizer=tokenizer)\n\n  # M2M100:\n  dscd = MemoryEfficientDSCDOnline(embed_dim=1024, tokenizer=tokenizer)\n================================================================================\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - ADVERSARIAL SELECTIVE BATCH NORMALIZATION\n# ==============================================================================\n\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.1\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_is_valid_token = False\n_has_get_tokenizer_special_tokens = False\n_has_should_track_token = False\n_is_valid_token_fn = None\n_get_tokenizer_special_tokens_fn = None\n_should_track_token_fn = None\n\ntry:\n    if 'is_valid_token' in dir():\n        _is_valid_token_fn = is_valid_token\n        _has_is_valid_token = True\n    elif 'is_valid_token' in globals():\n        _is_valid_token_fn = globals()['is_valid_token']\n        _has_is_valid_token = True\nexcept Exception:\n    pass\n\ntry:\n    if 'get_tokenizer_special_tokens' in dir():\n        _get_tokenizer_special_tokens_fn = get_tokenizer_special_tokens\n        _has_get_tokenizer_special_tokens = True\n    elif 'get_tokenizer_special_tokens' in globals():\n        _get_tokenizer_special_tokens_fn = globals()['get_tokenizer_special_tokens']\n        _has_get_tokenizer_special_tokens = True\nexcept Exception:\n    pass\n\ntry:\n    if 'should_track_token' in dir():\n        _should_track_token_fn = should_track_token\n        _has_should_track_token = True\n    elif 'should_track_token' in globals():\n        _should_track_token_fn = globals()['should_track_token']\n        _has_should_track_token = True\nexcept Exception:\n    pass\n\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = float(alpha)\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.alpha * grad_output, None\n\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    return GradientReversalFunction.apply(x, alpha)\n\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 50,\n        encoder_grl_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = int(embed_dim)\n\n        self.bn_source = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n        self.bn_target = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n\n        self.freq_threshold = float(freq_threshold)\n        self.uncertainty_threshold = float(uncertainty_threshold)\n        self.gate_threshold = float(gate_threshold)\n        self.warmup_steps = int(warmup_steps)\n        self.current_step = 0\n\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 1.0, \"xl\": 1.0, \"domain\": 1.0}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = float(encoder_grl_scale)\n\n        self.stats_reset_interval = 100\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens and _get_tokenizer_special_tokens_fn is not None:\n                    self.special_tokens = _get_tokenizer_special_tokens_fn(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[ASBN-INIT] Initialized MemoryEfficientASBNModule:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - warmup_steps: {self.warmup_steps}\")\n            print(f\"  - encoder_grl_scale: {self.encoder_grl_scale}\")\n            print(f\"  - GRL_ALPHA: {_GRL_ALPHA_START} ‚Üí {_GRL_ALPHA_END} over {_GRL_ALPHA_STEPS} steps\")\n            print(f\"  - thresholds: freq={self.freq_threshold}, uncert={self.uncertainty_threshold}, gate={self.gate_threshold}\")\n            print(f\"  - Function availability: should_track={_has_should_track_token}, is_valid={_has_is_valid_token}\")\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n\n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            ratio = _GRL_ALPHA_END / max(1e-8, _GRL_ALPHA_START if _GRL_ALPHA_START > 0 else 1e-3)\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n\n        return float(alpha)\n\n    def get_asbn_stats(self) -> Dict[str, float]:\n        return self.get_detailed_stats()\n\n    def get_detailed_stats(self) -> Dict[str, float]:\n        if self.stats[\"num_updates\"] > 0:\n            n = float(self.stats[\"num_updates\"])\n            return {\n                \"domain_loss\": self.stats[\"domain_loss\"] / n,\n                \"domain_accuracy\": self.stats[\"domain_accuracy\"] / n,\n                \"source_accuracy\": self.stats[\"source_accuracy\"] / n,\n                \"target_accuracy\": self.stats[\"target_accuracy\"] / n,\n                \"asbn_loss\": self.stats[\"asbn_loss\"] / n,\n                \"num_updates\": self.stats[\"num_updates\"],\n            }\n        return {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def reset_stats(self) -> None:\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def critic_parameters(self):\n        return (\n            list(self.d_domain.parameters())\n            + list(self.d_freq.parameters())\n            + list(self.d_ctx.parameters())\n            + list(self.d_xl.parameters())\n        )\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (\n                self.d_domain,\n                self.d_freq,\n                self.d_ctx,\n                self.d_xl,\n                self.bn_source,\n                self.bn_target,\n            ):\n                try:\n                    p = next(mod.parameters())\n                    if p.device != device:\n                        mod.to(device)\n                except StopIteration:\n                    mod.to(device)\n                except Exception:\n                    pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] Device migration failed:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    print(\"[ASBN] Device migration failed\")\n\n    def _expand_domain_labels(self, domain_labels: Optional[torch.Tensor], batch_size: int) -> Optional[torch.Tensor]:\n        if domain_labels is None:\n            return None\n\n        if domain_labels.dim() == 0:\n            domain_labels = domain_labels.unsqueeze(0)\n\n        if domain_labels.size(0) == 1 and batch_size > 1:\n            domain_labels = domain_labels.expand(batch_size).contiguous()\n        elif domain_labels.size(0) != batch_size:\n            if _DEBUG_DISCOVERY:\n                print(f\"[ASBN] Domain label size mismatch: {domain_labels.size(0)} vs batch {batch_size}, using first label\")\n            domain_labels = domain_labels[0].unsqueeze(0).expand(batch_size).contiguous()\n\n        return domain_labels\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n\n        try:\n            if proto_probs is None:\n                return pmax\n\n            if isinstance(proto_probs, torch.Tensor):\n                if proto_probs.dim() == 3:\n                    B, T, K = proto_probs.shape\n                    p = proto_probs.detach().to(device)\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    pmax[:b_max, :t_max] = p[:b_max, :t_max].max(dim=2)[0]\n                    return pmax\n\n                if proto_probs.dim() == 2:\n                    p = proto_probs.detach().to(device)\n                    if batch_size >= 1:\n                        t_max = min(seq_len, p.size(0))\n                        pmax[0, :t_max] = p[:t_max].max(dim=1)[0]\n                        return pmax\n\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            t_max = min(seq_len, row.size(0))\n                            pmax[b, :t_max] = row[:t_max].max(dim=1)[0].to(device)\n                        elif isinstance(row, (list, tuple)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        pmax[b, t] = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    pmax[0, t] = float(val.max().item())\n                                else:\n                                    pmax[0, t] = float(np.max(np.asarray(val, dtype=np.float32)))\n                            except Exception:\n                                pmax[0, t] = 0.5\n\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] parse_proto_probs exception: {e}\")\n\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device,\n                            default: float = 0.0) -> torch.Tensor:\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n\n        try:\n            if mat is None:\n                return out\n\n            if isinstance(mat, torch.Tensor):\n                if mat.dim() == 3:\n                    B, T, _ = mat.shape\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    out[:b_max, :t_max] = mat[:b_max, :t_max, 0].to(device)\n                elif mat.dim() == 2:\n                    if mat.size(0) == batch_size:\n                        t_max = min(seq_len, mat.size(1))\n                        out[:, :t_max] = mat[:, :t_max].to(device)\n                    elif batch_size == 1:\n                        t_max = min(seq_len, mat.size(0))\n                        out[0, :t_max] = mat[:t_max].to(device)\n                elif mat.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, mat.size(0))\n                    out[0, :t_max] = mat[:t_max].to(device)\n\n            elif isinstance(mat, (list, tuple)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                            t_max = min(seq_len, row.size(0))\n                            for t in range(t_max):\n                                out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                                except Exception:\n                                    out[b, t] = float(default)\n                elif batch_size == 1:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            out[0, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                        except Exception:\n                            out[0, t] = float(default)\n\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    pass\n\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor,\n                                    gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        base = float(self.lambda_base.get(lambda_type, 1.0))\n        lam = base * torch.ones_like(pmax)\n        lam = torch.clamp(lam, min=0.1, max=float(self.lambda_max))\n        lam = lam.contiguous()\n        lam = torch.where(torch.isfinite(lam), lam, torch.ones_like(lam))\n        return lam\n\n    def forward(\n        self,\n        h: torch.Tensor,\n        proto_probs: Any = None,\n        uncertainties: Any = None,\n        gates: Any = None,\n        token_word_map: Optional[List[Dict[int, str]]] = None,\n        domain_labels: Optional[torch.Tensor] = None,\n        global_step: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n\n        if global_step is not None:\n            self.current_step = int(global_step)\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            zero = torch.tensor(0.0, device=dev)\n            return h, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n\n        B, T, H = h.size()\n        device = h.device\n\n        domain_labels = self._expand_domain_labels(domain_labels, B)\n\n        h_normalized = h.clone()\n\n        if domain_labels is not None and B * T >= 2:\n            try:\n                self._ensure_discriminators_on_device(device)\n                h_flat = h.view(B * T, H)\n                domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n\n                source_mask = domain_expanded == 0\n                target_mask = domain_expanded == 1\n\n                h_norm_flat = h_flat.clone()\n\n                source_count = source_mask.sum().item()\n                target_count = target_mask.sum().item()\n\n                if source_count >= 2:\n                    self.bn_source.train(self.training)\n                    h_norm_flat[source_mask] = self.bn_source(h_flat[source_mask])\n                elif source_count == 1:\n                    self.bn_source.eval()\n                    with torch.no_grad():\n                        h_norm_flat[source_mask] = self.bn_source(h_flat[source_mask])\n\n                if target_count >= 2:\n                    self.bn_target.train(self.training)\n                    h_norm_flat[target_mask] = self.bn_target(h_flat[target_mask])\n                elif target_count == 1:\n                    self.bn_target.eval()\n                    with torch.no_grad():\n                        h_norm_flat[target_mask] = self.bn_target(h_flat[target_mask])\n\n                h_normalized = h_norm_flat.view(B, T, H)\n\n                if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n                    print(f\"[ASBN-BN] Applied BN: {source_count} source, {target_count} target tokens\")\n\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] BN failed: {e}\")\n                h_normalized = h\n\n        if self.current_step < self.warmup_steps:\n            if _DEBUG_DISCOVERY and self.current_step % 50 == 0:\n                print(f\"[ASBN] Warmup: {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n\n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n\n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n        batch_indices = torch.arange(B, device=device).unsqueeze(1).expand(B, T)\n\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                tracked = True\n\n                                if _has_should_track_token and _should_track_token_fn is not None:\n                                    tracked = bool(_should_track_token_fn(token_str, self.special_tokens, self.tokenizer, self.language))\n                                elif _has_is_valid_token and _is_valid_token_fn is not None:\n                                    tracked = bool(_is_valid_token_fn(token_str, self.special_tokens, self.tokenizer, self.language))\n\n                                if not tracked:\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    try:\n                        print(\"[ASBN] Token filtering failed:\", traceback.format_exc().splitlines()[-1])\n                    except Exception:\n                        pass\n\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        batch_idx = batch_indices.view(-1)[sel_idx]\n\n        if sel_idx.numel() == 0:\n            if _DEBUG_DISCOVERY:\n                print(\"[ASBN] No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n\n        h_flat = h_normalized.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n\n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n\n        grl_alpha = self.get_grl_alpha(global_step)\n\n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n\n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n\n        freq_logits = self.d_freq(freq_input_grl)\n        ctx_logits = self.d_ctx(ctx_input_grl)\n        xl_logits = self.d_xl(xl_input_grl)\n\n        freq_label = (pmax_flat > self.freq_threshold).long().to(device)\n        ctx_label = (U_flat < self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat > self.gate_threshold).long().to(device)\n\n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n\n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        mean_weighted = torch.mean(weighted)\n\n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n\n        if domain_labels is not None:\n            try:\n                domain_flat = domain_labels[batch_idx]\n\n                domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                domain_logits = self.d_domain(domain_input)\n\n                domain_loss = F.cross_entropy(domain_logits, domain_flat)\n\n                with torch.no_grad():\n                    domain_preds = torch.argmax(domain_logits, dim=1)\n                    domain_accuracy = (domain_preds == domain_flat).float().mean()\n\n                    source_mask = domain_flat == 0\n                    target_mask = domain_flat == 1\n\n                    if source_mask.any():\n                        source_acc = ((domain_preds[source_mask] == domain_flat[source_mask]).float().mean())\n                        self.stats[\"source_accuracy\"] += float(source_acc.item())\n\n                    if target_mask.any():\n                        target_acc = ((domain_preds[target_mask] == domain_flat[target_mask]).float().mean())\n                        self.stats[\"target_accuracy\"] += float(target_acc.item())\n\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] Domain loss failed: {e}\")\n\n        encoder_loss = self.encoder_grl_scale * (mean_weighted + domain_loss)\n\n        try:\n            with torch.no_grad():\n                self.stats[\"domain_loss\"] += float(domain_loss.item())\n                self.stats[\"domain_accuracy\"] += float(domain_accuracy.item())\n                self.stats[\"asbn_loss\"] += float(encoder_loss.item())\n                self.stats[\"num_updates\"] += 1\n\n                if self.stats[\"num_updates\"] >= self.stats_reset_interval:\n                    if _DEBUG_DISCOVERY:\n                        stats = self.get_detailed_stats()\n                        print(f\"\\n[ASBN-STATS] After {stats['num_updates']} updates:\")\n                        print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                        print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                        print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                        print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                        print(f\"  ASBN loss: {stats['asbn_loss']:.4f}\")\n                    self.reset_stats()\n        except Exception:\n            pass\n\n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            print(f\"\\n[ASBN-STEP-{self.current_step}]\")\n            print(f\"  GRL alpha: {grl_alpha:.3f}\")\n            print(f\"  Encoder loss: {encoder_loss.item():.4f}\")\n            print(f\"  Domain loss: {domain_loss.item():.4f}\")\n            print(f\"  Domain acc: {domain_accuracy.item():.2%}\")\n\n        return h_normalized, {\n            \"encoder_loss\": encoder_loss,\n            \"adversarial_loss\": mean_weighted,\n            \"domain_loss\": domain_loss,\n            \"domain_accuracy\": domain_accuracy,\n        }\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[ASBN-TEST] Testing ASBN module\")\n        print(\"=\" * 60)\n\n        try:\n            try:\n                device = next(self.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cpu\")\n\n            h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n            domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n\n            h_out, losses = self.forward(h, domain_labels=domain_labels)\n            assert h_out.shape == h.shape, \"Forward output shape mismatch\"\n            assert \"domain_loss\" in losses, \"Missing domain_loss\"\n            print(\"  ‚úì forward() with domain_labels passed\")\n\n            proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n            uncertainties = torch.rand(batch_size, seq_len, device=device)\n            gates = torch.rand(batch_size, seq_len, device=device)\n\n            self.train()\n            self.current_step = self.warmup_steps + 1\n\n            h_out, losses = self.forward(\n                h,\n                proto_probs=proto_probs,\n                uncertainties=uncertainties,\n                gates=gates,\n                domain_labels=domain_labels,\n                global_step=self.current_step,\n            )\n\n            assert losses[\"encoder_loss\"].item() >= 0.0, \"Encoder loss negative\"\n            assert 0.0 <= losses[\"domain_accuracy\"].item() <= 1.0, \"Domain accuracy out of range\"\n            print(\"  ‚úì forward() with full inputs passed\")\n\n            stats = self.get_detailed_stats()\n            assert \"domain_loss\" in stats, \"Missing domain_loss in stats\"\n            print(\"  ‚úì Statistics tracking passed\")\n\n            print(\"\\n‚úì All ASBN tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n\n        except Exception as e:\n            print(f\"\\n‚úó ASBN test failed: {e}\")\n            traceback.print_exc()\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN Module - VERIFIED CORRECT\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Warmup Steps: 50\")\nprint(f\"  - GRL Alpha: {_GRL_ALPHA_START:.3f} ‚Üí {_GRL_ALPHA_END:.3f} over {_GRL_ALPHA_STEPS} steps\")\nprint(f\"  - GRL Schedule: {_GRL_ALPHA_SCHEDULE}\")\nprint(f\"  - Encoder GRL Scale: 1.0\")\nprint(f\"  - Stats Reset Interval: 100\")\nprint(f\"  - ASBN Training: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"XrNq18UsH4J3","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.346958Z","iopub.execute_input":"2026-02-16T03:01:26.347605Z","iopub.status.idle":"2026-02-16T03:01:26.415680Z","shell.execute_reply.started":"2026-02-16T03:01:26.347581Z","shell.execute_reply":"2026-02-16T03:01:26.415034Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 4: ASBN Module - VERIFIED CORRECT\n================================================================================\nConfiguration:\n  - Warmup Steps: 50\n  - GRL Alpha: 0.100 ‚Üí 1.000 over 500 steps\n  - GRL Schedule: linear\n  - Encoder GRL Scale: 1.0\n  - Stats Reset Interval: 100\n  - ASBN Training: ENABLED\n================================================================================\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG MODULE - TRANSPARENT RATIONALE GENERATION\n# ==============================================================================\n\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport threading\nimport time\n\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\nexcept (NameError, ValueError, TypeError):\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\nexcept (NameError, ValueError, TypeError):\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\nexcept (NameError, ValueError, TypeError):\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept NameError:\n    _DEBUG_TIMING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TRG_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _TRG_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _TRG_SPAN_THRESHOLD = 0.20\n\ntry:\n    _TAU_HIGH = float(TAU_HIGH)\nexcept (NameError, ValueError, TypeError):\n    _TAU_HIGH = 0.85\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TAU_LOW = 0.15\n\ntry:\n    _TAU_ACCEPT = float(TAU_ACCEPT)\nexcept (NameError, ValueError, TypeError):\n    _TAU_ACCEPT = 0.80\n\ntry:\n    _TRG_TEMPERATURE = float(TRG_TEMPERATURE)\nexcept (NameError, ValueError, TypeError):\n    _TRG_TEMPERATURE = 1.0\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = (\n        int(MAX_EXPLANATIONS_PER_SENTENCE)\n        if \"MAX_EXPLANATIONS_PER_SENTENCE\" in globals()\n        else 10\n    )\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = False\n_has_get_tokenizer_special_tokens = False\n_has_get_cached_special_tokens = False\n_is_valid_token_fn = None\n_get_tokenizer_special_tokens_fn = None\n_get_cached_special_tokens_fn = None\n\ntry:\n    if 'is_valid_token' in dir():\n        _is_valid_token_fn = is_valid_token\n        _has_is_valid_token = True\n    elif 'is_valid_token' in globals():\n        _is_valid_token_fn = globals()['is_valid_token']\n        _has_is_valid_token = True\nexcept Exception:\n    pass\n\ntry:\n    if 'get_tokenizer_special_tokens' in dir():\n        _get_tokenizer_special_tokens_fn = get_tokenizer_special_tokens\n        _has_get_tokenizer_special_tokens = True\n    elif 'get_tokenizer_special_tokens' in globals():\n        _get_tokenizer_special_tokens_fn = globals()['get_tokenizer_special_tokens']\n        _has_get_tokenizer_special_tokens = True\nexcept Exception:\n    pass\n\ntry:\n    if 'get_cached_special_tokens' in dir():\n        _get_cached_special_tokens_fn = get_cached_special_tokens\n        _has_get_cached_special_tokens = True\n    elif 'get_cached_special_tokens' in globals():\n        _get_cached_special_tokens_fn = globals()['get_cached_special_tokens']\n        _has_get_cached_special_tokens = True\nexcept Exception:\n    pass\n\n_BENGALI_PUNCT_SET = set(['‡•§', '‡••'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_TRG_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n_FUNCTION_WORDS = {\n    '‡¶è‡¶¨‡¶Ç', '‡¶ì', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶§‡¶¨‡ßá', '‡¶Ø‡¶¶‡¶ø', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', '‡¶ï‡¶æ‡¶∞‡¶£', '‡¶Ø‡ßá‡¶Æ‡¶®',\n    '‡¶Ø‡¶ñ‡¶®', '‡¶§‡¶ñ‡¶®', '‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ', '‡¶∏‡ßá‡¶π‡ßá‡¶§‡ßÅ', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ', '‡¶¨‡¶æ',\n    '‡¶è‡¶á', '‡¶∏‡ßá‡¶á', '‡¶ê', '‡¶ì‡¶á', '‡¶ï‡ßã‡¶®', '‡¶ï‡ßã‡¶®‡ßã', '‡¶ï‡ßã‡¶®‡ßã', '‡¶Ø‡ßá', '‡¶Ø‡¶æ', '‡¶Ø‡¶ø‡¶®‡¶ø',\n    '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ï‡¶ú‡¶®', '‡¶ï‡¶Ø‡¶º‡ßá‡¶ï‡¶ü‡¶ø', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶∏‡¶¨', '‡¶∏‡¶ï‡¶≤', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶∏‡¶¨‡¶ï‡¶ø‡¶õ‡ßÅ',\n    '‡¶Ü‡¶Æ‡¶ø', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶∏‡ßá', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶§‡ßã‡¶Æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞‡¶æ',\n    '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡ßã‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞', '‡¶Ü‡¶™‡¶®‡¶æ‡¶¶‡ßá‡¶∞',\n    '‡¶ï‡¶ø', '‡¶ï‡ßÄ', '‡¶ï‡ßá', '‡¶ï‡ßá‡¶®', '‡¶ï‡¶ñ‡¶®', '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø', '‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá', '‡¶ï‡¶§‡¶ü‡¶æ',\n    '‡¶®‡¶æ', '‡¶®‡¶Ø‡¶º', '‡¶®‡ßá‡¶á', '‡¶®‡¶ø', '‡¶Ü‡¶õ‡ßá', '‡¶õ‡¶ø‡¶≤', '‡¶π‡¶¨‡ßá', '‡¶π‡¶Ø‡¶º',\n    '‡¶•‡ßá‡¶ï‡ßá', '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá', '‡¶∏‡¶æ‡¶•‡ßá', '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶â‡¶™‡¶∞',\n    '‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡ßá‡¶®', '‡¶ï‡¶∞‡¶õ‡ßá', '‡¶ï‡¶∞‡¶¨‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá', '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ', '‡¶π‡¶Ø‡¶º‡ßá', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá'\n}\n\n\ndef _is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n\n    clean = (\n        token.replace(\"‚ñÅ\", \"\")\n        .replace(\"ƒ†\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if not clean:\n        return False\n\n    if clean in _BENGALI_PUNCT_SET:\n        return True\n\n    if clean in _COMMON_PUNCT_SET:\n        return True\n\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n\n    return all(c in _TRG_PUNCT_SET for c in clean)\n\n\ndef _fallback_is_valid_token(\n    token: str, special_tokens: set, tokenizer=None, language: str = \"bn\"\n) -> bool:\n    if token is None:\n        return False\n\n    if not isinstance(token, str):\n        try:\n            token = str(token)\n        except Exception:\n            return False\n\n    token = token.strip()\n    if not token:\n        return False\n\n    if token in special_tokens:\n        return False\n\n    clean = (\n        token.replace(\"‚ñÅ\", \"\")\n        .replace(\"ƒ†\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if len(clean) < 2:\n        return False\n\n    if not any(c.isalpha() for c in clean):\n        return False\n\n    if _is_punctuation_only(token):\n        return False\n\n    if clean.isdigit():\n        return False\n\n    return True\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    if not isinstance(raw_token, str):\n        return False\n\n    try:\n        if token_word_map is not None and isinstance(token_word_map, dict):\n            if idx in token_word_map:\n                w = token_word_map[idx]\n                if isinstance(w, str) and w.strip():\n                    return True\n\n        if raw_token.startswith(\"‚ñÅ\") or raw_token.startswith(\"ƒ†\"):\n            return True\n\n        clean = (\n            raw_token.replace(\"‚ñÅ\", \"\")\n            .replace(\"ƒ†\", \"\")\n            .replace(\"##\", \"\")\n            .replace(\"</w>\", \"\")\n            .strip()\n        )\n\n        if len(clean) < 2:\n            return False\n\n        if _is_punctuation_only(raw_token):\n            return False\n\n        if token_word_map is None and any(c.isalpha() for c in clean):\n            return True\n\n        return False\n\n    except Exception:\n        return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on: '{evidence}'.   \"\n                \"Pattern matches learned data.   {alternatives_text}\"\n            ),\n            \"medium_confidence\": (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty ({uncertainty:.1%}).   {alternatives_text}\"\n            ),\n            \"low_confidence\": (\n                \"Uncertain; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'.   {alternatives_text} Review recommended.\"\n            ),\n            \"fallback\": (\"Token '{token}' analyzed.   Context: '{evidence}'.\"),\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n\n        token = (\n            str(evidence.get(\"token\", \"unknown\"))\n            .replace(\"‚ñÅ\", \"\")\n            .replace(\"ƒ†\", \"\")\n        )\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        uncertainty = float(evidence.get(\"uncertainty\", 0.5))\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = (\n            \", \".join(\n                [\n                    str(tok).replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\")\n                    for tok in evidence_tokens[:_TRG_EVIDENCE_K]\n                ]\n            )\n            or \"limited context\"\n        )\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)}.\"\n\n        if confidence >= _TAU_ACCEPT:\n            template_key = \"high_confidence\"\n        elif confidence >= _TRG_UNCERTAINTY_THRESHOLD:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(\n            template_key, self.explanation_templates[\"fallback\"]\n        )\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                uncertainty=uncertainty,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token '{token}' -> '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens and _get_tokenizer_special_tokens_fn is not None:\n                    self.special_tokens = _get_tokenizer_special_tokens_fn(tokenizer)\n                elif _has_get_cached_special_tokens and _get_cached_special_tokens_fn is not None:\n                    self.special_tokens = _get_cached_special_tokens_fn(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor,\n    ) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n\n        seq_len = (\n            len(tgt_preds)\n            if isinstance(tgt_preds, list)\n            else int(tgt_preds.size(0))\n        )\n        if span_end > seq_len:\n            return None\n\n        if span_start >= span_end:\n            return None\n\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n\n        if token_idx >= seq_len:\n            return None\n\n        try:\n            evidence_tokens: List[str] = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        evidence_tokens.append(f\"token_{i}\")\n\n            return evidence_tokens if evidence_tokens else None\n\n        except Exception:\n            return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n    ) -> Dict:\n        if not isinstance(tokens, list):\n            return self._create_fallback_evidence(token_idx, [])\n\n        if not isinstance(token_idx, int):\n            return self._create_fallback_evidence(0, tokens)\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(\n                max(0, min(token_idx, len(tokens) - 1)), tokens\n            )\n\n        raw_token = tokens[token_idx]\n\n        if _has_is_valid_token and _is_valid_token_fn is not None:\n            try:\n                is_valid = _is_valid_token_fn(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n\n            evidence_tokens: Optional[List[str]] = None\n            if decoder_attention is not None and isinstance(\n                decoder_attention, torch.Tensor\n            ):\n                try:\n                    if decoder_attention.dim() == 4:\n                        if (\n                            decoder_attention.size(0) > 1\n                            and decoder_attention.size(1) > 1\n                        ):\n                            attn_avg = decoder_attention.mean(dim=(0, 1))\n                        elif decoder_attention.size(0) > 1:\n                            attn_avg = decoder_attention.mean(dim=1)\n                        else:\n                            attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 3:\n                        attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 2:\n                        if token_idx < decoder_attention.size(0):\n                            vec = decoder_attention[token_idx]\n                        else:\n                            vec = decoder_attention.reshape(-1)\n                    elif decoder_attention.dim() == 1:\n                        vec = decoder_attention\n                    else:\n                        vec = None\n\n                    if vec is not None and vec.numel() > 0:\n                        k = min(5, int(vec.size(0)))\n                        top_k_indices = torch.topk(vec, k=k).indices.cpu().numpy()\n                        evidence_tokens = []\n                        for i in top_k_indices:\n                            if i < len(tokens) and i != token_idx:\n                                evidence_tokens.append(tokens[int(i)])\n\n                except Exception:\n                    evidence_tokens = None\n\n            if evidence_tokens is None:\n                evidence_tokens = self._extract_context_window(\n                    token_idx, tokens, token_word_map\n                )\n\n            seen: Dict[str, bool] = {}\n            dedup_evidence: List[str] = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen[t] = True\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:_TRG_EVIDENCE_K]\n\n            top_senses = self._compute_sense_alternatives_fast(\n                proto_probs, temperature=_TRG_TEMPERATURE\n            )\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            if (\n                token_word_map\n                and token_idx in token_word_map\n                and isinstance(token_word_map[token_idx], str)\n                and token_word_map[token_idx].strip()\n            ):\n                token_value = token_word_map[token_idx]\n            else:\n                token_value = raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"[TRG] Evidence error @ {token_idx}: {e}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _extract_context_window(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        token_word_map: Optional[dict],\n    ) -> List[str]:\n        context_window = 2\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens: List[str] = []\n\n        for i in range(start_idx, end_idx):\n            if i == token_idx or i >= len(tokens):\n                continue\n            rtok = tokens[i]\n            clean_token = (\n                str(rtok)\n                .replace(\"‚ñÅ\", \"\")\n                .replace(\"ƒ†\", \"\")\n                .replace(\"</w>\", \"\")\n                .strip()\n            )\n\n            if not _is_word_start(rtok, token_word_map, i):\n                if (\n                    token_word_map is None\n                    and len(clean_token) >= 2\n                    and any(c.isalpha() for c in clean_token)\n                ):\n                    pass\n                else:\n                    continue\n\n            if _has_is_valid_token and _is_valid_token_fn is not None:\n                try:\n                    ok = _is_valid_token_fn(\n                        rtok,\n                        self.special_tokens,\n                        self.tokenizer,\n                        language=self.language,\n                    )\n                except Exception:\n                    ok = _fallback_is_valid_token(\n                        rtok, self.special_tokens, self.tokenizer, self.language\n                    )\n            else:\n                ok = _fallback_is_valid_token(\n                    rtok, self.special_tokens, self.tokenizer, self.language\n                )\n\n            if ok and len(clean_token) > 0:\n                if (\n                    token_word_map\n                    and isinstance(token_word_map.get(i, \"\"), str)\n                    and token_word_map[i].strip()\n                ):\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    evidence_tokens.append(clean_token)\n\n        return evidence_tokens\n\n    def _safe_extract_proto_probs(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all and len(pp_all) > 0:\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return row[token_idx].detach().cpu().flatten()\n                    return row.detach().cpu().flatten()\n                if isinstance(row, (list, tuple)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().flatten()\n                        if isinstance(val, (list, tuple, np.ndarray)):\n                            return torch.as_tensor(\n                                val, dtype=torch.float32\n                            ).flatten()\n                        return torch.tensor([float(val)], dtype=torch.float32)\n                    if len(row) > 0:\n                        maybe = row[0]\n                        if isinstance(maybe, torch.Tensor):\n                            return maybe.detach().cpu().flatten()\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Proto_probs extraction failed for token {token_idx}, using default [1.0]\")\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    elif row.ndim == 1 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    else:\n                        return 0.0\n                elif isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    span_val = (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n                else:\n                    return 0.0\n\n                if span_val < 0.0 or span_val > 1.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Clamping span {span_val:.3f} -> [0.0, 1.0]\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n\n                    span_val = max(0.0, min(1.0, float(span_val)))\n                return span_val\n\n        except Exception:\n            pass\n        return 0.0\n\n    def compute_span(self, sense_probs) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n\n            if len(probs) < 2:\n                return 0.0\n\n            sorted_probs = sorted([float(p) for p in probs], reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n\n            return max(0.0, min(1.0, span))\n\n        except Exception:\n            return 0.0\n\n    def _compute_sense_alternatives_fast(\n        self, proto_probs: torch.Tensor, temperature: float = 1.0\n    ) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n\n            probs = proto_probs.flatten().float()\n\n            if probs.numel() == 0:\n                return [(\"unknown\", 0.5)]\n\n            probs = torch.clamp(probs, min=1e-10, max=1.0)\n\n            if temperature != 1.0 and probs.numel() > 1:\n                log_probs = torch.log(probs)\n                scaled_log_probs = log_probs / float(temperature)\n                probs = torch.softmax(scaled_log_probs, dim=0)\n\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [\n                    (f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item()))\n                    for i in range(top_k)\n                ]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(\n        self, token_idx: int, tokens: List[str]\n    ) -> Dict:\n        if isinstance(tokens, list) and 0 <= token_idx < len(tokens):\n            token = tokens[token_idx]\n        else:\n            token = \"UNK\"\n\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"get_discovered_homographs\"):\n                    homograph_tokens = set(\n                        self.dscd_module.get_discovered_homographs()\n                    )\n                elif hasattr(self.dscd_module, \"prototype_stores\"):\n                    for token, store in self.dscd_module.prototype_stores.items():\n                        if hasattr(store, \"size\") and store.size() >= 2:\n                            clean = (\n                                str(token)\n                                .replace(\"‚ñÅ\", \"\")\n                                .replace(\"ƒ†\", \"\")\n                                .replace(\"##\", \"\")\n                                .strip()\n                            )\n                            homograph_tokens.add(clean)\n        except Exception:\n            pass\n        return homograph_tokens\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(\n        self,\n        embed_dim: Optional[int] = None,\n        tokenizer=None,\n        language: str = \"bn\",\n        dscd_module=None,\n    ):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(\n            _TRG_GEN_EMBED\n        )\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n\n        if dscd_module is None:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"[TRG] No DSCD module - homograph detection disabled\")\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens and _get_tokenizer_special_tokens_fn is not None:\n                    self.special_tokens = _get_tokenizer_special_tokens_fn(tokenizer)\n                elif _has_get_cached_special_tokens and _get_cached_special_tokens_fn is not None:\n                    self.special_tokens = _get_cached_special_tokens_fn(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(\n            tokenizer, language=language, dscd_module=dscd_module\n        )\n\n        self.silver_buffer = deque(maxlen=int(_MAX_SILVER_BUFFER))\n        self._silver_lock = threading.Lock()\n\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"explanations_generated\": 0,\n            \"high_confidence_explanations\": 0,\n            \"low_confidence_explanations\": 0,\n            \"empty_evidence_count\": 0,\n            \"total_evidence_tokens\": 0,\n            \"tokens_filtered_word_start\": 0,\n            \"tokens_filtered_validity\": 0,\n            \"tokens_filtered_ambiguity\": 0,\n            \"dscd_homographs_explained\": 0,\n        }\n        self._stats_lock = threading.Lock()\n\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(\"[TRG] Initialized:\")\n            print(f\"  - Uncertainty: ADAPTIVE (base={_TRG_UNCERTAINTY_THRESHOLD:.2f})\")\n            print(f\"  - Span: ADAPTIVE (base={_TRG_SPAN_THRESHOLD:.2f})\")\n            print(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\n            print(\"  - Mode: DATA-DRIVEN + ADAPTIVE THRESHOLDS\")\n            print(f\"  - Function availability: is_valid={_has_is_valid_token}, get_special={_has_get_tokenizer_special_tokens}, get_cached={_has_get_cached_special_tokens}\")\n\n    def _update_stats(self, evidence: Dict, is_dscd_homograph: bool = False) -> None:\n        with self._stats_lock:\n            self.stats[\"explanations_generated\"] += 1\n\n            if is_dscd_homograph:\n                self.stats[\"dscd_homographs_explained\"] += 1\n\n            if not evidence.get(\"evidence_tokens\"):\n                self.stats[\"empty_evidence_count\"] += 1\n            else:\n                self.stats[\"total_evidence_tokens\"] += len(\n                    evidence[\"evidence_tokens\"]\n                )\n\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= _TAU_ACCEPT:\n                self.stats[\"high_confidence_explanations\"] += 1\n            elif confidence < _TRG_UNCERTAINTY_THRESHOLD:\n                self.stats[\"low_confidence_explanations\"] += 1\n\n            if self.stats[\"explanations_generated\"] >= self.stats_reset_interval:\n                if _DEBUG_DISCOVERY:\n                    current_stats = self.get_statistics()\n                    print(\n                        f\"\\n[TRG-STATS] After {self.stats['explanations_generated']}:\"\n                    )\n                    print(\n                        f\"  High conf: {current_stats['high_confidence_rate']:.2%}\"\n                    )\n                    print(\n                        f\"  DSCD: {current_stats['dscd_homograph_rate']:.2%}\"\n                    )\n                self.reset_statistics()\n\n    def _add_to_silver_buffer(\n        self, evidence: Dict, explanation: str, tokens: List[str]\n    ) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n\n            with self._silver_lock:\n                self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        is_dscd_homograph: bool = False,\n    ) -> Tuple[str, Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token and _is_valid_token_fn is not None:\n            try:\n                is_valid = _is_valid_token_fn(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx,\n                tokens,\n                dscd_outputs,\n                token_word_map=token_word_map,\n                decoder_attention=decoder_attention,\n            )\n\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence, is_dscd_homograph=is_dscd_homograph)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception:\n            return \"\", {}\n\n    @staticmethod\n    def _to_list_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n\n        try:\n            if isinstance(x, torch.Tensor):\n                x = x.detach().cpu()\n\n                if x.ndim == 0:\n                    return [float(x.item())]\n                if x.ndim == 1:\n                    return [float(v.item()) for v in x]\n                if x.ndim == 2:\n                    if x.size(0) == 1:\n                        return [float(v.item()) for v in x[0]]\n                    else:\n                        return [float(v.item()) for v in x.flatten()]\n                if x.ndim >= 3:\n                    return [float(v.item()) for v in x.flatten()]\n\n            if isinstance(x, (list, tuple)):\n                out: List[float] = []\n                for v in x:\n                    if isinstance(v, torch.Tensor):\n                        v = v.detach().cpu()\n                        if v.ndim == 0:\n                            out.append(float(v.item()))\n                        elif v.numel() > 0:\n                            out.append(float(v.flatten()[0].item()))\n                        else:\n                            out.append(0.0)\n                    elif isinstance(v, (int, float, np.number)):\n                        out.append(float(v))\n                    else:\n                        try:\n                            out.append(float(v))\n                        except Exception:\n                            out.append(0.0)\n                return out\n\n            if isinstance(x, (int, float, np.number)):\n                return [float(x)]\n\n            return [float(x)]\n\n        except Exception:\n            return []\n\n    def compute_uncertainty_adaptive(\n        self, proto_probs: Any, uncertainties: Any\n    ) -> Tuple[float, float]:\n        try:\n            U = self._to_list_helper(uncertainties)\n\n            if not U or len(U) == 0:\n                return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n            U_arr = np.array(U, dtype=np.float32)\n            U_arr = U_arr[np.isfinite(U_arr)]\n\n            if len(U_arr) == 0:\n                return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n            median_u = float(np.median(U_arr))\n            std_u = float(np.std(U_arr))\n\n            adaptive_threshold = median_u + 0.5 * std_u\n            adaptive_threshold = max(0.05, min(0.50, adaptive_threshold))\n\n            return float(adaptive_threshold), float(median_u)\n\n        except Exception:\n            return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n    def compute_span_adaptive(self, span_preds: Any) -> Tuple[float, float]:\n        try:\n            S = self._to_list_helper(span_preds)\n\n            if not S or len(S) == 0:\n                return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n            S_arr = np.array(S, dtype=np.float32)\n            S_arr = S_arr[np.isfinite(S_arr)]\n\n            if len(S_arr) == 0:\n                return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n            median_s = float(np.median(S_arr))\n            percentile_75 = float(np.percentile(S_arr, 75))\n\n            adaptive_threshold = 0.5 * median_s + 0.5 * percentile_75\n            adaptive_threshold = max(0.02, min(0.30, adaptive_threshold))\n\n            return float(adaptive_threshold), float(median_s)\n\n        except Exception:\n            return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        span_threshold: Optional[float] = None,\n        uncertainty_threshold: Optional[float] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        max_explanations: int = _MAX_EXPLANATIONS_PER_SENTENCE,\n    ) -> List[Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if span_threshold is None:\n            span_threshold = float(_TRG_SPAN_THRESHOLD)\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n\n        explanations: List[Dict] = []\n\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n\n            if not U_all or not U_all[0]:\n                return explanations\n\n            U = self._to_list_helper(U_all[0])\n            S = (\n                self._to_list_helper(S_all[0])\n                if S_all and S_all[0]\n                else [0.0] * len(tokens)\n            )\n\n            seq_len = len(tokens)\n            if len(U) < seq_len:\n                U.extend([0.5] * (seq_len - len(U)))\n            elif len(U) > seq_len:\n                U = U[:seq_len]\n\n            if len(S) < seq_len:\n                S.extend([0.0] * (seq_len - len(S)))\n            elif len(S) > seq_len:\n                S = S[:seq_len]\n\n            if not U:\n                return explanations\n\n            adaptive_u_threshold, median_u = self.compute_uncertainty_adaptive(\n                dscd_outputs.get(\"proto_probs\", None), U_all[0]\n            )\n            adaptive_s_threshold, median_s = self.compute_span_adaptive(S_all[0] if S_all else None)\n\n            strict_uncertainty = max(adaptive_u_threshold, uncertainty_threshold)\n            strict_span = max(adaptive_s_threshold, span_threshold)\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[TRG-ADAPTIVE] U: median={median_u:.3f}, thresh={strict_uncertainty:.3f}\")\n                print(f\"[TRG-ADAPTIVE] S: median={median_s:.3f}, thresh={strict_span:.3f}\")\n\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n\n            local_stats = {\n                \"tokens_filtered_word_start\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n            }\n\n            for idx in range(seq_len):\n                tok = tokens[idx]\n                clean_tok = tok.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").strip()\n\n                if _is_punctuation_only(tok):\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                if not _is_word_start(tok, token_word_map, idx):\n                    local_stats[\"tokens_filtered_word_start\"] += 1\n                    continue\n\n                if _has_is_valid_token and _is_valid_token_fn is not None:\n                    try:\n                        valid = _is_valid_token_fn(\n                            tok,\n                            self.special_tokens,\n                            self.tokenizer,\n                            language=self.language,\n                        )\n                    except Exception:\n                        valid = _fallback_is_valid_token(\n                            tok, self.special_tokens, self.tokenizer, self.language\n                        )\n                else:\n                    valid = _fallback_is_valid_token(\n                        tok, self.special_tokens, self.tokenizer, self.language\n                    )\n\n                if not valid:\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                if clean_tok in _FUNCTION_WORDS:\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                if len(clean_tok) < 3 and not any('\\u0980' <= c <= '\\u09FF' for c in clean_tok):\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                u = float(U[idx])\n                s = float(S[idx])\n\n                in_dscd = clean_tok in dscd_homographs\n\n                if in_dscd:\n                    priority = 1\n                elif s >= strict_span and u >= strict_uncertainty:\n                    priority = 2\n                elif s >= strict_span:\n                    priority = 3\n                elif u >= strict_uncertainty:\n                    priority = 4\n                else:\n                    local_stats[\"tokens_filtered_ambiguity\"] += 1\n                    continue\n\n                candidates.append((idx, u, s, clean_tok, priority, idx))\n\n            with self._stats_lock:\n                self.stats[\"tokens_filtered_word_start\"] += local_stats[\"tokens_filtered_word_start\"]\n                self.stats[\"tokens_filtered_validity\"] += local_stats[\"tokens_filtered_validity\"]\n                self.stats[\"tokens_filtered_ambiguity\"] += local_stats[\"tokens_filtered_ambiguity\"]\n\n            if not candidates:\n                return explanations\n\n            candidates.sort(key=lambda t: (t[4], -t[2], -t[1], t[5]))\n\n            for (token_idx, u, s, clean_tok, priority, _) in candidates[\n                : max_explanations\n            ]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=token_word_map,\n                        decoder_attention=decoder_attention,\n                        is_dscd_homograph=(priority == 1),\n                    )\n                    if explanation_text and evidence:\n                        explanations.append(\n                            {\n                                \"token_idx\": token_idx,\n                                \"token\": (\n                                    token_word_map[token_idx]\n                                    if token_word_map\n                                    and token_idx in token_word_map\n                                    else tokens[token_idx]\n                                    .replace(\"‚ñÅ\", \"\")\n                                    .replace(\"ƒ†\", \"\")\n                                ),\n                                \"explanation\": explanation_text,\n                                \"uncertainty\": u,\n                                \"span\": s,\n                                \"dscd_discovered\": (priority == 1),\n                                \"priority\": priority,\n                            }\n                        )\n                except Exception:\n                    continue\n\n        except Exception:\n            pass\n\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self._stats_lock:\n            total = max(self.stats[\"explanations_generated\"], 1)\n            if self.stats[\"explanations_generated\"] > 0:\n                avg_evidence_tokens = (\n                    self.stats[\"total_evidence_tokens\"] / total\n                )\n            else:\n                avg_evidence_tokens = 0.0\n\n            return {\n                **self.stats.copy(),\n                \"high_confidence_rate\": self.stats[\n                    \"high_confidence_explanations\"\n                ]\n                / total,\n                \"low_confidence_rate\": self.stats[\n                    \"low_confidence_explanations\"\n                ]\n                / total,\n                \"empty_evidence_rate\": self.stats[\"empty_evidence_count\"]\n                / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.stats[\n                    \"dscd_homographs_explained\"\n                ]\n                / total,\n            }\n\n    def reset_statistics(self) -> None:\n        with self._stats_lock:\n            self.stats = {\n                \"explanations_generated\": 0,\n                \"high_confidence_explanations\": 0,\n                \"low_confidence_explanations\": 0,\n                \"empty_evidence_count\": 0,\n                \"total_evidence_tokens\": 0,\n                \"tokens_filtered_word_start\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n                \"dscd_homographs_explained\": 0,\n            }\n\n    def clear_silver_buffer(self) -> None:\n        with self._silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[TRG-TEST] Testing\")\n        print(\"=\" * 60)\n\n        if not _ENABLE_TRG_INFERENCE:\n            print(\"TRG inference disabled, enabling for test...\")\n\n        try:\n            tokens = [\"‚ñÅ‡¶Ü‡¶Æ‡¶ø\", \"‚ñÅ‡¶ï‡¶≤\", \"‚ñÅ‡¶¨‡¶®‡ßç‡¶ß\", \"‚ñÅ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø\", \"‡•§\"]\n\n            dscd_outputs = {\n                \"proto_probs\": [[torch.tensor([0.6, 0.4]) for _ in tokens]],\n                \"uncertainties\": [[0.1, 0.5, 0.2, 0.1, 0.0]],\n                \"span_preds\": [[0.05, 0.3, 0.1, 0.05, 0.0]],\n                \"gates\": [[0.2, 0.8, 0.3, 0.2, 0.0]],\n            }\n\n            token_word_map = {\n                0: \"‡¶Ü‡¶Æ‡¶ø\",\n                1: \"‡¶ï‡¶≤\",\n                2: \"‡¶¨‡¶®‡ßç‡¶ß\",\n                3: \"‡¶ï‡¶∞‡ßá‡¶õ‡¶ø\",\n                4: \"‡•§\",\n            }\n\n            self.eval()\n\n            explanations = self.process_sentence_for_explanations(\n                tokens=tokens,\n                dscd_outputs=dscd_outputs,\n                token_word_map=token_word_map,\n                max_explanations=3,\n            )\n\n            print(f\"  ‚úì Generated {len(explanations)} explanations\")\n\n            if len(explanations) > 0:\n                for i, expl in enumerate(explanations, 1):\n                    print(\n                        f\"    {i}. '{expl['token']}' (u={expl['uncertainty']:.2f})\"\n                    )\n\n            stats = self.get_statistics()\n            print(f\"  ‚úì Stats: {stats['explanations_generated']} total\")\n\n            self.reset_statistics()\n            stats_after = self.get_statistics()\n            assert stats_after[\"explanations_generated\"] == 0\n            print(\"  ‚úì Reset OK\")\n\n            print(\"\\n‚úì All TRG tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n\n        except Exception as e:\n            print(f\"\\n‚úó Test failed: {e}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Module - VERIFIED CORRECT\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Uncertainty: ADAPTIVE (base={_TRG_UNCERTAINTY_THRESHOLD:.2f})\")\nprint(f\"  - Span: ADAPTIVE (base={_TRG_SPAN_THRESHOLD:.2f})\")\nprint(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\nprint(f\"  - TAU_HIGH: {_TAU_HIGH:.2f}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW:.2f}\")\nprint(f\"  - TAU_ACCEPT: {_TAU_ACCEPT:.2f}\")\nprint(f\"  - Evidence K: {_TRG_EVIDENCE_K}\")\nprint(f\"  - Max Explanations: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"svk-wKO7H4J3","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.417023Z","iopub.execute_input":"2026-02-16T03:01:26.417245Z","iopub.status.idle":"2026-02-16T03:01:26.526014Z","shell.execute_reply.started":"2026-02-16T03:01:26.417227Z","shell.execute_reply":"2026-02-16T03:01:26.525305Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 5: TRG Module - VERIFIED CORRECT\n================================================================================\nConfiguration:\n  - Uncertainty: ADAPTIVE (base=0.12)\n  - Span: ADAPTIVE (base=0.18)\n  - Temperature: 1.00\n  - TAU_HIGH: 0.88\n  - TAU_LOW: 0.12\n  - TAU_ACCEPT: 0.70\n  - Evidence K: 3\n  - Max Explanations: 10\n================================================================================\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 6: DUAL-PATH TATN MODEL - PATH 1 (WORD-LEVEL) + PATH 2 (SUBWORD-LEVEL) - BanglaT5\n# ===========================================================================================\nfrom typing import List, Dict, Optional, Any, Tuple, Union\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSeq2SeqLM, T5ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\nimport re\nimport math\n\nMODEL_VOCAB_SIZE = 32128\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    _TASK_PREFIX = \"translate Bengali to English: \"\n\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return int(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return float(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return bool(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\ndef _get_list_global(name: str, default: list) -> list:\n    try:\n        val = globals().get(name)\n        if val is not None and isinstance(val, (list, tuple)):\n            return list(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\n_DSCD_BUFFER_SIZE = _get_int_global(\"DSCD_BUFFER_SIZE\", 20)\n_DSCD_MAX_PROTOS = _get_int_global(\"DSCD_MAX_PROTOS\", 3)\n_DSCD_N_MIN = _get_int_global(\"DSCD_N_MIN\", 3)\n_DSCD_DISPERSION_THRESHOLD = _get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.20)\n\n_ENABLE_ASBN_TRAINING = _get_bool_global(\"ENABLE_ASBN_TRAINING\", False)\n_ENABLE_ASBN_INFERENCE = _get_bool_global(\"ENABLE_ASBN_INFERENCE\", False)\n_ENABLE_TRG_INFERENCE = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\n_MEMORY_CLEANUP_FREQUENCY = _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 100)\n\n_NUM_GPUS = _get_int_global(\n    \"NUM_GPUS\",\n    torch.cuda.device_count() if torch.cuda.is_available() else 1,\n)\n_USE_GC = _get_bool_global(\"GRADIENT_CHECKPOINTING\", True)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\n    \"DSCD_ENABLE_TRAINING_CLUSTERING\", True\n)\n\n_LAMBDA_ASBN = _get_float_global(\"LAMBDA_ASBN\", 0.0)\n_LAMBDA_DSCD = _get_float_global(\"LAMBDA_DSCD\", 0.15)\n_LAMBDA_TOKEN = _get_float_global(\"LAMBDA_TOKEN\", 0.0)\n_LAMBDA_CONFIDENCE = _get_float_global(\"LAMBDA_CONFIDENCE\", 0.0)\n_LAMBDA_LENGTH = _get_float_global(\"LAMBDA_LENGTH\", 0.0)\n\n_VERBOSE_LOGGING = _get_bool_global(\"VERBOSE_LOGGING\", False)\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\n_PERIODIC_DISCOVERY_FREQUENCY = _get_int_global(\n    \"PERIODIC_DISCOVERY_FREQUENCY\", 50\n)\n_VALIDATION_CHECK_INTERVAL = _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 200)\n\n_SPAN_THRESHOLD = _get_float_global(\"SPAN_THRESHOLD\", 0.20)\n_UNCERTAINTY_THRESHOLD = _get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.15)\n\n_TRG_UNCERTAINTY_THRESHOLD = _get_float_global(\n    \"TRG_UNCERTAINTY_THRESHOLD\", _get_float_global(\"TAU_LOW\", 0.15)\n)\n_TAU_LOW = _get_float_global(\"TAU_LOW\", 0.15)\n\n_TRAIN_DOMAIN = _get_int_global(\"TRAIN_DOMAIN\", 0)\n_TEST_DOMAIN = _get_int_global(\"TEST_DOMAIN\", 1)\n_USE_DOMAIN_LABELS = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    _LABEL_SMOOTHING_EPS = float(LABEL_SMOOTHING)\nexcept (NameError, ValueError, TypeError):\n    _LABEL_SMOOTHING_EPS = 0.0\n\ntry:\n    _RDROP_ALPHA = float(RDROP_ALPHA)\nexcept (NameError, ValueError, TypeError):\n    _RDROP_ALPHA = 0.0\n\ntry:\n    _USE_RDROP = bool(USE_RDROP)\nexcept (NameError, TypeError):\n    _USE_RDROP = False\n\n_USE_LORA = _get_bool_global(\"USE_LORA\", False)\n_LORA_RANK = _get_int_global(\"LORA_RANK\", 16)\n_LORA_ALPHA = _get_float_global(\"LORA_ALPHA\", 32.0)\n_LORA_DROPOUT = _get_float_global(\"LORA_DROPOUT\", 0.1)\n_LORA_TARGET_MODULES = _get_list_global(\"LORA_TARGET_MODULES\", [\"q\", \"v\"])\n\n_USE_8BIT = _get_bool_global(\"USE_8BIT\", False)\n_USE_4BIT = _get_bool_global(\"USE_4BIT\", False)\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\n_BENGALI_PUNCT_SET = set(['‡•§', '‡••'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n\ndef _is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    clean = (\n        token.replace(\"‚ñÅ\", \"\")\n        .replace(\"ƒ†\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n    if not clean:\n        return False\n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    if clean in _COMMON_PUNCT_SET:\n        return True\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    return all(c in _PUNCT_SET for c in clean)\n\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        return enc_output[0]\n    return None\n\n\ndef build_token_word_map_sentencepiece(\n    token_strings: List[str], fallback: bool = True\n) -> Dict[int, str]:\n    word_map: Dict[int, str] = {}\n    current_word = \"\"\n    start_idx = None\n    for i, token in enumerate(token_strings):\n        if not token or token.startswith(\"<\") or token.startswith(\"[\"):\n            continue\n        if token.startswith(\"‚ñÅ\"):\n            if current_word and start_idx is not None:\n                clean = current_word.replace(\"‚ñÅ\", \"\").strip()\n                if clean and len(clean) >= 2 and not _is_punctuation_only(current_word):\n                    word_map[start_idx] = clean\n            current_word = token\n            start_idx = i\n        else:\n            current_word += token\n    if current_word and start_idx is not None:\n        clean = current_word.replace(\"‚ñÅ\", \"\").strip()\n        if clean and len(clean) >= 2 and not _is_punctuation_only(current_word):\n            word_map[start_idx] = clean\n    if fallback and not word_map:\n        for i, tok in enumerate(token_strings):\n            clean = tok.replace(\"‚ñÅ\", \"\").strip()\n            if clean and len(clean) >= 2 and not _is_punctuation_only(tok):\n                word_map[i] = clean\n    return word_map\n\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int,\n    fallback_h: Optional[torch.Tensor] = None,\n) -> Dict[str, Any]:\n    if fallback_h is None:\n        fallback_h_augmented = torch.zeros(\n            batch_size, seq_len, embed_dim, device=device, dtype=torch.float32\n        )\n    else:\n        fallback_h_augmented = fallback_h.detach().clone()\n    defaults = {\n        \"h_augmented\": fallback_h_augmented,\n        \"proto_probs\": [\n            [\n                torch.tensor([1.0], device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"uncertainties\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"gates\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"span_preds\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"proto_assignments\": [\n            torch.zeros(seq_len, dtype=torch.long, device=device)\n            for _ in range(batch_size)\n        ],\n    }\n    if not isinstance(raw, dict):\n        return defaults\n    out = defaults.copy()\n    try:\n        if \"h_augmented\" in raw and raw[\"h_augmented\"] is not None:\n            h = raw[\"h_augmented\"]\n            if isinstance(h, torch.Tensor) and h.shape == (\n                batch_size,\n                seq_len,\n                embed_dim,\n            ):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = (\n                        h.to(device).reshape(batch_size, seq_len, embed_dim)\n                    )\n                except Exception:\n                    out[\"h_augmented\"] = fallback_h_augmented\n    except Exception:\n        out[\"h_augmented\"] = fallback_h_augmented\n    for list_key in (\"proto_probs\", \"uncertainties\", \"gates\", \"span_preds\"):\n        if list_key in raw and raw[list_key] is not None:\n            try:\n                val = raw[list_key]\n                if isinstance(val, list) and len(val) == batch_size:\n                    safe_batch = []\n                    for b_row in val:\n                        if isinstance(b_row, list):\n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < len(b_row):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            safe_row.append(v.detach().to(device))\n                                        else:\n                                            safe_row.append(\n                                                torch.as_tensor(\n                                                    v,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    [1.0],\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                        else:\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    0.0,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                except Exception:\n                                    safe_row.append(\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                    )\n                            safe_batch.append(safe_row)\n                        else:\n                            if list_key == \"proto_probs\":\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            [1.0],\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                            else:\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                    out[list_key] = safe_batch\n            except Exception:\n                pass\n    try:\n        if \"proto_assignments\" in raw and raw[\"proto_assignments\"] is not None:\n            pa = raw[\"proto_assignments\"]\n            if isinstance(pa, list) and len(pa) == batch_size:\n                safe_pa = []\n                for b_row in pa:\n                    try:\n                        if isinstance(b_row, torch.Tensor):\n                            safe_pa.append(b_row.detach().to(device).long())\n                        else:\n                            safe_pa.append(\n                                torch.tensor(\n                                    b_row, dtype=torch.long, device=device\n                                )\n                            )\n                    except Exception:\n                        safe_pa.append(\n                            torch.zeros(seq_len, dtype=torch.long, device=device)\n                        )\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n    return out\n\n\ndef _norm_scalar_matrix(uncertainties, gates, gate_threshold=0.01):\n    final_normalized = []\n    batch_size = len(uncertainties)\n    for b in range(batch_size):\n        u_row = uncertainties[b]\n        g_row = gates[b]\n        seq_len = len(u_row)\n        safe_row = []\n        for t in range(seq_len):\n            try:\n                u_val = float(u_row[t]) if t < len(u_row) else 0.0\n                g_val = float(g_row[t]) if t < len(g_row) else 0.0\n                if g_val < gate_threshold:\n                    norm_val = 0.0\n                else:\n                    norm_val = max(0.0, min(1.0, u_val))\n                safe_row.append(norm_val)\n            except Exception:\n                safe_row.append(0.0)\n        final_normalized.append(safe_row)\n    return final_normalized\n\n\ndef _norm_proto_probs(proto_probs):\n    return [\n        [pp if isinstance(pp, torch.Tensor) else torch.tensor([1.0]) for pp in row]\n        for row in proto_probs\n    ]\n\n\ndef _to_vec(x):\n    if isinstance(x, torch.Tensor):\n        return x.flatten().tolist()\n    elif isinstance(x, (list, tuple)):\n        return list(x)\n    elif isinstance(x, (int, float)):\n        return [float(x)]\n    else:\n        return [0.0]\n\n\ndef _extract_words_from_text(text: str) -> List[str]:\n    if not text or not isinstance(text, str):\n        return []\n    text = text.strip()\n    if not text:\n        return []\n    try:\n        words = re.findall(r'[\\u0980-\\u09FF]+|[a-zA-Z]+|\\d+', text)\n        words = [w for w in words if w and len(w) > 0 and not _is_punctuation_only(w)]\n        return words if words else []\n    except Exception:\n        return []\n\n\ndef _capitalize_first_alpha(s: str) -> str:\n    if not s or not isinstance(s, str):\n        return s\n    for idx, ch in enumerate(s):\n        if ch.isalpha():\n            return s[:idx] + ch.upper() + s[idx + 1 :]\n    return s\n\n\ndef _clean_decoded_text(s: str) -> str:\n    if s is None:\n        return \"\"\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    s = _capitalize_first_alpha(s)\n    return s\n\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n        self.lora_applied = False\n\n        print(\"=\" * 80)\n        print(\"CELL 6: INITIALIZING BANGLAT5 MODEL WITH LORA\")\n        print(\"=\" * 80)\n\n        print(\"\\n[STEP 1/9] Loading pretrained BanglaT5 model...\")\n        try:\n            self.t5 = AutoModelForSeq2SeqLM.from_pretrained(\n                \"csebuetnlp/banglat5\",\n                torch_dtype=torch.float32,\n                use_cache=False,\n            )\n            print(\"  ‚úÖ Model loaded successfully\")\n        except Exception as e:\n            print(f\"  ‚ùå CRITICAL: Failed to load model: {type(e).__name__}\")\n            print(f\"     Error: {str(e)}\")\n            raise\n\n        try:\n            self.t5.config.use_cache = False\n        except Exception:\n            pass\n\n        print(\"\\n[STEP 2/9] Moving model to GPU...\")\n        device_for_init = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        try:\n            self.t5 = self.t5.to(device_for_init)\n            print(f\"  ‚úÖ Model on device: {device_for_init}\")\n        except Exception as e:\n            print(f\"  ‚ùå CRITICAL: Failed to move model to {device_for_init}\")\n            print(f\"     Error: {type(e).__name__}: {str(e)}\")\n            raise\n\n        if _USE_LORA:\n            print(\"\\n[LORA INITIALIZATION]\")\n            \n            if _USE_8BIT or _USE_4BIT:\n                print(f\"  ‚ö†Ô∏è  WARNING: Quantization requested but not available\")\n                print(f\"     Kaggle CUDA 12.6 doesn't support bitsandbytes\")\n                print(f\"     ‚Üí Using Standard LoRA (FP16) instead\")\n                print(f\"     ‚Üí No performance impact (same BLEU/ChrF++)\")\n            \n            try:\n                from peft import LoraConfig, get_peft_model, TaskType\n                \n                lora_cfg = LoraConfig(\n                    r=_LORA_RANK,\n                    lora_alpha=_LORA_ALPHA,\n                    target_modules=_LORA_TARGET_MODULES,\n                    lora_dropout=_LORA_DROPOUT,\n                    bias=\"none\",\n                    task_type=TaskType.SEQ_2_SEQ_LM,\n                )\n                \n                print(f\"  Applying Standard LoRA (FP16) with config:\")\n                print(f\"    - Rank: {_LORA_RANK}\")\n                print(f\"    - Alpha: {_LORA_ALPHA}\")\n                print(f\"    - Dropout: {_LORA_DROPOUT}\")\n                print(f\"    - Target modules: {len(_LORA_TARGET_MODULES)} ({', '.join(_LORA_TARGET_MODULES)})\")\n                print(f\"    - Mode: FP16 (no quantization)\")\n                \n                self.t5 = get_peft_model(self.t5, lora_cfg)\n                self.lora_applied = True\n                \n                trainable_params = sum(p.numel() for p in self.t5.parameters() if p.requires_grad)\n                total_params = sum(p.numel() for p in self.t5.parameters())\n                \n                print(f\"  ‚úÖ Standard LoRA (FP16) applied successfully:\")\n                print(f\"     - Total params: {total_params:,}\")\n                print(f\"     - Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n                print(f\"     - Frozen params: {total_params - trainable_params:,}\")\n                print(f\"     - Expected GPU memory: ~2.5 GB\")\n                print(f\"     - Expected BLEU: 38-40\")\n                print(f\"     - Expected training time: ~3.5 hours\")\n                \n                if trainable_params == 0:\n                    raise RuntimeError(\n                        \"LoRA applied but 0 trainable params!\\n\"\n                        \"  Possible causes:\\n\"\n                        \"    - PEFT version mismatch\\n\"\n                        \"    - Target modules don't exist in model\\n\"\n                        \"  Fix: pip install peft==0.7.1\"\n                    )\n                \n                if trainable_params == total_params:\n                    raise RuntimeError(\n                        \"LoRA applied but ALL params trainable!\\n\"\n                        \"  LoRA is not working - adapters were not created\\n\"\n                        \"  Fix: Check PEFT installation and target module names\"\n                    )\n                \n                trainable_pct = trainable_params / total_params * 100\n                if trainable_pct < 0.5:\n                    print(f\"  ‚ö†Ô∏è  WARNING: Very low trainable params ({trainable_pct:.2f}%)\")\n                    print(f\"     This may limit model capacity - consider increasing rank\")\n                elif trainable_pct > 10.0:\n                    print(f\"  ‚ö†Ô∏è  WARNING: High trainable params ({trainable_pct:.2f}%)\")\n                    print(f\"     This is unusual for LoRA - verify configuration\")\n                else:\n                    print(f\"  ‚úÖ LoRA parameters in optimal range ({trainable_pct:.2f}%)\")\n                \n            except ImportError as e:\n                print(f\"  ‚ùå CRITICAL: PEFT library not available\")\n                print(f\"     Error: {type(e).__name__}: {e}\")\n                print(f\"     Install with: pip install peft==0.7.1\")\n                print(f\"     Then restart kernel and re-run Cell -1\")\n                raise RuntimeError(\"PEFT not available - cannot use LoRA\")\n            \n            except RuntimeError as e:\n                error_str = str(e).lower()\n                \n                if \"trainable params\" in error_str or \"lora applied\" in error_str:\n                    raise\n                \n                print(f\"  ‚ùå LoRA initialization failed: {type(e).__name__}\")\n                print(f\"     {str(e)[:200]}\")\n                raise RuntimeError(f\"LoRA setup failed: {type(e).__name__}\")\n            \n            except Exception as e:\n                print(f\"  ‚ùå Unexpected LoRA error: {type(e).__name__}\")\n                print(f\"     {str(e)[:200]}\")\n                print(f\"\\n  Debug info:\")\n                print(f\"     _USE_LORA: {_USE_LORA}\")\n                print(f\"     _LORA_RANK: {_LORA_RANK}\")\n                print(f\"     _LORA_TARGET_MODULES: {_LORA_TARGET_MODULES}\")\n                raise RuntimeError(f\"LoRA initialization failed: {type(e).__name__}\")\n        \n        else:\n            print(\"\\n[LORA] Disabled (USE_LORA=False)\")\n            print(\"  ‚ÑπÔ∏è  Using full fine-tuning mode\")\n            print(\"  ‚ÑπÔ∏è  All 220M parameters will be trained\")\n            print(\"  ‚ÑπÔ∏è  Expected GPU memory: ~4.0 GB\")\n            print(\"  ‚ÑπÔ∏è  Expected training time: ~9-10 hours\")\n            self.lora_applied = False\n\n        print(\"\\n[STEP 3/9] Testing T5 BEFORE any modifications...\")\n        test_input = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device_for_init)\n        test_labels = torch.tensor([[6, 7, 8, 9, 10]], dtype=torch.long).to(device_for_init)\n\n        baseline_loss = None\n        try:\n            with torch.no_grad():\n                test_output = self.t5(input_ids=test_input, labels=test_labels, return_dict=True)\n                baseline_loss = test_output.loss\n\n                if torch.isnan(baseline_loss) or torch.isinf(baseline_loss):\n                    print(f\"  ‚ùå CRITICAL: Model produces NaN/Inf loss BEFORE fixes!\")\n                    print(f\"     Baseline loss: {baseline_loss}\")\n                    print(f\"     Model is corrupted from download!\")\n                else:\n                    print(f\"  ‚úÖ Baseline test passed: loss={baseline_loss.item():.4f}\")\n                    print(f\"     (This is the pretrained model's natural loss)\")\n        except Exception as e:\n            print(f\"  ‚ùå CRITICAL: Baseline test failed: {type(e).__name__}\")\n            print(f\"     Error: {str(e)}\")\n            raise\n\n        print(\"\\n[STEP 4/9] Analyzing embedding layers...\")\n        encoder_embedding_layer = self.t5.get_input_embeddings()\n        decoder_embedding_layer = self.t5.get_output_embeddings()\n\n        print(\"  [4a] Encoder embeddings:\")\n        with torch.no_grad():\n            enc_emb_weight = encoder_embedding_layer.weight\n            enc_has_nan = torch.isnan(enc_emb_weight).any().item()\n            enc_has_inf = torch.isinf(enc_emb_weight).any().item()\n            enc_min = enc_emb_weight.min().item()\n            enc_max = enc_emb_weight.max().item()\n            enc_mean = enc_emb_weight.mean().item()\n            enc_std = enc_emb_weight.std().item()\n\n            print(f\"     Shape: {enc_emb_weight.shape}\")\n            print(f\"     Range: [{enc_min:.4f}, {enc_max:.4f}]\")\n            print(f\"     Mean: {enc_mean:.4f}, Std: {enc_std:.4f}\")\n            print(f\"     Has NaN: {enc_has_nan}, Has Inf: {enc_has_inf}\")\n\n            if enc_has_nan or enc_has_inf:\n                print(f\"     ‚ùå CORRUPTION DETECTED!\")\n                if enc_has_nan:\n                    nan_count = torch.isnan(enc_emb_weight).sum().item()\n                    print(f\"        NaN count: {nan_count}/{enc_emb_weight.numel()}\")\n                if enc_has_inf:\n                    inf_count = torch.isinf(enc_emb_weight).sum().item()\n                    print(f\"        Inf count: {inf_count}/{enc_emb_weight.numel()}\")\n            else:\n                print(f\"     ‚úÖ No NaN/Inf corruption\")\n\n        print(\"  [4b] Decoder embeddings:\")\n        with torch.no_grad():\n            dec_emb_weight = decoder_embedding_layer.weight\n            dec_has_nan = torch.isnan(dec_emb_weight).any().item()\n            dec_has_inf = torch.isinf(dec_emb_weight).any().item()\n            dec_min = dec_emb_weight.min().item()\n            dec_max = dec_emb_weight.max().item()\n            dec_mean = dec_emb_weight.mean().item()\n            dec_std = dec_emb_weight.std().item()\n\n            print(f\"     Shape: {dec_emb_weight.shape}\")\n            print(f\"     Range: [{dec_min:.4f}, {dec_max:.4f}]\")\n            print(f\"     Mean: {dec_mean:.4f}, Std: {dec_std:.4f}\")\n            print(f\"     Has NaN: {dec_has_nan}, Has Inf: {dec_has_inf}\")\n\n            if dec_has_nan or dec_has_inf:\n                print(f\"     ‚ùå CORRUPTION DETECTED!\")\n                if dec_has_nan:\n                    nan_count = torch.isnan(dec_emb_weight).sum().item()\n                    print(f\"        NaN count: {nan_count}/{dec_emb_weight.numel()}\")\n                if dec_has_inf:\n                    inf_count = torch.isinf(dec_emb_weight).sum().item()\n                    print(f\"        Inf count: {inf_count}/{dec_emb_weight.numel()}\")\n            else:\n                print(f\"     ‚úÖ No NaN/Inf corruption\")\n\n        if encoder_embedding_layer.weight.data_ptr() == decoder_embedding_layer.weight.data_ptr():\n            print(\"  [4c] ‚ÑπÔ∏è  Encoder/decoder share embeddings (T5 tied weights)\")\n        else:\n            print(\"  [4c] ‚ÑπÔ∏è  Encoder/decoder have separate embeddings\")\n\n        print(\"\\n[STEP 5/9] Detecting if BanglaT5 uses scaled embeddings...\")\n\n        is_scaled_embeddings = False\n        scale_factor = 1.0\n\n        try:\n            d_model = int(getattr(self.t5.config, \"d_model\", 768))\n            expected_scale = math.sqrt(d_model)\n\n            print(f\"  Model d_model: {d_model}\")\n            print(f\"  Expected sqrt(d_model): {expected_scale:.2f}\")\n\n            enc_ratio = max(abs(enc_max), abs(enc_min)) / max(abs(dec_max), abs(dec_min))\n            print(f\"  Encoder/Decoder magnitude ratio: {enc_ratio:.2f}\")\n\n            if 20.0 < enc_ratio < 35.0:\n                print(f\"  ‚ÑπÔ∏è  Ratio matches sqrt(d_model) scaling pattern!\")\n                print(f\"     This is NORMAL for T5 with scaled encoder embeddings\")\n                is_scaled_embeddings = True\n                scale_factor = expected_scale\n            elif abs(enc_max) > 50 or abs(enc_min) > 50:\n                print(f\"  ‚ö†Ô∏è  Encoder embeddings have extreme values but NOT scaled pattern\")\n                print(f\"     This may indicate corruption\")\n            else:\n                print(f\"  ‚úÖ Both embeddings in similar ranges - no scaling detected\")\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Scale detection failed: {type(e).__name__}\")\n\n        print(\"\\n[STEP 6/9] Applying INTELLIGENT fixes...\")\n\n        fixes_applied = False\n        encoder_was_clipped = False\n\n        print(\"  [6a] Checking non-embedding parameters...\")\n        param_fixes = 0\n        for name, param in self.t5.named_parameters():\n            if param.requires_grad and 'embed' not in name.lower():\n                with torch.no_grad():\n                    if torch.isnan(param).any() or torch.isinf(param).any():\n                        print(f\"     ‚ö†Ô∏è  Found NaN/Inf in {name}, resetting to zero\")\n                        param.zero_()\n                        param_fixes += 1\n                        fixes_applied = True\n\n        if param_fixes == 0:\n            print(f\"     ‚úÖ All non-embedding params are clean\")\n        else:\n            print(f\"     ‚ö†Ô∏è  Fixed {param_fixes} corrupted parameters\")\n\n        print(\"  [6b] Handling encoder embeddings...\")\n        with torch.no_grad():\n            if enc_has_nan or enc_has_inf:\n                print(f\"     ‚ùå CRITICAL CORRUPTION - Reinitializing encoder embeddings\")\n                nn.init.normal_(enc_emb_weight, mean=0.0, std=0.02)\n                fixes_applied = True\n                encoder_was_clipped = True\n                print(f\"     ‚úÖ Encoder embeddings reinitialized\")\n            elif is_scaled_embeddings:\n                print(f\"     ‚ÑπÔ∏è  Detected SCALED embeddings (factor: {scale_factor:.2f})\")\n                print(f\"     ‚Üí This is ARCHITECTURAL, not corruption\")\n                print(f\"     ‚Üí PRESERVING original scaled weights (no clipping)\")\n            elif abs(enc_max) > 100 or abs(enc_min) > 100:\n                print(f\"     ‚ö†Ô∏è  Extreme values without scaling pattern detected\")\n                print(f\"     Applying conservative clipping to [-100, 100]...\")\n                enc_emb_weight.clamp_(-100.0, 100.0)\n                fixes_applied = True\n                encoder_was_clipped = True\n                new_min = enc_emb_weight.min().item()\n                new_max = enc_emb_weight.max().item()\n                print(f\"     ‚úÖ Clipped to [{new_min:.2f}, {new_max:.2f}]\")\n            else:\n                print(f\"     ‚úÖ Encoder embeddings healthy\")\n                print(f\"     ‚Üí NO CLIPPING APPLIED (preserving pretrained weights)\")\n\n        print(\"  [6c] Handling decoder embeddings...\")\n        with torch.no_grad():\n            if dec_has_nan or dec_has_inf:\n                print(f\"     ‚ùå CRITICAL CORRUPTION - Reinitializing decoder embeddings\")\n                nn.init.normal_(dec_emb_weight, mean=0.0, std=0.02)\n                fixes_applied = True\n                print(f\"     ‚úÖ Decoder embeddings reinitialized\")\n            elif abs(dec_max) > 50 or abs(dec_min) > 50:\n                print(f\"     ‚ö†Ô∏è  Extreme values detected\")\n                print(f\"     Applying conservative clipping to [-50, 50]...\")\n                dec_emb_weight.clamp_(-50.0, 50.0)\n                fixes_applied = True\n                new_min = dec_emb_weight.min().item()\n                new_max = dec_emb_weight.max().item()\n                print(f\"     ‚úÖ Clipped to [{new_min:.2f}, {new_max:.2f}]\")\n            else:\n                print(f\"     ‚úÖ Decoder embeddings healthy\")\n                print(f\"     ‚Üí NO CLIPPING APPLIED (preserving pretrained weights)\")\n\n        if not fixes_applied:\n            print(\"  ‚úÖ No fixes needed - model is healthy!\")\n\n        print(\"\\n[STEP 7/9] Testing T5 AFTER modifications...\")\n        postfix_loss = None\n        try:\n            with torch.no_grad():\n                retest_output = self.t5(input_ids=test_input, labels=test_labels, return_dict=True)\n                postfix_loss = retest_output.loss\n\n                if torch.isnan(postfix_loss) or torch.isinf(postfix_loss):\n                    print(f\"  ‚ùå CRITICAL: Model produces NaN/Inf loss AFTER fixes!\")\n                    print(f\"     Post-fix loss: {postfix_loss}\")\n                    print(f\"     The fixes FAILED to repair the model!\")\n                else:\n                    print(f\"  ‚úÖ Post-fix test passed: loss={postfix_loss.item():.4f}\")\n        except Exception as e:\n            print(f\"  ‚ùå CRITICAL: Post-fix test failed: {type(e).__name__}\")\n            print(f\"     Error: {str(e)}\")\n            raise\n\n        print(\"\\n[STEP 8/9] Comparing before/after losses...\")\n        if baseline_loss is not None and postfix_loss is not None:\n            baseline_val = baseline_loss.item()\n            postfix_val = postfix_loss.item()\n            loss_change = postfix_val - baseline_val\n            loss_ratio = postfix_val / baseline_val if baseline_val > 0 else float('inf')\n\n            print(f\"  Baseline loss:  {baseline_val:.4f}\")\n            print(f\"  Post-fix loss:  {postfix_val:.4f}\")\n            print(f\"  Change:         {loss_change:+.4f} ({loss_ratio:.2f}x)\")\n\n            if encoder_was_clipped and loss_ratio > 1.5:\n                print(f\"  ‚ö†Ô∏è  WARNING: Loss increased significantly after encoder clipping\")\n                print(f\"     Loss ratio: {loss_ratio:.2f}x\")\n\n                if is_scaled_embeddings:\n                    print(f\"  ‚ÑπÔ∏è  Model likely needs scaled embeddings for proper function\")\n                    print(f\"     ‚Üí Reverting encoder embeddings to original scaled values...\")\n\n                    try:\n                        original_model = AutoModelForSeq2SeqLM.from_pretrained(\n                            \"csebuetnlp/banglat5\",\n                            torch_dtype=torch.float32,\n                        ).to(device_for_init)\n\n                        with torch.no_grad():\n                            self.t5.get_input_embeddings().weight.copy_(\n                                original_model.get_input_embeddings().weight\n                            )\n\n                        print(f\"  ‚úÖ Original encoder embeddings restored\")\n\n                        with torch.no_grad():\n                            final_test = self.t5(input_ids=test_input, labels=test_labels, return_dict=True)\n                            final_loss = final_test.loss\n                            print(f\"  ‚úÖ Final test after revert: loss={final_loss.item():.4f}\")\n\n                        del original_model\n                        torch.cuda.empty_cache()\n                    except Exception as e:\n                        print(f\"  ‚ùå Failed to revert: {type(e).__name__}\")\n\n            elif loss_ratio > 2.0:\n                print(f\"  ‚ùå CRITICAL: Loss MORE THAN DOUBLED!\")\n                print(f\"     The 'fixes' are DESTROYING the model!\")\n                print(f\"     ‚Üí Reverting to original pretrained weights...\")\n\n                try:\n                    self.t5 = AutoModelForSeq2SeqLM.from_pretrained(\n                        \"csebuetnlp/banglat5\",\n                        torch_dtype=torch.float32,\n                        use_cache=False,\n                    ).to(device_for_init)\n\n                    encoder_embedding_layer = self.t5.get_input_embeddings()\n                    decoder_embedding_layer = self.t5.get_output_embeddings()\n\n                    print(f\"  ‚úÖ Model reloaded with original pretrained weights\")\n\n                    with torch.no_grad():\n                        revert_test = self.t5(input_ids=test_input, labels=test_labels, return_dict=True)\n                        revert_loss = revert_test.loss\n                        print(f\"  ‚úÖ Reverted model test: loss={revert_loss.item():.4f}\")\n\n                except Exception as e:\n                    print(f\"  ‚ùå Failed to revert model: {type(e).__name__}\")\n                    raise\n\n            elif loss_ratio < 0.9:\n                print(f\"  ‚úÖ EXCELLENT: Loss improved by {(1-loss_ratio)*100:.1f}%\")\n                print(f\"     The fixes successfully improved the model!\")\n\n            else:\n                print(f\"  ‚úÖ GOOD: Loss stable (change: {(loss_ratio-1)*100:+.1f}%)\")\n                print(f\"     Model is healthy\")\n\n        print(\"\\n[STEP 9/9] Final validation...\")\n        try:\n            with torch.no_grad():\n                final_enc_emb = encoder_embedding_layer.weight\n                final_dec_emb = decoder_embedding_layer.weight\n\n                print(f\"  Encoder: [{final_enc_emb.min().item():.2f}, {final_enc_emb.max().item():.2f}]\")\n                print(f\"  Decoder: [{final_dec_emb.min().item():.2f}, {final_dec_emb.max().item():.2f}]\")\n\n                if is_scaled_embeddings:\n                    print(f\"  ‚ÑπÔ∏è  Using SCALED encoder embeddings (architectural feature)\")\n\n                test_dec_ids = torch.tensor([[0, 1, 2, 3, 4]], dtype=torch.long).to(device_for_init)\n                dec_test = decoder_embedding_layer(test_dec_ids)\n\n                if torch.isnan(dec_test).any() or torch.isinf(dec_test).any():\n                    print(f\"  ‚ùå Decoder produces NaN/Inf outputs!\")\n                else:\n                    print(f\"  ‚úÖ Both embeddings produce valid outputs\")\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Final validation error: {type(e).__name__}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"MODEL INITIALIZATION COMPLETE\")\n        print(\"=\" * 80)\n\n        tokenizer_vocab_size = len(self.tokenizer) if hasattr(self.tokenizer, \"__len__\") else getattr(self.tokenizer, \"vocab_size\", 32100)\n        model_vocab_size = int(getattr(self.t5.config, \"vocab_size\", 32128))\n\n        self.tokenizer_vocab_size = int(tokenizer_vocab_size)\n\n        if tokenizer_vocab_size != model_vocab_size:\n            print(f\"\\n[VOCAB] ‚ö†Ô∏è  Vocab size mismatch detected!\")\n            print(f\"  Tokenizer: {tokenizer_vocab_size}\")\n            print(f\"  Model: {model_vocab_size}\")\n\n            if tokenizer_vocab_size < model_vocab_size:\n                print(f\"[VOCAB] ‚úÖ Using model's full vocab size: {model_vocab_size}\")\n                print(f\"[VOCAB] Note: Model has {model_vocab_size - tokenizer_vocab_size} extra embeddings\")\n                print(f\"[VOCAB] ‚úÖ Preserving pretrained weights - NO RESIZE\")\n                self.vocab_size = model_vocab_size\n            else:\n                print(f\"[VOCAB] ‚ùå ERROR: Tokenizer vocab ({tokenizer_vocab_size}) > Model vocab ({model_vocab_size})\")\n                raise RuntimeError(\n                    f\"Tokenizer has more tokens than model!\\n\"\n                    f\"  Tokenizer: {tokenizer_vocab_size}\\n\"\n                    f\"  Model: {model_vocab_size}\"\n                )\n        else:\n            print(f\"\\n[VOCAB] ‚úÖ Vocab sizes match: {model_vocab_size}\")\n            self.vocab_size = model_vocab_size\n\n        try:\n            pad_token_id = getattr(self.tokenizer, 'pad_token_id', 0)\n            self.pad_token_id = int(pad_token_id)\n        except Exception as e:\n            raise RuntimeError(f\"Token setup failed: {e}\")\n\n        try:\n            if _USE_GC and hasattr(self.t5, \"gradient_checkpointing_enable\"):\n                self.t5.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = int(getattr(self.t5.config, \"d_model\", 768))\n\n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            try:\n                self.dscd = dscd_cls(\n                    embed_dim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffer_size=_DSCD_BUFFER_SIZE,\n                    max_protos=_DSCD_MAX_PROTOS,\n                    n_min=_DSCD_N_MIN,\n                    language=_SOURCE_LANGUAGE,\n                    dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                    enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                    max_clustering_points=500,\n                    max_candidates_per_step=1,\n                )\n                dscd_embed_dim = getattr(self.dscd, \"embed_dim\", None)\n                if dscd_embed_dim is not None and dscd_embed_dim != embed_dim:\n                    raise RuntimeError(\n                        f\"DSCD embed_dim mismatch! Expected {embed_dim}, got {dscd_embed_dim}\"\n                    )\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to instantiate MemoryEfficientDSCDOnline: {e}\"\n                )\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found in globals()\")\n\n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(\n                    embed_dim, tokenizer, language=_SOURCE_LANGUAGE\n                )\n                asbn_embed_dim = getattr(self.asbn, \"embed_dim\", None)\n                if asbn_embed_dim is not None and asbn_embed_dim != embed_dim:\n                    raise RuntimeError(\n                        f\"ASBN embed_dim mismatch! Expected {embed_dim}, got {asbn_embed_dim}\"\n                    )\n            except Exception as e:\n                print(f\"[TATN-INIT] ASBN init failed: {e}, using stub\")\n                self.asbn = self._build_stub_asbn()\n        else:\n            self.asbn = self._build_stub_asbn()\n\n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg = trg_cls(\n                    embed_dim,\n                    tokenizer,\n                    language=_SOURCE_LANGUAGE,\n                    dscd_module=self.dscd,\n                )\n            except Exception as e:\n                print(f\"[TATN-INIT] TRG init failed: {e}, using stub\")\n                self.trg = self._build_stub_trg()\n        else:\n            self.trg = self._build_stub_trg()\n\n        label_smoothing_cls = globals().get(\"LabelSmoothingLoss\", None)\n        if callable(label_smoothing_cls) and _LABEL_SMOOTHING_EPS > 0:\n            try:\n                self.label_smoothing_loss = label_smoothing_cls(\n                    num_classes=self.vocab_size,\n                    smoothing=_LABEL_SMOOTHING_EPS,\n                    ignore_index=-100\n                )\n            except Exception as e:\n                print(f\"[TATN-INIT] LabelSmoothingLoss init failed: {e}, using None\")\n                self.label_smoothing_loss = None\n        else:\n            self.label_smoothing_loss = None\n\n        rdrop_cls = globals().get(\"RDropLoss\", None)\n        if callable(rdrop_cls) and _USE_RDROP and _RDROP_ALPHA > 0:\n            try:\n                self.rdrop_loss = rdrop_cls(alpha=_RDROP_ALPHA)\n            except Exception as e:\n                print(f\"[TATN-INIT] RDropLoss init failed: {e}, using None\")\n                self.rdrop_loss = None\n        else:\n            self.rdrop_loss = None\n\n        actual_embed_dim = encoder_embedding_layer.embedding_dim\n        if actual_embed_dim != embed_dim:\n            raise RuntimeError(\n                f\"Embedding dimension mismatch! Config says {embed_dim}, \"\n                f\"but embedding layer has {actual_embed_dim}\"\n            )\n\n        print(\"\\n[TATN-INIT] ‚úÖ DUAL-PATH MemoryOptimizedTATNWithExplanations READY\")\n        print(f\"  - Model: csebuetnlp/banglat5\")\n        print(f\"  - Vocab size: {self.vocab_size}\")\n        print(f\"  - Embed dim: {embed_dim}\")\n        if is_scaled_embeddings:\n            print(f\"  - Encoder: SCALED embeddings (factor: {scale_factor:.2f})\")\n        if self.lora_applied:\n            trainable_params = sum(p.numel() for p in self.t5.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in self.t5.parameters())\n            print(f\"  - LoRA: ENABLED (r={_LORA_RANK}, alpha={_LORA_ALPHA})\")\n            print(f\"  - Trainable: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n        else:\n            print(f\"  - LoRA: DISABLED (fallback to full fine-tuning)\")\n            trainable_params = sum(p.numel() for p in self.t5.parameters() if p.requires_grad)\n            print(f\"  - Trainable: {trainable_params:,} (100.00%)\")\n        print(f\"  - Capitalization: ENABLED (first alphabetic char auto-capitalized)\")\n        print(\"=\" * 80 + \"\\n\")\n\n    def _build_stub_asbn(self):\n        class _StubASBN(nn.Module):\n            def forward(self, h, **kwargs):\n                dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                zero = torch.tensor(0.0, device=dev, requires_grad=True)\n                return h, {\n                    \"encoder_loss\": zero,\n                    \"adversarial_loss\": zero,\n                    \"domain_loss\": zero,\n                    \"domain_accuracy\": zero,\n                }\n            def critic_parameters(self):\n                return []\n            def reset_stats(self):\n                pass\n            def get_detailed_stats(self):\n                return {\n                    \"domain_loss\": 0.0,\n                    \"domain_accuracy\": 0.0,\n                    \"source_accuracy\": 0.0,\n                    \"target_accuracy\": 0.0,\n                    \"asbn_loss\": 0.0,\n                    \"num_updates\": 0,\n                }\n            def get_asbn_stats(self):\n                return self.get_detailed_stats()\n        return _StubASBN()\n\n    def _build_stub_trg(self):\n        class _StubTRG:\n            def process_sentence_for_explanations(self, *args, **kwargs):\n                return []\n            def get_statistics(self):\n                return {}\n            def reset_statistics(self):\n                pass\n        return _StubTRG()\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(\n        proto_probs_list, gates_list=None, min_gate: float = 0.01\n    ) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n        if dev is None:\n            return torch.tensor(0.0)\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    try:\n                        if float(gl[j]) < min_gate:\n                            continue\n                    except Exception:\n                        pass\n                try:\n                    p = torch.clamp(probs.to(dev).float(), 1e-8, 1.0)\n                    H = -torch.sum(p * torch.log(p))\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n\n    def _extract_word_embeddings(\n        self,\n        src_texts: List[str],\n        device: torch.device,\n        embed_dim: int,\n    ) -> Tuple[torch.Tensor, List[Dict[int, str]], List[List[str]]]:\n        batch_size = len(src_texts)\n        word_embeddings_batch = []\n        token_word_map_batch = []\n        words_batch = []\n        max_words = 0\n        try:\n            embedding_layer = self.t5.get_input_embeddings()\n        except Exception:\n            fallback_embs = torch.zeros(batch_size, 1, embed_dim, device=device)\n            fallback_maps = [{0: \"UNK\"} for _ in range(batch_size)]\n            fallback_words = [[\"UNK\"] for _ in range(batch_size)]\n            return fallback_embs, fallback_maps, fallback_words\n        for batch_idx, text in enumerate(src_texts):\n            if not text or not isinstance(text, str):\n                text = \"UNK\"\n            text = text.strip()\n            if not text:\n                text = \"UNK\"\n            words = _extract_words_from_text(text)\n            if not words or len(words) == 0:\n                words = [\"UNK\"]\n            words_batch.append(words)\n            word_embeddings = []\n            word_map = {}\n            for idx, word in enumerate(words):\n                try:\n                    if not word or len(word) == 0:\n                        word = \"UNK\"\n                    word_ids = self.tokenizer.encode(word, add_special_tokens=False)\n                    if not word_ids or len(word_ids) == 0:\n                        word_ids = [3]\n                    word_ids = [wid for wid in word_ids if 0 <= wid < self.vocab_size]\n                    if not word_ids:\n                        word_ids = [3]\n                    word_ids_tensor = torch.tensor([word_ids], dtype=torch.long, device=device)\n                    subword_embs = embedding_layer(word_ids_tensor)\n                    word_emb = subword_embs.mean(dim=1).squeeze(0)\n                    if torch.isnan(word_emb).any() or torch.isinf(word_emb).any():\n                        word_emb = torch.zeros(embed_dim, device=device)\n                    word_embeddings.append(word_emb)\n                    word_map[idx] = word\n                except Exception as e:\n                    fallback_emb = torch.zeros(embed_dim, device=device)\n                    word_embeddings.append(fallback_emb)\n                    word_map[idx] = word if word else \"UNK\"\n            if word_embeddings and len(word_embeddings) > 0:\n                try:\n                    word_embs_tensor = torch.stack(word_embeddings, dim=0)\n                    word_embeddings_batch.append(word_embs_tensor)\n                    token_word_map_batch.append(word_map)\n                    max_words = max(max_words, len(word_embeddings))\n                except Exception:\n                    fallback_emb = torch.zeros(1, embed_dim, device=device)\n                    word_embeddings_batch.append(fallback_emb)\n                    token_word_map_batch.append({0: \"UNK\"})\n                    max_words = max(max_words, 1)\n            else:\n                fallback_emb = torch.zeros(1, embed_dim, device=device)\n                word_embeddings_batch.append(fallback_emb)\n                token_word_map_batch.append({0: \"UNK\"})\n                max_words = max(max_words, 1)\n        if max_words == 0:\n            max_words = 1\n        try:\n            padded_word_embs = torch.zeros(batch_size, max_words, embed_dim, device=device)\n            for i, word_embs in enumerate(word_embeddings_batch):\n                try:\n                    length = word_embs.size(0)\n                    if length > max_words:\n                        length = max_words\n                    padded_word_embs[i, :length] = word_embs[:length]\n                except Exception:\n                    pass\n        except Exception:\n            padded_word_embs = torch.zeros(batch_size, 1, embed_dim, device=device)\n        return padded_word_embs, token_word_map_batch, words_batch\n\n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> List[dict]:\n        if token_word_map is not None and len(token_word_map) == batch_size:\n            valid_count = sum(\n                1 for m in token_word_map if isinstance(m, dict) and len(m) > 0\n            )\n            if valid_count == batch_size:\n                if _DEBUG_DISCOVERY:\n                    total_words = sum(len(m) for m in token_word_map)\n                    print(\n                        f\"[TATN-WORDMAP] Using provided word maps: {total_words} words\"\n                    )\n                return token_word_map\n        word_maps_batch: List[dict] = []\n        for b in range(batch_size):\n            try:\n                ids_b = input_ids[b].detach().cpu().tolist()\n                tokens = self.tokenizer.convert_ids_to_tokens(ids_b)\n                wm = build_token_word_map_sentencepiece(tokens, fallback=True)\n                if wm:\n                    word_maps_batch.append(wm)\n                else:\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n            except Exception:\n                word_maps_batch.append(\n                    {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                )\n        total_words = sum(len(m) for m in word_maps_batch)\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructed {total_words} words\")\n        return word_maps_batch\n\n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None,\n    ) -> Optional[torch.Tensor]:\n        if not _USE_DOMAIN_LABELS:\n            return None\n        try:\n            if self.training:\n                return torch.full(\n                    (batch_size,),\n                    _TRAIN_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n            else:\n                return torch.full(\n                    (batch_size,),\n                    _TEST_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n        except Exception:\n            return None\n\n    @staticmethod\n    def _safe_take_key_static(\n        dscd_struct: Dict[str, Any],\n        key: str,\n        b_index: int,\n        seq_len: int,\n        device: torch.device,\n    ):\n        if key == \"proto_probs\":\n            out = [\n                torch.tensor([1.0], dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n        else:\n            out = [\n                torch.tensor(0.0, dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n        try:\n            val = dscd_struct.get(key, None)\n            if val is None:\n                return out\n            if key == \"proto_probs\":\n                if isinstance(val, list) and len(val) > b_index:\n                    row = val[b_index]\n                    if isinstance(row, list):\n                        for t in range(min(seq_len, len(row))):\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.detach().to(device)\n                            else:\n                                try:\n                                    out[t] = torch.as_tensor(\n                                        v,\n                                        dtype=torch.float32,\n                                        device=device,\n                                    ).flatten()\n                                except Exception:\n                                    pass\n                return out\n            if isinstance(val, list) and len(val) > b_index:\n                row = val[b_index]\n                if isinstance(row, list):\n                    for t in range(min(seq_len, len(row))):\n                        v = row[t]\n                        try:\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.detach().to(device)\n                            else:\n                                out[t] = torch.tensor(\n                                    float(v), device=device\n                                )\n                        except Exception:\n                            pass\n                elif isinstance(row, torch.Tensor):\n                    if row.dim() == 1:\n                        for t in range(min(seq_len, int(row.size(0)))):\n                            try:\n                                out[t] = torch.tensor(\n                                    float(row[t].item()), device=device\n                                )\n                            except Exception:\n                                pass\n                return out\n            if isinstance(val, torch.Tensor):\n                if val.dim() >= 2 and int(val.size(0)) > b_index:\n                    for t in range(min(seq_len, int(val.size(1)))):\n                        try:\n                            if val.dim() == 3:\n                                v = val[b_index, t]\n                                if v.numel() == 1:\n                                    out[t] = torch.tensor(\n                                        float(v.item()), device=device\n                                    )\n                                else:\n                                    out[t] = v.detach().to(device)\n                            else:\n                                v = val[b_index, t]\n                                out[t] = torch.tensor(\n                                    float(v.item()), device=device\n                                )\n                        except Exception:\n                            pass\n        except Exception:\n            pass\n        return out\n\n    def forward_path1(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        domain_labels: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n        embed_dim = int(getattr(self.t5.config, \"d_model\", 768))\n        if src_texts is None or not isinstance(src_texts, list) or len(src_texts) != batch_size:\n            src_texts_extracted = []\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    text = self.tokenizer.decode(ids_b, skip_special_tokens=True)\n                    if not text or not text.strip():\n                        text = \"UNK\"\n                    src_texts_extracted.append(text.strip())\n                except Exception:\n                    src_texts_extracted.append(\"UNK\")\n            src_texts = src_texts_extracted\n        for i in range(len(src_texts)):\n            if not src_texts[i] or not isinstance(src_texts[i], str) or not src_texts[i].strip():\n                src_texts[i] = \"UNK\"\n        try:\n            h, token_word_map, words_batch = self._extract_word_embeddings(\n                src_texts, device, embed_dim\n            )\n        except Exception as e:\n            h = torch.zeros(batch_size, 1, embed_dim, device=device)\n            token_word_map = [{0: \"UNK\"} for _ in range(batch_size)]\n            words_batch = [[\"UNK\"] for _ in range(batch_size)]\n        max_words = h.size(1)\n        try:\n            h_detached = h.detach()\n            raw_dscd = self.dscd.forward(\n                h_detached,\n                token_types=None,\n                train_mode=self.training,\n                input_ids=None,\n                attention_mask=None,\n                token_word_map=token_word_map,\n            )\n        except Exception as e:\n            raw_dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [\n                    [\n                        torch.tensor([1.0], dtype=torch.float32, device=device)\n                        for _ in range(max_words)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [torch.tensor(0.0, device=device) for _ in range(max_words)]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [torch.tensor(0.0, device=device) for _ in range(max_words)]\n                    for _ in range(batch_size)\n                ],\n            }\n        try:\n            dscd = _normalize_dscd_outputs(\n                raw_dscd, batch_size, max_words, device, embed_dim, fallback_h=h\n            )\n        except Exception:\n            dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [\n                    [torch.tensor([1.0], device=device) for _ in range(max_words)]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [torch.tensor(0.0, device=device) for _ in range(max_words)]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [torch.tensor(0.0, device=device) for _ in range(max_words)]\n                    for _ in range(batch_size)\n                ],\n                \"proto_assignments\": [\n                    torch.zeros(max_words, dtype=torch.long, device=device)\n                    for _ in range(batch_size)\n                ],\n            }\n        h_aug = dscd.get(\"h_augmented\", h)\n        if domain_labels is None:\n            domain_labels = self._extract_domain_labels(batch_size=batch_size, device=device, src_texts=src_texts)\n        asbn_loss = torch.zeros(1, device=device, requires_grad=True)\n        if self.training and _ENABLE_ASBN_TRAINING and domain_labels is not None:\n            try:\n                h_asbn, asbn_losses = self.asbn.forward(\n                    h_aug,\n                    proto_probs=dscd.get(\"proto_probs\", None),\n                    uncertainties=dscd.get(\"uncertainties\", None),\n                    gates=dscd.get(\"gates\", None),\n                    token_word_map=token_word_map,\n                    domain_labels=domain_labels,\n                    global_step=current_step,\n                )\n                if isinstance(asbn_losses, dict):\n                    encoder_loss = asbn_losses.get(\"encoder_loss\", torch.zeros(1, device=device, requires_grad=True))\n                    if isinstance(encoder_loss, torch.Tensor):\n                        if torch.isfinite(encoder_loss):\n                            if encoder_loss.requires_grad:\n                                asbn_loss = encoder_loss\n                            else:\n                                asbn_loss = torch.tensor(float(encoder_loss.item()), device=device, requires_grad=True)\n                        else:\n                            asbn_loss = torch.zeros(1, device=device, requires_grad=True)\n            except Exception as e:\n                asbn_loss = torch.zeros(1, device=device, requires_grad=True)\n        dscd_reg = torch.zeros(1, device=device, requires_grad=True)\n        try:\n            dscd_reg_raw = self._entropy_reg_from_proto_probs_static(\n                dscd.get('proto_probs', []),\n                gates_list=dscd.get('gates', []),\n                min_gate=0.01,\n            )\n            if isinstance(dscd_reg_raw, torch.Tensor):\n                if torch.isfinite(dscd_reg_raw):\n                    if dscd_reg_raw.requires_grad:\n                        dscd_reg = torch.clamp(dscd_reg_raw.to(device), 0.0, 5.0)\n                    else:\n                        dscd_reg = torch.tensor(float(dscd_reg_raw.item()), device=device, requires_grad=True)\n                        dscd_reg = torch.clamp(dscd_reg, 0.0, 5.0)\n        except Exception:\n            dscd_reg = torch.zeros(1, device=device, requires_grad=True)\n        total_loss = _LAMBDA_ASBN * asbn_loss + _LAMBDA_DSCD * dscd_reg\n        if not isinstance(total_loss, torch.Tensor):\n            total_loss = torch.tensor(float(total_loss), device=device, requires_grad=True)\n        if not torch.isfinite(total_loss):\n            total_loss = torch.tensor(0.01, device=device, requires_grad=True)\n        if not total_loss.requires_grad:\n            total_loss = torch.tensor(float(total_loss.item()), device=device, requires_grad=True)\n        return total_loss\n\n    def forward_path2(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        labels: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        use_rdrop: bool = False,\n    ) -> torch.Tensor:\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n\n        if input_ids is None or attention_mask is None or labels is None:\n            raise ValueError(\"input_ids, attention_mask, and labels cannot be None\")\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        model_vocab_limit = getattr(self, \"vocab_size\", MODEL_VOCAB_SIZE)\n\n        if torch.any(input_ids >= model_vocab_limit) or torch.any(input_ids < 0):\n            invalid_count = torch.sum((input_ids >= model_vocab_limit) | (input_ids < 0)).item()\n            max_id = torch.max(input_ids).item()\n            if invalid_count > 0:\n                print(f\"[PATH2-EMERGENCY] Step {current_step}: input_ids out of bounds!\")\n                print(f\"  Count: {invalid_count}, Max: {max_id}, Limit: {model_vocab_limit-1}\")\n            input_ids = torch.clamp(input_ids, 0, model_vocab_limit - 1)\n\n        if torch.isnan(input_ids.float()).any() or torch.isinf(input_ids.float()).any():\n            print(f\"[PATH2-ERROR] NaN/Inf in input_ids! Replacing with pad tokens...\")\n            input_ids = torch.where(\n                torch.isnan(input_ids.float()) | torch.isinf(input_ids.float()),\n                torch.tensor(self.pad_token_id, dtype=input_ids.dtype, device=device),\n                input_ids\n            )\n\n        if torch.isnan(attention_mask.float()).any() or torch.isinf(attention_mask.float()).any():\n            print(f\"[PATH2-ERROR] NaN/Inf in attention_mask! Resetting to ones...\")\n            attention_mask = torch.ones_like(attention_mask)\n\n        valid_labels = (labels != -100)\n        if valid_labels.any():\n            label_min = labels[valid_labels].min().item()\n            label_max = labels[valid_labels].max().item()\n\n            if label_min < 0 or label_max >= model_vocab_limit:\n                invalid_label_count = torch.sum((labels[valid_labels] < 0) | (labels[valid_labels] >= model_vocab_limit)).item()\n                print(f\"[PATH2-EMERGENCY] Step {current_step}: labels out of bounds!\")\n                print(f\"  Range: [{label_min}, {label_max}], Limit: [0, {model_vocab_limit-1}]\")\n                print(f\"  Invalid count: {invalid_label_count}/{valid_labels.sum().item()}\")\n\n                labels = torch.where(\n                    valid_labels,\n                    torch.clamp(labels, 0, model_vocab_limit - 1),\n                    torch.tensor(-100, dtype=labels.dtype, device=device)\n                )\n\n                label_min = labels[valid_labels].min().item()\n                label_max = labels[valid_labels].max().item()\n                print(f\"  After clamping: [{label_min}, {label_max}]\")\n\n        if torch.isnan(labels.float()).any() or torch.isinf(labels.float()).any():\n            print(f\"[PATH2-ERROR] NaN/Inf in labels! Replacing with -100...\")\n            labels = torch.where(\n                torch.isnan(labels.float()) | torch.isinf(labels.float()),\n                torch.tensor(-100, dtype=labels.dtype, device=device),\n                labels\n            )\n\n        try:\n            with torch.cuda.amp.autocast(enabled=False):\n                input_ids = input_ids.float().long()\n                attention_mask = attention_mask.float().long()\n                labels = labels.float().long()\n\n                t5_outputs = self.t5(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    return_dict=True,\n                )\n\n                translation_loss = t5_outputs.loss\n\n                if translation_loss is None:\n                    print(f\"[PATH2-ERROR] T5 returned None loss!\")\n                    return torch.tensor(10.0, device=device, requires_grad=True)\n\n                if not isinstance(translation_loss, torch.Tensor):\n                    return torch.tensor(10.0, device=device, requires_grad=True)\n\n                if torch.isnan(translation_loss) or torch.isinf(translation_loss):\n                    print(f\"[PATH2-ERROR] T5 produced NaN/Inf loss!\")\n                    return torch.tensor(10.0, device=device, requires_grad=True)\n\n                if not translation_loss.requires_grad:\n                    translation_loss = torch.tensor(\n                        translation_loss.item(),\n                        device=device,\n                        dtype=torch.float32,\n                        requires_grad=True\n                    )\n\n                translation_loss = torch.clamp(translation_loss, 0.0, 100.0)\n\n        except Exception as e:\n            print(f\"[PATH2-ERROR] T5 forward failed: {type(e).__name__}: {str(e)[:200]}\")\n            return torch.tensor(10.0, device=device, requires_grad=True)\n\n        total_loss = translation_loss\n\n        if not torch.isfinite(total_loss):\n            print(f\"[PATH2-ERROR] total_loss is NaN/Inf: {total_loss}, returning fallback\")\n            return torch.tensor(10.0, device=device, requires_grad=True)\n\n        return total_loss\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = False,\n        return_dict: bool = True,\n        domain_labels: Optional[torch.Tensor] = None,\n        path: Optional[int] = None,\n        use_rdrop: bool = False,\n        **kwargs\n    ):\n        if path == 1:\n            return self.forward_path1(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                src_texts=src_texts,\n                token_word_map=token_word_map,\n                domain_labels=domain_labels,\n            )\n        elif path == 2:\n            if labels is not None and self.training:\n                return self.forward_path2(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    src_texts=src_texts,\n                    token_word_map=token_word_map,\n                    use_rdrop=use_rdrop,\n                )\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n        if torch.any(input_ids >= self.vocab_size) or torch.any(input_ids < 0):\n            input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n        if (\n            torch.cuda.is_available()\n            and _MEMORY_CLEANUP_FREQUENCY > 0\n            and current_step % _MEMORY_CLEANUP_FREQUENCY == 0\n        ):\n            for i in range(min(_NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            if gc.isenabled():\n                gc.collect()\n        if self.training and _DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if (\n                current_step - self.last_discovery_step\n                >= _PERIODIC_DISCOVERY_FREQUENCY\n            ):\n                try:\n                    self.dscd.periodic_discovery_check(\n                        global_step=current_step,\n                        frequency=_PERIODIC_DISCOVERY_FREQUENCY,\n                        cluster_missing=False,\n                    )\n                    self.last_discovery_step = current_step\n                except Exception:\n                    pass\n        if src_texts is None or not isinstance(src_texts, list) or len(src_texts) != batch_size:\n            src_texts_extracted = []\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    text = self.tokenizer.decode(ids_b, skip_special_tokens=True)\n                    if not text or not text.strip():\n                        text = \"UNK\"\n                    src_texts_extracted.append(text.strip())\n                except Exception:\n                    src_texts_extracted.append(\"UNK\")\n            src_texts = src_texts_extracted\n        try:\n            encoder_outputs_raw = self.t5.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True,\n            )\n        except Exception as e:\n            raise\n        h_enc = _safe_get_last_hidden_state(encoder_outputs_raw)\n        if h_enc is None:\n            h_enc = torch.zeros(batch_size, seq_len, int(getattr(self.t5.config, \"d_model\", 768)), device=device)\n        word_maps = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n        dscd_outputs = {}\n        if use_dscd:\n            try:\n                dscd_raw = self.dscd.forward(\n                    h_enc.detach(),\n                    token_types=None,\n                    train_mode=self.training,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_word_map=word_maps,\n                )\n                dscd_outputs = _normalize_dscd_outputs(\n                    dscd_raw,\n                    batch_size,\n                    seq_len,\n                    device,\n                    int(getattr(self.t5.config, \"d_model\", 768)),\n                    fallback_h=h_enc,\n                )\n            except Exception:\n                dscd_outputs = _normalize_dscd_outputs(\n                    {},\n                    batch_size,\n                    seq_len,\n                    device,\n                    int(getattr(self.t5.config, \"d_model\", 768)),\n                    fallback_h=h_enc,\n                )\n        h_aug = dscd_outputs.get(\"h_augmented\", h_enc)\n        if use_asbn and _ENABLE_ASBN_INFERENCE:\n            try:\n                h_aug, _ = self.asbn.forward(\n                    h_aug,\n                    proto_probs=dscd_outputs.get(\"proto_probs\", None),\n                    uncertainties=dscd_outputs.get(\"uncertainties\", None),\n                    gates=dscd_outputs.get(\"gates\", None),\n                    token_word_map=word_maps,\n                    domain_labels=None,\n                    global_step=current_step,\n                )\n            except Exception:\n                pass\n        try:\n            encoder_outputs_wrapped = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=getattr(encoder_outputs_raw, \"hidden_states\", None),\n                attentions=getattr(encoder_outputs_raw, \"attentions\", None),\n            )\n        except Exception:\n            encoder_outputs_wrapped = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=None,\n                attentions=None,\n            )\n        if return_dict:\n            return {\n                \"encoder_last_hidden_state\": h_enc,\n                \"sense_augmented_embeddings\": h_aug,\n                \"dscd_outputs\": dscd_outputs,\n            }\n        else:\n            return encoder_outputs_wrapped\n\n    def generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[Any] = None,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        max_length: Optional[int] = None,\n        num_beams: Optional[int] = None,\n        early_stopping: bool = True,\n        use_dscd: bool = True,\n        use_asbn: bool = False,\n        return_text: bool = True,\n        **kwargs\n    ):\n        if encoder_outputs is None:\n            if input_ids is None:\n                raise ValueError(\"Either input_ids or encoder_outputs must be provided\")\n            if attention_mask is None:\n                attention_mask = (input_ids != self.pad_token_id).long()\n            forward_outputs = self.forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                src_texts=src_texts,\n                token_word_map=token_word_map,\n                use_dscd=use_dscd,\n                use_asbn=use_asbn,\n                return_dict=True,\n            )\n            h_aug = forward_outputs.get(\"sense_augmented_embeddings\")\n            if h_aug is None:\n                h_aug = forward_outputs.get(\"encoder_last_hidden_state\")\n            try:\n                encoder_outputs = BaseModelOutput(\n                    last_hidden_state=h_aug,\n                    hidden_states=None,\n                    attentions=None,\n                )\n            except Exception:\n                encoder_outputs = h_aug\n        try:\n            generated = self.t5.generate(\n                input_ids=None,\n                attention_mask=attention_mask,\n                encoder_outputs=encoder_outputs,\n                max_length=max_length if max_length is not None else 64,\n                min_length=5,\n                num_beams=num_beams if num_beams is not None else 4,\n                early_stopping=early_stopping,\n                no_repeat_ngram_size=3,\n                length_penalty=0.8,\n                repetition_penalty=1.2,\n                **kwargs\n            )\n\n            if return_text and isinstance(generated, torch.Tensor):\n                translations = []\n                batch_size = generated.size(0)\n\n                for i in range(batch_size):\n                    gen_ids = generated[i]\n                    text = self.tokenizer.decode(gen_ids, skip_special_tokens=True)\n                    text = _clean_decoded_text(text)\n                    translations.append(text)\n\n                if len(translations) == 1:\n                    return translations[0]\n                else:\n                    return translations\n\n            return generated\n        except Exception as e:\n            raise\n\n\nprint(\"=\" * 80)\nprint(\"Cell 6: DUAL-PATH TATN Model - BanglaT5 [‚úÖ COMPLETE]\")\nprint(\"=\" * 80)\nprint(\"‚úÖ LoRA initialization aligned with Cell -1 and Cell 0\")\nprint(\"‚úÖ Skips quantization on Kaggle CUDA 12.6 (uses standard FP16 LoRA)\")\nprint(\"‚úÖ Comprehensive validation (0 trainable params / all trainable detection)\")\nprint(\"‚úÖ Clear error messages with actionable fix instructions\")\nprint(\"‚úÖ No fallback to full fine-tuning (raises error if LoRA fails)\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"KZbMDpIYH4J4","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.527438Z","iopub.execute_input":"2026-02-16T03:01:26.527631Z","iopub.status.idle":"2026-02-16T03:01:26.686320Z","shell.execute_reply.started":"2026-02-16T03:01:26.527615Z","shell.execute_reply":"2026-02-16T03:01:26.685788Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 6: DUAL-PATH TATN Model - BanglaT5 [‚úÖ COMPLETE]\n================================================================================\n‚úÖ LoRA initialization aligned with Cell -1 and Cell 0\n‚úÖ Skips quantization on Kaggle CUDA 12.6 (uses standard FP16 LoRA)\n‚úÖ Comprehensive validation (0 trainable params / all trainable detection)\n‚úÖ Clear error messages with actionable fix instructions\n‚úÖ No fallback to full fine-tuning (raises error if LoRA fails)\n================================================================================\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 7: DUAL-PATH TRAINING LOOP - BanglaT5 + Standard LoRA (FP16) [GRADSCALER FIXED]\n# ===========================================================================================\n\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_GRADIENTS = bool(DEBUG_GRADIENTS)\nexcept (NameError, TypeError):\n    _DEBUG_GRADIENTS = False\n\nDEBUG_PRINT_INTERVAL = 400\nGRADIENT_DIAGNOSTIC_INTERVAL = 100\n_cell7_dbg_counts = defaultdict(int)\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept (NameError, ValueError, TypeError):\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept (NameError, ValueError, TypeError):\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept (NameError, ValueError, TypeError):\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept (NameError, ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept (NameError, TypeError):\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept (NameError, ValueError, TypeError):\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _PERIODIC_DISCOVERY_FREQUENCY = 200\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _LAMBDA_TRG = float(LAMBDA_TRG)\nexcept (NameError, ValueError, TypeError):\n    _LAMBDA_TRG = 0.15\n\ntry:\n    _WARMUP_STEPS = int(WARMUP_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _WARMUP_STEPS = 200\n\ntry:\n    _USE_LR_SCHEDULER = bool(USE_LR_SCHEDULER)\nexcept (NameError, TypeError):\n    _USE_LR_SCHEDULER = True\n\ntry:\n    _USE_DUAL_PATH_TRAINING = bool(USE_DUAL_PATH_TRAINING)\nexcept (NameError, TypeError):\n    _USE_DUAL_PATH_TRAINING = True\n\ntry:\n    _USE_LORA = bool(USE_LORA)\nexcept (NameError, TypeError):\n    _USE_LORA = False\n\ntry:\n    _LORA_RANK = int(LORA_RANK)\nexcept (NameError, ValueError, TypeError):\n    _LORA_RANK = 32\n\ntry:\n    _LORA_ALPHA = float(LORA_ALPHA)\nexcept (NameError, ValueError, TypeError):\n    _LORA_ALPHA = 64.0\n\ntry:\n    _LR_NMT = float(LR_NMT)\nexcept (NameError, ValueError, TypeError):\n    _LR_NMT = 5e-4\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\", \"‡¶¨‡¶æ‡¶∞\", \"‡¶π‡¶æ‡¶∞\", \"‡¶§‡¶æ‡¶∞‡¶æ\",\n        \"‡¶™‡¶æ‡¶®‡¶ø\", \"‡¶¶‡¶≤\", \"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞\", \"‡¶®‡¶æ‡¶Æ\", \"‡¶ï‡¶•‡¶æ\", \"‡¶¨‡¶á\", \"‡¶ò‡¶∞\", \"‡¶Æ‡¶®\", \"‡¶π‡¶æ‡¶§\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n_BENGALI_PUNCT_SET = set(['‡•§', '‡••'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\ndef _is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    clean = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").replace(\"</w>\", \"\").strip()\n    if not clean:\n        return False\n    if clean in _BENGALI_PUNCT_SET or clean in _COMMON_PUNCT_SET:\n        return True\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    return all(c in _PUNCT_SET for c in clean)\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\ndef compute_gradient_diagnostics(model: torch.nn.Module, step: int, prefix: str = \"\") -> Dict[str, Any]:\n    diagnostics = {\n        'total_norm': 0.0,\n        'max_grad': 0.0,\n        'min_grad': 0.0,\n        'mean_grad': 0.0,\n        'num_params': 0,\n        'num_nan': 0,\n        'num_inf': 0,\n        'layer_stats': {},\n        'lora_stats': {},\n    }\n    try:\n        total_norm_sq = 0.0\n        all_grads = []\n        layer_norms = {}\n        lora_norms = {}\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                diagnostics['num_params'] += 1\n                grad_data = param.grad.data\n                if torch.isnan(grad_data).any():\n                    diagnostics['num_nan'] += 1\n                if torch.isinf(grad_data).any():\n                    diagnostics['num_inf'] += 1\n                param_norm = grad_data.norm(2).item()\n                total_norm_sq += param_norm ** 2\n                all_grads.extend(grad_data.flatten().cpu().tolist())\n                is_lora_param = 'lora_' in name.lower() or '.lora_' in name\n                if is_lora_param:\n                    lora_layer_name = name.split('.lora_')[0] if '.lora_' in name else name\n                    if lora_layer_name not in lora_norms:\n                        lora_norms[lora_layer_name] = []\n                    lora_norms[lora_layer_name].append(param_norm)\n                else:\n                    layer_name = name.split('.')[0] if '.' in name else name\n                    if layer_name not in layer_norms:\n                        layer_norms[layer_name] = []\n                    layer_norms[layer_name].append(param_norm)\n        if all_grads:\n            diagnostics['total_norm'] = math.sqrt(total_norm_sq)\n            diagnostics['max_grad'] = max(all_grads)\n            diagnostics['min_grad'] = min(all_grads)\n            diagnostics['mean_grad'] = sum(all_grads) / len(all_grads)\n        for layer_name, norms in layer_norms.items():\n            layer_total_norm = math.sqrt(sum(n**2 for n in norms))\n            diagnostics['layer_stats'][layer_name] = {'norm': layer_total_norm, 'count': len(norms)}\n        for lora_name, norms in lora_norms.items():\n            lora_total_norm = math.sqrt(sum(n**2 for n in norms))\n            diagnostics['lora_stats'][lora_name] = {'norm': lora_total_norm, 'count': len(norms)}\n    except Exception as e:\n        diagnostics['error'] = str(e)\n    return diagnostics\n\ndef print_gradient_diagnostics(diagnostics: Dict[str, Any], step: int, prefix: str = \"\"):\n    if not _DEBUG_GRADIENTS:\n        return\n    print(f\"\\n[GRAD-DIAG{prefix}] Step {step}:\")\n    print(f\"  Total Norm: {diagnostics['total_norm']:.6f}\")\n    print(f\"  Max Grad: {diagnostics['max_grad']:.6f}\")\n    print(f\"  Min Grad: {diagnostics['min_grad']:.6f}\")\n    print(f\"  Mean Grad: {diagnostics['mean_grad']:.6f}\")\n    print(f\"  Params with grad: {diagnostics['num_params']}\")\n    if diagnostics['num_nan'] > 0:\n        print(f\"  ‚ö†Ô∏è  NaN gradients: {diagnostics['num_nan']}\")\n    if diagnostics['num_inf'] > 0:\n        print(f\"  ‚ö†Ô∏è  Inf gradients: {diagnostics['num_inf']}\")\n    if diagnostics['total_norm'] < 1e-7:\n        print(f\"  ‚ö†Ô∏è  WARNING: Vanishing gradients detected!\")\n    elif diagnostics['total_norm'] > 100.0:\n        print(f\"  ‚ö†Ô∏è  WARNING: Exploding gradients detected!\")\n    if diagnostics['lora_stats']:\n        print(f\"\\n  LoRA Adapter Gradients:\")\n        sorted_lora = sorted(diagnostics['lora_stats'].items(), key=lambda x: x[1]['norm'], reverse=True)\n        for lora_name, stats in sorted_lora[:3]:\n            print(f\"    - {lora_name}: {stats['norm']:.6f} (LoRA)\")\n        if not any(s['norm'] > 1e-8 for s in diagnostics['lora_stats'].values()):\n            print(f\"  ‚ö†Ô∏è  WARNING: LoRA adapters not learning! (all norms ~0)\")\n    if diagnostics['layer_stats'] and (_VERBOSE_LOGGING or diagnostics['total_norm'] > 10.0):\n        print(f\"\\n  Layer-wise norms:\")\n        sorted_layers = sorted(diagnostics['layer_stats'].items(), key=lambda x: x[1]['norm'], reverse=True)\n        for layer_name, stats in sorted_layers[:5]:\n            print(f\"    - {layer_name}: {stats['norm']:.6f}\")\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n        if hasattr(dscd, 'get_discovered_homographs'):\n            discovered = dscd.get_discovered_homographs()\n            return set(w for w in discovered if not _is_punctuation_only(w))\n        homographs = set()\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        stores_snapshot = {}\n        if lock:\n            with lock:\n                stores_snapshot = dict(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = dict(dscd.prototype_stores.items())\n        for token, store in stores_snapshot.items():\n            try:\n                if store.size() >= 1:\n                    clean_token = str(token).replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').strip().lower()\n                    if clean_token and not _is_punctuation_only(clean_token):\n                        homographs.add(clean_token)\n            except Exception:\n                continue\n        return homographs\n    except Exception:\n        return set()\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module,\n    tokenizer,\n    epoch: int,\n    global_step: int,\n    source_lang: str,\n    target_lang: str,\n    max_length: int,\n    device: torch.device\n) -> Dict[str, Any]:\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n    if not isinstance(device, torch.device):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    dscd_homographs = _get_dscd_homographs(model)\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[VALIDATION] DSCD discovered homographs: {len(dscd_homographs)}\")\n        if dscd_homographs:\n            print(f\"[VALIDATION] Sample: {list(dscd_homographs)[:10]}\")\n    validation_results = {\n        'epoch': epoch,\n        'step': global_step,\n        'translations_success': 0,\n        'translations_failed': 0,\n        'explanations_generated': 0,\n        'dscd_homographs_explained': 0,\n        'reference_homographs_explained': 0,\n        'avg_explanation_confidence': 0.0,\n        'dscd_quality_score': 0.0,\n        'dscd_multi_sense_tokens': 0,\n        'dscd_total_prototypes': 0,\n        'asbn_domain_loss': 0.0,\n        'asbn_domain_accuracy': 0.0,\n        'asbn_source_accuracy': 0.0,\n        'asbn_target_accuracy': 0.0,\n        'trg_total_explanations': 0,\n        'validation_completed': False,\n    }\n    try:\n        core_model.eval()\n        val_sentences = [\n            (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap\", \"‡¶ï‡¶≤=tap/call\"),\n            (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"Tomorrow I will buy a book\", \"‡¶ï‡¶æ‡¶≤=tomorrow/yesterday\"),\n            (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"The leaf has fallen\", \"‡¶™‡¶æ‡¶§‡¶æ=leaf/page\"),\n            (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\", \"He went to the bank\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï=bank/embankment\"),\n            (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø‡•§\", \"I am fine\", \"No ambiguity\"),\n            (\"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"She speaks sweetly\", \"No ambiguity\"),\n            (\"‡¶è‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶á‡•§\", \"This is my book\", \"No ambiguity\"),\n            (\"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã‡•§\", \"Weather is good today\", \"No ambiguity\"),\n            (\"‡¶´‡¶≤ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶∏‡ßç‡¶¨‡¶æ‡¶¶‡ßÅ‡•§\", \"The fruit is delicious\", \"‡¶´‡¶≤=fruit/result\"),\n            (\"‡¶Æ‡¶æ‡¶•‡¶æ ‡¶¨‡ßç‡¶Ø‡¶•‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡•§\", \"Head is aching\", \"‡¶Æ‡¶æ‡¶•‡¶æ=head/top\"),\n        ]\n        print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n        print(\"-\" * 80)\n        confidences = []\n        dscd_homograph_words_detected = set()\n        reference_homograph_words_detected = set()\n        try:\n            try:\n                tokenizer.src_lang = source_lang\n                tokenizer.tgt_lang = target_lang\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    actual_src = getattr(tokenizer, 'src_lang', 'NOT_SET')\n                    actual_tgt = getattr(tokenizer, 'tgt_lang', 'NOT_SET')\n                    print(f\"[VALIDATION] Tokenizer languages set: src={actual_src}, tgt={actual_tgt}\")\n            except Exception as e:\n                print(f\"[VALIDATION] Warning: Could not set tokenizer languages: {type(e).__name__}\")\n            for idx, (src, expected, note) in enumerate(val_sentences, 1):\n                try:\n                    translation = \"\"\n                    explanation_status = \"\"\n                    error_detail = \"\"\n                    if 'translate_with_explanations' in globals():\n                        try:\n                            res = translate_with_explanations(\n                                model,\n                                tokenizer,\n                                src,\n                                source_lang=source_lang,\n                                target_lang=target_lang,\n                                device=device,\n                                max_length=max_length,\n                            )\n                            translation = str(res.get('translation', ''))\n                            exps = res.get('explanations', [])\n                            error_info = res.get('error', '')\n                            if translation and translation.strip():\n                                validation_results['translations_success'] += 1\n                            else:\n                                validation_results['translations_failed'] += 1\n                                if error_info:\n                                    error_detail = f\" ({error_info})\"\n                                else:\n                                    error_detail = \" (empty result)\"\n                            validation_results['explanations_generated'] += len(exps)\n                            if exps:\n                                explanation_status = f\"{len(exps)} expl\"\n                                for exp in exps:\n                                    try:\n                                        conf = exp.get('confidence', 0.5)\n                                        confidences.append(float(conf))\n                                        word = exp.get('ambiguous_word', '').strip()\n                                        clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '').lower()\n                                        if clean_word and not _is_punctuation_only(clean_word):\n                                            if clean_word in dscd_homographs:\n                                                validation_results['dscd_homographs_explained'] += 1\n                                                dscd_homograph_words_detected.add(clean_word)\n                                            if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                                                validation_results['reference_homographs_explained'] += 1\n                                                reference_homograph_words_detected.add(clean_word)\n                                    except Exception:\n                                        pass\n                            else:\n                                explanation_status = \"no expl\"\n                        except Exception as e:\n                            explanation_status = f\"error: {type(e).__name__}\"\n                            error_detail = f\" ({str(e)[:50]})\"\n                            translation = \"\"\n                            validation_results['translations_failed'] += 1\n                    else:\n                        explanation_status = \"unavailable\"\n                        error_detail = \" (function not found)\"\n                        validation_results['translations_failed'] += 1\n                    if translation and translation.strip():\n                        print(f\"  {idx:2d}. {explanation_status:15s} {note[:30]:30s} -> {translation[:200]}\")\n                    else:\n                        print(f\"  {idx:2d}. Translation failed: {note[:30]:30s}{error_detail}\")\n                        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                            print(f\"       [DEBUG] Bengali input was: {src[:50]}\")\n                except Exception as e:\n                    validation_results['translations_failed'] += 1\n                    print(f\"  {idx:2d}. ERROR: {note[:30]:30s} -> {type(e).__name__}\")\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n        finally:\n            if torch.cuda.is_available():\n                try:\n                    torch.cuda.synchronize()\n                except Exception:\n                    pass\n            clear_all_gpu_caches()\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n        try:\n            dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n            if dscd and hasattr(dscd, 'validate_prototypes'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n                if lock:\n                    with lock:\n                        quality_results = dscd.validate_prototypes(cluster_missing=False)\n                else:\n                    quality_results = dscd.validate_prototypes(cluster_missing=False)\n                validation_results['dscd_quality_score'] = quality_results.get('quality_score', 0.0)\n                validation_results['dscd_multi_sense_tokens'] = quality_results.get('multi_sense_tokens', 0)\n                validation_results['dscd_total_prototypes'] = quality_results.get('total_prototypes', 0)\n                print(f\"  - Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n                print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n                print(f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\")\n            else:\n                print(\"  - Validation not available\")\n        except Exception as e:\n            print(f\"  - Validation failed: {type(e).__name__}\")\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] ASBN Training Statistics:\")\n        try:\n            asbn = core_model.asbn if hasattr(core_model, 'asbn') else None\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                asbn_stats = asbn.get_detailed_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get('domain_loss', 0.0)\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get('domain_accuracy', 0.0)\n                validation_results['asbn_source_accuracy'] = asbn_stats.get('source_accuracy', 0.0)\n                validation_results['asbn_target_accuracy'] = asbn_stats.get('target_accuracy', 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n                print(f\"  - Source Accuracy: {validation_results['asbn_source_accuracy']:.2%}\")\n                print(f\"  - Target Accuracy: {validation_results['asbn_target_accuracy']:.2%}\")\n            elif asbn and hasattr(asbn, 'get_asbn_stats'):\n                asbn_stats = asbn.get_asbn_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get('domain_loss', 0.0)\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get('domain_accuracy', 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n            else:\n                print(\"  - ASBN statistics not available\")\n        except Exception as e:\n            print(f\"  - ASBN stats retrieval failed: {type(e).__name__}\")\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] TRG Explanation Statistics:\")\n        try:\n            trg = core_model.trg if hasattr(core_model, 'trg') else None\n            if trg and hasattr(trg, 'get_statistics'):\n                trg_stats = trg.get_statistics()\n                validation_results['trg_total_explanations'] = trg_stats.get('explanations_generated', 0)\n                print(f\"  - Total explanations: {validation_results['trg_total_explanations']}\")\n                print(f\"  - High confidence rate: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                print(f\"  - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n            else:\n                print(\"  - TRG statistics not available\")\n        except Exception as e:\n            print(f\"  - TRG stats retrieval failed: {type(e).__name__}\")\n        if confidences:\n            validation_results['avg_explanation_confidence'] = sum(confidences) / len(confidences)\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\")\n        print(f\"  - Explanations generated: {validation_results['explanations_generated']}\")\n        print(f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\")\n        print(f\"  - DSCD homographs explained: {validation_results['dscd_homographs_explained']}\")\n        print(f\"  - Reference homographs explained: {validation_results['reference_homographs_explained']}\")\n        if dscd_homograph_words_detected:\n            print(f\"  - DSCD homographs detected: {', '.join(sorted(dscd_homograph_words_detected))}\")\n        print(f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n        print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n        print(f\"  - ASBN Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n        warnings = []\n        if validation_results['translations_failed'] > len(val_sentences) // 2:\n            warnings.append(\"High translation failure rate\")\n        if validation_results['explanations_generated'] == 0:\n            warnings.append(\"No explanations generated\")\n        if validation_results['dscd_quality_score'] < 0.3:\n            warnings.append(\"Low DSCD quality score\")\n        if validation_results['dscd_multi_sense_tokens'] < 10:\n            warnings.append(\"Very few multi-sense tokens\")\n        if warnings:\n            print(\"\\n[VALIDATION] Health Warnings:\")\n            for w in warnings:\n                print(f\"  - {w}\")\n        else:\n            print(\"\\n[VALIDATION] All systems healthy\")\n        validation_results['validation_completed'] = True\n    except Exception as e:\n        print(f\"\\n[VALIDATION] Critical error: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n        validation_results['validation_completed'] = False\n    finally:\n        if was_training:\n            core_model.train()\n        clear_all_gpu_caches()\n    print(\"=\" * 80 + \"\\n\")\n    return validation_results\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return 0\n        stores = getattr(dscd, 'prototype_stores', None)\n        if stores is None:\n            return 0\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        if lock:\n            with lock:\n                return len(stores)\n        else:\n            return len(stores)\n    except Exception:\n        return 0\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        return getattr(core, 'dscd', None)\n    except Exception:\n        return None\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n    try:\n        print(\"\\n[CLUSTER] Top 5 clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n        dscd_homographs = _get_dscd_homographs(model)\n        items = []\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        if lock:\n            with lock:\n                stores_snapshot = list(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = list(dscd.prototype_stores.items())\n        for token, store in stores_snapshot:\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                protos = store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n                clean_token = str(token).replace('‚ñÅ', '').replace('ƒ†', '').strip().lower()\n                if _is_punctuation_only(clean_token):\n                    continue\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n                items.append((token, total_count, protos, mu, tau))\n            except Exception:\n                continue\n        items.sort(key=lambda x: x[1], reverse=True)\n        for i, (tok, cnt, prot, mu, tau) in enumerate(items[:top_n], 1):\n            token_display = str(tok)[:12]\n            print(f\"{i:<6}{token_display:<15}{cnt:<12}{prot:<10}{mu:<15.6f}{tau:<12.6f}\")\n        print(\"-\" * 90)\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}\")\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return\n        if hasattr(dscd, 'get_prototype_summary'):\n            summary = dscd.get_prototype_summary()\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Step {global_step}:\")\n                print(f\"  - Total tokens: {summary.get('total_tokens', 0)}\")\n                print(f\"  - Homographs: {summary.get('num_homographs', 0)}\")\n                print(f\"  - Total prototypes: {summary.get('total_prototypes', 0)}\")\n                print(f\"  - Quality score: {summary.get('quality_score', 0.0):.1%}\")\n        if hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Discovery events: {total_discovered}\")\n                recent = dscd.discovered_log[-3:] if len(dscd.discovered_log) >= 3 else dscd.discovered_log\n                for entry in recent:\n                    discovered = entry.get('discovered', 0)\n                    candidates = entry.get('candidates', 0)\n                    print(f\"  - {discovered}/{candidates} homographs discovered\")\n        else:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] No discoveries yet at step {global_step}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[DISCOVERY-STATUS] Error: {e}\")\n\ndef _print_path_loss_summary(training_stats: Dict[str, Any], validate_every: int, global_step: int, use_dual_path: bool):\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"LOSS SUMMARY AT STEP {global_step}\")\n    print(\"=\" * 80)\n    lookback_window = min(validate_every, len(training_stats['path1_losses']), len(training_stats['path2_losses']))\n    if use_dual_path and lookback_window > 0:\n        recent_p1_fwd = training_stats['path1_losses'][-lookback_window:] if training_stats['path1_losses'] else []\n        recent_p2_fwd = training_stats['path2_losses'][-lookback_window:] if training_stats['path2_losses'] else []\n        recent_bwd = training_stats['backward_losses'][-lookback_window:] if training_stats['backward_losses'] else []\n        p1_fwd_avg = float(np.mean(recent_p1_fwd)) if recent_p1_fwd else 0.0\n        p2_fwd_avg = float(np.mean(recent_p2_fwd)) if recent_p2_fwd else 0.0\n        bwd_avg = float(np.mean(recent_bwd)) if recent_bwd else 0.0\n        p1_count = training_stats['path1_batches']\n        p2_count = training_stats['path2_batches']\n        print(f\"\\nPATH 1 (DSCD Word-Level):\")\n        print(f\"  - Forward Loss:  {p1_fwd_avg:.4f}\")\n        print(f\"  - Backward Loss: {bwd_avg:.4f}\")\n        print(f\"  - Total Batches: {p1_count}\")\n        print(f\"\\nPATH 2 (Translation Subword):\")\n        print(f\"  - Forward Loss:  {p2_fwd_avg:.4f}\")\n        print(f\"  - Backward Loss: {bwd_avg:.4f}\")\n        print(f\"  - Total Batches: {p2_count}\")\n        print(f\"\\nCOMBINED:\")\n        print(f\"  - Total Batches: {p1_count + p2_count}\")\n        print(f\"  - Optimizer Updates: {training_stats['optimizer_updates']}\")\n    else:\n        recent_fwd = training_stats['total_loss'][-lookback_window:] if training_stats['total_loss'] else []\n        recent_bwd = training_stats['backward_losses'][-lookback_window:] if training_stats['backward_losses'] else []\n        fwd_avg = float(np.mean(recent_fwd)) if recent_fwd else 0.0\n        bwd_avg = float(np.mean(recent_bwd)) if recent_bwd else 0.0\n        print(f\"\\nSINGLE PATH MODE (Path 2 Only):\")\n        print(f\"  - Forward Loss:  {fwd_avg:.4f}\")\n        print(f\"  - Backward Loss: {bwd_avg:.4f}\")\n        print(f\"  - Total Batches: {training_stats['batches_processed']}\")\n        print(f\"  - Optimizer Updates: {training_stats['optimizer_updates']}\")\n    print(\"=\" * 80 + \"\\n\")\n\ndef count_trainable_parameters(model: torch.nn.Module) -> Dict[str, int]:\n    total_params = 0\n    trainable_params = 0\n    lora_params = 0\n    frozen_params = 0\n    for name, param in model.named_parameters():\n        num_params = param.numel()\n        total_params += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n            if 'lora_' in name.lower() or '.lora_' in name:\n                lora_params += num_params\n        else:\n            frozen_params += num_params\n    return {\n        'total': total_params,\n        'trainable': trainable_params,\n        'lora': lora_params,\n        'frozen': frozen_params,\n        'trainable_pct': 100.0 * trainable_params / total_params if total_params > 0 else 0.0,\n        'lora_pct': 100.0 * lora_params / total_params if total_params > 0 else 0.0,\n    }\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True,\n    use_dual_path: bool = None,\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n    if use_dual_path is None:\n        use_dual_path = _USE_DUAL_PATH_TRAINING\n    print(f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, accum_steps={accumulation_steps}\")\n    print(f\"[TRAIN] Validation: {'enabled' if enable_validation and validate_every > 0 else 'disabled'}\")\n    print(f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\")\n    print(f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(f\"[TRAIN] Dual-path training: {'ENABLED' if use_dual_path else 'DISABLED (default path=2)'}\")\n    print(f\"[TRAIN] Gradient diagnostics: {'ENABLED' if _DEBUG_GRADIENTS else 'DISABLED'}\")\n    param_stats = count_trainable_parameters(model)\n    print(f\"\\n[TRAIN] Parameter Statistics:\")\n    print(f\"  Total parameters: {param_stats['total']/1e6:.2f}M\")\n    print(f\"  Trainable parameters: {param_stats['trainable']/1e6:.2f}M ({param_stats['trainable_pct']:.2f}%)\")\n    print(f\"  LoRA parameters: {param_stats['lora']/1e6:.2f}M ({param_stats['lora_pct']:.2f}%)\")\n    print(f\"  Frozen parameters: {param_stats['frozen']/1e6:.2f}M\")\n    if _USE_LORA:\n        if param_stats['lora'] == 0:\n            print(f\"\\n[TRAIN] ‚ö†Ô∏è  WARNING: LoRA enabled but NO LoRA params found!\")\n            print(f\"[TRAIN] LoRA may not be correctly applied. Check Cell 6.\")\n        elif param_stats['trainable_pct'] > 10.0:\n            print(f\"\\n[TRAIN] ‚ö†Ô∏è  WARNING: LoRA enabled but {param_stats['trainable_pct']:.1f}% params trainable!\")\n            print(f\"[TRAIN] Expected <5% for LoRA. LoRA may not be working correctly.\")\n        else:\n            print(f\"\\n[TRAIN] ‚úÖ LoRA correctly applied ({param_stats['lora']/1e6:.2f}M LoRA params)\")\n    print(\"[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt after all epochs\\n\")\n    if 'translate_with_explanations' not in globals():\n        print(\"[TRAIN] ‚ö†Ô∏è  WARNING: translate_with_explanations not found in globals!\")\n        print(\"[TRAIN] Validation will fail. Please ensure Cell 8 is executed before training.\")\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n    scheduler = None\n    if _USE_LR_SCHEDULER:\n        try:\n            from transformers import get_cosine_schedule_with_warmup\n            total_steps = len(train_loader) * epochs\n            warmup_steps = _WARMUP_STEPS\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=warmup_steps,\n                num_training_steps=total_steps\n            )\n            initial_lr_after_scheduler = optimizer.param_groups[0]['lr']\n            print(f\"[TRAIN] ‚úÖ Learning rate scheduler created:\")\n            print(f\"[TRAIN]    - Type: Cosine with warmup\")\n            print(f\"[TRAIN]    - Total steps: {total_steps}\")\n            print(f\"[TRAIN]    - Warmup steps: {warmup_steps}\")\n            print(f\"[TRAIN]    - LR after scheduler init: {initial_lr_after_scheduler:.2e}\")\n            if initial_lr_after_scheduler == 0.0:\n                print(f\"[TRAIN]    ‚ö†Ô∏è  Scheduler set LR to 0 (warmup starts at step 0)\")\n                print(f\"[TRAIN]    Applying emergency fix: setting LR to 1% of target...\")\n                target_lr = _LR_NMT\n                for group in optimizer.param_groups:\n                    group['lr'] = target_lr * 0.01\n                print(f\"[TRAIN]    ‚úÖ LR corrected: 0.00e+00 ‚Üí {optimizer.param_groups[0]['lr']:.2e}\")\n                print(f\"[TRAIN]    Warmup will now increase from 1% ‚Üí 100% over {warmup_steps} steps\")\n            if _USE_LORA:\n                print(f\"[TRAIN]    - LoRA mode: Using LR {_LR_NMT:.2e}\")\n                print(f\"[TRAIN]    - Trainable params: {param_stats['trainable']/1e6:.2f}M (LoRA adapters only)\")\n            print(f\"[TRAIN]    - Expected: Train loss will converge to <1.5 (from current ~2.4)\")\n            print(f\"[TRAIN]    - Expected BLEU gain: +10-15 points\\n\")\n        except ImportError:\n            print(\"[TRAIN] ‚ö†Ô∏è  WARNING: transformers not available, cannot create scheduler\")\n            print(\"[TRAIN] Training will proceed without LR scheduling (lower BLEU expected)\\n\")\n            scheduler = None\n        except Exception as e:\n            print(f\"[TRAIN] ‚ö†Ô∏è  WARNING: Scheduler creation failed: {type(e).__name__}\")\n            print(\"[TRAIN] Training will proceed without LR scheduling\\n\")\n            scheduler = None\n    else:\n        print(\"[TRAIN] Learning rate scheduler disabled (USE_LR_SCHEDULER=False)\\n\")\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"nan_losses\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"asbn_domain_loss_history\": [],\n        \"trg_explanation_history\": [],\n        \"discovery_runs\": 0,\n        \"discovery_homographs_found\": 0,\n        \"learning_rates\": [],\n        \"path1_batches\": 0,\n        \"path2_batches\": 0,\n        \"path1_losses\": [],\n        \"path2_losses\": [],\n        \"gradient_norms\": [],\n        \"gradient_clips\": 0,\n        \"lora_gradient_norms\": [],\n    }\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n    cached_cluster_count = 0\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n        print(f\"\\n{'='*80}\")\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(f\"{'='*80}\\n\")\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            trg = getattr(core, 'trg', None)\n            if trg and hasattr(trg, 'reset_statistics'):\n                try:\n                    trg.reset_statistics()\n                    print(f\"[TRAIN] TRG statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] TRG stats reset failed: {e}\")\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'reset_stats'):\n                try:\n                    asbn.reset_stats()\n                    print(f\"[TRAIN] ASBN statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] ASBN stats reset failed: {e}\")\n        try:\n            optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n        progress = None\n        batch_idx = 0\n        try:\n            progress = tqdm(\n                total=len(train_loader),\n                desc=f\"Epoch {epoch}/{epochs}\",\n                ncols=110,\n                leave=False,\n                position=0,\n                file=sys.stdout\n            )\n            for batch in train_loader:\n                batch_idx += 1\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n                if (_DEBUG_DISCOVERY or _VERBOSE_LOGGING) and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    print(f\"\\n[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} GlobalStep {global_step}\")\n                    _check_discovery_status(model, global_step)\n                if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                    if global_step % _PERIODIC_DISCOVERY_FREQUENCY == 0:\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            dscd = getattr(core, 'dscd', None)\n                            if dscd and hasattr(dscd, 'periodic_discovery_check'):\n                                print(f\"\\n[DISCOVERY] Running periodic check at step {global_step}...\")\n                                num_discovered = dscd.periodic_discovery_check(\n                                    global_step=global_step,\n                                    frequency=_PERIODIC_DISCOVERY_FREQUENCY,\n                                    cluster_missing=False\n                                )\n                                training_stats['discovery_runs'] += 1\n                                training_stats['discovery_homographs_found'] += num_discovered\n                                if num_discovered > 0:\n                                    print(f\"[DISCOVERY] Found {num_discovered} new homographs!\")\n                                else:\n                                    print(f\"[DISCOVERY] No new homographs found this check\")\n                                cached_cluster_count = _get_cluster_count(model)\n                        except Exception as e:\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                print(f\"[DISCOVERY] Periodic check failed: {type(e).__name__}: {str(e)[:200]}\")\n                if enable_validation and validate_every and validate_every > 0 and (global_step % validate_every == 0):\n                    if accumulated_steps == 0:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            dscd = getattr(core, 'dscd', None)\n                            if dscd and hasattr(dscd, 'cleanup_memory'):\n                                print(f\"[VALIDATION] Running DSCD cleanup before validation...\")\n                                dscd.cleanup_memory()\n                        except Exception as e:\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                print(f\"[VALIDATION] DSCD cleanup failed: {type(e).__name__}\")\n                        _print_path_loss_summary(training_stats, validate_every, global_step, use_dual_path)\n                        val_result = comprehensive_epoch_validation(\n                            model,\n                            tokenizer,\n                            epoch,\n                            global_step,\n                            _SOURCE_LANGUAGE,\n                            _TARGET_LANGUAGE,\n                            _MAX_LENGTH,\n                            _DEVICE,\n                        )\n                        if val_result:\n                            training_stats['epoch_validations'].append(val_result)\n                        cached_cluster_count = _get_cluster_count(model)\n                    else:\n                        pending_validation = True\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    progress.update(1)\n                    continue\n                try:\n                    input_ids = batch[\"input_ids\"]\n                    attention_mask = batch[\"attention_mask\"]\n                    labels = batch[\"labels\"]\n                    batch_size = int(input_ids.size(0))\n                    domain_labels = batch.get(\"domain_labels\", None)\n                    if domain_labels is not None:\n                        if not isinstance(domain_labels, torch.Tensor):\n                            domain_labels = None\n                        elif domain_labels.dim() == 0:\n                            domain_labels = domain_labels.unsqueeze(0)\n                    if domain_labels is None:\n                        domain_labels = torch.full(\n                            (batch_size,),\n                            _TRAIN_DOMAIN,\n                            dtype=torch.long,\n                            device=torch.device('cpu')\n                        )\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        keep = (batch_size // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            progress.update(1)\n                            continue\n                        if keep != batch_size:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n                            domain_labels = domain_labels[:keep]\n                            batch_size = keep\n                    input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                    attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                    labels = labels.to(_DEVICE, non_blocking=True)\n                    domain_labels = domain_labels.to(_DEVICE, non_blocking=True)\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        progress.update(1)\n                        continue\n                    if use_dual_path:\n                        selected_path = 1 if batch_idx % 2 == 1 else 2\n                    else:\n                        selected_path = 2\n                    if selected_path == 1:\n                        training_stats[\"path1_batches\"] += 1\n                        forward_kwargs = {\n                            \"input_ids\": input_ids,\n                            \"attention_mask\": attention_mask,\n                            \"src_texts\": batch.get(\"src_text\", None),\n                            \"token_word_map\": batch.get(\"token_word_map\", None),\n                            \"domain_labels\": domain_labels,\n                        }\n                        amp_ctx = get_amp_ctx()\n                        with amp_ctx:\n                            try:\n                                core = model.module if hasattr(model, 'module') else model\n                                if hasattr(core, 'forward_path1'):\n                                    forward_out = core.forward_path1(**forward_kwargs)\n                                else:\n                                    forward_kwargs[\"labels\"] = None\n                                    forward_kwargs[\"path\"] = 1\n                                    forward_out = model(**forward_kwargs)\n                            except RuntimeError as e:\n                                print(f\"\\n[TRAIN-ERROR] Path 1 forward failed at step {global_step}: {type(e).__name__}\")\n                                print(f\"  Skipping batch...\")\n                                training_stats[\"runtime_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p1_forward_error\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            except Exception as e:\n                                print(f\"\\n[TRAIN-ERROR] Path 1 exception at step {global_step}: {type(e).__name__}\")\n                                training_stats[\"exceptions\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p1_exception\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            if isinstance(forward_out, torch.Tensor):\n                                loss_tensor = forward_out\n                            elif isinstance(forward_out, dict):\n                                if \"loss\" in forward_out:\n                                    loss_tensor = forward_out[\"loss\"]\n                                elif \"asbn_loss\" in forward_out:\n                                    loss_tensor = forward_out[\"asbn_loss\"]\n                                else:\n                                    print(f\"\\n[TRAIN-ERROR] Path 1 returned dict without loss at step {global_step}\")\n                                    training_stats[\"skipped_batches\"] += 1\n                                    skip_reasons[\"p1_no_loss\"] += 1\n                                    try:\n                                        optimizer.zero_grad(set_to_none=True)\n                                    except Exception:\n                                        pass\n                                    progress.update(1)\n                                    continue\n                            elif isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                                loss_tensor = forward_out[0]\n                            else:\n                                print(f\"\\n[TRAIN-ERROR] Path 1 returned unrecognized type at step {global_step}\")\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p1_bad_return\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            if not isinstance(loss_tensor, torch.Tensor):\n                                loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE)\n                            else:\n                                loss_tensor = loss_tensor.to(_DEVICE)\n                            if not torch.isfinite(loss_tensor):\n                                print(f\"\\n[TRAIN-SKIP] Path 1 NaN/Inf loss at step {global_step}, skipping batch\")\n                                training_stats[\"nan_losses\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p1_nan_loss\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    if p.grad is not None:\n                                        p.grad = None\n                                clear_all_gpu_caches()\n                                progress.update(1)\n                                continue\n                            if loss_tensor.numel() > 1:\n                                loss_val = float(loss_tensor.mean().item())\n                                loss_tensor = loss_tensor.mean()\n                            else:\n                                loss_val = float(loss_tensor.item())\n                            last_forward_loss = loss_val\n                            epoch_losses.append(loss_val)\n                            training_stats[\"total_loss\"].append(loss_val)\n                            training_stats[\"path1_losses\"].append(loss_val)\n                    else:\n                        training_stats[\"path2_batches\"] += 1\n                        forward_kwargs = {\n                            \"input_ids\": input_ids,\n                            \"attention_mask\": attention_mask,\n                            \"labels\": labels,\n                            \"src_texts\": batch.get(\"src_text\", None),\n                            \"token_word_map\": batch.get(\"token_word_map\", None),\n                        }\n                        amp_ctx = get_amp_ctx()\n                        with amp_ctx:\n                            try:\n                                core = model.module if hasattr(model, 'module') else model\n                                if hasattr(core, 'forward_path2'):\n                                    forward_out = core.forward_path2(**forward_kwargs, use_rdrop=False)\n                                else:\n                                    forward_kwargs[\"path\"] = 2\n                                    forward_out = model(**forward_kwargs)\n                            except RuntimeError as e:\n                                print(f\"\\n[TRAIN-ERROR] Path 2 forward failed at step {global_step}: {type(e).__name__}\")\n                                print(f\"  Error: {str(e)[:100]}\")\n                                print(f\"  Skipping batch...\")\n                                training_stats[\"runtime_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p2_forward_error\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            except Exception as e:\n                                print(f\"\\n[TRAIN-ERROR] Path 2 exception at step {global_step}: {type(e).__name__}\")\n                                training_stats[\"exceptions\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p2_exception\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            if isinstance(forward_out, torch.Tensor):\n                                loss_tensor = forward_out\n                            elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                                loss_tensor = forward_out[\"loss\"]\n                            elif isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                                loss_tensor = forward_out[0]\n                            else:\n                                print(f\"\\n[TRAIN-ERROR] Path 2 returned unrecognized type at step {global_step}\")\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p2_bad_return\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                progress.update(1)\n                                continue\n                            if not isinstance(loss_tensor, torch.Tensor):\n                                loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE)\n                            else:\n                                loss_tensor = loss_tensor.to(_DEVICE)\n                            if not torch.isfinite(loss_tensor):\n                                print(f\"\\n[TRAIN-SKIP] Path 2 NaN/Inf loss at step {global_step}, skipping batch\")\n                                training_stats[\"nan_losses\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"p2_nan_loss\"] += 1\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    if p.grad is not None:\n                                        p.grad = None\n                                clear_all_gpu_caches()\n                                progress.update(1)\n                                continue\n                            if loss_tensor.numel() > 1:\n                                loss_val = float(loss_tensor.mean().item())\n                                loss_tensor = loss_tensor.mean()\n                            else:\n                                loss_val = float(loss_tensor.item())\n                            last_forward_loss = loss_val\n                            epoch_losses.append(loss_val)\n                            training_stats[\"total_loss\"].append(loss_val)\n                            training_stats[\"path2_losses\"].append(loss_val)\n                    loss_scaled = loss_tensor / max(1, accumulation_steps)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n                    try:\n                        if scaler.is_enabled():\n                            scaler.scale(loss_scaled).backward()\n                        else:\n                            loss_scaled.backward()\n                        if _DEBUG_GRADIENTS and global_step % GRADIENT_DIAGNOSTIC_INTERVAL == 0:\n                            grad_diag = compute_gradient_diagnostics(model, global_step, prefix=f\"-P{selected_path}\")\n                            print_gradient_diagnostics(grad_diag, global_step, prefix=f\"-P{selected_path}\")\n                            training_stats[\"gradient_norms\"].append(grad_diag['total_norm'])\n                            if grad_diag['lora_stats']:\n                                lora_total_norm = math.sqrt(sum(\n                                    s['norm']**2 for s in grad_diag['lora_stats'].values()\n                                ))\n                                training_stats[\"lora_gradient_norms\"].append(lora_total_norm)\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                    except RuntimeError as e:\n                        if \"out of memory\" in str(e).lower():\n                            training_stats[\"oom_errors\"] += 1\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"oom_backward\"] += 1\n                            print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                            try:\n                                optimizer.zero_grad(set_to_none=True)\n                            except Exception:\n                                pass\n                            for p in model.parameters():\n                                p.grad = None\n                            if torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n                            gc.collect()\n                            accumulated_steps = 0\n                            progress.update(1)\n                            continue\n                        else:\n                            raise\n                    accumulated_steps += 1\n                    if accumulated_steps >= accumulation_steps:\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            \n                            has_nan_inf = False\n                            for name, param in core.named_parameters():\n                                if param.grad is not None:\n                                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n                                        print(f\"[TRAIN-WARN] Step {global_step}: NaN/Inf gradient in {name}, skipping step\")\n                                        has_nan_inf = True\n                                        break\n                            \n                            if has_nan_inf:\n                                optimizer.zero_grad(set_to_none=True)\n                                accumulated_steps = 0\n                                progress.update(1)\n                                continue\n                            \n                            if _GRAD_CLIP_NORM > 0:\n                                if scaler.is_enabled():\n                                    try:\n                                        scaler.unscale_(optimizer)\n                                        print(f\"[DEBUG-GRAD] Step {global_step}: scaler.unscale_() SUCCESS\")\n                                    except (RuntimeError, AssertionError) as unscale_error:\n                                        error_msg = str(unscale_error)\n                                        print(f\"\\n{'='*80}\")\n                                        print(f\"[ERROR-LOCATION] scaler.unscale_() at step {global_step}\")\n                                        print(f\"{'='*80}\")\n                                        print(f\"Error Type: {type(unscale_error).__name__}\")\n                                        print(f\"Error Message: {error_msg}\")\n                                        \n                                        if \"No inf checks\" in error_msg or \"AssertionError\" in str(type(unscale_error).__name__):\n                                            print(f\"\\n[DIAGNOSIS] This is the 'No inf checks recorded' error!\")\n                                            print(f\"  ‚Üí CAUSE: Scaler hasn't tracked this optimizer for current step\")\n                                            print(f\"  ‚Üí FIX: Skipping unscale, gradients will be clipped in scaled form\")\n                                            print(f\"  ‚Üí IMPACT: No problem - scaler.step() will handle unscaling\")\n                                            print(f\"{'='*80}\\n\")\n                                        else:\n                                            print(f\"\\n[DIAGNOSIS] Different scaler error - re-raising\")\n                                            print(f\"{'='*80}\\n\")\n                                            raise\n                                \n                                has_grads = any(p.grad is not None for p in core.parameters() if p.requires_grad)\n                                if has_grads:\n                                    grad_norm = torch.nn.utils.clip_grad_norm_(\n                                        [p for p in core.parameters() if p.requires_grad],\n                                        _GRAD_CLIP_NORM\n                                    )\n                                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):\n                                        print(f\"[TRAIN-WARN] Step {global_step}: NaN/Inf gradient norm, skipping step\")\n                                        optimizer.zero_grad(set_to_none=True)\n                                        accumulated_steps = 0\n                                        progress.update(1)\n                                        continue\n                                    if grad_norm > _GRAD_CLIP_NORM:\n                                        training_stats[\"gradient_clips\"] += 1\n                                        if _DEBUG_GRADIENTS:\n                                            print(f\"[GRAD-CLIP] Step {global_step}: {grad_norm:.6f} ‚Üí {_GRAD_CLIP_NORM}\")\n                            current_lr = optimizer.param_groups[0]['lr']\n                            if current_lr == 0.0:\n                                print(f\"\\n{'='*80}\")\n                                print(f\"‚ö†Ô∏è  WARNING: LR is 0.0 at step {global_step}\")\n                                print(f\"{'='*80}\")\n                                print(f\"[FIX] Setting LR to 1% of target to prevent AssertionError...\")\n                                for group in optimizer.param_groups:\n                                    group['lr'] = _LR_NMT * 0.01\n                                print(f\"[FIX] LR corrected: 0.00e+00 ‚Üí {optimizer.param_groups[0]['lr']:.2e}\")\n                                print(f\"{'='*80}\\n\")\n                            if scaler.is_enabled():\n                                try:\n                                    scaler.step(optimizer)\n                                    scaler.update()\n                                except AssertionError as e:\n                                    print(f\"\\n{'='*80}\")\n                                    print(f\"‚ùå AssertionError at step {global_step}\")\n                                    print(f\"{'='*80}\")\n                                    print(f\"Error: {str(e)}\")\n                                    current_lr = optimizer.param_groups[0]['lr']\n                                    print(f\"\\n[DIAGNOSTIC] Current LR: {current_lr:.2e}\")\n                                    \n                                    scaler.update()\n                                    \n                                    if current_lr == 0.0:\n                                        print(f\"  ‚ùå CAUSE: LR is 0.0 (scheduler warmup issue)\")\n                                        print(f\"  üîß FIX: Setting LR to 1% of target...\")\n                                        for group in optimizer.param_groups:\n                                            group['lr'] = _LR_NMT * 0.01\n                                        print(f\"  ‚úÖ LR corrected to {optimizer.param_groups[0]['lr']:.2e}\")\n                                    \n                                    print(f\"  ‚ö†Ô∏è  Skipping this optimizer step, resetting scaler state\")\n                                    print(f\"{'='*80}\\n\")\n                                    \n                                    optimizer.zero_grad(set_to_none=True)\n                                    accumulated_steps = 0\n                            else:\n                                try:\n                                    optimizer.step()\n                                except AssertionError as e:\n                                    print(f\"\\n{'='*80}\")\n                                    print(f\"‚ùå AssertionError (non-AMP) at step {global_step}\")\n                                    print(f\"{'='*80}\")\n                                    print(f\"Error: {str(e)}\")\n                                    current_lr = optimizer.param_groups[0]['lr']\n                                    print(f\"Current LR: {current_lr:.2e}\")\n                                    if current_lr == 0.0:\n                                        print(f\"üîß Applying emergency fix...\")\n                                        for group in optimizer.param_groups:\n                                            group['lr'] = _LR_NMT * 0.01\n                                        optimizer.step()\n                                        print(f\"‚úÖ Fixed and retried\")\n                                    else:\n                                        raise\n                                    print(f\"{'='*80}\\n\")\n                            if scheduler is not None:\n                                scheduler.step()\n                                current_lr = optimizer.param_groups[0]['lr']\n                                training_stats['learning_rates'].append(current_lr)\n                                if global_step % DEBUG_PRINT_INTERVAL == 0 and (_DEBUG_DISCOVERY or _VERBOSE_LOGGING):\n                                    print(f\"[TRAIN-DEBUG] Current learning rate: {current_lr:.2e}\")\n                            optimizer.zero_grad(set_to_none=True)\n                            training_stats[\"optimizer_updates\"] += 1\n                            if torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    p.grad = None\n                                if torch.cuda.is_available():\n                                    torch.cuda.empty_cache()\n                                gc.collect()\n                                accumulated_steps = 0\n                                progress.update(1)\n                                continue\n                            else:\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n                                print(f\"\\n[ERROR] Runtime error during optimizer step: {type(e).__name__}\")\n                        except Exception as e:\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n                            print(f\"\\n[ERROR] Exception during optimizer step: {type(e).__name__}\")\n                        finally:\n                            accumulated_steps = 0\n                            if pending_validation:\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                try:\n                                    core = model.module if hasattr(model, 'module') else model\n                                    dscd = getattr(core, 'dscd', None)\n                                    if dscd and hasattr(dscd, 'cleanup_memory'):\n                                        print(f\"[VALIDATION] Running DSCD cleanup before validation...\")\n                                        dscd.cleanup_memory()\n                                except Exception as e:\n                                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                        print(f\"[VALIDATION] DSCD cleanup failed: {type(e).__name__}\")\n                                _print_path_loss_summary(training_stats, validate_every, global_step, use_dual_path)\n                                val_result = comprehensive_epoch_validation(\n                                    model,\n                                    tokenizer,\n                                    epoch,\n                                    global_step,\n                                    _SOURCE_LANGUAGE,\n                                    _TARGET_LANGUAGE,\n                                    _MAX_LENGTH,\n                                    _DEVICE,\n                                )\n                                if val_result:\n                                    training_stats['epoch_validations'].append(val_result)\n                                pending_validation = False\n                                cached_cluster_count = _get_cluster_count(model)\n                    if global_step % DEBUG_PRINT_INTERVAL == 0:\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cached_cluster_count = _get_cluster_count(model)\n                        path_str = f\"p{selected_path}\" if use_dual_path else \"p2\"\n                        print(f\"[TRAIN-DEBUG] step={global_step} {path_str} loss={last_forward_loss:.4f} clusters={cached_cluster_count}\")\n                        _print_top_clusters(model, top_n=5)\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            dscd = getattr(core, 'dscd', None)\n                            if dscd and hasattr(dscd, 'cleanup_memory'):\n                                dscd.cleanup_memory()\n                                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                    print(f\"[MEMORY] DSCD cleanup executed at step {global_step}\")\n                        except Exception as e:\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                print(f\"[MEMORY] DSCD cleanup failed: {type(e).__name__}\")\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            p.grad = None\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                        gc.collect()\n                        accumulated_steps = 0\n                        progress.update(1)\n                        continue\n                    else:\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        print(f\"\\n{'='*80}\")\n                        print(f\"RUNTIME ERROR - Step {global_step}\")\n                        print(f\"{'='*80}\")\n                        print(f\"Error: {str(e)}\")\n                        print(f\"Path: {selected_path}\")\n                        print(f\"Batch size: {batch_size}\")\n                        traceback.print_exc()\n                        print(f\"{'='*80}\\n\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        progress.update(1)\n                        continue\n                except Exception as e:\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    print(f\"\\n[EXCEPTION] Exception at step {global_step}: {type(e).__name__}\")\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    progress.update(1)\n                    continue\n                processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n                expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n                success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n                next_disc = 0\n                try:\n                    if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                        next_disc = _PERIODIC_DISCOVERY_FREQUENCY - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                except Exception:\n                    next_disc = 0\n                postfix = {\n                    'fwd': f\"{last_forward_loss:.2f}\",\n                    'bwd': f\"{last_backward_loss:.2f}\",\n                    'rate': f\"{success_rate:.1f}%\",\n                    'disc': next_disc\n                }\n                if use_dual_path:\n                    postfix['path'] = f\"{selected_path}\"\n                progress.set_postfix(postfix, refresh=False)\n                progress.update(1)\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n        if accumulated_steps > 0:\n            try:\n                core = model.module if hasattr(model, 'module') else model\n                if _GRAD_CLIP_NORM > 0:\n                    if scaler.is_enabled():\n                        try:\n                            scaler.unscale_(optimizer)\n                        except (RuntimeError, AssertionError):\n                            pass\n                    has_grads = any(p.grad is not None for p in core.parameters() if p.requires_grad)\n                    if has_grads:\n                        torch.nn.utils.clip_grad_norm_(\n                            [p for p in core.parameters() if p.requires_grad],\n                            _GRAD_CLIP_NORM\n                        )\n                if scaler.is_enabled():\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    optimizer.step()\n                if scheduler is not None:\n                    scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}\")\n            finally:\n                accumulated_steps = 0\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        print(f\"\\n{'='*80}\")\n        print(f\"EPOCH {epoch}/{epochs} COMPLETE\")\n        print(f\"{'='*80}\")\n        print(f\"Duration: {epoch_duration_min:.2f} min\")\n        print(f\"Avg loss: {avg_epoch_loss:.4f}\")\n        print(f\"Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\"Skipped batches: {training_stats['skipped_batches']}\")\n        if skip_reasons:\n            print(f\"\\nSkip reasons:\")\n            for reason, count in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"  - {reason}: {count}\")\n        print(f\"{'='*80}\\n\")\n        training_stats['epoch_losses'].append(avg_epoch_loss)\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    print(f\"\\n{'='*80}\")\n    print(f\"TRAINING COMPLETE\")\n    print(f\"{'='*80}\")\n    print(f\"Total epochs: {epochs}\")\n    print(f\"Total optimizer updates: {training_stats['optimizer_updates']}\")\n    print(f\"Total batches processed: {training_stats['batches_processed']}\")\n    print(f\"Total batches skipped: {training_stats['skipped_batches']}\")\n    print(f\"OOM errors: {training_stats['oom_errors']}\")\n    print(f\"Runtime errors: {training_stats['runtime_errors']}\")\n    print(f\"Exceptions: {training_stats['exceptions']}\")\n    print(f\"NaN losses: {training_stats['nan_losses']}\")\n    print(f\"Gradient clips: {training_stats['gradient_clips']}\")\n    print(f\"{'='*80}\\n\")\n    return model\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training Loop [‚úÖ ALL FIXES APPLIED]\")\nprint(\"=\" * 80)\nprint(\"‚úÖ FIX #1: scaler.unscale_() wrapped in try-except\")\nprint(\"‚úÖ FIX #2: scaler.update() called in AssertionError except block\")\nprint(\"   - Prevents state corruption\")\nprint(\"   - Stops cascading 'unscale_() already called' errors\")\nprint(\"‚úÖ FIX #3: LR scheduler warmup fix (already present)\")\nprint(\"‚úÖ FIX #4: Optimizer step LR check (already present)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.689031Z","iopub.execute_input":"2026-02-16T03:01:26.689561Z","iopub.status.idle":"2026-02-16T03:01:26.837868Z","shell.execute_reply.started":"2026-02-16T03:01:26.689540Z","shell.execute_reply":"2026-02-16T03:01:26.837333Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 7: Training Loop [‚úÖ ALL FIXES APPLIED]\n================================================================================\n‚úÖ FIX #1: scaler.unscale_() wrapped in try-except\n‚úÖ FIX #2: scaler.update() called in AssertionError except block\n   - Prevents state corruption\n   - Stops cascading 'unscale_() already called' errors\n‚úÖ FIX #3: LR scheduler warmup fix (already present)\n‚úÖ FIX #4: Optimizer step LR check (already present)\n================================================================================\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 8: INFERENCE & EVALUATION PIPELINE (DUAL-PATH + LORA COMPATIBLE) - BanglaT5\n# ===========================================================================================\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nimport re\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\n\n# -------------------------\n# Environment / defaults\n# -------------------------\ntry:\n    SOURCE_LANG = str(SOURCE_LANGUAGE)\n    TARGET_LANG = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    SOURCE_LANG = \"bn\"\n    TARGET_LANG = \"en\"\n\ntry:\n    TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    TASK_PREFIX = \"translate Bengali to English: \"\n\ntry:\n    MAXLEN = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    MAXLEN = 128\n\ntry:\n    DEVICE = DEVICE\nexcept (NameError, TypeError):\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    VERBOSE_LOGGING = False\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    DEBUG_DISCOVERY = False\n\ntry:\n    DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    DEBUG_TIMING = False\n\ntry:\n    USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    USE_MULTI_GPU = torch.cuda.is_available() and (torch.cuda.device_count() > 1)\n\ntry:\n    SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    SPAN_THRESHOLD = 0.10\n\ntry:\n    UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    UNCERTAINTY_THRESHOLD = 0.10\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST = {\n        \"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\", \"‡¶¨‡¶æ‡¶∞\", \"‡¶π‡¶æ‡¶∞\", \"‡¶§‡¶æ‡¶∞‡¶æ\",\n        \"‡¶™‡¶æ‡¶®‡¶ø\", \"‡¶¶‡¶≤\", \"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞\", \"‡¶®‡¶æ‡¶Æ\", \"‡¶ï‡¶•‡¶æ\", \"‡¶¨‡¶á\", \"‡¶ò‡¶∞\", \"‡¶Æ‡¶®\", \"‡¶π‡¶æ‡¶§\"\n    }\n    HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    TEST_DOMAIN = 1\n\ntry:\n    EVAL_NUM_BEAMS = int(EVAL_NUM_BEAMS)\nexcept (NameError, ValueError, TypeError):\n    EVAL_NUM_BEAMS = 8\n\ntry:\n    EVAL_LENGTH_PENALTY = float(EVAL_LENGTH_PENALTY)\nexcept (NameError, ValueError, TypeError):\n    EVAL_LENGTH_PENALTY = 1.0\n\ntry:\n    EVAL_NO_REPEAT_NGRAM_SIZE = int(EVAL_NO_REPEAT_NGRAM_SIZE)\nexcept (NameError, ValueError, TypeError):\n    EVAL_NO_REPEAT_NGRAM_SIZE = 2\n\ntry:\n    EVAL_MIN_LENGTH = int(EVAL_MIN_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    EVAL_MIN_LENGTH = 3\n\n# ===================================================================\n# ‚úÖ FIX #1: ADD LORA GLOBALS\n# ===================================================================\ntry:\n    USE_LORA = bool(USE_LORA)\nexcept (NameError, TypeError):\n    USE_LORA = False\n\nBENGALI_PUNCT_SET = set(['‡•§', '‡••'])\nCOMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\nPUNCT_SET = BENGALI_PUNCT_SET | COMMON_PUNCT_SET\n\n\ndef is_punctuation_only(token: str) -> bool:\n    \"\"\"‚úÖ UNCHANGED: Punctuation detection\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n\n    clean = (\n        token.replace(\"‚ñÅ\", \"\")\n        .replace(\"ƒ†\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if not clean:\n        return False\n\n    if clean in BENGALI_PUNCT_SET:\n        return True\n\n    if clean in COMMON_PUNCT_SET:\n        return True\n\n    if len(clean) == 1 and not clean.isalnum():\n        return True\n\n    return all(c in PUNCT_SET for c in clean)\n\n\ndef clean_token(token: str) -> str:\n    \"\"\"‚úÖ UNCHANGED: Token cleaning\"\"\"\n    if not isinstance(token, str):\n        return \"\"\n    cleaned = token.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").replace(\"##\", \"\").strip()\n    for punct in [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\"]:\n        cleaned = cleaned.replace(punct, \"\")\n    return cleaned.lower()\n\n\n# ===================================================================\n# ‚úÖ FIX #2: IMPROVED CAPITALIZATION FUNCTION\n# ===================================================================\ndef capitalize_first_char(text: str) -> str:\n    \"\"\"\n    Capitalizes the first alphabetic character in the text.\n    Works for both ASCII (English) and Unicode (Bengali/other languages).\n    Preserves leading whitespace and punctuation.\n    \n    Examples:\n        \"hello world\" -> \"Hello world\"\n        \" hello world\" -> \" Hello world\"\n        \"123 hello\" -> \"123 Hello\"\n        \"‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã\" -> \"‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã\" (Bengali already capitalized if applicable)\n    \"\"\"\n    if not text or not isinstance(text, str) or len(text) == 0:\n        return text\n    \n    # Find first alphabetic character\n    for idx, char in enumerate(text):\n        if char.isalpha():\n            # Capitalize it (works for ASCII and some Unicode)\n            return text[:idx] + char.upper() + text[idx + 1:]\n    \n    # No alphabetic character found, return as-is\n    return text\n\n\ndef clean_and_capitalize_translation(text: str) -> str:\n    \"\"\"\n    Cleans and capitalizes decoded text from T5 generation.\n    - Strips leading/trailing whitespace\n    - Normalizes multiple spaces to single space\n    - Capitalizes first alphabetic character\n    - Preserves inner spacing and punctuation\n    \"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Capitalize first alphabetic character\n    text = capitalize_first_char(text)\n    \n    return text\n\n\ndef get_dscd_homographs(model: torch.nn.Module) -> set:\n    \"\"\"‚úÖ UNCHANGED: DSCD homograph extraction (model-agnostic)\"\"\"\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                discovered = dscd.get_discovered_homographs()\n                return set(w for w in discovered if not is_punctuation_only(w))\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                for token, store in dscd.prototype_stores.items():\n                    try:\n                        if store.size() >= 2:\n                            clean_tok = clean_token(str(token))\n                            if clean_tok and not is_punctuation_only(str(token)):\n                                homographs.add(clean_tok)\n                    except Exception:\n                        continue\n        else:\n            for token, store in dscd.prototype_stores.items():\n                try:\n                    if store.size() >= 2:\n                        clean_tok = clean_token(str(token))\n                        if clean_tok and not is_punctuation_only(str(token)):\n                            homographs.add(clean_tok)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\nclass InferenceStatistics:\n    \"\"\"‚úÖ UNCHANGED: Statistics tracking (model-agnostic)\"\"\"\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self._lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.avg_span = 0.0\n            self.avg_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        with self._lock:\n            self.total_inferences += 1\n\n            if result.get('translation') and result['translation'] != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get('explanations', [])\n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                try:\n                    conf = exp.get('confidence', 0.5)\n                    self.total_confidence += float(conf)\n\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf < 0.4:\n                        self.low_confidence_explanations += 1\n\n                    word = str(exp.get('ambiguous_word', '')).strip()\n\n                    if is_punctuation_only(word):\n                        continue\n\n                    clean_word = clean_token(word)\n\n                    if not clean_word:\n                        continue\n\n                    self.token_counts[clean_word] += 1\n                    self.token_confidences[clean_word].append(float(conf))\n\n                    if dscd_homographs and clean_word in dscd_homographs:\n                        self.dscd_homographs_explained.add(clean_word)\n\n                    if clean_word in HOMOGRAPH_REFERENCE_LIST:\n                        self.reference_homographs_explained.add(clean_word)\n\n                    self.avg_span += float(exp.get('span', 0.0))\n                    self.avg_uncertainty += float(exp.get('uncertainty', 0.0))\n\n                except Exception:\n                    pass\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self._lock:\n            total_exp = max(self.total_explanations, 1)\n\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n\n            return {\n                'total_inferences': self.total_inferences,\n                'successful_translations': self.successful_translations,\n                'failed_translations': self.failed_translations,\n                'success_rate': self.successful_translations / max(self.total_inferences, 1),\n                'total_explanations': self.total_explanations,\n                'explanations_per_inference': self.total_explanations / max(self.total_inferences, 1),\n                'high_confidence_rate': self.high_confidence_explanations / total_exp,\n                'low_confidence_rate': self.low_confidence_explanations / total_exp,\n                'avg_confidence': self.total_confidence / total_exp,\n                'avg_span': self.avg_span / total_exp,\n                'avg_uncertainty': self.avg_uncertainty / total_exp,\n                'dscd_homographs_explained': list(self.dscd_homographs_explained),\n                'reference_homographs_explained': list(self.reference_homographs_explained),\n                'dscd_empty_warnings': self.dscd_empty_warnings,\n                'unique_tokens_explained': unique_tokens,\n                'diversity_ratio': diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"\\n\" + \"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2%}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n\n        if summary['dscd_homographs_explained']:\n            print(f\"\\nDSCD homographs explained ({len(summary['dscd_homographs_explained'])})\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'])}\")\n\n        if summary['reference_homographs_explained']:\n            print(f\"\\nReference homographs explained ({len(summary['reference_homographs_explained'])})\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'])}\")\n\n        if summary['dscd_empty_warnings'] > 0:\n            print(f\"\\nDSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80 + \"\\n\")\n\n\nINFERENCE_STATS = InferenceStatistics()\n\n\ndef to_device_batch(enc: Any, device: torch.device):\n    \"\"\"‚úÖ UNCHANGED: Device transfer utility\"\"\"\n    try:\n        if hasattr(enc, \"to\") and callable(getattr(enc, \"to\")):\n            return enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(enc, dict):\n        out = {}\n        try:\n            for k, v in enc.items():\n                try:\n                    if isinstance(v, torch.Tensor):\n                        out[k] = v.to(device)\n                    elif isinstance(v, dict):\n                        out[k] = to_device_batch(v, device)\n                    elif isinstance(v, (list, tuple)):\n                        out[k] = [\n                            t.to(device) if isinstance(t, torch.Tensor) else t\n                            for t in v\n                        ]\n                    else:\n                        out[k] = v\n                except Exception:\n                    out[k] = v\n            return out\n        except Exception:\n            return enc\n\n    return enc\n\n\ndef extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    \"\"\"‚úÖ UNCHANGED: DSCD output extraction\"\"\"\n    if raw_out is None:\n        return {}\n\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"proto_probs\" in raw_out or \"uncertainties\" in raw_out:\n            return raw_out\n\n        for key in (\"dscd_outputs\", \"dscd\", \"dscd_out\"):\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n\n        return raw_out\n\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return extract_dscd_outputs(item)\n\n    return {}\n\n\ndef is_subword_token(token: str) -> bool:\n    \"\"\"‚úÖ UNCHANGED: Subword detection\"\"\"\n    if not token or len(token.strip()) == 0:\n        return True\n\n    token = token.strip()\n\n    if is_punctuation_only(token):\n        return True\n\n    if (\n        token.startswith(\"##\")\n        or token.startswith(\"‚ñÅ‚ñÅ\")\n        or token.startswith(\"@@\")\n        or token.startswith(\"‚ñÅ\")\n    ):\n        return True\n\n    if len(token) < 2:\n        return True\n\n    if (len(token) == 1 and token in PUNCT_SET) or token.isdigit():\n        return True\n\n    return False\n\n\ndef should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    \"\"\"‚úÖ UNCHANGED: Explanation filtering\"\"\"\n    try:\n        token = expl.get('ambiguous_word', expl.get('token', ''))\n\n        if is_punctuation_only(str(token)):\n            return True\n\n        span = float(expl.get('span', 0.0))\n        uncertainty = float(expl.get('uncertainty', 0.0))\n\n        if is_subword_token(str(token)):\n            return True\n\n        if span < span_th and uncertainty < u_th:\n            return True\n\n        return False\n    except Exception:\n        return True\n\n\ndef has_bengali_chars(text: str) -> bool:\n    \"\"\"‚úÖ UNCHANGED: Bengali character detection\"\"\"\n    if not text or not isinstance(text, str):\n        return False\n    return any('\\u0980' <= c <= '\\u09FF' for c in text)\n\n\n@torch.inference_mode()\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    source_lang: str = \"bn\",\n    target_lang: str = \"en\",\n    device: Optional[torch.device] = None,\n    max_length: Optional[int] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"\n    ‚úÖ CHANGED: \n    1. Improved capitalization handling\n    2. Better empty translation safeguards\n    3. LoRA-compatible inference mode\n    \"\"\"\n    device = DEVICE if device is None else device\n    max_len = MAXLEN if max_length is None else int(max_length)\n    span_th = SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    span_th = min(span_th, 0.10)\n    u_th = min(u_th, 0.10)\n\n    # ‚úÖ FIX #3: IMPROVED EMPTY INPUT HANDLING\n    if not input_sentence or not isinstance(input_sentence, str) or not input_sentence.strip():\n        return {\n            \"input_sentence\": input_sentence if input_sentence else \"\",\n            \"translation\": \"\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": \"Empty or invalid input\"\n        }\n\n    if not has_bengali_chars(input_sentence):\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] WARNING: Input does not contain Bengali characters: {input_sentence[:50]}\")\n\n    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n        print(f\"\\n[INF] Starting inference (BanglaT5):\")\n        print(f\"[INF]   Input: {input_sentence[:60]}\")\n        print(f\"[INF]   Task prefix: '{TASK_PREFIX}'\")\n        print(f\"[INF]   Thresholds: span={span_th:.2f}, uncertainty={u_th:.2f}\")\n        if USE_LORA:\n            print(f\"[INF]   LoRA mode: ENABLED\")\n\n    try:\n        core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd and hasattr(dscd, 'cleanup_memory'):\n            if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                print(\"[INF] Running DSCD cleanup before inference...\")\n            dscd.cleanup_memory()\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] DSCD cleanup failed: {type(e).__name__}\")\n\n    dscd_homographs = get_dscd_homographs(model)\n\n    try:\n        # ‚úÖ CHANGED: Add task prefix for T5\n        prefixed_input = TASK_PREFIX + input_sentence\n\n        enc = tokenizer(\n            prefixed_input,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len,\n        )\n        enc = to_device_batch(enc, device)\n\n        model.eval()\n        core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n        src_texts = [input_sentence]\n\n        dscd_validated = False\n        try:\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n            if dscd:\n                lock = getattr(dscd, 'buffer_lock', None) or getattr(dscd, 'clustering_lock', None)\n\n                num_stores = 0\n                multi_sense = 0\n\n                if lock:\n                    try:\n                        with lock:\n                            num_stores = len(dscd.prototype_stores)\n                            multi_sense = sum(\n                                1\n                                for store in dscd.prototype_stores.values()\n                                if hasattr(store, 'centroids') and len(store.centroids) >= 2\n                            )\n                    except Exception:\n                        pass\n                else:\n                    try:\n                        num_stores = len(dscd.prototype_stores)\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if hasattr(store, 'centroids') and len(store.centroids) >= 2\n                        )\n                    except Exception:\n                        pass\n\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(\n                        f\"[INF] DSCD state: {num_stores} tokens, \"\n                        f\"{multi_sense} multi-sense, {len(dscd_homographs)} discovered\"\n                    )\n\n                if num_stores == 0:\n                    if track_stats:\n                        INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[INF] DSCD validation failed: {e}\")\n\n        with torch.inference_mode():\n            dscd_out_dict: Dict[str, Any] = {}\n\n            try:\n                if not hasattr(core, \"t5\"):\n                    raise RuntimeError(\"Model backend missing .t5 attribute\")\n\n                if DEBUG_DISCOVERY:\n                    print(\"[INF] Step 1: Running DSCD-augmented forward pass for explanations...\")\n\n                fwd_outputs = core(\n                    input_ids=enc.get(\"input_ids\"),\n                    attention_mask=enc.get(\"attention_mask\"),\n                    src_texts=src_texts,\n                    token_word_map=None,\n                    labels=None,\n                    return_dict=True,\n                    path=2\n                )\n\n                dscd_out_dict = extract_dscd_outputs(fwd_outputs)\n\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] DSCD outputs extracted: {list(dscd_out_dict.keys()) if isinstance(dscd_out_dict, dict) else 'NOT_DICT'}\")\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD forward error: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                dscd_out_dict = {}\n\n            try:\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Step 2: Running generation...\")\n\n                with torch.no_grad():\n                    fwd_out = core(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=src_texts,\n                        token_word_map=None,\n                        labels=None,\n                        return_dict=True,\n                        path=2\n                    )\n\n                h_sense = fwd_out.get('encoder_last_hidden_state', None)\n                if h_sense is None:\n                    h_sense = fwd_out.get('sense_augmented_embeddings', None)\n                if h_sense is None:\n                    h_sense = fwd_out.get('last_hidden_state', None)\n\n                if h_sense is None:\n                    raise ValueError(\"No encoder outputs found in forward pass\")\n\n                encoder_outputs_wrapped = BaseModelOutput(\n                    last_hidden_state=h_sense,\n                    hidden_states=None,\n                    attentions=None\n                )\n\n                try:\n                    generated = core.t5.generate(\n                        input_ids=None,\n                        encoder_outputs=encoder_outputs_wrapped,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=max_len,\n                        min_length=EVAL_MIN_LENGTH,\n                        num_beams=EVAL_NUM_BEAMS,\n                        early_stopping=True,\n                        length_penalty=EVAL_LENGTH_PENALTY,\n                        no_repeat_ngram_size=EVAL_NO_REPEAT_NGRAM_SIZE,\n                        repetition_penalty=1.2,\n                    )\n\n                except RuntimeError as oom_err:\n                    if \"out of memory\" in str(oom_err).lower():\n                        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                            print(\"[INF] OOM during generation, retrying with reduced settings\")\n                        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n                        with torch.no_grad():\n                            fwd_out = core(\n                                input_ids=enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                src_texts=src_texts,\n                                token_word_map=None,\n                                labels=None,\n                                return_dict=True,\n                                path=2\n                            )\n\n                        h_sense = (\n                            fwd_out.get('encoder_last_hidden_state', None) or\n                            fwd_out.get('sense_augmented_embeddings', None) or\n                            fwd_out.get('last_hidden_state', None)\n                        )\n\n                        encoder_outputs_wrapped = BaseModelOutput(last_hidden_state=h_sense)\n\n                        generated = core.t5.generate(\n                            input_ids=None,\n                            encoder_outputs=encoder_outputs_wrapped,\n                            attention_mask=enc.get(\"attention_mask\"),\n                            max_length=min(max_len, 128),\n                            min_length=EVAL_MIN_LENGTH,\n                            num_beams=max(1, min(4, EVAL_NUM_BEAMS)),\n                            early_stopping=True,\n                            length_penalty=EVAL_LENGTH_PENALTY,\n                            no_repeat_ngram_size=EVAL_NO_REPEAT_NGRAM_SIZE,\n                            repetition_penalty=1.2,\n                        )\n                    else:\n                        raise\n\n                # ‚úÖ FIX #4: IMPROVED DECODING WITH CAPITALIZATION\n                if generated is None:\n                    translation = \"\"\n                else:\n                    try:\n                        gen_ids = generated[0] if isinstance(generated, (list, tuple)) else generated[0]\n                        gen_ids = gen_ids.detach().cpu().numpy().tolist() if hasattr(gen_ids, \"detach\") else gen_ids\n                        translation = tokenizer.decode(gen_ids, skip_special_tokens=True)\n                        \n                        # ‚úÖ NEW: Clean and capitalize\n                        translation = clean_and_capitalize_translation(translation)\n                        \n                    except Exception:\n                        try:\n                            translation = tokenizer.decode(generated[0], skip_special_tokens=True)\n                            translation = clean_and_capitalize_translation(translation)\n                        except Exception:\n                            translation = \"\"\n\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Translation: {translation[:60] if translation else 'EMPTY'}\")\n\n                # ‚úÖ FIX #5: BETTER EMPTY TRANSLATION HANDLING\n                if not translation or not translation.strip():\n                    error_msg = \"Empty generation result\"\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(f\"[INF] ERROR: {error_msg}\")\n                        print(f\"[INF] Input was: {input_sentence[:50]}\")\n                        print(f\"[INF] Generated IDs: {generated[0].tolist() if generated is not None else 'None'}\")\n                    \n                    return {\n                        \"input_sentence\": input_sentence,\n                        \"translation\": \"\",\n                        \"ambiguous_words_detected\": 0,\n                        \"explanations\": [],\n                        \"quality_metrics\": {},\n                        \"dscd_validated\": dscd_validated,\n                        \"error\": error_msg\n                    }\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] Generation error: {type(e).__name__}: {str(e)}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n\n                return {\n                    \"input_sentence\": input_sentence,\n                    \"translation\": \"\",\n                    \"ambiguous_words_detected\": 0,\n                    \"explanations\": [],\n                    \"quality_metrics\": {},\n                    \"dscd_validated\": dscd_validated,\n                    \"error\": f\"Generation failed: {type(e).__name__}\"\n                }\n\n            if DEBUG_DISCOVERY:\n                print(\"[INF] Step 3: Calling TRG to generate explanations...\")\n\n            out_explanations: List[Dict[str, Any]] = []\n\n            try:\n                trg = core.trg if hasattr(core, 'trg') else None\n\n                if trg and hasattr(trg, 'process_sentence_for_explanations'):\n                    try:\n                        tokens_list = tokenizer.convert_ids_to_tokens(enc['input_ids'][0].tolist())\n\n                        if DEBUG_DISCOVERY:\n                            print(f\"[INF] Calling TRG with {len(tokens_list)} tokens\")\n\n                        trg_explanations = trg.process_sentence_for_explanations(\n                            tokens=tokens_list,\n                            dscd_outputs=dscd_out_dict,\n                            token_word_map=None,\n                            uncertainty_threshold=u_th,\n                            decoder_attention=None\n                        )\n\n                        if DEBUG_DISCOVERY:\n                            print(f\"[INF] TRG returned {len(trg_explanations) if isinstance(trg_explanations, list) else 0} explanations\")\n\n                        if isinstance(trg_explanations, list):\n                            for exp in trg_explanations:\n                                try:\n                                    raw_word = exp.get('token', '')\n\n                                    if is_punctuation_only(str(raw_word)):\n                                        continue\n\n                                    clean_word = clean_token(str(raw_word)) if raw_word else ''\n\n                                    if not clean_word:\n                                        continue\n\n                                    if should_filter_explanation(exp, span_th, u_th):\n                                        continue\n\n                                    s = float(exp.get('span', 0.0))\n                                    u = float(exp.get('uncertainty', 0.0))\n                                    confidence = max(s, u)\n\n                                    expl_text = exp.get('explanation', '')\n                                    if not expl_text:\n                                        continue\n\n                                    out_explanations.append({\n                                        \"ambiguous_word\": clean_word,\n                                        \"position\": exp.get(\"token_idx\", \"N/A\"),\n                                        \"explanation\": expl_text,\n                                        \"uncertainty\": u,\n                                        \"span\": s,\n                                        \"confidence\": confidence,\n                                        \"is_real_amb\": bool((s > span_th) or (u > u_th)),\n                                    })\n                                except Exception as e:\n                                    if DEBUG_DISCOVERY:\n                                        print(f\"[INF] Error processing TRG explanation: {e}\")\n                                    continue\n\n                    except Exception as e:\n                        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                            print(f\"[INF] TRG processing failed: {e}\")\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] TRG not available or missing process_sentence_for_explanations()\")\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] TRG invocation error: {e}\")\n\n            real_amb_count = sum(1 for e in out_explanations if e.get('is_real_amb', False))\n\n            quality_metrics = {\n                'total_raw_explanations': len(out_explanations),\n                'filtered_explanations': 0,\n                'high_confidence_count': sum(1 for e in out_explanations if e.get('confidence', 0) >= 0.65),\n                'low_confidence_count': sum(1 for e in out_explanations if e.get('confidence', 0) < 0.4),\n                'avg_confidence': sum(e.get('confidence', 0) for e in out_explanations) / max(len(out_explanations), 1),\n                'avg_span': sum(e.get('span', 0) for e in out_explanations) / max(len(out_explanations), 1),\n                'avg_uncertainty': sum(e.get('uncertainty', 0) for e in out_explanations) / max(len(out_explanations), 1),\n            }\n\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[INF] Final: {len(out_explanations)} explanations \"\n                    f\"(real ambiguous: {real_amb_count})\"\n                )\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n\n            if track_stats:\n                INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n            return result\n\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": f\"{type(e).__name__}: {str(e)[:150]}\",\n        }\n\n        if track_stats:\n            INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    \"\"\"‚úÖ CHANGED: Now shows capitalization in demo\"\"\"\n    if sentences is None:\n        sentences = [\n            \"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\",\n            \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\",\n            \"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\",\n            \"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\",\n            \"‡¶Ü‡¶ú ‡¶≠‡¶æ‡¶≤ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡•§\",\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations (BanglaT5 + Capitalization)\")\n    print(\"=\" * 80)\n\n    INFERENCE_STATS.reset()\n\n    for s in sentences:\n        print(f\"\\nInput: {s}\")\n        res = translate_with_explanations(model, tokenizer, s, source_lang=\"bn\", target_lang=\"en\")\n        translation = res.get(\"translation\", \"\")\n        \n        # ‚úÖ NEW: Highlight capitalization\n        if translation and len(translation) > 0:\n            first_char = translation[0]\n            print(f\"Translation: {translation}\")\n            if first_char.isupper():\n                print(f\"  ‚úÖ Capitalized: '{first_char}' is uppercase\")\n            else:\n                print(f\"  ‚ö†Ô∏è  Not capitalized: '{first_char}' is not uppercase\")\n        else:\n            print(\"Translation: (empty)\")\n            \n        print(\"Ambiguous words detected:\", res.get(\"ambiguous_words_detected\", 0))\n\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(\n                f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, \"\n                f\"high={quality.get('high_confidence_count', 0)}, \"\n                f\"low={quality.get('low_confidence_count', 0)}\"\n            )\n\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(\n                    f\"  {idx}. '{ex['ambiguous_word']}' \"\n                    f\"pos={ex['position']} conf={ex.get('confidence', 0):.3f}\"\n                )\n                print(\"     \", ex.get(\"explanation\", \"\")[:200])\n        else:\n            print(\"  No explanations\")\n\n    print(\"=\" * 80)\n    INFERENCE_STATS.print_summary()\n\n\ndef dscd_discovery_warmup(\n    model,\n    tokenizer,\n    num_sents: int = 8000,\n    batch_size: int = 64,\n    max_len: Optional[int] = None,\n):\n    \"\"\"\n    ‚úÖ CHANGED: Adds task prefix to warmup sentences\n    \"\"\"\n    if max_len is None:\n        max_len = MAXLEN\n\n    core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component\")\n            return\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[WARMUP] Starting DSCD discovery warmup (BanglaT5)\")\n        print(\"=\" * 80)\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_n_min = getattr(dscd, \"n_min\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"n_min\"):\n                dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for (bn, _) in pairs][:num_sents]\n            else:\n                base = [\n                    \"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\",\n                    \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\",\n                    \"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\",\n                    \"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\",\n                ]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n        except Exception:\n            texts = [\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\"] * num_sents\n\n        processed = 0\n        core.eval()\n\n        print(f\"\\n[WARMUP] Processing {len(texts)} sentences (batch={batch_size})...\")\n\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                \n                # ‚úÖ CHANGED: Add task prefix to each sentence\n                prefixed_batch = [TASK_PREFIX + sent for sent in batch]\n                \n                try:\n                    enc = tokenizer(\n                        prefixed_batch,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_len,\n                    )\n                    enc = to_device_batch(enc, DEVICE)\n\n                    core(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=batch,\n                        token_word_map=None,\n                        labels=None,\n                        return_dict=True,\n                        path=2\n                    )\n\n                    processed += len(batch)\n\n                    current_time = time.time()\n                    if (i // batch_size) % 10 == 0 or (current_time - last_print) > 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n                        print(\n                            f\"[WARMUP] {processed}/{len(texts)} \"\n                            f\"({processed/len(texts)*100:.1f}%) | \"\n                            f\"{rate:.1f} sent/s | ETA {eta:.0f}s\"\n                        )\n                        last_print = current_time\n\n                    del enc\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        print(f\"[WARMUP] OOM at batch {i//batch_size}, cleaning up...\")\n                        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n                        gc.collect()\n                        continue\n                    else:\n                        print(f\"[WARMUP] Batch {i//batch_size} failed: {str(e)[:100]}\")\n                        continue\n                except Exception as e:\n                    print(f\"[WARMUP] Batch {i//batch_size} failed: {str(e)[:100]}\")\n                    continue\n\n        total_time = time.time() - start_time\n        print(\n            f\"\\n[WARMUP] Completed in {total_time:.1f}s \"\n            f\"({processed/total_time:.1f} sent/s)\"\n        )\n        print(\"-\" * 80)\n\n        try:\n            if dscd and hasattr(dscd, 'cleanup_memory'):\n                print(\"[WARMUP] Running DSCD cleanup after warmup...\")\n                dscd.cleanup_memory()\n        except Exception as e:\n            if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                print(f\"[WARMUP] DSCD cleanup failed: {type(e).__name__}\")\n\n        try:\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            num_types = len(stores)\n            total_protos = (\n                sum(store.size() for store in stores.values()) if stores else 0\n            )\n            multi = (\n                sum(1 for store in stores.values() if store.size() >= 2)\n                if stores\n                else 0\n            )\n\n            print(\"[WARMUP] Summary:\")\n            print(f\"  - Token types: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n\n            dscd_homographs = get_dscd_homographs(model)\n\n            print(f\"\\n[WARMUP] Discovered Homographs: {len(dscd_homographs)}\")\n            if dscd_homographs:\n                print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n            reference_found = dscd_homographs.intersection(HOMOGRAPH_REFERENCE_LIST)\n\n            print(f\"\\n[WARMUP] Reference List Comparison:\")\n            print(f\"  - Reference list: {len(HOMOGRAPH_REFERENCE_LIST)} words\")\n            print(f\"  - Found in DSCD: {len(reference_found)}\")\n            print(\n                f\"  - Coverage: {len(reference_found)/len(HOMOGRAPH_REFERENCE_LIST):.1%}\"\n            )\n\n            if num_types == 0:\n                print(\"\\n[WARMUP] CRITICAL: NO PROTOTYPES CREATED\")\n            elif len(reference_found) < len(HOMOGRAPH_REFERENCE_LIST) // 2:\n                print(\"\\n[WARMUP] WARNING: < 50% reference coverage\")\n            else:\n                print(\"\\n[WARMUP] SUCCESS\")\n\n        except Exception as e:\n            print(f\"[WARMUP] Validation failed: {e}\")\n\n    finally:\n        try:\n            if dscd is not None:\n                if hasattr(dscd, \"enable_training_clustering\"):\n                    dscd.enable_training_clustering = orig_enable\n                if hasattr(dscd, \"n_min\") and orig_n_min is not None:\n                    dscd.n_min = orig_n_min\n                if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                    dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n        print(\"=\" * 80 + \"\\n\")\n\n\ndef final_evaluation_with_bleu(\n    model,\n    tokenizer,\n    test_data: List[Tuple[str, str]],\n    device: Optional[torch.device] = None,\n    max_length: Optional[int] = None,\n    batch_size: int = 16,\n) -> Dict[str, Any]:\n    \"\"\"‚úÖ CHANGED: Now reports capitalization rate\"\"\"\n    device = DEVICE if device is None else device\n    max_len = MAXLEN if max_length is None else int(max_length)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL EVALUATION WITH BLEU/CHRF++ (BanglaT5 + Capitalization)\")\n    print(\"=\" * 80)\n    print(f\"Test samples: {len(test_data)}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Max length: {max_len}\")\n    print(\"=\" * 80 + \"\\n\")\n\n    try:\n        core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd and hasattr(dscd, 'cleanup_memory'):\n            print(\"[EVAL] Running DSCD cleanup before evaluation...\")\n            dscd.cleanup_memory()\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[EVAL] DSCD cleanup failed: {type(e).__name__}\")\n\n    INFERENCE_STATS.reset()\n\n    predictions = []\n    references = []\n    translations_with_explanations = []\n    capitalization_count = 0  # ‚úÖ NEW: Track capitalization\n\n    model.eval()\n\n    try:\n        from sacrebleu.metrics import BLEU, CHRF\n        bleu_metric = BLEU()\n        chrf_metric = CHRF()\n        metrics_available = True\n    except ImportError:\n        print(\"[EVAL] WARNING: sacrebleu not available, BLEU/CHRF scores will not be computed\")\n        metrics_available = False\n\n    start_time = time.time()\n\n    with torch.inference_mode():\n        for i in range(0, len(test_data), batch_size):\n            batch = test_data[i:i+batch_size]\n\n            for src, ref in batch:\n                try:\n                    result = translate_with_explanations(\n                        model,\n                        tokenizer,\n                        src,\n                        source_lang=\"bn\",\n                        target_lang=\"en\",\n                        device=device,\n                        max_length=max_len,\n                        track_stats=True\n                    )\n\n                    translation = result.get('translation', '')\n                    \n                    # ‚úÖ NEW: Count capitalizations\n                    if translation and len(translation) > 0 and translation[0].isupper():\n                        capitalization_count += 1\n\n                    predictions.append(translation)\n                    references.append(ref)\n                    translations_with_explanations.append({\n                        'source': src,\n                        'reference': ref,\n                        'translation': translation,\n                        'explanations': result.get('explanations', []),\n                        'ambiguous_words': result.get('ambiguous_words_detected', 0)\n                    })\n\n                except Exception as e:\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(f\"[EVAL] Translation failed for: {src[:50]} - {type(e).__name__}\")\n                    predictions.append(\"\")\n                    references.append(ref)\n                    translations_with_explanations.append({\n                        'source': src,\n                        'reference': ref,\n                        'translation': \"ERROR\",\n                        'explanations': [],\n                        'ambiguous_words': 0\n                    })\n\n            if (i // batch_size) % 10 == 0:\n                elapsed = time.time() - start_time\n                processed = min(i + batch_size, len(test_data))\n                rate = processed / elapsed if elapsed > 0 else 0\n                eta = (len(test_data) - processed) / rate if rate > 0 else 0\n                print(f\"[EVAL] {processed}/{len(test_data)} ({processed/len(test_data)*100:.1f}%) | {rate:.1f} sent/s | ETA {eta:.0f}s\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n[EVAL] Translation completed in {total_time:.1f}s ({len(test_data)/total_time:.1f} sent/s)\")\n\n    results = {\n        'total_samples': len(test_data),\n        'successful_translations': sum(1 for p in predictions if p and p != \"ERROR\"),\n        'failed_translations': sum(1 for p in predictions if not p or p == \"ERROR\"),\n        'total_time': total_time,\n        'throughput': len(test_data) / total_time,\n        'predictions': predictions,\n        'references': references,\n        'translations_with_explanations': translations_with_explanations,\n        'capitalization_count': capitalization_count,  # ‚úÖ NEW\n        'capitalization_rate': capitalization_count / len(test_data) if len(test_data) > 0 else 0.0,  # ‚úÖ NEW\n    }\n\n    if metrics_available and predictions and references:\n        try:\n            valid_preds = []\n            valid_refs = []\n            for p, r in zip(predictions, references):\n                if p and p != \"ERROR\" and r:\n                    valid_preds.append(p)\n                    valid_refs.append(r)\n\n            if valid_preds:\n                bleu_score = bleu_metric.corpus_score(valid_preds, [valid_refs])\n                chrf_score = chrf_metric.corpus_score(valid_preds, [valid_refs])\n\n                results['bleu'] = float(bleu_score.score)\n                results['chrf'] = float(chrf_score.score)\n\n                print(\"\\n\" + \"=\" * 80)\n                print(\"METRIC SCORES\")\n                print(\"=\" * 80)\n                print(f\"BLEU:    {results['bleu']:.2f}\")\n                print(f\"CHRF++:  {results['chrf']:.2f}\")\n                print(f\"Valid samples: {len(valid_preds)}/{len(predictions)}\")\n                print(f\"‚úÖ Capitalization rate: {results['capitalization_rate']:.1%} ({capitalization_count}/{len(test_data)})\")  # ‚úÖ NEW\n                print(\"=\" * 80)\n            else:\n                print(\"[EVAL] WARNING: No valid translations for BLEU/CHRF computation\")\n                results['bleu'] = 0.0\n                results['chrf'] = 0.0\n        except Exception as e:\n            print(f\"[EVAL] Metric computation failed: {type(e).__name__}: {str(e)[:100]}\")\n            results['bleu'] = 0.0\n            results['chrf'] = 0.0\n    else:\n        results['bleu'] = 0.0\n        results['chrf'] = 0.0\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"Total samples: {results['total_samples']}\")\n    print(f\"Successful: {results['successful_translations']}\")\n    print(f\"Failed: {results['failed_translations']}\")\n    print(f\"Success rate: {results['successful_translations']/results['total_samples']:.1%}\")\n    print(f\"Throughput: {results['throughput']:.1f} sent/s\")\n    print(f\"‚úÖ Capitalization rate: {results['capitalization_rate']:.1%}\")  # ‚úÖ NEW\n    print(\"=\" * 80 + \"\\n\")\n\n    INFERENCE_STATS.print_summary()\n\n    return results\n\n\ndef load_checkpoint_for_resume(\n    model: torch.nn.Module, optimizer, checkpoint_path: str\n) -> Tuple[bool, int, int, float]:\n    \"\"\"‚úÖ UNCHANGED: Checkpoint loading (model-agnostic)\"\"\"\n    if not os.path.exists(checkpoint_path):\n        print(f\"[CHECKPOINT] Not found: {checkpoint_path}\")\n        return False, 0, 0, 0.0\n\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] Load failed: {e}\")\n        return False, 0, 0, 0.0\n\n    core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    state = ckpt.get(\"model_state_dict\", ckpt)\n    try:\n        core.load_state_dict(state, strict=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] model.load_state_dict failed: {e}\")\n\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                core.load_state_dict(new_state, strict=False)\n        except Exception:\n            pass\n\n    try:\n        if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n    except Exception as e:\n        print(f\"[CHECKPOINT] optimizer.load_state_dict failed: {e}\")\n\n    try:\n        if \"dscd_state\" in ckpt and ckpt[\"dscd_state\"]:\n            dscd_state = ckpt[\"dscd_state\"]\n\n            print(\"[CHECKPOINT] Restoring DSCD...\")\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n\n            if dscd and hasattr(dscd, 'load_state_dict'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        dscd.load_state_dict(dscd_state)\n                        num_tokens = len(dscd.prototype_stores)\n                        total_protos = sum(\n                            store.size() for store in dscd.prototype_stores.values()\n                        )\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if store.size() >= 2\n                        )\n                else:\n                    dscd.load_state_dict(dscd_state)\n                    num_tokens = len(dscd.prototype_stores)\n                    total_protos = sum(\n                        store.size() for store in dscd.prototype_stores.values()\n                    )\n                    multi_sense = sum(\n                        1\n                        for store in dscd.prototype_stores.values()\n                        if store.size() >= 2\n                    )\n\n                print(\"[CHECKPOINT] DSCD restored:\")\n                print(f\"  - Tokens: {num_tokens}\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multi_sense}\")\n\n                if num_tokens == 0:\n                    print(\n                        \"[CHECKPOINT] WARNING: DSCD state empty - consider running warmup\"\n                    )\n            else:\n                print(\"[CHECKPOINT] Model has no dscd.load_state_dict\")\n        else:\n            print(\"[CHECKPOINT] No DSCD state in checkpoint\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] DSCD restore failed: {e}\")\n\n    epoch = int(ckpt.get(\"epochs_trained\", ckpt.get(\"epoch\", 0)))\n    step = int(\n        ckpt.get(\n            \"global_steps\", ckpt.get(\"global_step\", ckpt.get(\"step\", 0))\n        )\n    )\n    avg_loss = float(\n        ckpt.get(\n            \"final_train_loss\",\n            ckpt.get(\"avg_epoch_loss\", ckpt.get(\"avg_loss\", 0.0)),\n        )\n    )\n\n    print(f\"[CHECKPOINT] Loaded: epoch={epoch} step={step} loss={avg_loss:.6f}\")\n    return True, epoch, step, avg_loss\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 8: Inference & Evaluation Pipeline [‚úÖ FULLY FIXED + CAPITALIZATION]\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Model: BanglaT5 (csebuetnlp/banglat5)\")\nprint(f\"  - Task prefix: '{TASK_PREFIX}'\")\nprint(f\"  - Source language: {SOURCE_LANG}\")\nprint(f\"  - Target language: {TARGET_LANG}\")\nprint(f\"  - LoRA mode: {'ENABLED' if USE_LORA else 'DISABLED'}\")\nprint(f\"  - Span threshold: {SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Max length: {MAXLEN}\")\nprint(f\"  - Device: {DEVICE}\")\nprint(f\"  - Eval num beams: {EVAL_NUM_BEAMS}\")\nprint(f\"  - Eval length penalty: {EVAL_LENGTH_PENALTY}\")\nprint(f\"  - Eval no repeat ngram size: {EVAL_NO_REPEAT_NGRAM_SIZE}\")\nprint(f\"  - Eval min length: {EVAL_MIN_LENGTH}\")\nprint(\"\\n‚úÖ FIXES APPLIED:\")\nprint(\"  ‚úÖ FIX #1: Added LoRA compatibility check\")\nprint(\"  ‚úÖ FIX #2: Improved capitalization function (works for all languages)\")\nprint(\"  ‚úÖ FIX #3: Better empty input handling\")\nprint(\"  ‚úÖ FIX #4: Improved decoding with capitalization\")\nprint(\"  ‚úÖ FIX #5: Better empty translation safeguards\")\nprint(\"  ‚úÖ FIX #6: Capitalization rate reporting in evaluation\")\nprint(\"\\n‚ö° CAPITALIZATION:\")\nprint(\"  - ‚úÖ capitalize_first_char(): Finds first alphabetic char and uppercases it\")\nprint(\"  - ‚úÖ clean_and_capitalize_translation(): Cleans + capitalizes output\")\nprint(\"  - ‚úÖ Works for English (ASCII) and Bengali (Unicode)\")\nprint(\"  - ‚úÖ Preserves leading whitespace/punctuation\")\nprint(\"  - ‚úÖ Applied to ALL translations (demo, warmup, eval)\")\nprint(\"\\n‚ö° INFERENCE FLOW:\")\nprint(\"  1. Add task prefix to input\")\nprint(\"  2. Tokenize prefixed input\")\nprint(\"  3. Run DSCD-augmented forward (path=2)\")\nprint(\"  4. Extract sense-augmented encoder outputs\")\nprint(\"  5. Generate with core.t5.generate()\")\nprint(\"  6. Decode + clean + ‚úÖ CAPITALIZE\")\nprint(\"  7. Extract TRG explanations\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"7Dxg7ck0H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.839024Z","iopub.execute_input":"2026-02-16T03:01:26.839251Z","iopub.status.idle":"2026-02-16T03:01:26.951667Z","shell.execute_reply.started":"2026-02-16T03:01:26.839231Z","shell.execute_reply":"2026-02-16T03:01:26.951029Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 8: Inference & Evaluation Pipeline [‚úÖ FULLY FIXED + CAPITALIZATION]\n================================================================================\nConfiguration:\n  - Model: BanglaT5 (csebuetnlp/banglat5)\n  - Task prefix: 'translate Bengali to English: '\n  - Source language: bn\n  - Target language: en\n  - LoRA mode: ENABLED\n  - Span threshold: 0.18\n  - Uncertainty threshold: 0.12\n  - Max length: 128\n  - Device: cuda:0\n  - Eval num beams: 8\n  - Eval length penalty: 1.2\n  - Eval no repeat ngram size: 2\n  - Eval min length: 3\n\n‚úÖ FIXES APPLIED:\n  ‚úÖ FIX #1: Added LoRA compatibility check\n  ‚úÖ FIX #2: Improved capitalization function (works for all languages)\n  ‚úÖ FIX #3: Better empty input handling\n  ‚úÖ FIX #4: Improved decoding with capitalization\n  ‚úÖ FIX #5: Better empty translation safeguards\n  ‚úÖ FIX #6: Capitalization rate reporting in evaluation\n\n‚ö° CAPITALIZATION:\n  - ‚úÖ capitalize_first_char(): Finds first alphabetic char and uppercases it\n  - ‚úÖ clean_and_capitalize_translation(): Cleans + capitalizes output\n  - ‚úÖ Works for English (ASCII) and Bengali (Unicode)\n  - ‚úÖ Preserves leading whitespace/punctuation\n  - ‚úÖ Applied to ALL translations (demo, warmup, eval)\n\n‚ö° INFERENCE FLOW:\n  1. Add task prefix to input\n  2. Tokenize prefixed input\n  3. Run DSCD-augmented forward (path=2)\n  4. Extract sense-augmented encoder outputs\n  5. Generate with core.t5.generate()\n  6. Decode + clean + ‚úÖ CAPITALIZE\n  7. Extract TRG explanations\n================================================================================\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION - BanglaT5 + Standard LoRA (FP16)\n# ===========================================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.10\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.10\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 64\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\", \"‡¶¨‡¶æ‡¶∞\", \"‡¶π‡¶æ‡¶∞\", \"‡¶§‡¶æ‡¶∞‡¶æ\",\n        \"‡¶™‡¶æ‡¶®‡¶ø\", \"‡¶¶‡¶≤\", \"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞\", \"‡¶®‡¶æ‡¶Æ\", \"‡¶ï‡¶•‡¶æ\", \"‡¶¨‡¶á\", \"‡¶ò‡¶∞\", \"‡¶Æ‡¶®\", \"‡¶π‡¶æ‡¶§\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    _USE_LORA = bool(USE_LORA)\nexcept (NameError, TypeError):\n    _USE_LORA = False\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                return len(stores)\n        else:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            return len(stores)\n    except Exception:\n        return 0\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                for token, store in prototype_stores.items():\n                    try:\n                        if hasattr(store, 'size') and store.size() >= 1:\n                            clean_token = (\n                                str(token)\n                                .replace('‚ñÅ', '')\n                                .replace('ƒ†', '')\n                                .replace('##', '')\n                                .strip()\n                                .lower()\n                            )\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            for token, store in prototype_stores.items():\n                try:\n                    if hasattr(store, 'size') and store.size() >= 1:\n                        clean_token = (\n                            str(token)\n                            .replace('‚ñÅ', '')\n                            .replace('ƒ†', '')\n                            .replace('##', '')\n                            .strip()\n                            .lower()\n                        )\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = len(getattr(store, \"centroids\", []))\n            except Exception:\n                n_protos = 0\n            cluster_info.append({\n                'token': token,\n                'count': total_count,\n                'protos': n_protos,\n                'mu': getattr(store, \"mu\", 0.0),\n                'tau': getattr(store, \"tau\", 0.0)\n            })\n\n        cluster_info.sort(key=lambda x: x['count'], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info['token'])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n\n        print(\"-\" * 90)\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef _compute_similarity(pred: str, expected: str) -> float:\n    try:\n        pred_words = set(pred.lower().split())\n        exp_words = set(expected.lower().split())\n        \n        if not exp_words:\n            return 0.0\n        \n        intersection = len(pred_words & exp_words)\n        union = len(pred_words | exp_words)\n        \n        if union == 0:\n            return 0.0\n        \n        jaccard = intersection / union\n        recall = intersection / len(exp_words)\n        similarity = 0.6 * recall + 0.4 * jaccard\n        \n        return similarity\n    except Exception:\n        return 0.0\n\n\ndef _check_capitalization(text: str) -> Dict[str, Any]:\n    if not text or not isinstance(text, str) or len(text) == 0:\n        return {\n            'is_capitalized': False,\n            'first_char': '',\n            'issue': 'empty_text'\n        }\n    \n    for idx, char in enumerate(text):\n        if char.isalpha():\n            return {\n                'is_capitalized': char.isupper(),\n                'first_char': char,\n                'first_char_index': idx,\n                'issue': None if char.isupper() else 'not_uppercase'\n            }\n    \n    return {\n        'is_capitalized': False,\n        'first_char': '',\n        'issue': 'no_alphabetic_char'\n    }\n\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION\")\n    print(\"=\" * 80)\n\n    if 'translate_with_explanations' not in globals():\n        print(\"[EVAL] ERROR: translate_with_explanations not found!\")\n        print(\"[EVAL] Cell 8 must be executed first.\")\n        return {\n            \"error\": \"translate_with_explanations not found\",\n            \"total_tests\": 0,\n            \"successful_translations\": 0,\n        }\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap\", \"‡¶ï‡¶≤ = tap/call\", [\"‡¶ï‡¶≤\"]),\n        (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"Tomorrow I will buy a book\", \"‡¶ï‡¶æ‡¶≤ = tomorrow/yesterday\", [\"‡¶ï‡¶æ‡¶≤\"]),\n        (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"The leaf has fallen\", \"‡¶™‡¶æ‡¶§‡¶æ = leaf/page\", [\"‡¶™‡¶æ‡¶§‡¶æ\"]),\n        (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\", \"He went to the bank\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï = bank/embankment\", [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"]),\n        (\"‡¶´‡¶≤ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶∏‡ßç‡¶¨‡¶æ‡¶¶‡ßÅ‡•§\", \"The fruit is delicious\", \"‡¶´‡¶≤ = fruit/result\", [\"‡¶´‡¶≤\"]),\n        (\"‡¶Æ‡¶æ‡¶•‡¶æ ‡¶¨‡ßç‡¶Ø‡¶•‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡•§\", \"Head is aching\", \"‡¶Æ‡¶æ‡¶•‡¶æ = head/top\", [\"‡¶Æ‡¶æ‡¶•‡¶æ\"]),\n        (\"‡¶ï‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶è‡¶∏‡ßá‡¶õ‡ßá‡•§\", \"A call came from the tap\", \"Multiple ‡¶ï‡¶≤\", [\"‡¶ï‡¶≤\"]),\n        (\"‡¶ï‡¶æ‡¶≤‡¶ï‡ßá ‡¶ï‡¶æ‡¶≤ ‡¶Æ‡ßá‡¶ò ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶ó‡ßá‡¶õ‡ßá‡•§\", \"Yesterday black clouds were seen\", \"Multiple ‡¶ï‡¶æ‡¶≤\", [\"‡¶ï‡¶æ‡¶≤\"]),\n        (\"‡¶Ü‡¶ú ‡¶≠‡¶æ‡¶≤ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡•§\", \"Weather is good today\", \"Simple\", []),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø‡•§\", \"I am fine\", \"Simple\", []),\n        (\"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"She speaks sweetly\", \"Simple\", []),\n        (\"‡¶è‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶á‡•§\", \"This is my book\", \"Simple\", []),\n        (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡¶® ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶¨‡¶∏‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡¶®‡•§\",\n         \"He works at the bank and sits on the embankment\",\n         \"Long with multiple\", [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    print(\"\\n[EVALUATION] Checking LoRA status...\")\n    try:\n        total_params = sum(p.numel() for p in core_model.parameters())\n        trainable_params = sum(p.numel() for p in core_model.parameters() if p.requires_grad)\n        lora_enabled = getattr(core_model, 'lora_applied', False)\n        \n        if lora_enabled:\n            print(f\"  ‚úÖ LoRA Mode: Standard LoRA (FP16)\")\n            print(f\"     Total params: {total_params:,}\")\n            print(f\"     Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n            \n            trainable_pct = trainable_params / total_params\n            if trainable_pct > 0.1:\n                print(f\"  ‚ö†Ô∏è  WARNING: Trainable params > 10% (unusual for LoRA)\")\n            elif trainable_pct < 0.005:\n                print(f\"  ‚ö†Ô∏è  WARNING: Trainable params < 0.5% (very aggressive LoRA)\")\n            else:\n                print(f\"  ‚úÖ LoRA parameters in expected range (0.5-10%)\")\n        else:\n            print(f\"  ‚ÑπÔ∏è  LoRA disabled - using full fine-tuning\")\n            print(f\"     Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n            \n            if trainable_params == 0:\n                print(f\"  ‚ùå ERROR: 0 trainable params!\")\n                raise RuntimeError(\"No trainable parameters detected\")\n    except Exception as e:\n        print(f\"  ‚ö†Ô∏è  LoRA status check failed: {type(e).__name__}\")\n\n    capitalization_metrics = {\n        'total_checked': 0,\n        'capitalized_count': 0,\n        'not_capitalized_count': 0,\n        'empty_translations': 0,\n        'no_alpha_char': 0,\n        'capitalization_issues': [],\n    }\n\n    quality_metrics = {\n        'total_confidence': 0.0,\n        'confidence_samples': 0,\n        'high_confidence_count': 0,\n        'medium_confidence_count': 0,\n        'low_confidence_count': 0,\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n    }\n\n    homograph_tracking = {\n        'test_expected_homographs': set(),\n        'dscd_discovered_homographs': set(),\n        'explained_homographs': set(),\n        'homograph_explanations': defaultdict(list),\n    }\n\n    error_tracking = {\n        'translation_failures': 0,\n        'dscd_failures': 0,\n        'trg_failures': 0,\n        'timeout_errors': 0,\n        'oom_errors': 0,\n        'other_errors': 0,\n        'error_details': [],\n        'per_test_status': [],\n    }\n\n    timing_metrics = {\n        'total_time': 0.0,\n        'per_test_times': [],\n        'avg_test_time': 0.0,\n    }\n\n    sample_translations = []\n\n    discovery_validated = False\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd and hasattr(dscd, 'discovered_log'):\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    discovered_log = getattr(dscd, 'discovered_log', [])\n                    if discovered_log:\n                        discovery_validated = True\n                        last_discovery = discovered_log[-1]\n                        discovered = last_discovery.get('discovered', 0)\n                        candidates = last_discovery.get('candidates', 0)\n                        if _DEBUG_DISCOVERY:\n                            print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n            else:\n                discovered_log = getattr(dscd, 'discovered_log', [])\n                if discovered_log:\n                    discovery_validated = True\n                    last_discovery = discovered_log[-1]\n                    discovered = last_discovery.get('discovered', 0)\n                    candidates = last_discovery.get('candidates', 0)\n                    if _DEBUG_DISCOVERY:\n                        print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] No discovery log found\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] Discovery validation failed: {e}\")\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn:\n            if hasattr(asbn, 'get_detailed_stats'):\n                asbn_stats = asbn.get_detailed_stats()\n            elif hasattr(asbn, 'get_asbn_stats'):\n                asbn_stats = asbn.get_asbn_stats()\n\n            if asbn_stats and _DEBUG_DISCOVERY:\n                print(f\"[EVAL] ASBN: domain_acc={asbn_stats.get('domain_accuracy', 0):.2%}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN stats failed: {e}\")\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg\", None)\n        if trg and hasattr(trg, 'get_statistics'):\n            trg_stats = trg.get_statistics()\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] TRG: {trg_stats.get('explanations_generated', 0)} total\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] TRG stats failed: {e}\")\n\n    homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking['dscd_discovered_homographs'] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", None)\n                        store_count = len(stores) if stores else 0\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", None)\n                    store_count = len(stores) if stores else 0\n\n                if store_count == 0 and 'dscd_discovery_warmup' in globals():\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(model, tokenizer, num_sents=4000, batch_size=64)\n                        homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n                    except Exception as e:\n                        print(f\"[EVAL] Warmup failed: {e}\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n        tokenizer.tgt_lang = _TARGET_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    for _, _, _, expected_homos in test_sentences:\n        homograph_tracking['test_expected_homographs'].update([h.lower() for h in expected_homos])\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            'test_id': idx,\n            'success': False,\n            'translation_ok': False,\n            'explanations_count': 0,\n            'error': None,\n        }\n\n        try:\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                source_lang=_SOURCE_LANGUAGE,\n                target_lang=_TARGET_LANGUAGE,\n                device=_DEVICE,\n                max_length=_MAX_LENGTH,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                track_stats=False\n            )\n\n            if result is None or not isinstance(result, dict):\n                print(f\"[EVAL] Invalid result type: {type(result)}\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'invalid_result'\n                error_tracking['per_test_status'].append(test_status)\n                continue\n\n            if 'error' in result and result['error']:\n                print(f\"[EVAL] Translation error: {result['error']}\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'translation_error'\n                error_tracking['per_test_status'].append(test_status)\n                continue\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n\n            cap_check = _check_capitalization(translation)\n            capitalization_metrics['total_checked'] += 1\n            \n            if cap_check['is_capitalized']:\n                capitalization_metrics['capitalized_count'] += 1\n            else:\n                capitalization_metrics['not_capitalized_count'] += 1\n                capitalization_metrics['capitalization_issues'].append({\n                    'test_id': idx,\n                    'translation': translation[:50],\n                    'issue': cap_check['issue'],\n                    'first_char': cap_check.get('first_char', ''),\n                })\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            \n            if cap_check['is_capitalized']:\n                print(f\"‚úÖ Capitalized: '{cap_check['first_char']}' is uppercase\")\n            else:\n                print(f\"‚ö†Ô∏è  NOT capitalized: {cap_check['issue']}\")\n                if cap_check.get('first_char'):\n                    print(f\"   First char: '{cap_check['first_char']}' (should be uppercase)\")\n            \n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if len(sample_translations) < 5:\n                sample_translations.append({\n                    'test_id': idx,\n                    'source': src_text,\n                    'translation': translation,\n                    'expected': expected_translation,\n                    'capitalized': cap_check['is_capitalized'],\n                    'similarity': similarity,\n                })\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n\n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0))\n                    u_val = float(expl.get(\"uncertainty\", 0.0))\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n\n                    marker = f\"[S>{_SPAN_THRESHOLD:.2f}]\" if span_val > _SPAN_THRESHOLD else \"          \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics['confidences'].append(conf_val)\n                    quality_metrics['spans'].append(span_val)\n                    quality_metrics['uncertainties'].append(u_val)\n                    quality_metrics['total_confidence'] = quality_metrics.get('total_confidence', 0.0) + conf_val\n                    quality_metrics['confidence_samples'] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics['high_confidence_count'] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics['medium_confidence_count'] += 1\n                    else:\n                        quality_metrics['low_confidence_count'] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = str(word).replace('‚ñÅ', '').replace('ƒ†', '').strip().lower()\n                    homograph_tracking['explained_homographs'].add(clean_word)\n                    homograph_tracking['homograph_explanations'][clean_word].append({\n                        'sentence': src_text,\n                        'confidence': conf_val,\n                        'span': span_val,\n                        'uncertainty': u_val,\n                    })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status['explanations_count'] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            if translation and translation.strip() and translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            ):\n                successful_translations += 1\n                test_status['translation_ok'] = True\n                test_status['success'] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'translation_failed'\n\n            del result\n            if explanations:\n                del explanations\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking['oom_errors'] += 1\n                test_status['error'] = 'oom'\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking['timeout_errors'] += 1\n                test_status['error'] = 'timeout'\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'runtime'\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking['other_errors'] += 1\n            test_status['error'] = type(e).__name__\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking['per_test_status'].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics['per_test_times'].append(test_time)\n\n        print(\"-\" * 60)\n\n        if idx % 3 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    timing_metrics['total_time'] = time.time() - eval_start\n    if timing_metrics['per_test_times']:\n        timing_metrics['avg_test_time'] = (\n            sum(timing_metrics['per_test_times']) / len(timing_metrics['per_test_times'])\n        )\n\n    capitalization_metrics['capitalization_rate'] = (\n        capitalization_metrics['capitalized_count'] / capitalization_metrics['total_checked']\n        if capitalization_metrics['total_checked'] > 0\n        else 0.0\n    )\n\n    if quality_metrics['confidence_samples'] > 0:\n        quality_metrics['avg_confidence'] = (\n            quality_metrics['total_confidence'] / quality_metrics['confidence_samples']\n        )\n        quality_metrics['avg_span'] = (\n            sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n            if quality_metrics['spans']\n            else 0.0\n        )\n        quality_metrics['avg_uncertainty'] = (\n            sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n            if quality_metrics['uncertainties']\n            else 0.0\n        )\n\n        if quality_metrics['confidences']:\n            sorted_conf = sorted(quality_metrics['confidences'])\n            quality_metrics['confidence_p25'] = sorted_conf[len(sorted_conf) // 4]\n            quality_metrics['confidence_p50'] = sorted_conf[len(sorted_conf) // 2]\n            quality_metrics['confidence_p75'] = sorted_conf[3 * len(sorted_conf) // 4]\n    else:\n        quality_metrics['avg_confidence'] = 0.0\n        quality_metrics['avg_span'] = 0.0\n        quality_metrics['avg_uncertainty'] = 0.0\n\n    explained_from_dscd = homograph_tracking['explained_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    test_expected_discovered = homograph_tracking['test_expected_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    homograph_tracking['explained_from_dscd_rate'] = (\n        len(explained_from_dscd) / len(homograph_tracking['dscd_discovered_homographs'])\n        if homograph_tracking['dscd_discovered_homographs']\n        else 0.0\n    )\n    homograph_tracking['test_expected_discovery_rate'] = (\n        len(test_expected_discovered) / len(homograph_tracking['test_expected_homographs'])\n        if homograph_tracking['test_expected_homographs']\n        else 0.0\n    )\n    homograph_tracking['reference_discovery_rate'] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototype_stores\") or {})\n            else:\n                stores = dict(getattr(dscd, \"prototype_stores\") or {})\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for key, store in stores.items():\n                try:\n                    sz = int(store.size()) if hasattr(store, \"size\") else 0\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n            }\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] DSCD stats failed: {e}\")\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n\n    print(f\"\\n[CAPITALIZATION]\")\n    print(f\"  Total checked: {capitalization_metrics['total_checked']}\")\n    print(f\"  Capitalized: {capitalization_metrics['capitalized_count']}\")\n    print(f\"  Not capitalized: {capitalization_metrics['not_capitalized_count']}\")\n    print(f\"  ‚úÖ Capitalization rate: {capitalization_metrics['capitalization_rate']:.1%}\")\n    \n    if capitalization_metrics['not_capitalized_count'] > 0:\n        print(f\"\\n  ‚ö†Ô∏è  Capitalization issues found:\")\n        for issue in capitalization_metrics['capitalization_issues'][:5]:\n            print(f\"    - Test {issue['test_id']}: {issue['issue']}\")\n            print(f\"      Translation: {issue['translation']}\")\n            if issue.get('first_char'):\n                print(f\"      First char: '{issue['first_char']}' (expected uppercase)\")\n\n    if sample_translations:\n        print(f\"\\n[SAMPLE TRANSLATIONS]\")\n        for sample in sample_translations:\n            cap_marker = \"‚úÖ\" if sample['capitalized'] else \"‚ö†Ô∏è \"\n            print(f\"\\n  Test {sample['test_id']} {cap_marker}\")\n            print(f\"    Source: {sample['source'][:60]}\")\n            print(f\"    Output: {sample['translation'][:60]}\")\n            print(f\"    Expected: {sample['expected'][:60]}\")\n            print(f\"    Similarity: {sample['similarity']:.1%}\")\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n\n    if 'confidence_p50' in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: \"\n            f\"{quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking['explained_homographs']:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking['explained_homographs'])[:10]:\n            exps = homograph_tracking['homograph_explanations'].get(homo, [])\n            count = len(exps)\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            in_dscd = \"[D]\" if homo in homograph_tracking['dscd_discovered_homographs'] else \"   \"\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\")\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats['total_words'] > 0:\n        print(\n            f\"  Multi-sense ratio: \"\n            f\"{dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        print(f\"  Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n        if 'source_accuracy' in asbn_stats:\n            print(f\"  Source accuracy: {asbn_stats['source_accuracy']:.2%}\")\n            print(f\"  Target accuracy: {asbn_stats['target_accuracy']:.2%}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        print(f\"  Total explanations: {trg_stats.get('explanations_generated', 0)}\")\n        print(f\"  High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum([\n        error_tracking['translation_failures'],\n        error_tracking['dscd_failures'],\n        error_tracking['trg_failures'],\n        error_tracking['timeout_errors'],\n        error_tracking['oom_errors'],\n        error_tracking['other_errors'],\n    ])\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = float(baseline_metrics.get('success_rate_pct', 0))\n            current_success = (\n                successful_translations / total_tests * 100.0\n            ) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n\n            baseline_expl = int(baseline_metrics.get('total_explanations', 0))\n            expl_delta = total_explanations - baseline_expl\n\n            baseline_quality_dict = baseline_metrics.get('quality_metrics', {})\n            if isinstance(baseline_quality_dict, dict):\n                baseline_quality = float(baseline_quality_dict.get('avg_confidence', 0))\n            else:\n                baseline_quality = 0.0\n            quality_delta = quality_metrics['avg_confidence'] - baseline_quality\n\n            print(f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics['avg_confidence']:.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n\n            baseline_homo_dict = baseline_metrics.get('homograph_tracking', {})\n            if isinstance(baseline_homo_dict, dict):\n                baseline_homo_rate = float(baseline_homo_dict.get('explained_from_dscd_rate', 0))\n                homo_delta = (\n                    homograph_tracking['explained_from_dscd_rate'] - baseline_homo_rate\n                )\n                print(\n                    f\"  Explanation rate: \"\n                    f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                    f\"({homo_delta:+.1%})\"\n                )\n        except Exception as e:\n            print(f\"  Comparison failed: {type(e).__name__}\")\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"No explanations generated\")\n    if dscd_stats['total_words'] < 100:\n        warnings.append(\"Very few prototypes (<100)\")\n    if quality_metrics['low_confidence_count'] > quality_metrics['high_confidence_count']:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking['explained_from_dscd_rate'] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    if asbn_stats and asbn_stats.get('domain_accuracy', 0) < 0.5:\n        warnings.append(\"ASBN domain accuracy <50%\")\n    if capitalization_metrics['capitalization_rate'] < 0.9:\n        warnings.append(f\"Low capitalization rate (<90%): {capitalization_metrics['capitalization_rate']:.1%}\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] ‚úÖ All systems nominal\")\n\n    print(\"=\" * 80)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n        \"capitalization_metrics\": capitalization_metrics,\n        \"sample_translations\": sample_translations,\n    }\n\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\"*60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\"*60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model,\n            tokenizer,\n            run_warmup=False,\n            compare_baseline=False\n        )\n\n        assert 'total_tests' in result\n        assert 'quality_metrics' in result\n        assert 'homograph_tracking' in result\n        assert 'capitalization_metrics' in result\n\n        print(\"‚úÖ Evaluation pipeline test passed\")\n        print(f\"‚úÖ Capitalization rate: {result['capitalization_metrics']['capitalization_rate']:.1%}\")\n        print(\"=\"*60 + \"\\n\")\n        return True\n\n    except Exception as e:\n        print(f\"‚ùå Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\"*60 + \"\\n\")\n        return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Comprehensive Evaluation - BanglaT5 + Standard LoRA (FP16)\")\nprint(\"=\" * 80)\nprint(\"Evaluation metrics:\")\nprint(\"  - Translation quality (success rate)\")\nprint(\"  - Capitalization rate\")\nprint(\"  - Ambiguity detection\")\nprint(\"  - Explanation quality\")\nprint(\"  - Homograph discovery\")\nprint(\"  - DSCD prototypes\")\nprint(\"  - ASBN domain accuracy\")\nprint(\"  - TRG statistics\")\nprint(\"  - Performance timing\")\nprint(f\"\\nConfiguration:\")\nprint(f\"  Source: {_SOURCE_LANGUAGE} | Target: {_TARGET_LANGUAGE}\")\nprint(f\"  LoRA: {'ENABLED' if _USE_LORA else 'DISABLED'}\")\nprint(f\"  Thresholds: span={_SPAN_THRESHOLD}, uncertainty={_UNCERTAINTY_THRESHOLD}\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"8uL574F8H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:26.952770Z","iopub.execute_input":"2026-02-16T03:01:26.952992Z","iopub.status.idle":"2026-02-16T03:01:27.034908Z","shell.execute_reply.started":"2026-02-16T03:01:26.952973Z","shell.execute_reply":"2026-02-16T03:01:27.034349Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 9: Comprehensive Evaluation - BanglaT5 + Standard LoRA (FP16)\n================================================================================\nEvaluation metrics:\n  - Translation quality (success rate)\n  - Capitalization rate\n  - Ambiguity detection\n  - Explanation quality\n  - Homograph discovery\n  - DSCD prototypes\n  - ASBN domain accuracy\n  - TRG statistics\n  - Performance timing\n\nConfiguration:\n  Source: bn | Target: en\n  LoRA: ENABLED\n  Thresholds: span=0.18, uncertainty=0.12\n================================================================================\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 10: TATN MAIN PIPELINE (DUAL-PATH + LORA COMPATIBLE) - BanglaT5\n# ===========================================================================================\n\nimport os\nimport sys\nimport time\nimport traceback\nimport inspect\nfrom typing import Tuple, Optional, Dict, Any\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers.modeling_outputs import BaseModelOutput\nimport collections\n\ntry:\n    if hasattr(torch.serialization, 'add_safe_globals'):\n        torch.serialization.add_safe_globals([\n            collections.defaultdict,\n            collections.OrderedDict,\n            collections.deque\n        ])\n        print(\"‚úì Registered safe globals for PyTorch 2.6+\")\nexcept (AttributeError, Exception):\n    pass\n\ndef _g(name, default):\n    return globals().get(name, default)\n\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 52))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 5e-6))\n    _WEIGHT_DECAY = float(_g(\"WEIGHT_DECAY\", 0.01))\n    _GRAD_CLIP_NORM = float(_g(\"GRAD_CLIP_NORM\", 1.0))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", False))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 500))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 50))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 4000))\n    _SPAN_THRESHOLD = float(_g(\"SPAN_THRESHOLD\", 0.20))\n    _UNCERTAINTY_THRESHOLD = float(_g(\"UNCERTAINTY_THRESHOLD\", 0.15))\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(_g(\"HOMOGRAPH_REFERENCE_LIST_BN\",\n        [\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\", \"‡¶¨‡¶æ‡¶∞\", \"‡¶π‡¶æ‡¶∞\", \"‡¶§‡¶æ‡¶∞‡¶æ\"]))\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = bool(_g(\"FREEZE_ENCODER\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\n    _DEBUG_DISCOVERY = bool(_g(\"DEBUG_DISCOVERY\", False))\n    _TASK_PREFIX = str(_g(\"TASK_PREFIX\", \"translate Bengali to English: \"))\n    _USE_LORA = bool(_g(\"USE_LORA\", False))\n    _LORA_RANK = int(_g(\"LORA_RANK\", 32))\n    _LORA_ALPHA = float(_g(\"LORA_ALPHA\", 64.0))\n    _LORA_DROPOUT = float(_g(\"LORA_DROPOUT\", 0.1))\n    _LORA_TARGET_MODULES = _g(\"LORA_TARGET_MODULES\", [\"q\", \"v\", \"k\", \"o\"])\n    \nexcept (ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 52\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 5e-6\n    _WEIGHT_DECAY = 0.01\n    _GRAD_CLIP_NORM = 1.0\n    _ENABLE_ASBN_TRAINING = False\n    _VALIDATION_CHECK_INTERVAL = 500\n    _PERIODIC_DISCOVERY_FREQUENCY = 50\n    _DSCD_WARMUP_SAMPLES = 4000\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.15\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"}\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = False\n    _DEBUG_TIMING = False\n    _DEBUG_DISCOVERY = False\n    _TASK_PREFIX = \"translate Bengali to English: \"\n    _USE_LORA = False\n    _LORA_RANK = 32\n    _LORA_ALPHA = 64.0\n    _LORA_DROPOUT = 0.1\n    _LORA_TARGET_MODULES = [\"q\", \"v\", \"k\", \"o\"]\n\n_CHECKPOINT_DIR = \"/kaggle/working\"\n_CHECKPOINT_PATH = os.path.join(_CHECKPOINT_DIR, \"tatn_final.pt\")\n\n\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            globals()[\"clear_all_gpu_caches\"]()\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, None)\n        if result is None:\n            return default\n    return result\n\n\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import AutoTokenizer\n        \n        tok = AutoTokenizer.from_pretrained(\n            model_name,\n            local_files_only=local_files_only\n        )\n        \n        required = ['encode', 'decode', 'convert_ids_to_tokens', '__call__']\n        for method in required:\n            if not hasattr(tok, method):\n                raise RuntimeError(f\"Tokenizer missing: {method}\")\n        return tok\n    except Exception as e:\n        print(f\"[TOKENIZER] Load failed: {e}\")\n        raise\n\n\ndef _get_dscd_stores_safe(dscd):\n    try:\n        prototype_stores = getattr(dscd, 'prototype_stores', None)\n        if prototype_stores is None:\n            return {}\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        try:\n            if lock:\n                try:\n                    with lock:\n                        return dict(prototype_stores)\n                except Exception:\n                    return dict(prototype_stores)\n            else:\n                return dict(prototype_stores)\n        except Exception:\n            return {}\n    except Exception:\n        return {}\n\n\ndef _get_core_model(model):\n    return model.module if hasattr(model, \"module\") else model\n\n\ndef count_trainable_parameters(model: nn.Module) -> Dict[str, Any]:\n    total_params = 0\n    trainable_params = 0\n    lora_params = 0\n    frozen_params = 0\n    \n    for name, param in model.named_parameters():\n        num_params = param.numel()\n        total_params += num_params\n        \n        if param.requires_grad:\n            trainable_params += num_params\n            if 'lora_' in name.lower() or '.lora_' in name:\n                lora_params += num_params\n        else:\n            frozen_params += num_params\n    \n    return {\n        'total': total_params,\n        'trainable': trainable_params,\n        'lora': lora_params,\n        'frozen': frozen_params,\n        'trainable_pct': 100.0 * trainable_params / total_params if total_params > 0 else 0.0,\n        'lora_pct': 100.0 * lora_params / total_params if total_params > 0 else 0.0,\n    }\n\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        _safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    \n    if _USE_LORA:\n        print(f\"\\n[PIPELINE] ‚úÖ LoRA ENABLED:\")\n        print(f\"  - Rank: {_LORA_RANK}\")\n        print(f\"  - Alpha: {_LORA_ALPHA}\")\n        print(f\"  - Dropout: {_LORA_DROPOUT}\")\n        print(f\"  - Target modules: {_LORA_TARGET_MODULES}\")\n    else:\n        print(f\"\\n[PIPELINE] LoRA DISABLED (full fine-tuning)\")\n    \n    return True\n\n\ndef validate_component_compatibility(model_core, tokenizer):\n    print(\"\\n[VALIDATION] Checking component compatibility...\")\n\n    issues = []\n\n    try:\n        model_vocab = model_core.vocab_size\n        tokenizer_vocab = len(tokenizer) if hasattr(tokenizer, \"__len__\") else getattr(tokenizer, \"vocab_size\", 0)\n\n        if model_vocab < tokenizer_vocab:\n            issues.append(f\"CRITICAL: model vocab ({model_vocab}) < tokenizer vocab ({tokenizer_vocab})\")\n        elif model_vocab > tokenizer_vocab:\n            print(f\"  ‚úÖ Vocabulary: model={model_vocab}, tokenizer={tokenizer_vocab}\")\n            print(f\"     Note: Model has {model_vocab - tokenizer_vocab} extra tokens (preserves pretrained weights)\")\n        else:\n            print(f\"  ‚úÖ Vocabulary: {model_vocab}\")\n    except Exception as e:\n        issues.append(f\"Vocab check failed: {e}\")\n\n    try:\n        model_embed_dim = int(getattr(model_core.t5.config, \"d_model\", 768))\n        print(f\"  ‚úÖ Model embed_dim: {model_embed_dim}\")\n\n        if hasattr(model_core, 'dscd'):\n            dscd_embed_dim = getattr(model_core.dscd, 'embed_dim', None)\n            if dscd_embed_dim is not None and dscd_embed_dim != model_embed_dim:\n                issues.append(f\"DSCD embed_dim mismatch: {dscd_embed_dim} != {model_embed_dim}\")\n            else:\n                print(f\"  ‚úÖ DSCD embed_dim: {dscd_embed_dim}\")\n\n        if hasattr(model_core, 'asbn'):\n            asbn_embed_dim = getattr(model_core.asbn, 'embed_dim', None)\n            if asbn_embed_dim is not None and asbn_embed_dim != model_embed_dim:\n                issues.append(f\"ASBN embed_dim mismatch: {asbn_embed_dim} != {model_embed_dim}\")\n            else:\n                print(f\"  ‚úÖ ASBN embed_dim: {asbn_embed_dim}\")\n    except Exception as e:\n        issues.append(f\"Embed_dim check failed: {e}\")\n\n    try:\n        embedding_layer = model_core.t5.get_input_embeddings()\n        if embedding_layer is None:\n            issues.append(\"Model has no input embeddings\")\n        else:\n            actual_embed_dim = embedding_layer.embedding_dim\n            actual_vocab_size = embedding_layer.num_embeddings\n            print(f\"  ‚úÖ Embedding layer: dim={actual_embed_dim}, vocab={actual_vocab_size}\")\n    except Exception as e:\n        issues.append(f\"Embedding layer check failed: {e}\")\n\n    if _USE_LORA:\n        try:\n            param_stats = count_trainable_parameters(model_core)\n            \n            print(f\"\\n[VALIDATION] LoRA Parameter Check:\")\n            print(f\"  Total params: {param_stats['total']/1e6:.2f}M\")\n            print(f\"  Trainable params: {param_stats['trainable']/1e6:.2f}M ({param_stats['trainable_pct']:.2f}%)\")\n            print(f\"  LoRA params: {param_stats['lora']/1e6:.2f}M ({param_stats['lora_pct']:.2f}%)\")\n            print(f\"  Frozen params: {param_stats['frozen']/1e6:.2f}M\")\n            \n            if param_stats['lora'] == 0:\n                issues.append(\"LoRA enabled but NO LoRA params found! Check Cell 6.\")\n            elif param_stats['trainable_pct'] > 10.0:\n                issues.append(f\"LoRA enabled but {param_stats['trainable_pct']:.1f}% params trainable (expected <5%)\")\n            else:\n                print(f\"  ‚úÖ LoRA correctly applied\")\n        except Exception as e:\n            issues.append(f\"LoRA param check failed: {e}\")\n\n    if issues:\n        print(\"\\n[VALIDATION] ‚ùå FAILED - Issues found:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n        raise RuntimeError(\"Component compatibility validation failed\")\n    else:\n        print(\"[VALIDATION] ‚úÖ All components compatible\")\n\n    return True\n\n\ndef validate_dataset_compatibility(dataset, tokenizer, model_vocab_size):\n    print(\"\\n[VALIDATION] Checking dataset compatibility...\")\n\n    try:\n        sample_batch = []\n        for i in range(min(5, len(dataset))):\n            try:\n                sample_batch.append(dataset[i])\n            except Exception:\n                continue\n\n        if not sample_batch:\n            print(\"[VALIDATION] ‚ö†Ô∏è  Could not load samples\")\n            return True\n\n        max_input_id = 0\n        min_input_id = float('inf')\n\n        for item in sample_batch:\n            input_ids = item.get('input_ids', None)\n            if input_ids is not None:\n                if isinstance(input_ids, torch.Tensor):\n                    max_input_id = max(max_input_id, input_ids.max().item())\n                    min_input_id = min(min_input_id, input_ids.min().item())\n                elif isinstance(input_ids, list):\n                    max_input_id = max(max_input_id, max(input_ids))\n                    min_input_id = min(min_input_id, min(input_ids))\n\n        print(f\"  Input IDs range: [{min_input_id}, {max_input_id}]\")\n        print(f\"  Model vocab size: {model_vocab_size}\")\n\n        if max_input_id >= model_vocab_size:\n            raise RuntimeError(\n                f\"Dataset contains out-of-bounds token IDs!\\n\"\n                f\"  Max ID: {max_input_id}\\n\"\n                f\"  Vocab size: {model_vocab_size}\\n\"\n                f\"  ‚Üí Cell 2 tokenization error or vocab mismatch\"\n            )\n\n        if min_input_id < 0:\n            raise RuntimeError(f\"Dataset contains negative token IDs: {min_input_id}\")\n\n        print(\"[VALIDATION] ‚úÖ Dataset token IDs valid\")\n        return True\n\n    except Exception as e:\n        print(f\"[VALIDATION] Dataset check failed: {e}\")\n        raise\n\n\ndef test_model_forward_pass(model, tokenizer, device):\n    print(\"\\n[VALIDATION] Testing model with translate_with_explanations...\")\n    \n    try:\n        if 'translate_with_explanations' not in globals():\n            print(\"  ‚ùå translate_with_explanations() not found\")\n            print(\"     Run Cell 8 before Cell 10\")\n            raise RuntimeError(\"Cell 8 (translate_with_explanations) not loaded\")\n        \n        core_model = model.module if hasattr(model, 'module') else model\n        was_training = core_model.training\n        core_model.eval()\n        \n        test_sentences = [\n            \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ó‡¶æ‡¶® ‡¶ó‡¶æ‡¶á‡•§\",\n            \"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã‡•§\",\n            \"‡¶§‡¶ø‡¶®‡¶ø ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡¶®‡•§\"\n        ]\n        \n        print(f\"  Testing {len(test_sentences)} sentences...\")\n        \n        capitalized_count = 0\n        total_tests = len(test_sentences)\n        \n        for idx, test_sentence in enumerate(test_sentences, 1):\n            try:\n                result = translate_with_explanations(\n                    model,\n                    tokenizer,\n                    test_sentence,\n                    source_lang=_SOURCE_LANGUAGE,\n                    target_lang=_TARGET_LANGUAGE,\n                    device=device,\n                    max_length=64,\n                )\n                \n                if not isinstance(result, dict):\n                    raise RuntimeError(f\"Expected dict, got {type(result)}\")\n                \n                translation = result.get('translation', '')\n                \n                if not translation:\n                    raise RuntimeError(\"Empty translation\")\n                \n                if not isinstance(translation, str):\n                    raise RuntimeError(f\"Translation is {type(translation)}, expected str\")\n                \n                is_capitalized = False\n                first_char = ''\n                for char in translation:\n                    if char.isalpha():\n                        first_char = char\n                        is_capitalized = char.isupper()\n                        break\n                \n                if is_capitalized:\n                    capitalized_count += 1\n                \n                cap_marker = \"‚úÖ\" if is_capitalized else \"‚ö†Ô∏è \"\n                print(f\"  {idx}. {cap_marker} '{translation[:50]}'\")\n                if not is_capitalized and first_char:\n                    print(f\"      First char: '{first_char}' (should be uppercase)\")\n            \n            except Exception as e:\n                print(f\"  {idx}. ‚ùå Failed: {type(e).__name__}\")\n                raise\n        \n        capitalization_rate = capitalized_count / total_tests\n        \n        print(f\"\\n  ‚úÖ All translations successful\")\n        print(f\"  ‚úÖ Capitalization rate: {capitalization_rate:.1%} ({capitalized_count}/{total_tests})\")\n        \n        if capitalization_rate < 1.0:\n            print(f\"  ‚ö†Ô∏è  WARNING: Not all translations capitalized!\")\n            print(f\"      Expected: 100%, Got: {capitalization_rate:.1%}\")\n            print(f\"      ‚Üí Check Cell 8 capitalization function\")\n        else:\n            print(f\"  ‚úÖ All translations properly capitalized\")\n        \n        print(f\"  ‚úÖ Model validation passed\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"[VALIDATION] ‚ùå Model test failed: {e}\")\n        raise RuntimeError(f\"Model forward pass validation failed: {e}\")\n    \n    finally:\n        if was_training:\n            core_model.train()\n\n\ndef main_pipeline() -> Tuple[object, object]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN MAIN PIPELINE (DUAL-PATH + LORA COMPATIBLE) - BanglaT5\")\n    print(\"=\" * 80)\n    print(f\"Configuration:\")\n    print(f\"  - Model: BanglaT5 (csebuetnlp/banglat5)\")\n    print(f\"  - LoRA: {'ENABLED' if _USE_LORA else 'DISABLED'}\")\n    if _USE_LORA:\n        print(f\"    ‚Ä¢ Rank: {_LORA_RANK}\")\n        print(f\"    ‚Ä¢ Alpha: {_LORA_ALPHA}\")\n        print(f\"    ‚Ä¢ Target modules: {len(_LORA_TARGET_MODULES)} ({', '.join(_LORA_TARGET_MODULES)})\")\n    print(f\"  - Task prefix: '{_TASK_PREFIX}'\")\n    print(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - ASBN Training: {'DISABLED' if not _ENABLE_ASBN_TRAINING else 'ENABLED'}\")\n    print(f\"  - Epochs: {_EPOCHS}\")\n    print(f\"  - Batch size: {_BATCH_SIZE}\")\n    print(\"=\" * 80)\n\n    pipeline_start = time.time()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    initialize_environment()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 1] Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"csebuetnlp/banglat5\")\n    \n    try:\n        if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:\n            if hasattr(tokenizer, 'add_special_tokens'):\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n    except Exception:\n        pass\n\n    vocab_size = getattr(tokenizer, 'vocab_size', None)\n    if vocab_size is None:\n        try:\n            vocab_size = len(tokenizer)\n        except Exception:\n            vocab_size = 32100\n\n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size})\")\n\n    if \"validate_tokenizer_vocab\" in globals():\n        try:\n            print(\"[PHASE 1] Validating tokenizer vocabulary...\")\n            validate_tokenizer_vocab(tokenizer, expected_vocab_size=None)\n        except Exception as e:\n            print(f\"[PHASE 1] Tokenizer validation warning: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(f\"\\n[PHASE 2] Loading data ({_NUM_SAMPLES} samples)...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals():\n        raise RuntimeError(\"MemoryEfficientDataset not found - run Cell 2\")\n    \n    print(\"\\n[PHASE 3] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        raise RuntimeError(\"Model class not found - run Cell 6\")\n\n    model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n\n    try:\n        validate_component_compatibility(model_core, tokenizer)\n    except Exception as e:\n        print(f\"[PHASE 3] ‚ùå Component validation failed: {e}\")\n        raise\n\n    print(f\"[PIPELINE] Creating dataset with model vocab_size={model_core.vocab_size}\")\n    dataset = MemoryEfficientDataset(\n        pairs, \n        tokenizer, \n        max_length=_MAX_LENGTH,\n        vocab_size=model_core.vocab_size\n    )\n\n    collate_fn = globals().get(\"safe_collate\", None)\n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = create_optimized_dataloader(dataset, batch_size=_BATCH_SIZE, shuffle=True)\n        except Exception:\n            dataloader_kwargs = {\n                'batch_size': _BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0,\n                'pin_memory': torch.cuda.is_available()\n            }\n            if collate_fn is not None:\n                dataloader_kwargs['collate_fn'] = collate_fn\n            train_loader = DataLoader(dataset, **dataloader_kwargs)\n    else:\n        dataloader_kwargs = {\n            'batch_size': _BATCH_SIZE,\n            'shuffle': True,\n            'num_workers': 0,\n            'pin_memory': torch.cuda.is_available()\n        }\n        if collate_fn is not None:\n            dataloader_kwargs['collate_fn'] = collate_fn\n        train_loader = DataLoader(dataset, **dataloader_kwargs)\n\n    try:\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {len(train_loader)} batches\")\n    except Exception:\n        print(\"[PHASE 2] Dataset loaded\")\n\n    del pairs\n    _safe_clear_gpu_caches()\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    try:\n        validate_dataset_compatibility(dataset, tokenizer, model_core.vocab_size)\n    except Exception as e:\n        print(f\"[PHASE 3] ‚ùå Dataset validation failed: {e}\")\n        raise\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        device_ids = list(range(_NUM_GPUS))\n        print(f\"[PHASE 3] Applying DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        print(f\"[PHASE 3] Single GPU/CPU mode\")\n        model = model_core\n\n    print(f\"[PHASE 3] Moving model to device: {_DEVICE}\")\n    model = model.to(_DEVICE)\n    \n    core_model = _get_core_model(model)\n\n    try:\n        test_model_forward_pass(model, tokenizer, _DEVICE)\n    except Exception as e:\n        print(f\"[PHASE 3] ‚ùå Forward pass test failed: {e}\")\n        raise\n\n    if _FREEZE_ENCODER:\n        try:\n            for p in core_model.t5.encoder.parameters():\n                p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n\n    print(f\"[PHASE 3] Model initialized and validated\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 4] Setting up optimizers...\")\n    print(f\"[PHASE 4] Extracting parameters from {'wrapped' if hasattr(model, 'module') else 'unwrapped'} model...\")\n    \n    try:\n        base_params = []\n        lora_params = []\n        \n        for name, param in core_model.named_parameters():\n            if not param.requires_grad:\n                continue\n            \n            if 'lora_' in name.lower() or '.lora_' in name:\n                lora_params.append(param)\n            else:\n                base_params.append(param)\n        \n        print(f\"[PHASE 4] Parameter extraction:\")\n        print(f\"  - Base params: {len(base_params)} (DSCD/ASBN/TRG)\")\n        print(f\"  - LoRA params: {len(lora_params)}\")\n        \n        if _USE_LORA:\n            if not lora_params:\n                print(f\"[PHASE 4] ‚ùå CRITICAL: LoRA enabled but NO LoRA params found!\")\n                print(f\"[PHASE 4] Cell 6 may have failed. Using all trainable params...\")\n                lora_params = base_params + lora_params\n                base_params = []\n        \n        lora_param_count = sum(p.numel() for p in lora_params)\n        base_param_count = sum(p.numel() for p in base_params)\n        \n        print(f\"[PHASE 4] Parameter counts:\")\n        print(f\"  - LoRA params: {lora_param_count:,} ({lora_param_count/1e6:.2f}M)\")\n        print(f\"  - Base params: {base_param_count:,} ({base_param_count/1e6:.2f}M)\")\n        \n        optimizer_groups = []\n        \n        if lora_params:\n            optimizer_groups.append({\n                'params': lora_params,\n                'lr': _LR_NMT,\n                'weight_decay': _WEIGHT_DECAY * 0.1 if _USE_LORA else _WEIGHT_DECAY,\n            })\n            print(f\"[PHASE 4] Added LoRA param group (LR: {_LR_NMT:.2e}, {len(lora_params)} tensors)\")\n        \n        if base_params:\n            optimizer_groups.append({\n                'params': base_params,\n                'lr': _LR_NMT if not _USE_LORA else _LR_NMT * 0.5,\n                'weight_decay': _WEIGHT_DECAY,\n            })\n            print(f\"[PHASE 4] Added base param group (DSCD/ASBN/TRG, LR: {_LR_NMT if not _USE_LORA else _LR_NMT * 0.5:.2e}, {len(base_params)} tensors)\")\n        \n        if not optimizer_groups:\n            raise RuntimeError(\"No trainable parameters found! Check model initialization.\")\n        \n        optimizer = torch.optim.AdamW(\n            optimizer_groups,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n        )\n        \n        optimizer_param_count = sum(p.numel() for group in optimizer.param_groups for p in group['params'])\n        optimizer_tensor_count = sum(len(group['params']) for group in optimizer.param_groups)\n        \n        print(f\"[PHASE 4] ‚úÖ Optimizer created:\")\n        print(f\"  - Param groups: {len(optimizer.param_groups)}\")\n        print(f\"  - Total param tensors: {optimizer_tensor_count}\")\n        print(f\"  - Total params: {optimizer_param_count:,} ({optimizer_param_count/1e6:.2f}M)\")\n        print(f\"  - Initial LR: {optimizer.param_groups[0]['lr']:.2e}\")\n        print(f\"  - DataParallel: {'YES' if hasattr(model, 'module') else 'NO'}\")\n        \n        if optimizer_param_count == 0:\n            raise RuntimeError(\"Optimizer managing 0 parameters!\")\n        \n        print(f\"\\n[PHASE 4] Validating optimizer<->model connection...\")\n        optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])\n        model_param_ids = set(id(p) for p in core_model.parameters() if p.requires_grad)\n        model_trainable_count = len(model_param_ids)\n        \n        if optimizer_param_ids == model_param_ids:\n            print(f\"[PHASE 4] ‚úÖ Optimizer params EXACTLY match model params ({optimizer_tensor_count} tensors)\")\n            print(f\"[PHASE 4] ‚úÖ ALL trainable params will receive gradients\")\n        else:\n            missing_count = len(model_param_ids - optimizer_param_ids)\n            extra_count = len(optimizer_param_ids - model_param_ids)\n            print(f\"[PHASE 4] ‚ùå CRITICAL: Parameter mismatch!\")\n            print(f\"  - Optimizer: {optimizer_tensor_count} tensors, {optimizer_param_count:,} params\")\n            print(f\"  - Model trainable: {model_trainable_count} tensors\")\n            print(f\"  - Missing from optimizer: {missing_count} tensors\")\n            print(f\"  - Extra in optimizer: {extra_count} tensors\")\n            raise RuntimeError(f\"Optimizer missing {missing_count} trainable parameters!\")\n        \n        asbn_optimizer = None\n        try:\n            if hasattr(core_model, 'asbn'):\n                asbn = core_model.asbn\n                critic_params = [p for p in asbn.critic_parameters() if p.requires_grad]\n                \n                if critic_params:\n                    asbn_optimizer = torch.optim.AdamW(\n                        critic_params,\n                        lr=_LR_PHI,\n                        weight_decay=0.0001,\n                    )\n                    print(f\"[PHASE 4] ASBN optimizer created ({len(critic_params)} params)\")\n        except Exception as e:\n            print(f\"[PHASE 4] ‚ö†Ô∏è  ASBN optimizer creation failed: {type(e).__name__}\")\n            asbn_optimizer = None\n        \n        print(f\"[PHASE 4] Optimizers ready\\n\")\n        \n    except Exception as e:\n        print(f\"[PHASE 4] ‚ùå CRITICAL: Optimizer creation failed\")\n        print(f\"  Error: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        raise RuntimeError(f\"Optimizer setup failed: {e}\")\n\n    print(\"\\n[PHASE 5] Training...\")\n    print(f\"  - ASBN Training: {'DISABLED' if not _ENABLE_ASBN_TRAINING else 'ENABLED'}\")\n    print(f\"  - ASBN Optimizer: {'None (ASBN disabled)' if asbn_optimizer is None else 'Active'}\")\n    if _USE_LORA:\n        print(f\"  - LoRA Training: ENABLED ({len(lora_params)} LoRA param tensors)\")\n\n    trained_model = model\n    training_stats = None\n\n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            try:\n                trg = getattr(core_model, 'trg', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=asbn_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=(_VALIDATION_CHECK_INTERVAL > 0)\n            )\n            print(\"[PHASE 5] Training complete\")\n        except Exception as e:\n            print(f\"[PHASE 5] Training failed: {e}\")\n            traceback.print_exc()\n            trained_model = model\n    else:\n        print(\"[PHASE 5] Skipping training (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n\n    del train_loader, dataset\n    _safe_clear_gpu_caches()\n\n    core_model = _get_core_model(trained_model)\n\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 6] Discovery check...\")\n    discovery_success = False\n\n    try:\n        dscd = getattr(core_model, 'dscd', None)\n        if dscd is None:\n            print(\"[PHASE 6] No DSCD module\")\n        else:\n            print(\"[PHASE 6] Running periodic discovery check...\")\n            if hasattr(dscd, 'periodic_discovery_check'):\n                try:\n                    sig = inspect.signature(dscd.periodic_discovery_check)\n                    params = list(sig.parameters.keys())\n                    print(f\"[PHASE 6] periodic_discovery_check params: {params}\")\n\n                    total_steps = int(_EPOCHS * _NUM_SAMPLES // _BATCH_SIZE)\n\n                    if 'cluster_missing' in params:\n                        if len(params) >= 3:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, _PERIODIC_DISCOVERY_FREQUENCY, cluster_missing=False)\n                        elif len(params) >= 2:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, cluster_missing=False)\n                        else:\n                            num_discovered = dscd.periodic_discovery_check(cluster_missing=False)\n                    else:\n                        if len(params) >= 2:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, _PERIODIC_DISCOVERY_FREQUENCY)\n                        elif len(params) >= 1:\n                            num_discovered = dscd.periodic_discovery_check(total_steps)\n                        else:\n                            num_discovered = dscd.periodic_discovery_check()\n\n                    discovery_success = True\n                    print(f\"[PHASE 6] Discovery complete: {num_discovered} homographs found\")\n                except Exception as e:\n                    print(f\"[PHASE 6] periodic_discovery_check failed: {e}\")\n                    try:\n                        if hasattr(dscd, 'discover_homographs'):\n                            num_discovered = dscd.discover_homographs()\n                            discovery_success = True\n                            print(f\"[PHASE 6] Fallback discovery: {num_discovered} homographs\")\n                        else:\n                            print(\"[PHASE 6] discover_homographs not available\")\n                    except Exception as e2:\n                        print(f\"[PHASE 6] Fallback discovery failed: {e2}\")\n            else:\n                print(\"[PHASE 6] periodic_discovery_check not available\")\n                if hasattr(dscd, 'discover_homographs'):\n                    try:\n                        num_discovered = dscd.discover_homographs()\n                        discovery_success = True\n                        print(f\"[PHASE 6] discover_homographs: {num_discovered} homographs\")\n                    except Exception as e:\n                        print(f\"[PHASE 6] discover_homographs failed: {e}\")\n\n            stores = _get_dscd_stores_safe(dscd)\n\n            def _store_size(s):\n                try:\n                    if callable(getattr(s, \"size\", None)):\n                        return int(s.size())\n                    return int(getattr(s, \"size\", 0))\n                except Exception:\n                    return 0\n\n            total_protos = sum(_store_size(store) for store in stores.values())\n            multi_sense = sum(1 for store in stores.values() if _store_size(store) >= 2)\n\n            print(\"[PHASE 6] Discovery state:\")\n            print(f\"  - Tokens: {len(stores)}\")\n            print(f\"  - Prototypes: {total_protos}\")\n            print(f\"  - Multi-sense: {multi_sense}\")\n\n            if len(stores) == 0:\n                print(\"[PHASE 6] WARNING: No prototypes created\")\n            else:\n                discovery_success = True\n    except Exception as e:\n        print(f\"[PHASE 6] Discovery failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Discovery: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 7] DSCD warmup...\")\n    if \"dscd_discovery_warmup\" in globals():\n        try:\n            warmup_samples = min(4000, _DSCD_WARMUP_SAMPLES)\n            print(f\"[PHASE 7] Processing {warmup_samples} warmup samples...\")\n            warmup_start = time.time()\n            dscd_discovery_warmup(trained_model, tokenizer, num_sents=warmup_samples, batch_size=64, max_len=_MAX_LENGTH)\n            warmup_duration = time.time() - warmup_start\n            print(f\"[PHASE 7] Warmup complete ({warmup_samples} samples in {warmup_duration:.1f}s)\")\n        except Exception as e:\n            print(f\"[PHASE 7] Warmup failed: {e}\")\n    else:\n        print(\"[PHASE 7] Skipping warmup (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Warmup: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 8] Baseline evaluation...\")\n    baseline_metrics = None\n\n    try:\n        dscd_baseline = getattr(core_model, 'dscd', None)\n        has_prototypes = False\n\n        if dscd_baseline:\n            stores = _get_dscd_stores_safe(dscd_baseline)\n            has_prototypes = len(stores) > 0\n\n        if not has_prototypes:\n            print(\"[PHASE 8] Skipping baseline (no prototypes)\")\n        elif \"comprehensive_post_training_testing\" in globals():\n            try:\n                trg = getattr(core_model, 'trg', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n\n            print(\"[PHASE 8] Running baseline evaluation...\")\n            baseline_metrics = comprehensive_post_training_testing(trained_model, tokenizer, run_warmup=False)\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            baseline_cap = baseline_metrics.get('capitalization_metrics', {}).get('capitalization_rate', 0)\n            print(f\"[PHASE 8] Baseline: {baseline_success:.1f}% success, {baseline_expl} explanations, {baseline_cap:.1%} capitalized\")\n        else:\n            print(\"[PHASE 8] Skipping baseline (function not found)\")\n    except Exception as e:\n        print(f\"[PHASE 8] Baseline failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Baseline: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 9] Post-training evaluation...\")\n    eval_results: Dict[str, Any] = {}\n\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            try:\n                trg = getattr(core_model, 'trg', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n\n            print(\"[PHASE 9] Running evaluation...\")\n            eval_results = comprehensive_post_training_testing(\n                trained_model,\n                tokenizer,\n                run_warmup=False,\n                compare_baseline=(baseline_metrics is not None),\n                baseline_metrics=baseline_metrics\n            )\n            final_success = eval_results.get('success_rate_pct', 0)\n            final_expl = eval_results.get('total_explanations', 0)\n            final_cap = eval_results.get('capitalization_metrics', {}).get('capitalization_rate', 0)\n            print(f\"[PHASE 9] Evaluation: {final_success:.1f}% success, {final_expl} explanations, {final_cap:.1%} capitalized\")\n        except Exception as e:\n            print(f\"[PHASE 9] Evaluation failed: {e}\")\n    else:\n        print(\"[PHASE 9] Skipping evaluation (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 10] Saving checkpoint...\")\n    try:\n        os.makedirs(_CHECKPOINT_DIR, exist_ok=True)\n        was_training = getattr(core_model, \"training\", False)\n        core_model.eval()\n        try:\n            model_state = core_model.state_dict()\n            dscd_state = {}\n\n            if hasattr(core_model, 'dscd'):\n                dscd_save = core_model.dscd\n                if hasattr(dscd_save, 'state_dict'):\n                    lock = None\n                    if hasattr(dscd_save, 'buffer_lock'):\n                        lock = dscd_save.buffer_lock\n                    elif hasattr(dscd_save, 'clustering_lock'):\n                        lock = dscd_save.clustering_lock\n\n                    try:\n                        if lock:\n                            try:\n                                with lock:\n                                    dscd_state = dscd_save.state_dict()\n                            except Exception:\n                                dscd_state = dscd_save.state_dict()\n                        else:\n                            dscd_state = dscd_save.state_dict()\n                    except Exception as e:\n                        print(f\"[PHASE 10] DSCD state_dict failed: {e}\")\n                        dscd_state = {}\n\n            lora_state = {}\n            if _USE_LORA:\n                try:\n                    if hasattr(core_model, 'get_peft_model_state_dict'):\n                        lora_state = core_model.get_peft_model_state_dict()\n                        print(f\"[PHASE 10] LoRA state extracted (PEFT method)\")\n                    else:\n                        lora_state = {\n                            name: param.data\n                            for name, param in core_model.named_parameters()\n                            if 'lora_' in name.lower() or '.lora_' in name\n                        }\n                        print(f\"[PHASE 10] LoRA state extracted ({len(lora_state)} params)\")\n                except Exception as e:\n                    print(f\"[PHASE 10] LoRA state extraction failed: {e}\")\n                    lora_state = {}\n\n            optimizer_state = None\n            if optimizer is not None:\n                try:\n                    optimizer_state = optimizer.state_dict()\n                    if 'state' in optimizer_state:\n                        for param_state in optimizer_state['state'].values():\n                            if isinstance(param_state, dict) and 'momentum_buffer' in param_state:\n                                try:\n                                    del param_state['momentum_buffer']\n                                except Exception:\n                                    pass\n                except Exception:\n                    optimizer_state = None\n\n            param_stats = count_trainable_parameters(core_model)\n\n            checkpoint = {\n                'model_state_dict': model_state,\n                'dscd_state': dscd_state,\n                'lora_state': lora_state,\n                'optimizer_state_dict': optimizer_state,\n                'training_stats': training_stats,\n                'baseline_metrics': baseline_metrics,\n                'eval_results': eval_results,\n                'discovery_success': discovery_success,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'param_stats': param_stats,\n                'config': {\n                    'model': 'BanglaT5',\n                    'task_prefix': _TASK_PREFIX,\n                    'epochs': _EPOCHS,\n                    'batch_size': _BATCH_SIZE,\n                    'span_threshold': _SPAN_THRESHOLD,\n                    'uncertainty_threshold': _UNCERTAINTY_THRESHOLD,\n                    'discovery_frequency': _PERIODIC_DISCOVERY_FREQUENCY,\n                    'vocab_size': vocab_size,\n                    'asbn_training_enabled': _ENABLE_ASBN_TRAINING,\n                    'use_lora': _USE_LORA,\n                    'lora_rank': _LORA_RANK if _USE_LORA else None,\n                    'lora_alpha': _LORA_ALPHA if _USE_LORA else None,\n                    'lora_dropout': _LORA_DROPOUT if _USE_LORA else None,\n                    'lora_target_modules': _LORA_TARGET_MODULES if _USE_LORA else None,\n                }\n            }\n            torch.save(checkpoint, _CHECKPOINT_PATH)\n\n            try:\n                import mmap\n                with open(_CHECKPOINT_PATH, 'rb') as f:\n                    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as m:\n                        size_mb = len(m) / 1024**2\n\n                print(f\"[PHASE 10] Checkpoint saved: {_CHECKPOINT_PATH}\")\n                print(f\"  - Size: {size_mb:.2f} MB\")\n                \n                if _USE_LORA:\n                    print(f\"  - LoRA: ENABLED\")\n                    print(f\"    ‚Ä¢ LoRA params saved: {len(lora_state)}\")\n                    print(f\"    ‚Ä¢ Total trainable: {param_stats['trainable']/1e6:.2f}M ({param_stats['trainable_pct']:.2f}%)\")\n\n                try:\n                    verify_keys = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n                    has_model = 'model_state_dict' in verify_keys and len(verify_keys['model_state_dict']) > 0\n                    has_dscd = 'dscd_state' in verify_keys and len(verify_keys.get('dscd_state', {})) > 0\n                    has_lora = 'lora_state' in verify_keys and len(verify_keys.get('lora_state', {})) > 0\n                    print(f\"  - Model: {'OK' if has_model else 'MISSING'}\")\n                    print(f\"  - DSCD: {'OK' if has_dscd else 'MISSING'}\")\n                    if _USE_LORA:\n                        print(f\"  - LoRA: {'OK' if has_lora else 'MISSING'}\")\n\n                    if has_dscd:\n                        try:\n                            dscd_verify_state = verify_keys.get('dscd_state', {})\n                            num_tokens = 0\n                            if 'prototype_stores' in dscd_verify_state:\n                                num_tokens = len(dscd_verify_state['prototype_stores'])\n                            print(f\"  - DSCD tokens: {num_tokens}\")\n                        except Exception:\n                            print(f\"  - DSCD tokens: unknown\")\n\n                    del verify_keys\n                except Exception as e:\n                    print(f\"[PHASE 10] Checkpoint verification warning: {e}\")\n            except Exception:\n                print(f\"[PHASE 10] Checkpoint saved: {_CHECKPOINT_PATH}\")\n        finally:\n            if was_training:\n                try:\n                    core_model.train()\n                except Exception:\n                    pass\n    except Exception as e:\n        print(f\"[PHASE 10] Checkpoint failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Checkpoint: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 11] Final validation...\")\n    try:\n        dscd_ok = False\n        if hasattr(core_model, 'dscd'):\n            stores = _get_dscd_stores_safe(core_model.dscd)\n            dscd_ok = len(stores) > 0\n\n        asbn_ok = hasattr(core_model, 'asbn') and hasattr(core_model.asbn, 'forward')\n        trg_ok = hasattr(core_model, 'trg') and hasattr(core_model.trg, 'process_sentence_for_explanations')\n\n        print(f\"[PHASE 11] Component validation:\")\n        print(f\"  - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n        print(f\"  - ASBN: {'OK' if asbn_ok else 'MISSING'} {'(DISABLED)' if not _ENABLE_ASBN_TRAINING else '(ENABLED)'}\")\n        print(f\"  - TRG: {'OK' if trg_ok else 'MISSING'}\")\n        \n        if _USE_LORA:\n            param_stats = count_trainable_parameters(core_model)\n            lora_ok = param_stats['lora'] > 0\n            print(f\"  - LoRA: {'OK' if lora_ok else 'MISSING'} ({param_stats['lora']/1e6:.2f}M params)\")\n\n        all_ok = dscd_ok and asbn_ok and trg_ok\n        if all_ok:\n            print(\"[PHASE 11] ‚úÖ All components validated\")\n        else:\n            print(\"[PHASE 11] ‚ö†Ô∏è  Some components missing\")\n    except Exception as e:\n        print(f\"[PHASE 11] Validation failed: {e}\")\n\n    pipeline_time = time.time() - pipeline_start\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETE - FINAL SUMMARY (BanglaT5)\")\n    print(\"=\" * 80)\n    print(f\"\\n[TIMING]\")\n    print(f\"  Total time: {pipeline_time:.2f}s ({pipeline_time/60:.2f} min)\")\n\n    print(f\"\\n[TRAINING]\")\n    if training_stats:\n        total_loss = training_stats.get('total_loss', [])\n        optimizer_updates = training_stats.get('optimizer_updates', 0)\n        print(f\"  Completed: {optimizer_updates} optimizer updates\")\n        if total_loss:\n            recent_loss = sum(total_loss[-100:]) / len(total_loss[-100:])\n            print(f\"  - Final loss: {recent_loss:.6f}\")\n    else:\n        print(\"  No stats available\")\n\n    print(f\"\\n[DISCOVERY]\")\n    if discovery_success:\n        print(\"  ‚úÖ Success\")\n    else:\n        print(\"  ‚ö†Ô∏è  Issues detected\")\n\n    print(f\"\\n[EVALUATION]\")\n    if baseline_metrics and eval_results:\n        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n        final_success = eval_results.get('success_rate_pct', 0)\n        improvement = final_success - baseline_success\n\n        print(f\"  Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n        print(f\"  Improvement: {improvement:+.1f}%\")\n\n        baseline_cap_metrics = baseline_metrics.get('capitalization_metrics', {})\n        final_cap_metrics = eval_results.get('capitalization_metrics', {})\n        \n        baseline_cap = baseline_cap_metrics.get('capitalization_rate', 0) if isinstance(baseline_cap_metrics, dict) else 0\n        final_cap = final_cap_metrics.get('capitalization_rate', 0) if isinstance(final_cap_metrics, dict) else 0\n        \n        if baseline_cap > 0 or final_cap > 0:\n            print(f\"  ‚úÖ Capitalization: {baseline_cap:.1%} -> {final_cap:.1%}\")\n\n        baseline_dscd_stats = baseline_metrics.get('dscd_stats', {})\n        final_dscd_stats = eval_results.get('dscd_stats', {})\n\n        baseline_dscd = baseline_dscd_stats.get('multi_sense_words', 0) if isinstance(baseline_dscd_stats, dict) else 0\n        final_dscd = final_dscd_stats.get('multi_sense_words', 0) if isinstance(final_dscd_stats, dict) else 0\n\n        if baseline_dscd is not None and final_dscd is not None:\n            print(f\"  DSCD multi-sense: {baseline_dscd} -> {final_dscd}\")\n\n        baseline_asbn_stats = baseline_metrics.get('asbn_stats', {})\n        final_asbn_stats = eval_results.get('asbn_stats', {})\n\n        baseline_asbn = baseline_asbn_stats.get('domain_accuracy', 0) if isinstance(baseline_asbn_stats, dict) else 0\n        final_asbn = final_asbn_stats.get('domain_accuracy', 0) if isinstance(final_asbn_stats, dict) else 0\n\n        if baseline_asbn is not None and final_asbn is not None:\n            print(f\"  ASBN accuracy: {baseline_asbn:.2%} -> {final_asbn:.2%} {'(DISABLED)' if not _ENABLE_ASBN_TRAINING else ''}\")\n    elif eval_results:\n        print(f\"  Success rate: {eval_results.get('success_rate_pct', 0):.1f}%\")\n        cap_metrics = eval_results.get('capitalization_metrics', {})\n        if isinstance(cap_metrics, dict):\n            cap_rate = cap_metrics.get('capitalization_rate', 0)\n            if cap_rate > 0:\n                print(f\"  ‚úÖ Capitalization rate: {cap_rate:.1%}\")\n    else:\n        print(\"  No results\")\n\n    print(f\"\\n[CHECKPOINT]\")\n    if os.path.exists(_CHECKPOINT_PATH):\n        try:\n            size_mb = os.path.getsize(_CHECKPOINT_PATH) / 1024**2\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n            print(f\"  - Size: {size_mb:.2f} MB\")\n            print(f\"  - Model: BanglaT5\")\n            if _USE_LORA:\n                param_stats = count_trainable_parameters(core_model)\n                print(f\"  - LoRA: ENABLED ({param_stats['lora']/1e6:.2f}M params)\")\n        except Exception:\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n    else:\n        print(\"  Not saved\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Usage: trained_model, tokenizer = main_pipeline()\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    return trained_model, tokenizer\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 10: Main Pipeline [‚úÖ COMPLETE]\")\nprint(\"=\" * 80)\nprint(\"‚úÖ FIX: DSCD/ASBN/TRG params now ALWAYS added to optimizer (removed 'and not _USE_LORA')\")\nprint(\"‚úÖ Base params use 0.5x LR when LoRA enabled (prevents overwhelming LoRA updates)\")\nprint(\"‚úÖ Enhanced validation: exact tensor count matching\")\nprint(\"‚úÖ Clear error messages if param mismatch detected\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"kEux2BVXH4J5","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:27.036104Z","iopub.execute_input":"2026-02-16T03:01:27.036299Z","iopub.status.idle":"2026-02-16T03:01:27.135076Z","shell.execute_reply.started":"2026-02-16T03:01:27.036283Z","shell.execute_reply":"2026-02-16T03:01:27.134532Z"}},"outputs":[{"name":"stdout","text":"‚úì Registered safe globals for PyTorch 2.6+\n\n================================================================================\nCell 10: Main Pipeline [‚úÖ COMPLETE]\n================================================================================\n‚úÖ FIX: DSCD/ASBN/TRG params now ALWAYS added to optimizer (removed 'and not _USE_LORA')\n‚úÖ Base params use 0.5x LR when LoRA enabled (prevents overwhelming LoRA updates)\n‚úÖ Enhanced validation: exact tensor count matching\n‚úÖ Clear error messages if param mismatch detected\n================================================================================\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 11: MAIN EXECUTION WRAPPER (DUAL-PATH + LORA + CAPITALIZATION) - BanglaT5\n# ===========================================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\n\ntry:\n    _NUM_SAMPLES = int(globals().get('NUM_SAMPLES', 30000))\n    _EPOCHS = int(globals().get('EPOCHS', 1))\n    _BATCH_SIZE = int(globals().get('BATCH_SIZE', 4))\n    _ACCUMULATION_STEPS = int(globals().get('ACCUMULATION_STEPS', 16))\n\n    raw_device = globals().get('DEVICE', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        _DEVICE = torch.device(str(raw_device))\n\n    _ENABLE_ASBN_TRAINING = bool(globals().get('ENABLE_ASBN_TRAINING', False))\n    _ENABLE_TRG_INFERENCE = bool(globals().get('ENABLE_TRG_INFERENCE', True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(globals().get('PERIODIC_DISCOVERY_FREQUENCY', 50))\n    _VERBOSE_LOGGING = bool(globals().get('VERBOSE_LOGGING', False))\n    _DEBUG_DISCOVERY = bool(globals().get('DEBUG_DISCOVERY', False))\n    _DEBUG_TIMING = bool(globals().get('DEBUG_TIMING', False))\n    _NUM_GPUS = int(globals().get('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(globals().get('USE_MULTI_GPU', _NUM_GPUS > 1))\n    _SPAN_THRESHOLD = float(globals().get('SPAN_THRESHOLD', 0.20))\n    _UNCERTAINTY_THRESHOLD = float(globals().get('UNCERTAINTY_THRESHOLD', 0.15))\n    _MAX_LENGTH = int(globals().get('MAX_LENGTH', 52))\n    _SOURCE_LANGUAGE = str(globals().get('SOURCE_LANGUAGE', 'bn'))\n    _TARGET_LANGUAGE = str(globals().get('TARGET_LANGUAGE', 'en'))\n    _TASK_PREFIX = str(globals().get('TASK_PREFIX', 'translate Bengali to English: '))\n\n    # ===================================================================\n    # ‚úÖ FIX #1: ADD LORA GLOBALS\n    # ===================================================================\n    _USE_LORA = bool(globals().get('USE_LORA', False))\n    _LORA_RANK = int(globals().get('LORA_RANK', 32))\n    _LORA_ALPHA = float(globals().get('LORA_ALPHA', 64.0))\n    _LORA_DROPOUT = float(globals().get('LORA_DROPOUT', 0.1))\n\n    raw_list = globals().get('HOMOGRAPH_REFERENCE_LIST_BN', [\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_list)\n    cell0_loaded = 'NUM_SAMPLES' in globals()\n\nexcept (NameError, TypeError, ValueError) as e:\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 1\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = False\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 50\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.15\n    _MAX_LENGTH = 52\n    _SOURCE_LANGUAGE = 'bn'\n    _TARGET_LANGUAGE = 'en'\n    _TASK_PREFIX = 'translate Bengali to English: '\n    _USE_LORA = False\n    _LORA_RANK = 32\n    _LORA_ALPHA = 64.0\n    _LORA_DROPOUT = 0.1\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n    cell0_loaded = False\n    print(\"[EXEC] Using fallback configuration (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/content/model/tatn_final.pt\"\n\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\n\ndef _format_duration(seconds: float) -> str:\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}min\"\n    else:\n        return f\"{seconds/3600:.2f}hr\"\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        if key not in result:\n            return default\n        result = result[key]\n    return result if result is not None else default\n\n\ndef _get_dscd_homographs(model):\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n\n        if dscd and hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        if dscd and hasattr(dscd, 'prototype_stores'):\n            homographs = set()\n\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            try:\n                if lock:\n                    try:\n                        with lock:\n                            stores = dict(dscd.prototype_stores)\n                    except Exception:\n                        stores = dict(dscd.prototype_stores)\n                else:\n                    stores = dict(dscd.prototype_stores)\n            except Exception:\n                return set()\n\n            for token, store in stores.items():\n                try:\n                    size_ok = False\n                    if hasattr(store, 'size'):\n                        size_attr = getattr(store, 'size')\n                        if callable(size_attr):\n                            try:\n                                size_val = size_attr()\n                                size_ok = int(size_val) >= 1\n                            except Exception:\n                                size_ok = False\n                        elif isinstance(size_attr, int):\n                            size_ok = size_attr >= 1\n\n                    if size_ok:\n                        clean = str(token).replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').strip().lower()\n                        if clean:\n                            homographs.add(clean)\n                except Exception:\n                    continue\n            return homographs\n    except Exception:\n        pass\n    return set()\n\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\n# ===================================================================\n# ‚úÖ FIX #2: ADD CAPITALIZATION CHECK FUNCTION\n# ===================================================================\ndef check_capitalization(text: str) -> dict:\n    \"\"\"\n    Checks if translation is properly capitalized.\n    Returns dict with is_capitalized, first_char, issue.\n    \"\"\"\n    if not text or not isinstance(text, str) or len(text) == 0:\n        return {\n            'is_capitalized': False,\n            'first_char': '',\n            'issue': 'empty_text'\n        }\n    \n    # Find first alphabetic character\n    for idx, char in enumerate(text):\n        if char.isalpha():\n            return {\n                'is_capitalized': char.isupper(),\n                'first_char': char,\n                'first_char_index': idx,\n                'issue': None if char.isupper() else 'not_uppercase'\n            }\n    \n    # No alphabetic character found\n    return {\n        'is_capitalized': False,\n        'first_char': '',\n        'issue': 'no_alphabetic_char'\n    }\n\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN (DUAL-PATH + LORA + CAPITALIZATION) - BanglaT5\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Model: BanglaT5 (csebuetnlp/banglat5)\")\n    print(f\"  Task prefix: '{_TASK_PREFIX}'\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    \n    # ‚úÖ NEW: Show LoRA status\n    if _USE_LORA:\n        print(f\"  LoRA: ENABLED\")\n        print(f\"    ‚Ä¢ Rank: {_LORA_RANK}\")\n        print(f\"    ‚Ä¢ Alpha: {_LORA_ALPHA}\")\n        print(f\"    ‚Ä¢ Dropout: {_LORA_DROPOUT}\")\n    else:\n        print(f\"  LoRA: DISABLED (full fine-tuning)\")\n    \n    print(f\"  Source language: {_SOURCE_LANGUAGE}\")\n    print(f\"  Target language: {_TARGET_LANGUAGE}\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  Max length: {_MAX_LENGTH}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"  Batch per GPU: {per_gpu}\")\n\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    if 'main_pipeline' not in globals():\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n\n            if _DEBUG_TIMING:\n                print(\"   Expected: ~15-45 min (config dependent)\")\n                if _USE_LORA:\n                    print(\"   (LoRA mode: ~30% faster)\")\n\n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n\n            if \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n                print(\"   Note: BanglaT5 uses AutoTokenizer (no src_lang/tgt_lang)\")\n\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n\n                print(\"\\nFixes:\")\n                print(\"   1. Reduce BATCH_SIZE (try 2-4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10k-20k)\")\n                print(\"   3. Increase ACCUMULATION_STEPS (32-64)\")\n                if _USE_LORA:\n                    print(\"   Note: LoRA already reduces memory by ~50%\")\n\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    checkpoint_dict = None\n\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024**2)\n                print(f\"  File: {_CHECKPOINT_PATH}\")\n                print(f\"  Size: {size_mb:.1f} MB\")\n\n                checkpoint_dict = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n\n                has_model = 'model_state_dict' in checkpoint_dict and checkpoint_dict['model_state_dict'] is not None and len(checkpoint_dict['model_state_dict']) > 0\n                has_dscd = 'dscd_state' in checkpoint_dict and checkpoint_dict.get('dscd_state') is not None and len(checkpoint_dict.get('dscd_state', {})) > 0\n                \n                # ‚úÖ NEW: Check LoRA state\n                has_lora = 'lora_state' in checkpoint_dict and checkpoint_dict.get('lora_state') is not None and len(checkpoint_dict.get('lora_state', {})) > 0\n\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n                \n                if _USE_LORA:\n                    print(f\"  LoRA: {'Present' if has_lora else 'MISSING'}\")\n                    if not has_lora:\n                        print(f\"     ‚ö†Ô∏è  WARNING: LoRA enabled but no LoRA state in checkpoint!\")\n\n                try:\n                    config = checkpoint_dict.get('config', {})\n                    model_type = config.get('model', 'Unknown')\n                    use_lora_ckpt = config.get('use_lora', False)\n                    \n                    print(f\"  Model Type: {model_type}\")\n                    \n                    if _USE_LORA:\n                        if use_lora_ckpt:\n                            lora_rank_ckpt = config.get('lora_rank', 0)\n                            print(f\"  LoRA Config: rank={lora_rank_ckpt}\")\n                        else:\n                            print(f\"  ‚ö†Ô∏è  WARNING: Current config uses LoRA but checkpoint doesn't\")\n                    \n                    if model_type != 'BanglaT5' and model_type != 'Unknown':\n                        print(f\"  ‚ö†Ô∏è  WARNING: Checkpoint is from {model_type}, not BanglaT5\")\n                        print(f\"     Compatibility issues may occur\")\n                except Exception:\n                    pass\n\n                if has_dscd:\n                    try:\n                        dscd_state = checkpoint_dict['dscd_state']\n                        num_tokens = 0\n\n                        if 'prototype_stores' in dscd_state:\n                            num_tokens = len(dscd_state['prototype_stores'])\n                        elif 'prototype_stores_data' in dscd_state:\n                            num_tokens = len(dscd_state['prototype_stores_data'])\n\n                        print(f\"  Tokens: {num_tokens}\")\n\n                        if num_tokens > 0:\n                            checkpoint_valid = True\n                            print(\"  Status: VALID\")\n                        else:\n                            print(\"  Status: EMPTY DSCD\")\n                    except Exception as e:\n                        print(f\"  Status: VALIDATION ERROR ({str(e)[:50]})\")\n                else:\n                    print(\"  Status: MISSING DSCD\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n\n        except Exception as e:\n            print(f\"  Validation failed: {e}\")\n            checkpoint_dict = None\n\n        print(\"\\n[COMPONENTS]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd = getattr(core, 'dscd', None)\n            if dscd and hasattr(dscd, 'get_prototype_summary'):\n                try:\n                    dscd_stats = dscd.get_prototype_summary()\n                    print(\"  DSCD:\")\n                    print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                    print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                    print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                except Exception:\n                    pass\n\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                try:\n                    asbn_stats = asbn.get_detailed_stats()\n                    print(\"  ASBN:\")\n                    print(f\"    - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%} {'(DISABLED)' if not _ENABLE_ASBN_TRAINING else ''}\")\n                    if 'source_accuracy' in asbn_stats:\n                        print(f\"    - Source: {asbn_stats['source_accuracy']:.2%}\")\n                        print(f\"    - Target: {asbn_stats['target_accuracy']:.2%}\")\n                except Exception:\n                    pass\n\n            trg = getattr(core, 'trg', None)\n            if trg and hasattr(trg, 'get_statistics'):\n                try:\n                    trg_stats = trg.get_statistics()\n                    print(\"  TRG:\")\n                    print(f\"    - Explanations: {trg_stats.get('explanations_generated', 0)}\")\n                    print(f\"    - High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                    print(f\"    - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n                except Exception:\n                    pass\n            \n            # ‚úÖ NEW: Show LoRA stats\n            if _USE_LORA:\n                try:\n                    if 'count_trainable_parameters' in globals():\n                        param_stats = count_trainable_parameters(core)\n                        print(\"  LoRA:\")\n                        print(f\"    - Total params: {param_stats['total']/1e6:.2f}M\")\n                        print(f\"    - Trainable: {param_stats['trainable']/1e6:.2f}M ({param_stats['trainable_pct']:.2f}%)\")\n                        print(f\"    - LoRA: {param_stats['lora']/1e6:.2f}M ({param_stats['lora_pct']:.2f}%)\")\n                except Exception as e:\n                    print(f\"  LoRA stats failed: {e}\")\n\n        except Exception as e:\n            print(f\"  Stats failed: {e}\")\n\n        print(\"\\n[METRICS]\")\n\n        try:\n            if checkpoint_dict is not None:\n                training_stats = checkpoint_dict.get('training_stats', {})\n                if training_stats:\n                    total_loss = training_stats.get('total_loss', [])\n                    updates = training_stats.get('optimizer_updates', 0)\n\n                    print(\"  Training:\")\n                    print(f\"    - Updates: {updates}\")\n                    if total_loss:\n                        if len(total_loss) >= 100:\n                            final = sum(total_loss[-100:]) / len(total_loss[-100:])\n                        else:\n                            final = sum(total_loss) / len(total_loss)\n                        print(f\"    - Final loss: {final:.6f}\")\n\n                eval_results = checkpoint_dict.get('eval_results', {})\n                baseline = checkpoint_dict.get('baseline_metrics', {})\n\n                if eval_results:\n                    final_success = eval_results.get('success_rate_pct', 0)\n                    total_expl = eval_results.get('total_explanations', 0)\n\n                    print(\"  Evaluation:\")\n                    if baseline:\n                        baseline_success = baseline.get('success_rate_pct', 0)\n                        improvement = final_success - baseline_success\n                        print(f\"    - Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n                        print(f\"    - Improvement: {improvement:+.1f}%\")\n                    else:\n                        print(f\"    - Success: {final_success:.1f}%\")\n\n                    print(f\"    - Explanations: {total_expl}\")\n\n                    # ‚úÖ NEW: Show capitalization metrics\n                    cap_metrics = eval_results.get('capitalization_metrics', {})\n                    if isinstance(cap_metrics, dict) and cap_metrics:\n                        cap_rate = cap_metrics.get('capitalization_rate', 0)\n                        cap_count = cap_metrics.get('capitalized_count', 0)\n                        cap_total = cap_metrics.get('total_checked', 0)\n                        print(f\"    - ‚úÖ Capitalization: {cap_rate:.1%} ({cap_count}/{cap_total})\")\n\n                    quality = eval_results.get('quality_metrics', {})\n                    if quality:\n                        print(f\"    - Avg confidence: {quality.get('avg_confidence', 0):.3f}\")\n            elif os.path.exists(_CHECKPOINT_PATH):\n                print(\"  Checkpoint loaded but invalid format\")\n            else:\n                print(\"  No checkpoint available\")\n\n        except Exception as e:\n            print(f\"  Metrics failed: {e}\")\n\n        del checkpoint_dict\n        _safe_cleanup()\n\n        # ===================================================================\n        # ‚úÖ FIX #3: IMPROVED INFERENCE VALIDATION WITH CAPITALIZATION\n        # ===================================================================\n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing disambiguation on ambiguous sentences...\")\n        print(\"-\" * 80)\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n        inference_times = []\n        capitalization_count = 0  # ‚úÖ NEW\n\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n        test_sentences = [\n            (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"‡¶ï‡¶≤ (tap/call)\"),\n            (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"‡¶ï‡¶æ‡¶≤ (tomorrow/yesterday)\"),\n            (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"‡¶™‡¶æ‡¶§‡¶æ (leaf/page)\"),\n        ]\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"ERROR: translate_with_explanations not available\")\n                print(\"   -> Run Cell 8 before Cell 11\")\n                print(\"   -> Cell 8 contains the translation function with capitalization\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}.  {desc}\")\n                        print(f\"   Input: {sentence}\")\n\n                        inf_start = time.time()\n\n                        res = translate_with_explanations(\n                            trained_model,\n                            tokenizer,\n                            sentence,\n                            source_lang=_SOURCE_LANGUAGE,\n                            target_lang=_TARGET_LANGUAGE,\n                            device=_DEVICE,\n                            max_length=_MAX_LENGTH,\n                            span_threshold=_SPAN_THRESHOLD,\n                            uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                            track_stats=True\n                        )\n\n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n\n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations', []) or []\n\n                            # ‚úÖ NEW: Check capitalization\n                            cap_check = check_capitalization(translation)\n                            \n                            if cap_check['is_capitalized']:\n                                capitalization_count += 1\n                                cap_marker = \"‚úÖ\"\n                            else:\n                                cap_marker = \"‚ö†Ô∏è \"\n\n                            print(f\"   {cap_marker} Translation: {translation}\")\n                            \n                            if not cap_check['is_capitalized']:\n                                print(f\"      Issue: {cap_check['issue']}\")\n                                if cap_check.get('first_char'):\n                                    print(f\"      First char: '{cap_check['first_char']}' (should be uppercase)\")\n                            \n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs:\n                                for exp in exs:\n                                    word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                    clean = str(word).replace('‚ñÅ', '').replace('ƒ†', '').strip().lower()\n\n                                    if clean in dscd_homographs:\n                                        dscd_homographs_detected.add(clean)\n\n                                    try:\n                                        conf = float(exp.get('confidence', 0.5))\n                                        span = float(exp.get('span', 0.0))\n                                        u = float(exp.get('uncertainty', 0.0))\n                                        print(f\"   -> '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                    except Exception:\n                                        print(f\"   -> '{word}': (no metrics)\")\n\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n\n                        _safe_cleanup()\n\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__} - {str(e)[:100]}\")\n                        inference_failed += 1\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n                \n                # ‚úÖ NEW: Report capitalization\n                cap_rate = capitalization_count / len(test_sentences) if len(test_sentences) > 0 else 0.0\n                print(f\"‚úÖ Capitalization: {cap_rate:.1%} ({capitalization_count}/{len(test_sentences)})\")\n                \n                if cap_rate < 1.0:\n                    print(f\"‚ö†Ô∏è  WARNING: Not all translations capitalized!\")\n                    print(f\"   Expected: 100%, Got: {cap_rate:.1%}\")\n                    print(f\"   ‚Üí Check Cell 8 capitalization function\")\n\n                if inference_times:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg per sentence\")\n\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                else:\n                    print(\"No DSCD homographs detected\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries (run warmup)\")\n                    else:\n                        print(f\"   -> Check TRG thresholds (span={_SPAN_THRESHOLD}, u={_UNCERTAINTY_THRESHOLD})\")\n\n                if 'INFERENCE_STATS' in globals():\n                    try:\n                        print(\"\\n\" + \"-\" * 80)\n                        print(\"AGGREGATED STATISTICS (from Cell 8):\")\n                        print(\"-\" * 80)\n                        INFERENCE_STATS.print_summary()\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(f\"Failed to print INFERENCE_STATS: {e}\")\n                else:\n                    if _DEBUG_DISCOVERY:\n                        print(\"\\nINFERENCE_STATS not available (Cell 8 not loaded)\")\n\n        except Exception as e:\n            print(f\"Validation failed: {e}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        # ===================================================================\n        # ‚úÖ FIX #4: ENHANCED SYSTEM TEST\n        # ===================================================================\n        print(\"\\n[SYSTEM TEST]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd_ok = hasattr(core, 'dscd') and hasattr(core.dscd, 'forward')\n            asbn_ok = hasattr(core, 'asbn') and hasattr(core.asbn, 'forward')\n            trg_ok = hasattr(core, 'trg') and hasattr(core.trg, 'process_sentence_for_explanations')\n            t5_ok = hasattr(core, 't5') and hasattr(core.t5, 'forward')\n            generate_ok = hasattr(core, 'generate')\n            \n            # ‚úÖ NEW: Check capitalization function\n            has_return_text = False\n            has_cap_function = False\n            \n            if generate_ok:\n                try:\n                    import inspect\n                    sig = inspect.signature(core.generate)\n                    has_return_text = 'return_text' in sig.parameters\n                except Exception:\n                    pass\n            \n            # Check if Cell 8 capitalization functions exist\n            if 'capitalize_first_char' in globals():\n                has_cap_function = True\n            elif 'clean_and_capitalize_translation' in globals():\n                has_cap_function = True\n\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'} {'(DISABLED)' if not _ENABLE_ASBN_TRAINING else ''}\")\n            print(f\"    - TRG: {'OK' if trg_ok else 'MISSING'}\")\n            print(f\"    - BanglaT5: {'OK' if t5_ok else 'MISSING'}\")\n            print(f\"    - generate(): {'OK' if generate_ok else 'MISSING'}\")\n            \n            # ‚úÖ NEW: Capitalization status\n            if has_cap_function:\n                print(f\"    - Capitalization: ‚úÖ ENABLED (Cell 8)\")\n            else:\n                print(f\"    - Capitalization: ‚ö†Ô∏è  MISSING (Cell 8 not loaded properly)\")\n\n            translate_fn_ok = 'translate_with_explanations' in globals()\n            print(f\"    - translate_with_explanations(): {'OK' if translate_fn_ok else 'MISSING (Cell 8 not loaded)'}\")\n            \n            # ‚úÖ NEW: LoRA status\n            if _USE_LORA:\n                lora_ok = False\n                try:\n                    if 'count_trainable_parameters' in globals():\n                        param_stats = count_trainable_parameters(core)\n                        lora_ok = param_stats['lora'] > 0\n                        print(f\"    - LoRA: {'OK' if lora_ok else 'MISSING'} ({param_stats['lora']/1e6:.2f}M params)\")\n                except Exception:\n                    print(f\"    - LoRA: UNKNOWN\")\n\n            all_ok = dscd_ok and asbn_ok and trg_ok and t5_ok and generate_ok and translate_fn_ok and has_cap_function\n\n            if all_ok:\n                print(\"  ‚úÖ All components operational\")\n                if _USE_LORA and lora_ok:\n                    print(\"  ‚úÖ LoRA adapters active\")\n            elif not translate_fn_ok:\n                print(\"  ‚ö†Ô∏è  WARNING: translate_with_explanations() missing\")\n                print(\"     Run Cell 8 before using the model\")\n            elif not has_cap_function:\n                print(\"  ‚ö†Ô∏è  WARNING: Capitalization function missing\")\n                print(\"     Translations will not be capitalized\")\n                print(\"     Run Cell 8 to enable capitalization\")\n            elif not generate_ok:\n                print(\"  ‚ö†Ô∏è  WARNING: generate() missing\")\n                print(\"     Cell 6 may need to be fixed\")\n            else:\n                print(\"  ‚ö†Ô∏è  Some components missing\")\n\n        except Exception as e:\n            print(f\"  Test failed: {e}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n\n        print(\"\\n1. Single translation (with automatic capitalization):\")\n        print(f\"   result = translate_with_explanations(trained_model, tokenizer, '‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§', source_lang='{_SOURCE_LANGUAGE}', target_lang='{_TARGET_LANGUAGE}', device=_DEVICE, max_length={_MAX_LENGTH})\")\n        print(f\"   print(result['translation'])  # 'I turned off the tap' (capitalized)\")\n\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(f\"       res = translate_with_explanations(trained_model, tokenizer, sent, source_lang='{_SOURCE_LANGUAGE}', target_lang='{_TARGET_LANGUAGE}', device=_DEVICE, max_length={_MAX_LENGTH})\")\n        print(\"       print(res['translation'])  # All automatically capitalized\")\n\n        print(\"\\n3. Load checkpoint:\")\n        print(f\"   ckpt = torch.load('{_CHECKPOINT_PATH}', weights_only=False)\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n        if _USE_LORA:\n            print(\"   # LoRA state is included in model_state_dict\")\n\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n        print(\"   print(f\\\"Capitalization: {results['capitalization_metrics']['capitalization_rate']:.1%}\\\")\")\n\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if not checkpoint_valid:\n            print(\"\\nCheckpoint needs verification - re-run Cell 10 if needed\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n\n        print(\"\\n[DIAGNOSTICS]\")\n\n        components = {\n            'Cell 0': 'NUM_SAMPLES' in globals(),\n            'Cell 1': 'reconstruct_word_spans' in globals(),\n            'Cell 2': 'MemoryEfficientDataset' in globals(),\n            'Cell 3': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8': 'translate_with_explanations' in globals(),\n            'Cell 9': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10': 'main_pipeline' in globals(),\n        }\n\n        all_present = True\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status} {comp}\")\n            if not present:\n                all_present = False\n\n        # ‚úÖ NEW: Check capitalization functions\n        print(\"\\n[CAPITALIZATION CHECK]\")\n        has_cap = 'capitalize_first_char' in globals() or 'clean_and_capitalize_translation' in globals()\n        print(f\"  {'OK' if has_cap else 'MISSING'} Capitalization functions (Cell 8)\")\n\n        print(\"\\n[RECOVERY]\")\n\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"  ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"  Then RESTART kernel and re-run Cells 0-11\")\n            print(\"\\n-> For BanglaT5:\")\n            print(\"  - Uses AutoTokenizer (no src_lang/tgt_lang)\")\n            print(\"  - Task prefix is automatically added\")\n            print(\"  - Vocab size: 32100 (not 250054)\")\n\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory in Cell 0:\")\n            if _USE_LORA:\n                print(\"  # LoRA already reduces memory by ~50%\")\n                print(\"  BATCH_SIZE = 2\")\n                print(\"  NUM_SAMPLES = 10000\")\n                print(\"  ACCUMULATION_STEPS = 32\")\n            else:\n                print(\"  BATCH_SIZE = 2\")\n                print(\"  NUM_SAMPLES = 15000\")\n                print(\"  ACCUMULATION_STEPS = 32\")\n                print(\"  # Or enable LoRA:\")\n                print(\"  USE_LORA = True\")\n                print(\"  LORA_RANK = 16\")\n            print(\"  Then re-run Cells 0-11\")\n\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0:\")\n            print(\"  VERBOSE_LOGGING = True\")\n            print(\"  DEBUG_DISCOVERY = True\")\n            print(\"  Then re-run Cell 11 for details\")\n\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check checkpoint exists:\")\n            print(f\"  os.path.exists('{_CHECKPOINT_PATH}')\")\n            print(\"  If yes, can load and skip training\")\n            print(\"  If no, re-run Cell 11\")\n\n        else:\n            print(\"\\n-> General steps:\")\n            print(\"  1. Enable DEBUG in Cell 0\")\n            print(\"  2. Re-run Cells 0-11\")\n            print(\"  3. Check GPU: torch.cuda.is_available()\")\n            print(\"  4. Verify data loaded\")\n            print(\"  5. Verify BanglaT5 compatibility:\")\n            print(\"     - Check model.t5 exists (not model.mbart)\")\n            print(\"     - Verify vocab_size = 32100\")\n            print(\"     - Confirm AutoTokenizer loaded\")\n            print(\"     - Check Cell 6 capitalization fix applied\")\n            print(\"     - Verify Cell 8 translate_with_explanations() exists\")\n            print(\"     - Verify Cell 8 capitalization functions exist\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"Model: BanglaT5\")\n    if _USE_LORA:\n        print(f\"LoRA: ENABLED (rank={_LORA_RANK})\")\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS\")\n        if 'checkpoint_valid' in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID\")\n        else:\n            print(\"Checkpoint: CHECK NEEDED\")\n        \n        # ‚úÖ NEW: Report capitalization\n        if 'capitalization_count' in locals() and 'test_sentences' in locals():\n            cap_rate = capitalization_count / len(test_sentences) if len(test_sentences) > 0 else 0.0\n            print(f\"‚úÖ Capitalization: {cap_rate:.1%}\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n\n    print(\"=\" * 80)\n\n    _safe_cleanup()\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution Wrapper [‚úÖ FULLY FIXED + LORA + CAPITALIZATION]\")\nprint(\"=\" * 80)\nprint(\"This cell:\")\nprint(\"  - Loads configuration from Cell 0\")\nprint(\"  - Executes main_pipeline() from Cell 10\")\nprint(\"  - Validates checkpoint integrity (including LoRA state)\")\nprint(\"  - Tests inference with sample sentences\")\nprint(\"  - ‚úÖ Verifies capitalization on all translations\")\nprint(\"  - Provides comprehensive diagnostics\")\nprint(\"  - Shows usage examples for next steps\")\nprint()\nprint(f\"Current config:\")\nprint(f\"  - Model: BanglaT5 (csebuetnlp/banglat5)\")\nprint(f\"  - LoRA: {'ENABLED' if _USE_LORA else 'DISABLED'}\")\nif _USE_LORA:\n    print(f\"    ‚Ä¢ Rank: {_LORA_RANK}\")\n    print(f\"    ‚Ä¢ Alpha: {_LORA_ALPHA}\")\nprint(f\"  - Task prefix: '{_TASK_PREFIX}'\")\nprint(f\"  - Samples: {_NUM_SAMPLES}\")\nprint(f\"  - Epochs: {_EPOCHS}\")\nprint(f\"  - Batch size: {_BATCH_SIZE}\")\nprint(f\"  - Device: {_DEVICE}\")\nprint(f\"  - Multi-GPU: {_USE_MULTI_GPU}\")\nprint(f\"  - ASBN Training: {'DISABLED' if not _ENABLE_ASBN_TRAINING else 'ENABLED'}\")\nprint(f\"  - Discovery Frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\nprint(f\"  - Checkpoint: {_CHECKPOINT_PATH}\")\nprint(\"\\n‚úÖ FIXES APPLIED:\")\nprint(\"  ‚úÖ FIX #1: Added LoRA globals and detection\")\nprint(\"  ‚úÖ FIX #2: Added capitalization check function\")\nprint(\"  ‚úÖ FIX #3: Enhanced inference validation with capitalization\")\nprint(\"  ‚úÖ FIX #4: Enhanced system test (checks Cell 8 cap functions)\")\nprint(\"  ‚úÖ FIX #5: LoRA state validation in checkpoint\")\nprint(\"  ‚úÖ FIX #6: Capitalization rate reporting\")\nprint(\"  ‚úÖ FIX #7: Shows capitalization per test sentence\")\nprint(\"  ‚úÖ FIX #8: Warns if capitalization < 100%\")\nprint(\"\\n‚úÖ CAPITALIZATION:\")\nprint(\"  ‚úÖ All translations automatically capitalized by Cell 8\")\nprint(\"  ‚úÖ Per-sentence capitalization check (‚úÖ or ‚ö†Ô∏è)\")\nprint(\"  ‚úÖ Overall capitalization rate (target: 100%)\")\nprint(\"  ‚úÖ Warnings if not all translations capitalized\")\nprint(\"\\n‚úÖ LORA:\")\nprint(\"  ‚úÖ Shows LoRA status in config\")\nprint(\"  ‚úÖ Validates LoRA state in checkpoint\")\nprint(\"  ‚úÖ Reports trainable param count\")\nprint(\"  ‚úÖ Warns if LoRA enabled but missing in checkpoint\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"id":"9n4Hrn1wH4J6","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:01:27.136367Z","iopub.execute_input":"2026-02-16T03:01:27.136677Z","iopub.status.idle":"2026-02-16T03:06:31.404241Z","shell.execute_reply.started":"2026-02-16T03:01:27.136639Z","shell.execute_reply":"2026-02-16T03:06:31.403621Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nMEMORY-OPTIMIZED TATN (DUAL-PATH + LORA + CAPITALIZATION) - BanglaT5\n================================================================================\nUser: manas0003\nStarted: 2026-02-16 03:01:27 UTC\n\n[CONFIGURATION]\n  Model: BanglaT5 (csebuetnlp/banglat5)\n  Task prefix: 'translate Bengali to English: '\n  Cell 0 status: Loaded\n  Samples: 200000\n  Epochs: 3\n  Batch Size: 32\n  Accumulation: 8\n  Device: cuda:0\n  Multi-GPU: ENABLED (2 GPUs)\n  LoRA: ENABLED\n    ‚Ä¢ Rank: 32\n    ‚Ä¢ Alpha: 64.0\n    ‚Ä¢ Dropout: 0.1\n  Source language: bn\n  Target language: en\n  Span threshold: 0.18\n  Uncertainty threshold: 0.12\n  Max length: 128\n  Discovery frequency: 400\n  Batch per GPU: 16\n  ASBN: Enabled\n  TRG: Enabled\n  Debug: Disabled\n================================================================================\n\nStarting pipeline...\n   Expected: ~15-45 min (config dependent)\n   (LoRA mode: ~30% faster)\n\n================================================================================\nTATN MAIN PIPELINE (DUAL-PATH + LORA COMPATIBLE) - BanglaT5\n================================================================================\nConfiguration:\n  - Model: BanglaT5 (csebuetnlp/banglat5)\n  - LoRA: ENABLED\n    ‚Ä¢ Rank: 32\n    ‚Ä¢ Alpha: 64.0\n    ‚Ä¢ Target modules: 5 (q, v, k, o, wi)\n  - Task prefix: 'translate Bengali to English: '\n  - Span threshold: 0.18\n  - Uncertainty threshold: 0.12\n  - Discovery frequency: 400\n  - ASBN Training: ENABLED\n  - Epochs: 3\n  - Batch size: 32\n================================================================================\n[PIPELINE] Initializing environment...\n[PIPELINE] GPUs: 2\n  GPU 0: Tesla T4 (14.6 GB)\n  GPU 1: Tesla T4 (14.6 GB)\n\n[PIPELINE] ‚úÖ LoRA ENABLED:\n  - Rank: 32\n  - Alpha: 64.0\n  - Dropout: 0.1\n  - Target modules: ['q', 'v', 'k', 'o', 'wi']\n[TIMING] Initialization: 0.61s\n\n[PHASE 1] Loading tokenizer...\n[PHASE 1] Tokenizer loaded (vocab: 32100)\n[PHASE 1] Validating tokenizer vocabulary...\n[TOKENIZER-VALIDATION] Actual vocab size: 32100\n[TOKENIZER-VALIDATION] Checking T5 special tokens:\n  <pad> ‚Üí 0\n  </s> ‚Üí 1\n  <unk> ‚Üí 2\n  <extra_id_0> ‚Üí 32099\n[TOKENIZER-VALIDATION] ‚úÖ T5 sentinel tokens detected\n[TOKENIZER-VALIDATION] ‚úÖ Special tokens valid\n[TIMING] Tokenizer: 0.52s\n\n[PHASE 2] Loading data (200000 samples)...\n[CELL2] Loading up to 200000 samples from local CSV: /kaggle/input/datasets/manas00000003/sam-dataset/bn_en_qe0.6_adequacy_filtered_500000_1000000.csv\n[CELL2] Reading CSV file...\n[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn‚Üíen task.\n[CELL2] Swap successful: src=Bengali, tgt=English\n[CELL2] Processing 200000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200000/200000 [00:05<00:00, 38874.59it/s]\n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 199758 pairs from CSV, skipped 242 rows\n\n[PHASE 3] Initializing model...\n================================================================================\nCELL 6: INITIALIZING BANGLAT5 MODEL WITH LORA\n================================================================================\n\n[STEP 1/9] Loading pretrained BanglaT5 model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15aadf1450254584a2e08aa5a2f85911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d128c9c6c8884a6986282ed67e89ca12"}},"metadata":{}},{"name":"stdout","text":"  ‚úÖ Model loaded successfully\n\n[STEP 2/9] Moving model to GPU...\n  ‚úÖ Model on device: cuda\n\n[LORA INITIALIZATION]\n  Applying Standard LoRA (FP16) with config:\n    - Rank: 32\n    - Alpha: 64.0\n    - Dropout: 0.1\n    - Target modules: 5 (q, v, k, o, wi)\n    - Mode: FP16 (no quantization)\n  ‚úÖ Standard LoRA (FP16) applied successfully:\n     - Total params: 254,655,744\n     - Trainable params: 7,077,888 (2.78%)\n     - Frozen params: 247,577,856\n     - Expected GPU memory: ~2.5 GB\n     - Expected BLEU: 38-40\n     - Expected training time: ~3.5 hours\n  ‚úÖ LoRA parameters in optimal range (2.78%)\n\n[STEP 3/9] Testing T5 BEFORE any modifications...\n  ‚úÖ Baseline test passed: loss=11.2268\n     (This is the pretrained model's natural loss)\n\n[STEP 4/9] Analyzing embedding layers...\n  [4a] Encoder embeddings:\n     Shape: torch.Size([32128, 768])\n     Range: [-296.0000, 233.0000]\n     Mean: -0.1264, Std: 22.0425\n     Has NaN: False, Has Inf: False\n     ‚úÖ No NaN/Inf corruption\n  [4b] Decoder embeddings:\n     Shape: torch.Size([32128, 768])\n     Range: [-9.8750, 10.0625]\n     Mean: -0.0016, Std: 1.1979\n     Has NaN: False, Has Inf: False\n     ‚úÖ No NaN/Inf corruption\n  [4c] ‚ÑπÔ∏è  Encoder/decoder have separate embeddings\n\n[STEP 5/9] Detecting if BanglaT5 uses scaled embeddings...\n  Model d_model: 768\n  Expected sqrt(d_model): 27.71\n  Encoder/Decoder magnitude ratio: 29.42\n  ‚ÑπÔ∏è  Ratio matches sqrt(d_model) scaling pattern!\n     This is NORMAL for T5 with scaled encoder embeddings\n\n[STEP 6/9] Applying INTELLIGENT fixes...\n  [6a] Checking non-embedding parameters...\n     ‚úÖ All non-embedding params are clean\n  [6b] Handling encoder embeddings...\n     ‚ÑπÔ∏è  Detected SCALED embeddings (factor: 27.71)\n     ‚Üí This is ARCHITECTURAL, not corruption\n     ‚Üí PRESERVING original scaled weights (no clipping)\n  [6c] Handling decoder embeddings...\n     ‚úÖ Decoder embeddings healthy\n     ‚Üí NO CLIPPING APPLIED (preserving pretrained weights)\n  ‚úÖ No fixes needed - model is healthy!\n\n[STEP 7/9] Testing T5 AFTER modifications...\n  ‚úÖ Post-fix test passed: loss=11.2268\n\n[STEP 8/9] Comparing before/after losses...\n  Baseline loss:  11.2268\n  Post-fix loss:  11.2268\n  Change:         +0.0000 (1.00x)\n  ‚úÖ GOOD: Loss stable (change: +0.0%)\n     Model is healthy\n\n[STEP 9/9] Final validation...\n  Encoder: [-296.00, 233.00]\n  Decoder: [-9.88, 10.06]\n  ‚ÑπÔ∏è  Using SCALED encoder embeddings (architectural feature)\n  ‚ö†Ô∏è  Final validation error: RuntimeError\n\n================================================================================\nMODEL INITIALIZATION COMPLETE\n================================================================================\n\n[VOCAB] ‚ö†Ô∏è  Vocab size mismatch detected!\n  Tokenizer: 32100\n  Model: 32128\n[VOCAB] ‚úÖ Using model's full vocab size: 32128\n[VOCAB] Note: Model has 28 extra embeddings\n[VOCAB] ‚úÖ Preserving pretrained weights - NO RESIZE\n\n[TATN-INIT] ‚úÖ DUAL-PATH MemoryOptimizedTATNWithExplanations READY\n  - Model: csebuetnlp/banglat5\n  - Vocab size: 32128\n  - Embed dim: 768\n  - Encoder: SCALED embeddings (factor: 27.71)\n  - LoRA: ENABLED (r=32, alpha=64.0)\n  - Trainable: 7,077,888 (2.78%)\n  - Capitalization: ENABLED (first alphabetic char auto-capitalized)\n================================================================================\n\n\n[VALIDATION] Checking component compatibility...\n  ‚úÖ Vocabulary: model=32128, tokenizer=32100\n     Note: Model has 28 extra tokens (preserves pretrained weights)\n  ‚úÖ Model embed_dim: 768\n  ‚úÖ DSCD embed_dim: 768\n  ‚úÖ ASBN embed_dim: 768\n  ‚úÖ Embedding layer: dim=768, vocab=32128\n\n[VALIDATION] LoRA Parameter Check:\n  Total params: 255.39M\n  Trainable params: 7.82M (3.06%)\n  LoRA params: 7.08M (2.77%)\n  Frozen params: 247.58M\n  ‚úÖ LoRA correctly applied\n[VALIDATION] ‚úÖ All components compatible\n[PIPELINE] Creating dataset with model vocab_size=32128\n[CELL2] Dataset using provided vocab_size: 32128\n[CELL2] Dataset initialized: 199758 valid pairs, 0 invalid, split=train\n[CELL2] DataLoader created: total_batch=32, per_gpu=16, workers=0\n[PHASE 2] Dataset: 199758 samples, 6243 batches\n[TIMING] Data loading: 14.33s\n\n[VALIDATION] Checking dataset compatibility...\n  Input IDs range: [1, 23265]\n  Model vocab size: 32128\n[VALIDATION] ‚úÖ Dataset token IDs valid\n[PHASE 3] Applying DataParallel on [0, 1]\n[PHASE 3] Moving model to device: cuda:0\n\n[VALIDATION] Testing model with translate_with_explanations...\n  Testing 3 sentences...\n  1. ‚úÖ 'I sing in Bengali. ‡¶Ü‡¶Æ‡¶ø ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø‡¶§‡ßá ‡¶ó‡¶æ‡¶® ‡¶ó‡¶æ‡¶á‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®'\n  2. ‚úÖ 'Today it is good weather. ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®, \"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ '\n  3. ‚úÖ 'He is going to go to school. ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú'\n\n  ‚úÖ All translations successful\n  ‚úÖ Capitalization rate: 100.0% (3/3)\n  ‚úÖ All translations properly capitalized\n  ‚úÖ Model validation passed\n[PHASE 3] Model initialized and validated\n[TIMING] Model init: 5.69s\n\n[PHASE 4] Setting up optimizers...\n[PHASE 4] Extracting parameters from wrapped model...\n[PHASE 4] Parameter extraction:\n  - Base params: 33 (DSCD/ASBN/TRG)\n  - LoRA params: 288\n[PHASE 4] Parameter counts:\n  - LoRA params: 7,077,888 (7.08M)\n  - Base params: 738,541 (0.74M)\n[PHASE 4] Added LoRA param group (LR: 5.00e-04, 288 tensors)\n[PHASE 4] Added base param group (DSCD/ASBN/TRG, LR: 2.50e-04, 33 tensors)\n[PHASE 4] ‚úÖ Optimizer created:\n  - Param groups: 2\n  - Total param tensors: 321\n  - Total params: 7,816,429 (7.82M)\n  - Initial LR: 5.00e-04\n  - DataParallel: YES\n\n[PHASE 4] Validating optimizer<->model connection...\n[PHASE 4] ‚úÖ Optimizer params EXACTLY match model params (321 tensors)\n[PHASE 4] ‚úÖ ALL trainable params will receive gradients\n[PHASE 4] ASBN optimizer created (18 params)\n[PHASE 4] Optimizers ready\n\n\n[PHASE 5] Training...\n  - ASBN Training: ENABLED\n  - ASBN Optimizer: Active\n  - LoRA Training: ENABLED (288 LoRA param tensors)\n[TRAIN] Starting training: epochs=3, batch=32, accum_steps=8\n[TRAIN] Validation: enabled\n[TRAIN] DP enabled: True, GPUs: 2, Device: cuda:0\n[TRAIN] Discovery frequency: 400 steps\n[TRAIN] Dual-path training: ENABLED\n[TRAIN] Gradient diagnostics: DISABLED\n\n[TRAIN] Parameter Statistics:\n  Total parameters: 255.39M\n  Trainable parameters: 7.82M (3.06%)\n  LoRA parameters: 7.08M (2.77%)\n  Frozen parameters: 247.58M\n\n[TRAIN] ‚úÖ LoRA correctly applied (7.08M LoRA params)\n[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt after all epochs\n\n[TRAIN] ‚úÖ Learning rate scheduler created:\n[TRAIN]    - Type: Cosine with warmup\n[TRAIN]    - Total steps: 18729\n[TRAIN]    - Warmup steps: 600\n[TRAIN]    - LR after scheduler init: 0.00e+00\n[TRAIN]    ‚ö†Ô∏è  Scheduler set LR to 0 (warmup starts at step 0)\n[TRAIN]    Applying emergency fix: setting LR to 1% of target...\n[TRAIN]    ‚úÖ LR corrected: 0.00e+00 ‚Üí 5.00e-06\n[TRAIN]    Warmup will now increase from 1% ‚Üí 100% over 600 steps\n[TRAIN]    - LoRA mode: Using LR 5.00e-04\n[TRAIN]    - Trainable params: 7.82M (LoRA adapters only)\n[TRAIN]    - Expected: Train loss will converge to <1.5 (from current ~2.4)\n[TRAIN]    - Expected BLEU gain: +10-15 points\n\n\n================================================================================\nEPOCH 1/3 STARTED\n================================================================================\n\n[TRAIN] TRG statistics reset for epoch 1\n[TRAIN] ASBN statistics reset for epoch 1\nEpoch 1/3:   0%|            | 7/6243 [00:03<58:30,  1.78it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=393, path=1][DEBUG-GRAD] Step 8: scaler.unscale_() SUCCESS\n\n================================================================================\n‚ùå AssertionError at step 8\n================================================================================\nError: No inf checks were recorded for this optimizer.\n\n[DIAGNOSTIC] Current LR: 5.00e-06\n\n[ERROR] Exception during optimizer step: AssertionError\nEpoch 1/3:   0%|           | 15/6243 [00:08<54:23,  1.91it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=385, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 16\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   0%|           | 23/6243 [00:12<52:44,  1.97it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=377, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 24\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   0%|           | 31/6243 [00:16<56:55,  1.82it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=369, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 32\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|           | 39/6243 [00:21<52:41,  1.96it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=361, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 40\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|         | 47/6243 [00:26<1:01:47,  1.67it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=353, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 48\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|         | 55/6243 [00:31<1:05:56,  1.56it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=345, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 56\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|         | 63/6243 [00:36<1:00:28,  1.70it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=337, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 64\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|         | 71/6243 [00:41<1:00:17,  1.71it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=329, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 72\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|         | 79/6243 [00:46<1:08:27,  1.50it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=321, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 80\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   1%|‚ñè        | 87/6243 [00:52<1:11:53,  1.43it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=313, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 88\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè        | 95/6243 [00:57<1:07:31,  1.52it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=305, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 96\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 103/6243 [01:02<1:08:34,  1.49it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=297, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 104\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 111/6243 [01:08<1:04:26,  1.59it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=289, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 112\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 119/6243 [01:13<1:03:30,  1.61it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=281, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 120\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 127/6243 [01:18<1:02:16,  1.64it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=273, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 128\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 135/6243 [01:23<1:00:24,  1.69it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=265, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 136\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 143/6243 [01:28<1:06:34,  1.53it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=257, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 144\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   2%|‚ñè       | 151/6243 [01:33<1:06:11,  1.53it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=249, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 152\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñè       | 159/6243 [01:39<1:13:06,  1.39it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=241, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 160\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñè       | 167/6243 [01:45<1:05:21,  1.55it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=233, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 168\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñè       | 175/6243 [01:50<1:10:49,  1.43it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=225, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 176\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñè       | 183/6243 [01:56<1:09:00,  1.46it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=217, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 184\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñè       | 191/6243 [02:02<1:10:36,  1.43it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=209, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 192\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñé       | 199/6243 [02:07<1:09:19,  1.45it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=201, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 200\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñé       | 207/6243 [02:14<1:10:11,  1.43it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=193, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 208\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   3%|‚ñé       | 215/6243 [02:19<1:08:38,  1.46it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=185, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 216\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 223/6243 [02:25<1:05:52,  1.52it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=177, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 224\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 231/6243 [02:30<1:13:08,  1.37it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=169, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 232\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 239/6243 [02:36<1:04:53,  1.54it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=161, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 240\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 247/6243 [02:41<1:06:56,  1.49it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=153, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 248\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 255/6243 [02:47<1:16:03,  1.31it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=145, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 256\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 263/6243 [02:53<1:05:28,  1.52it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=137, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 264\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 271/6243 [02:59<1:05:53,  1.51it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=129, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 272\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   4%|‚ñé       | 279/6243 [03:04<1:06:55,  1.49it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=121, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 280\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñé       | 287/6243 [03:10<1:18:31,  1.26it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=113, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 288\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç       | 295/6243 [03:15<1:05:16,  1.52it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=105, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 296\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 303/6243 [03:22<1:20:18,  1.23it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=97, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 304\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 311/6243 [03:27<1:06:40,  1.48it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=89, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 312\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 319/6243 [03:33<1:06:30,  1.48it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=81, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 320\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 327/6243 [03:38<1:06:50,  1.48it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=73, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 328\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 335/6243 [03:44<1:06:01,  1.49it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=65, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 336\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   5%|‚ñç        | 343/6243 [03:49<1:07:10,  1.46it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=57, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 344\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñå        | 351/6243 [03:56<1:23:45,  1.17it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=49, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 352\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñå        | 359/6243 [04:01<1:06:39,  1.47it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=41, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 360\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñå        | 367/6243 [04:07<1:05:20,  1.50it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=33, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 368\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñå        | 375/6243 [04:12<1:09:36,  1.40it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=25, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 376\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñå        | 383/6243 [04:18<1:06:19,  1.47it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=17, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 384\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñã         | 391/6243 [04:24<1:07:45,  1.44it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=9, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 392\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\nEpoch 1/3:   6%|‚ñã         | 399/6243 [04:30<1:10:30,  1.38it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=1, path=1]\n[DISCOVERY] Running periodic check at step 400...\n\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 400\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\n[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=0.96 resv=1.04\n  GPU 1: alloc=0.00 resv=0.00\n[TRAIN-DEBUG] step=400 p2 loss=17.9395 clusters=14781\n\n[CLUSTER] Top 5 clusters:\n------------------------------------------------------------------------------------------\nRank  Token          Count       Protos    Mu             Tau         \n------------------------------------------------------------------------------------------\n1     ‡¶Ü‡¶Æ‡¶ø            30          1         0.000000       0.000000    \n2     ‡¶≠‡¶æ‡¶≤‡ßã           30          1         0.000000       0.000000    \n3     ‡¶§‡¶ø‡¶®‡¶ø           30          1         0.000000       0.000000    \n4     ‡¶ï‡¶∞‡ßá            30          1         0.000000       0.000000    \n5     ‡¶ï‡¶∞‡¶æ‡¶∞           30          1         0.000000       0.000000    \n------------------------------------------------------------------------------------------\nEpoch 1/3:   7%|‚ñå       | 407/6243 [04:36<1:11:10,  1.37it/s, fwd=0.00, bwd=0.00, rate=0.0%, disc=393, path=1]\n================================================================================\n[ERROR-LOCATION] scaler.unscale_() at step 408\n================================================================================\nError Type: RuntimeError\nError Message: unscale_() has already been called on this optimizer since the last update().\n\n[DIAGNOSIS] Different scaler error - re-raising\n================================================================================\n\n\n[ERROR] Runtime error during optimizer step: RuntimeError\n                                                                                                              \nInterrupted by user\n\n================================================================================\nPIPELINE FAILED\n================================================================================\n\nCategory: USER_INTERRUPT\nDetails: Manual stop\n\n[DIAGNOSTICS]\n  OK Cell 0\n  OK Cell 1\n  OK Cell 2\n  OK Cell 3\n  OK Cell 4\n  OK Cell 5\n  OK Cell 6\n  OK Cell 7\n  OK Cell 8\n  OK Cell 9\n  OK Cell 10\n\n[CAPITALIZATION CHECK]\n  OK Capitalization functions (Cell 8)\n\n[RECOVERY]\n\n-> Check checkpoint exists:\n  os.path.exists('/content/model/tatn_final.pt')\n  If yes, can load and skip training\n  If no, re-run Cell 11\n\n================================================================================\n\n================================================================================\nEXECUTION SUMMARY\n================================================================================\nModel: BanglaT5\nLoRA: ENABLED (rank=32)\nUser: manas0003\nStarted: 2026-02-16 03:01:27 UTC\nFinished: 2026-02-16 03:06:31 UTC\nDuration: 5.1min\nStatus: FAILED (USER_INTERRUPT)\n================================================================================\n\n================================================================================\nCell 11: Execution Wrapper [‚úÖ FULLY FIXED + LORA + CAPITALIZATION]\n================================================================================\nThis cell:\n  - Loads configuration from Cell 0\n  - Executes main_pipeline() from Cell 10\n  - Validates checkpoint integrity (including LoRA state)\n  - Tests inference with sample sentences\n  - ‚úÖ Verifies capitalization on all translations\n  - Provides comprehensive diagnostics\n  - Shows usage examples for next steps\n\nCurrent config:\n  - Model: BanglaT5 (csebuetnlp/banglat5)\n  - LoRA: ENABLED\n    ‚Ä¢ Rank: 32\n    ‚Ä¢ Alpha: 64.0\n  - Task prefix: 'translate Bengali to English: '\n  - Samples: 200000\n  - Epochs: 3\n  - Batch size: 32\n  - Device: cuda:0\n  - Multi-GPU: True\n  - ASBN Training: ENABLED\n  - Discovery Frequency: 400\n  - Checkpoint: /content/model/tatn_final.pt\n\n‚úÖ FIXES APPLIED:\n  ‚úÖ FIX #1: Added LoRA globals and detection\n  ‚úÖ FIX #2: Added capitalization check function\n  ‚úÖ FIX #3: Enhanced inference validation with capitalization\n  ‚úÖ FIX #4: Enhanced system test (checks Cell 8 cap functions)\n  ‚úÖ FIX #5: LoRA state validation in checkpoint\n  ‚úÖ FIX #6: Capitalization rate reporting\n  ‚úÖ FIX #7: Shows capitalization per test sentence\n  ‚úÖ FIX #8: Warns if capitalization < 100%\n\n‚úÖ CAPITALIZATION:\n  ‚úÖ All translations automatically capitalized by Cell 8\n  ‚úÖ Per-sentence capitalization check (‚úÖ or ‚ö†Ô∏è)\n  ‚úÖ Overall capitalization rate (target: 100%)\n  ‚úÖ Warnings if not all translations capitalized\n\n‚úÖ LORA:\n  ‚úÖ Shows LoRA status in config\n  ‚úÖ Validates LoRA state in checkpoint\n  ‚úÖ Reports trainable param count\n  ‚úÖ Warns if LoRA enabled but missing in checkpoint\n================================================================================\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ===========================================================================================\n# EMERGENCY DIAGNOSTIC: Find AssertionError Source (FIXED VARIABLE NAMES)\n# ===========================================================================================\n\nimport torch\nimport traceback\nimport gc\n\nprint(\"=\" * 80)\nprint(\"DIAGNOSTIC: Analyzing AssertionError in Optimizer\")\nprint(\"=\" * 80)\n\n# Check what's available in globals\nprint(\"\\n[STEP 0] Checking available variables...\")\navailable_vars = [v for v in dir() if not v.startswith('_')]\nmodel_vars = [v for v in available_vars if 'model' in v.lower()]\nprint(f\"  Available variables containing 'model': {model_vars}\")\n\n# Try to get model\nmodel_to_check = None\ntry:\n    if 'trained_model' in globals():\n        model_to_check = trained_model\n        print(f\"  ‚úÖ Found 'trained_model'\")\n    elif 'model' in globals():\n        model_to_check = model\n        print(f\"  ‚úÖ Found 'model'\")\n    else:\n        print(f\"  ‚ùå No model found in globals!\")\n        print(f\"  Available: {model_vars}\")\n        raise RuntimeError(\"Model not found\")\nexcept Exception as e:\n    print(f\"‚ùå Model not accessible: {e}\")\n    print(\"\\n‚ö†Ô∏è  SOLUTION: Run this INSIDE Cell 10, not in a separate cell!\")\n    print(\"   Add this diagnostic code at the END of main_pipeline() function,\")\n    print(\"   right BEFORE the 'return trained_model, tokenizer' line.\")\n    raise\n\ncore = model_to_check.module if hasattr(model_to_check, 'module') else model_to_check\n\nprint(\"\\n[STEP 1] Model found, now creating test optimizer...\")\nprint(f\"  Model type: {type(core).__name__}\")\n\n# Recreate optimizer logic from Cell 10\ntry:\n    _USE_LORA = USE_LORA if 'USE_LORA' in globals() else False\n    _LR_NMT = LR_NMT if 'LR_NMT' in globals() else 5e-4\n    _WEIGHT_DECAY = WEIGHT_DECAY if 'WEIGHT_DECAY' in globals() else 0.001\n    \n    print(f\"  USE_LORA: {_USE_LORA}\")\n    print(f\"  LR_NMT: {_LR_NMT:.2e}\")\n    \n    base_params = []\n    lora_params = []\n    \n    for name, param in core.named_parameters():\n        if not param.requires_grad:\n            continue\n        \n        if 'lora_' in name.lower() or '.lora_' in name:\n            lora_params.append(param)\n        else:\n            base_params.append(param)\n    \n    print(f\"\\n[STEP 2] Parameter extraction:\")\n    print(f\"  - Base params: {len(base_params)}\")\n    print(f\"  - LoRA params: {len(lora_params)}\")\n    \n    lora_param_count = sum(p.numel() for p in lora_params)\n    base_param_count = sum(p.numel() for p in base_params)\n    \n    print(f\"  - LoRA param count: {lora_param_count:,} ({lora_param_count/1e6:.2f}M)\")\n    print(f\"  - Base param count: {base_param_count:,} ({base_param_count/1e6:.2f}M)\")\n    \n    optimizer_groups = []\n    \n    if lora_params:\n        optimizer_groups.append({\n            'params': lora_params,\n            'lr': _LR_NMT,\n            'weight_decay': _WEIGHT_DECAY * 0.1 if _USE_LORA else _WEIGHT_DECAY,\n        })\n        print(f\"  Added LoRA param group (LR: {_LR_NMT:.2e})\")\n    \n    if base_params and not _USE_LORA:\n        optimizer_groups.append({\n            'params': base_params,\n            'lr': _LR_NMT,\n            'weight_decay': _WEIGHT_DECAY,\n        })\n        print(f\"  Added base param group (LR: {_LR_NMT:.2e})\")\n    \n    if not optimizer_groups:\n        raise RuntimeError(\"No trainable parameters found!\")\n    \n    opt = torch.optim.AdamW(\n        optimizer_groups,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n    )\n    \n    optimizer_total = sum(p.numel() for group in opt.param_groups for p in group['params'])\n    print(f\"\\n[STEP 3] Optimizer created:\")\n    print(f\"  - Total params managed: {optimizer_total:,} ({optimizer_total/1e6:.2f}M)\")\n    print(f\"  - Initial LR: {opt.param_groups[0]['lr']:.2e}\")\n    \n    # Now create scheduler\n    try:\n        from transformers import get_cosine_schedule_with_warmup\n        \n        _WARMUP_STEPS = WARMUP_STEPS if 'WARMUP_STEPS' in globals() else 600\n        _EPOCHS = EPOCHS if 'EPOCHS' in globals() else 3\n        _NUM_SAMPLES = NUM_SAMPLES if 'NUM_SAMPLES' in globals() else 200000\n        _BATCH_SIZE = BATCH_SIZE if 'BATCH_SIZE' in globals() else 32\n        \n        total_steps = int(_EPOCHS * _NUM_SAMPLES // _BATCH_SIZE)\n        \n        print(f\"\\n[STEP 4] Creating scheduler...\")\n        print(f\"  - Warmup steps: {_WARMUP_STEPS}\")\n        print(f\"  - Total steps: {total_steps}\")\n        print(f\"  - LR before scheduler: {opt.param_groups[0]['lr']:.2e}\")\n        \n        scheduler = get_cosine_schedule_with_warmup(\n            opt,\n            num_warmup_steps=_WARMUP_STEPS,\n            num_training_steps=total_steps\n        )\n        \n        print(f\"  - LR after scheduler: {opt.param_groups[0]['lr']:.2e}\")\n        \n        if opt.param_groups[0]['lr'] == 0.0:\n            print(\"\\n  ‚ùå FOUND THE BUG!\")\n            print(\"  Scheduler set LR to 0.0 (warmup starts at step 0)\")\n            print(\"  This causes AdamW to fail because:\")\n            print(\"    1. No parameter updates occur (LR=0)\")\n            print(\"    2. Optimizer state (exp_avg, exp_avg_sq) remains uninitialized\")\n            print(\"    3. On next step, AdamW tries to access uninitialized state ‚Üí AssertionError\")\n        \n    except ImportError:\n        print(\"\\n[STEP 4] transformers not available, skipping scheduler test\")\n        scheduler = None\n    \n    print(\"\\n[STEP 5] Simulating training step...\")\n    \n    test_input = torch.randint(0, 32100, (2, 10)).to(core.device)\n    test_labels = torch.randint(0, 32100, (2, 10)).to(core.device)\n    test_attn = torch.ones_like(test_input)\n    \n    core.train()\n    opt.zero_grad()\n    \n    print(\"  Running forward pass...\")\n    try:\n        if hasattr(core, 'forward_path2'):\n            loss = core.forward_path2(\n                input_ids=test_input,\n                attention_mask=test_attn,\n                labels=test_labels,\n                use_rdrop=False\n            )\n        else:\n            output = core.t5(\n                input_ids=test_input,\n                attention_mask=test_attn,\n                labels=test_labels\n            )\n            loss = output.loss\n        \n        print(f\"  ‚úÖ Forward pass succeeded, loss: {loss.item():.4f}\")\n        \n        print(\"  Running backward pass...\")\n        loss.backward()\n        \n        grad_count = sum(1 for p in core.parameters() if p.requires_grad and p.grad is not None)\n        total_trainable = sum(1 for p in core.parameters() if p.requires_grad)\n        print(f\"  ‚úÖ Backward succeeded, {grad_count}/{total_trainable} params have gradients\")\n        \n        print(\"\\n  Attempting optimizer.step()...\")\n        print(f\"  Current LR: {opt.param_groups[0]['lr']:.2e}\")\n        \n        try:\n            opt.step()\n            print(\"  ‚úÖ optimizer.step() SUCCEEDED!\")\n            print(\"\\n  üéâ NO BUG FOUND - optimizer works correctly!\")\n            \n        except AssertionError as e:\n            print(f\"  ‚ùå AssertionError caught: {str(e)}\")\n            print(\"\\n  [DEEP ANALYSIS]\")\n            print(\"  Checking optimizer state...\")\n            \n            for group_idx, group in enumerate(opt.param_groups):\n                print(f\"\\n  Group {group_idx}:\")\n                print(f\"    LR: {group['lr']:.2e}\")\n                \n                for param_idx, param in enumerate(group['params'][:3]):\n                    print(f\"\\n    Param {param_idx}:\")\n                    print(f\"      requires_grad: {param.requires_grad}\")\n                    print(f\"      grad is None: {param.grad is None}\")\n                    \n                    if param.grad is not None:\n                        print(f\"      grad norm: {param.grad.norm().item():.6f}\")\n                    \n                    param_state = opt.state.get(id(param), {})\n                    print(f\"      optimizer state keys: {list(param_state.keys())}\")\n                    \n                    if 'step' in param_state:\n                        print(f\"      step: {param_state['step']}\")\n                    if 'exp_avg' in param_state:\n                        print(f\"      exp_avg initialized: True\")\n                    else:\n                        print(f\"      exp_avg initialized: False ‚Üê BUG!\")\n            \n            print(\"\\n  ‚ùå ROOT CAUSE:\")\n            print(\"  Scheduler set LR=0 at step 0 (warmup)\")\n            print(\"  ‚Üí No params updated on first optimizer.step()\")\n            print(\"  ‚Üí Optimizer state never initialized\")\n            print(\"  ‚Üí Next step tries to access exp_avg ‚Üí AssertionError\")\n            \n            print(\"\\n  ‚úÖ SOLUTION:\")\n            print(\"  Start warmup at 1% of target LR (not 0%)\")\n            print(\"  This ensures optimizer state gets initialized on first step\")\n        \n        except Exception as e:\n            print(f\"  ‚ùå Different error: {type(e).__name__}: {str(e)}\")\n            traceback.print_exc()\n    \n    except Exception as e:\n        print(f\"‚ùå Forward/backward failed: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n\nexcept Exception as e:\n    print(f\"‚ùå Diagnostic failed: {type(e).__name__}: {str(e)}\")\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIAGNOSTIC COMPLETE\")\nprint(\"=\" * 80)\nprint(\"\\nIf you saw 'FOUND THE BUG!' above, apply the scheduler fix from Cell 7.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.405284Z","iopub.execute_input":"2026-02-16T03:06:31.405519Z","iopub.status.idle":"2026-02-16T03:06:31.429074Z","shell.execute_reply.started":"2026-02-16T03:06:31.405500Z","shell.execute_reply":"2026-02-16T03:06:31.428347Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDIAGNOSTIC: Analyzing AssertionError in Optimizer\n================================================================================\n\n[STEP 0] Checking available variables...\n  Available variables containing 'model': ['AutoModelForSeq2SeqLM', 'BaseModelOutput', 'FREEZE_BASE_MODEL', 'HF_MODEL', 'MODEL_VOCAB_SIZE', 'PeftModel', 'd_model', 'get_peft_model', 'test_model_forward_pass', 'trained_model']\n  ‚úÖ Found 'trained_model'\n\n[STEP 1] Model found, now creating test optimizer...\n  Model type: NoneType\n  USE_LORA: True\n  LR_NMT: 5.00e-04\n‚ùå Diagnostic failed: AttributeError: 'NoneType' object has no attribute 'named_parameters'\n\n================================================================================\nDIAGNOSTIC COMPLETE\n================================================================================\n\nIf you saw 'FOUND THE BUG!' above, apply the scheduler fix from Cell 7.\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_55/1661972160.py\", line 56, in <cell line: 0>\n    for name, param in core.named_parameters():\n                       ^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'named_parameters'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ===========================\n# CHECKPOINT RECOVERY DIAGNOSTIC\n# ===========================\n\nimport os\nimport torch\n\nCHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\nprint(\"=\"*80)\nprint(\"CHECKPOINT RECOVERY DIAGNOSTIC\")\nprint(\"=\"*80)\n\n# Check 1: File exists?\nif not os.path.exists(CHECKPOINT_PATH):\n    print(f\"‚ùå File not found: {CHECKPOINT_PATH}\")\nelse:\n    file_size = os.path.getsize(CHECKPOINT_PATH) / (1024**2)  # MB\n    print(f\"‚úÖ File exists: {file_size:.2f} MB\")\n\n    # Check 2: Can we load it?\n    print(\"\\nüîç Attempting to load checkpoint...\")\n    try:\n        checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n        print(\"‚úÖ Checkpoint loaded successfully!\")\n\n        # Check 3: What's inside?\n        print(\"\\nüì¶ Checkpoint contents:\")\n        for key in checkpoint.keys():\n            if isinstance(checkpoint[key], dict):\n                print(f\"   ‚úÖ {key}: {len(checkpoint[key])} items\")\n            else:\n                print(f\"   ‚úÖ {key}: {type(checkpoint[key])}\")\n\n        # Check 4: Critical components\n        print(\"\\nüîç Critical components:\")\n        has_model = 'model_state_dict' in checkpoint and len(checkpoint['model_state_dict']) > 0\n        has_dscd = 'dscd_state' in checkpoint and len(checkpoint.get('dscd_state', {})) > 0\n        has_metrics = 'eval_results' in checkpoint\n\n        print(f\"   {'‚úÖ' if has_model else '‚ùå'} Model state: {'OK' if has_model else 'MISSING'}\")\n        print(f\"   {'‚úÖ' if has_dscd else '‚ö†Ô∏è'} DSCD state: {'OK' if has_dscd else 'MISSING'}\")\n        print(f\"   {'‚úÖ' if has_metrics else '‚ö†Ô∏è'} Metrics: {'OK' if has_metrics else 'MISSING'}\")\n\n        if has_metrics:\n            print(\"\\nüìä Training Results:\")\n            eval_res = checkpoint.get('eval_results', {})\n            print(f\"   BLEU: {eval_res.get('bleu', 'N/A')}\")\n            print(f\"   chrF: {eval_res.get('chrf', 'N/A')}\")\n\n        print(\"\\n‚úÖ CHECKPOINT IS VALID! You can resume training or evaluate.\")\n        print(\"\\nüí° To load it, run Cell 11 with RESUME_FROM_CHECKPOINT=True\")\n\n    except EOFError as e:\n        print(f\"‚ùå CORRUPTED (incomplete file): {e}\")\n        print(\"\\nüí° Possible causes:\")\n        print(\"   - Google Drive sync was interrupted\")\n        print(\"   - Notebook crashed during save\")\n        print(\"   - File was partially overwritten\")\n\n    except Exception as e:\n        print(f\"‚ùå LOAD FAILED: {type(e).__name__}: {e}\")\n        print(\"\\nüí° Checkpoint is damaged beyond recovery.\")\n","metadata":{"id":"WxULMmlm5_o-","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.429944Z","iopub.execute_input":"2026-02-16T03:06:31.430170Z","iopub.status.idle":"2026-02-16T03:06:31.445779Z","shell.execute_reply.started":"2026-02-16T03:06:31.430144Z","shell.execute_reply":"2026-02-16T03:06:31.445134Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCHECKPOINT RECOVERY DIAGNOSTIC\n================================================================================\n‚ùå File not found: /kaggle/working/tatn_final.pt\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12: TRANSLATION TEST WITH AMBIGUOUS WORD DETECTION & SENSE DISAMBIGUATION\n# FIXED FOR BANGLAT5\n# ==============================================================================\nimport os\nimport time\nimport json\nimport traceback\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom collections import defaultdict\nimport torch\nimport torch.nn.functional as F\nimport gc\nimport pandas as pd\n\ntry:\n    _DEVICE = DEVICE if isinstance(DEVICE, torch.device) else torch.device(str(DEVICE)) if isinstance(DEVICE, str) else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    _TASK_PREFIX = \"translate Bengali to English: \"\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 12: TRANSLATION TEST WITH SENSE DISAMBIGUATION (BanglaT5)\")\nprint(\"=\" * 80)\nprint(f\"Device: {_DEVICE}\")\nprint(f\"Translation: {_SOURCE_LANGUAGE} ‚Üí {_TARGET_LANGUAGE}\")\nprint(f\"Task prefix: '{_TASK_PREFIX}'\")\nprint(\"=\" * 80 + \"\\n\")\n\n\n# ==============================================================================\n# STEP 1: DEFINE TEST SENTENCES WITH EXPECTED TRANSLATIONS\n# ==============================================================================\nTEST_SENTENCES = [\n    {\"id\": 1, \"input\": \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ú‡¶Æ‡¶æ ‡¶ï‡¶∞‡¶ø‡•§\", \"expected\": \"I deposit money in the bank.\", \"ambiguous_words\": [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"], \"expected_senses\": {\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\": \"financial institution\"}},\n    {\"id\": 2, \"input\": \"‡¶®‡¶¶‡ßÄ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ó‡¶æ‡¶õ ‡¶Ü‡¶õ‡ßá‡•§\", \"expected\": \"There are many trees on the river bank.\", \"ambiguous_words\": [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"], \"expected_senses\": {\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\": \"riverbank/embankment\"}},\n    {\"id\": 3, \"input\": \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶Ø‡¶æ‡¶¨‡•§\", \"expected\": \"I will go to the market tomorrow.\", \"ambiguous_words\": [\"‡¶ï‡¶æ‡¶≤\"], \"expected_senses\": {\"‡¶ï‡¶æ‡¶≤\": \"tomorrow\"}},\n    {\"id\": 4, \"input\": \"‡¶ï‡¶æ‡¶≤ ‡¶Ö‡¶®‡ßç‡¶ß‡¶ï‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶§ ‡¶õ‡¶ø‡¶≤‡•§\", \"expected\": \"It was a dark black night.\", \"ambiguous_words\": [\"‡¶ï‡¶æ‡¶≤\"], \"expected_senses\": {\"‡¶ï‡¶æ‡¶≤\": \"black/dark\"}},\n    {\"id\": 5, \"input\": \"‡¶ó‡¶æ‡¶õ‡ßá‡¶∞ ‡¶™‡¶æ‡¶§‡¶æ ‡¶∏‡¶¨‡ßÅ‡¶ú‡•§\", \"expected\": \"The leaves of the tree are green.\", \"ambiguous_words\": [\"‡¶™‡¶æ‡¶§‡¶æ\"], \"expected_senses\": {\"‡¶™‡¶æ‡¶§‡¶æ\": \"leaf\"}},\n    {\"id\": 6, \"input\": \"‡¶¨‡¶á ‡¶™‡¶æ‡¶§‡¶æ ‡¶â‡¶≤‡ßç‡¶ü‡¶æ‡¶ì‡•§\", \"expected\": \"Turn the pages of the book.\", \"ambiguous_words\": [\"‡¶™‡¶æ‡¶§‡¶æ\"], \"expected_senses\": {\"‡¶™‡¶æ‡¶§‡¶æ\": \"page\"}},\n    {\"id\": 7, \"input\": \"‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤ ‡¶ñ‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶¨‡¶≤ ‡¶≤‡¶æ‡¶•‡¶ø ‡¶Æ‡¶æ‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§\", \"expected\": \"In football, the ball is kicked.\", \"ambiguous_words\": [\"‡¶¨‡¶≤\"], \"expected_senses\": {\"‡¶¨‡¶≤\": \"ball\"}},\n    {\"id\": 8, \"input\": \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶≤ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶ø‡¶§‡¶¨‡•§\", \"expected\": \"My strength is more so I will win.\", \"ambiguous_words\": [\"‡¶¨‡¶≤\"], \"expected_senses\": {\"‡¶¨‡¶≤\": \"strength/force\"}},\n    {\"id\": 9, \"input\": \"‡¶ö‡ßã‡¶∞‡¶ï‡ßá ‡¶ß‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§\", \"expected\": \"The thief has been caught.\", \"ambiguous_words\": [\"‡¶ß‡¶∞‡¶æ\"], \"expected_senses\": {\"‡¶ß‡¶∞‡¶æ\": \"caught\"}},\n    {\"id\": 10, \"input\": \"‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá ‡¶ö‡¶æ‡¶Å‡¶¶ ‡¶ß‡¶∞‡¶æ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§\", \"expected\": \"The moon has appeared in the sky.\", \"ambiguous_words\": [\"‡¶ß‡¶∞‡¶æ\"], \"expected_senses\": {\"‡¶ß‡¶∞‡¶æ\": \"appeared\"}},\n    {\"id\": 11, \"input\": \"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"expected\": \"She speaks very sweetly.\", \"ambiguous_words\": [\"‡¶ï‡¶•‡¶æ\"], \"expected_senses\": {\"‡¶ï‡¶•‡¶æ\": \"words/speech\"}},\n    {\"id\": 12, \"input\": \"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡¶® ‡¶è‡¶¨‡¶Ç ‡¶®‡¶¶‡ßÄ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶¨‡¶∏‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡¶®‡•§\", \"expected\": \"He works at the bank and sits on the river bank.\", \"ambiguous_words\": [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"], \"expected_senses\": {\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï_1\": \"financial institution\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï_2\": \"riverbank\"}},\n    {\"id\": 13, \"input\": \"‡¶ï‡¶æ‡¶≤ ‡¶ï‡¶æ‡¶≤‡ßã ‡¶Æ‡ßá‡¶ò ‡¶õ‡¶ø‡¶≤‡•§\", \"expected\": \"Yesterday there were black clouds.\", \"ambiguous_words\": [\"‡¶ï‡¶æ‡¶≤\"], \"expected_senses\": {\"‡¶ï‡¶æ‡¶≤\": \"yesterday\"}},\n]\n\nprint(f\"[STEP 1] Loaded {len(TEST_SENTENCES)} test sentences\")\n\n\n# ==============================================================================\n# STEP 2: LOAD TRAINED MODEL AND PROTOTYPES\n# ==============================================================================\nMODEL_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\nPROTOTYPE_DIR = \"/kaggle/working/\"\n\nmodel = None\ntokenizer = None\nprototypes_data = {}\n\ntry:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"[STEP 2] Loading Trained Model...\")\n    print(\"=\" * 80)\n\n    if not os.path.exists(MODEL_CHECKPOINT_PATH):\n        available_models = [f for f in os.listdir(\"/content/model/\") if f.endswith('.pt')]\n        print(f\"‚ùå Model not found: {MODEL_CHECKPOINT_PATH}\")\n        print(f\"üìÇ Available models in /content/model/:\")\n        for model_file in available_models:\n            print(f\"   - {model_file}\")\n        raise FileNotFoundError(f\"Model checkpoint not found: {MODEL_CHECKPOINT_PATH}\\nAvailable models: {available_models}\")\n\n    print(f\"üìÇ Loading from: {MODEL_CHECKPOINT_PATH}\")\n    checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location=_DEVICE, weights_only=False)\n    print(f\"‚úÖ Checkpoint loaded successfully\")\n    print(f\"   Keys in checkpoint: {list(checkpoint.keys())}\")\n\n    if \"tokenizer\" in checkpoint and checkpoint[\"tokenizer\"] is not None:\n        tokenizer = checkpoint[\"tokenizer\"]\n        print(\"‚úÖ Tokenizer loaded from checkpoint\")\n    else:\n        print(\"‚ö†Ô∏è  Tokenizer not in checkpoint, loading from HuggingFace...\")\n        from transformers import AutoTokenizer\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n            print(\"‚úÖ Loaded BanglaT5 tokenizer from HuggingFace\")\n        except Exception as e:\n            print(f\"‚ùå Failed to load BanglaT5 tokenizer: {e}\")\n            raise\n\n    if \"model_state_dict\" in checkpoint:\n        model_state = checkpoint[\"model_state_dict\"]\n    elif \"model\" in checkpoint:\n        model_state = checkpoint[\"model\"]\n    else:\n        raise ValueError(f\"No model state found in checkpoint. Keys: {list(checkpoint.keys())}\")\n\n    try:\n        TATNModelClass = globals().get(\"MemoryOptimizedTATNWithExplanations\")\n        if TATNModelClass is None:\n            raise RuntimeError(\"TATN model class not found. Run Cell 6 first.\")\n\n        print(f\"üîß Initializing model...\")\n        model = TATNModelClass(tokenizer)\n\n        print(f\"üîß Loading model state...\")\n        load_res = model.load_state_dict(model_state, strict=False)\n        \n        try:\n            missing_keys = load_res.missing_keys if hasattr(load_res, 'missing_keys') else []\n            unexpected_keys = load_res.unexpected_keys if hasattr(load_res, 'unexpected_keys') else []\n        except Exception:\n            missing_keys, unexpected_keys = [], []\n\n        if missing_keys:\n            print(f\"‚ö†Ô∏è  Missing keys: {len(missing_keys)}\")\n            if len(missing_keys) <= 10:\n                for key in missing_keys:\n                    print(f\"   - {key}\")\n\n        if unexpected_keys:\n            print(f\"‚ö†Ô∏è  Unexpected keys: {len(unexpected_keys)}\")\n            if len(unexpected_keys) <= 10:\n                for key in unexpected_keys[:10]:\n                    print(f\"   - {key}\")\n\n        model.to(_DEVICE)\n        model.eval()\n        print(f\"‚úÖ Model loaded successfully\")\n        print(f\"   Device: {next(model.parameters()).device}\")\n        print(f\"   Global step: {checkpoint.get('global_steps', checkpoint.get('step', 'unknown'))}\")\n        print(f\"   Epoch: {checkpoint.get('epochs_trained', checkpoint.get('epoch', 'unknown'))}\")\n\n    except Exception as e:\n        print(f\"‚ùå Failed to load model: {e}\")\n        traceback.print_exc()\n        raise\n\n    try:\n        print(f\"\\n[STEP 2.1] Loading DSCD Prototypes...\")\n\n        if \"dscd_state\" in checkpoint:\n            dscd_state = checkpoint[\"dscd_state\"]\n            print(f\"   ‚úì Found 'dscd_state' key\")\n\n            if isinstance(dscd_state, dict) and \"prototype_stores_data\" in dscd_state:\n                prototype_stores_raw = dscd_state[\"prototype_stores_data\"]\n                print(f\"   ‚úì Found 'prototype_stores_data' key!\")\n                print(f\"   Total tokens: {len(prototype_stores_raw) if isinstance(prototype_stores_raw, dict) else 'N/A'}\")\n\n                if isinstance(prototype_stores_raw, dict) and len(prototype_stores_raw) > 0:\n                    valid_prototypes = {}\n                    single_sense_count = 0\n                    for token, proto_list in prototype_stores_raw.items():\n                        if isinstance(proto_list, list):\n                            if len(proto_list) >= 2:\n                                valid_prototypes[token] = proto_list\n                            elif len(proto_list) == 1:\n                                single_sense_count += 1\n\n                    if len(valid_prototypes) == 0:\n                        valid_prototypes = prototype_stores_raw.copy()\n\n                    if hasattr(model, 'dscd'):\n                        try:\n                            model.dscd._prototype_stores = valid_prototypes\n                            print(f\"   ‚úÖ Injected {len(valid_prototypes)} prototypes into model.dscd\")\n                        except Exception:\n                            pass\n\n                    for token, proto_list in list(valid_prototypes.items())[:2000]:\n                        clean_token = token.replace('‚ñÅ', '').strip()\n                        num_senses = len(proto_list) if isinstance(proto_list, list) else 0\n                        prototypes_data[clean_token] = {\n                            \"token\": token,\n                            \"num_senses\": num_senses,\n                            \"prototypes\": proto_list\n                        }\n\n            elif isinstance(dscd_state, dict) and \"_prototype_stores\" in dscd_state:\n                prototype_stores = dscd_state[\"_prototype_stores\"]\n                if isinstance(prototype_stores, dict) and len(prototype_stores) > 0:\n                    if hasattr(model, 'dscd'):\n                        try:\n                            model.dscd._prototype_stores = prototype_stores\n                        except Exception:\n                            pass\n                    for token, proto_list in list(prototype_stores.items())[:2000]:\n                        clean_token = token.replace('‚ñÅ', '').strip()\n                        prototypes_data[clean_token] = {\n                            \"token\": token,\n                            \"num_senses\": len(proto_list) if isinstance(proto_list, list) else 0,\n                            \"prototypes\": proto_list\n                        }\n\n        if not prototypes_data and hasattr(model, 'dscd') and hasattr(model.dscd, '_prototype_stores'):\n            try:\n                prototype_stores = model.dscd._prototype_stores\n                if isinstance(prototype_stores, dict) and len(prototype_stores) > 0:\n                    for token, proto_list in list(prototype_stores.items())[:2000]:\n                        clean_token = token.replace('‚ñÅ', '').strip()\n                        prototypes_data[clean_token] = {\n                            \"token\": token,\n                            \"num_senses\": len(proto_list) if isinstance(proto_list, list) else 0,\n                            \"prototypes\": proto_list\n                        }\n                    print(f\"   ‚úÖ Found prototypes in model.dscd._prototype_stores: {len(prototype_stores)}\")\n            except Exception:\n                pass\n\n        print(f\"\\n{'='*80}\")\n        if not prototypes_data:\n            print(f\"‚ùå CRITICAL: No prototypes found!\")\n            print(f\"\\n‚ö†Ô∏è  Cell 12 will run WITHOUT prototypes\")\n            print(f\"   Translations will work, but:\")\n            print(f\"   - No homograph detection\")\n            print(f\"   - No sense disambiguation\")\n        else:\n            print(f\"‚úÖ PROTOTYPE LOADING COMPLETE!\")\n            print(f\"   Total prototypes loaded: {len(prototypes_data)}\")\n            avg_senses = sum(p[\"num_senses\"] for p in prototypes_data.values()) / len(prototypes_data)\n            print(f\"   Average prototypes per token: {avg_senses:.1f}\")\n            multi_sense = sum(1 for p in prototypes_data.values() if p[\"num_senses\"] >= 2)\n            single_sense = sum(1 for p in prototypes_data.values() if p[\"num_senses\"] == 1)\n            print(f\"   Multi-sense tokens (‚â•2): {multi_sense}\")\n            print(f\"   Single-sense tokens (=1): {single_sense}\")\n        print(f\"{'='*80}\\n\")\n\n    except Exception as e:\n        print(f\"\\n‚ùå FAILED TO LOAD PROTOTYPES: {e}\")\n        traceback.print_exc()\n\nexcept Exception as e:\n    print(f\"\\n‚ùå FAILED TO LOAD MODEL: {e}\")\n    traceback.print_exc()\n    print(\"\\nPlease ensure:\")\n    print(\"  1. Cell 0-11 have been run\")\n    print(\"  2. Training completed successfully\")\n    print(\"  3. Model checkpoint exists at:\", MODEL_CHECKPOINT_PATH)\n    raise\n\n\n# ==============================================================================\n# STEP 3: HELPER FUNCTIONS\n# ==============================================================================\ndef compute_similarity(text1: str, text2: str) -> float:\n    \"\"\"Compute word-level Jaccard similarity between two texts\"\"\"\n    words1 = set(str(text1).lower().split())\n    words2 = set(str(text2).lower().split())\n\n    if not words1 and not words2:\n        return 100.0\n    if not words1 or not words2:\n        return 0.0\n\n    intersection = len(words1 & words2)\n    union = len(words1 | words2)\n\n    return (intersection / union) * 100.0\n\n\ndef find_sense_from_prototypes(word: str, embedding: torch.Tensor, prototypes_data: Dict) -> Optional[Dict]:\n    \"\"\"Find which sense the word belongs to based on prototype similarity\"\"\"\n    if word not in prototypes_data:\n        return None\n\n    proto_info = prototypes_data[word]\n    proto_list = proto_info.get(\"prototypes\", [])\n\n    if not proto_list or not isinstance(proto_list, list):\n        return None\n\n    best_sense_idx = -1\n    best_similarity = -1.0\n    similarities = []\n\n    try:\n        emb_device = embedding.device if hasattr(embedding, \"device\") else _DEVICE\n        embedding_norm = F.normalize(embedding.flatten().unsqueeze(0).to(emb_device), dim=1)\n\n        for sense_idx, proto in enumerate(proto_list):\n            if isinstance(proto, dict) and \"centroid\" in proto:\n                centroid = proto[\"centroid\"]\n            elif isinstance(proto, torch.Tensor):\n                centroid = proto\n            else:\n                continue\n\n            try:\n                centroid = centroid.to(embedding_norm.device)\n            except Exception:\n                pass\n\n            centroid_norm = F.normalize(centroid.flatten().unsqueeze(0), dim=1)\n            similarity = F.cosine_similarity(embedding_norm, centroid_norm).item()\n            similarities.append(similarity)\n\n            if similarity > best_similarity:\n                best_similarity = similarity\n                best_sense_idx = sense_idx\n\n        if best_sense_idx >= 0:\n            return {\n                \"sense_index\": best_sense_idx,\n                \"similarity\": best_similarity,\n                \"num_senses\": len(proto_list),\n                \"all_similarities\": similarities\n            }\n\n    except Exception:\n        pass\n\n    return None\n\n\ndef translate_with_analysis(\n    sentence: str,\n    model,\n    tokenizer,\n    prototypes_data: Dict,\n    max_length: int = 64\n) -> Dict[str, Any]:\n    \"\"\"Translate sentence and analyze ambiguous words\"\"\"\n    result = {\n        \"input\": sentence,\n        \"translation\": \"\",\n        \"ambiguous_detections\": [],\n        \"sense_disambiguations\": [],\n        \"explanations\": [],\n        \"error\": None\n    }\n\n    try:\n        input_text = _TASK_PREFIX + sentence\n        \n        inputs = tokenizer(\n            input_text,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length\n        )\n\n        input_ids = inputs[\"input_ids\"].to(_DEVICE)\n        attention_mask = inputs[\"attention_mask\"].to(_DEVICE)\n\n        with torch.no_grad():\n            forward_outputs = model.forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                src_texts=[sentence],\n                labels=None,\n                use_dscd=True,\n                use_asbn=False,\n                return_dict=True\n            )\n\n            encoder_outputs = None\n            dscd_outputs = {}\n            explanations = []\n\n            if isinstance(forward_outputs, dict):\n                encoder_outputs = forward_outputs.get(\"sense_augmented_embeddings\") or forward_outputs.get(\"encoder_last_hidden_state\")\n                dscd_outputs = forward_outputs.get(\"dscd_outputs\", {}) or {}\n                explanations = forward_outputs.get(\"explanations\", [[]])[0] if forward_outputs.get(\"explanations\") is not None else []\n                result[\"explanations\"] = explanations\n\n            tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().tolist())\n            uncertainties = dscd_outputs.get(\"uncertainties\", [[]])[0] if dscd_outputs else []\n            span_preds = dscd_outputs.get(\"span_preds\", [[]])[0] if dscd_outputs else []\n            h_augmented = dscd_outputs.get(\"h_augmented\") if dscd_outputs else None\n\n            for idx, token in enumerate(tokens):\n                clean_token = token.replace('‚ñÅ', '').replace('##', '').strip()\n                if len(clean_token) < 1:\n                    continue\n\n                try:\n                    unc_val = uncertainties[idx] if idx < len(uncertainties) else 0.0\n                    if isinstance(unc_val, torch.Tensor):\n                        uncertainty = float(unc_val.item())\n                    else:\n                        uncertainty = float(unc_val)\n                except Exception:\n                    uncertainty = 0.0\n\n                try:\n                    span_val = span_preds[idx] if idx < len(span_preds) else 0.0\n                    if isinstance(span_val, torch.Tensor):\n                        span = float(span_val.item())\n                    else:\n                        span = float(span_val)\n                except Exception:\n                    span = 0.0\n\n                is_in_prototypes = clean_token in prototypes_data\n\n                if uncertainty > 0.10 or span > 0.10 or is_in_prototypes:\n                    detection = {\n                        \"word\": clean_token,\n                        \"token\": token,\n                        \"position\": idx,\n                        \"uncertainty\": uncertainty,\n                        \"span\": span,\n                        \"is_homograph\": is_in_prototypes\n                    }\n\n                    if is_in_prototypes and h_augmented is not None:\n                        try:\n                            embedding = h_augmented[0, idx, :].detach()\n                            sense_info = find_sense_from_prototypes(clean_token, embedding, prototypes_data)\n                            if sense_info:\n                                detection[\"sense_info\"] = sense_info\n                                result[\"sense_disambiguations\"].append({\n                                    \"word\": clean_token,\n                                    \"selected_sense\": sense_info[\"sense_index\"],\n                                    \"confidence\": sense_info[\"similarity\"],\n                                    \"num_senses\": sense_info[\"num_senses\"],\n                                    \"reason\": f\"Matched sense {sense_info['sense_index']+1}/{sense_info['num_senses']} with {sense_info['similarity']:.1%} confidence\"\n                                })\n                        except Exception:\n                            pass\n\n                    result[\"ambiguous_detections\"].append(detection)\n\n        try:\n            generated = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                min_length=5,\n                num_beams=4,\n                early_stopping=True,\n                no_repeat_ngram_size=3,\n                length_penalty=0.8,\n                repetition_penalty=1.2,\n                use_dscd=True,\n                use_asbn=False,\n            )\n            \n            translation = tokenizer.decode(generated[0], skip_special_tokens=True)\n            \n            if translation and len(translation) > 0:\n                translation = translation[0].upper() + translation[1:]\n            \n            result[\"translation\"] = translation\n            \n        except Exception as e:\n            result[\"error\"] = f\"Generation failed: {type(e).__name__}: {e}\"\n            traceback.print_exc()\n            return result\n\n    except Exception as e:\n        result[\"error\"] = str(e)\n        result[\"translation\"] = \"[ERROR]\"\n        traceback.print_exc()\n\n    return result\n\n\n# ==============================================================================\n# STEP 4: RUN TRANSLATION TESTS\n# ==============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"[STEP 4] Running Translation Tests...\")\nprint(\"=\" * 80 + \"\\n\")\n\nall_results = []\n\nfor test_case in TEST_SENTENCES:\n    print(f\"\\n{'='*60}\")\n    print(f\"TEST {test_case['id']}/{len(TEST_SENTENCES)}\")\n    print(f\"{'='*60}\")\n\n    print(f\"\\nüìù INPUT ({_SOURCE_LANGUAGE}):\")\n    print(f\"   {test_case['input']}\")\n\n    print(f\"\\nüéØ EXPECTED ({_TARGET_LANGUAGE}):\")\n    print(f\"   {test_case['expected']}\")\n\n    result = translate_with_analysis(\n        test_case['input'],\n        model,\n        tokenizer,\n        prototypes_data,\n        max_length=64\n    )\n\n    if result[\"error\"]:\n        print(f\"\\n‚ùå ERROR: {result['error']}\")\n        similarity = 0.0\n    else:\n        print(f\"\\nü§ñ TRANSLATION ({_TARGET_LANGUAGE}):\")\n        print(f\"   {result['translation']}\")\n\n        similarity = compute_similarity(result[\"translation\"], test_case[\"expected\"])\n        print(f\"\\nüìä SIMILARITY: {similarity:.1f}%\")\n\n        if similarity >= 70:\n            print(f\"   ‚úÖ EXCELLENT\")\n        elif similarity >= 50:\n            print(f\"   ‚úì GOOD\")\n        elif similarity >= 30:\n            print(f\"   ~ ACCEPTABLE\")\n        else:\n            print(f\"   ‚ùå NEEDS IMPROVEMENT\")\n\n    num_ambiguous = len(result[\"ambiguous_detections\"])\n    print(f\"\\nüîç AMBIGUOUS WORDS DETECTED: {num_ambiguous}\")\n\n    if num_ambiguous > 0:\n        for detection in result[\"ambiguous_detections\"]:\n            word = detection[\"word\"]\n            uncertainty = detection[\"uncertainty\"]\n            span = detection[\"span\"]\n            is_homograph = detection[\"is_homograph\"]\n\n            marker = \"üü¢\" if is_homograph else \"üü°\"\n            status = \"HOMOGRAPH\" if is_homograph else \"uncertain\"\n\n            print(f\"\\n   {marker} '{word}' ({status})\")\n            print(f\"      Uncertainty: {uncertainty:.3f}\")\n            print(f\"      Span: {span:.3f}\")\n\n            if \"sense_info\" in detection:\n                sense_info = detection[\"sense_info\"]\n                print(f\"      ‚úì SENSE DETECTED: {sense_info['sense_index']+1}/{sense_info['num_senses']}\")\n                print(f\"      ‚úì CONFIDENCE: {sense_info['similarity']:.1%}\")\n\n                if len(sense_info.get('all_similarities', [])) > 1:\n                    print(f\"      All similarities: {[f'{s:.2f}' for s in sense_info['all_similarities']]}\")\n\n    if len(result[\"sense_disambiguations\"]) > 0:\n        print(f\"\\nüí° SENSE DISAMBIGUATION:\")\n        for disamb in result[\"sense_disambiguations\"]:\n            print(f\"   ‚úì '{disamb['word']}': {disamb['reason']}\")\n\n    if len(result[\"explanations\"]) > 0:\n        print(f\"\\nüìñ EXPLANATIONS ({len(result['explanations'])}):\")\n        for i, exp in enumerate(result[\"explanations\"][:3], 1):\n            if isinstance(exp, dict):\n                word = exp.get('token', 'unknown')\n                explanation = exp.get('explanation', 'N/A')\n                print(f\"   {i}. {word}: {explanation}\")\n\n    result[\"test_id\"] = test_case[\"id\"]\n    result[\"expected\"] = test_case[\"expected\"]\n    result[\"similarity\"] = similarity\n    all_results.append(result)\n\n    print(f\"\\n{'='*60}\\n\")\n\n\n# ==============================================================================\n# STEP 5: SUMMARY REPORT\n# ==============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"[STEP 5] SUMMARY REPORT\")\nprint(\"=\" * 80)\n\ntotal_tests = len(all_results)\nsuccessful_tests = sum(1 for r in all_results if r[\"error\"] is None)\navg_similarity = sum(r[\"similarity\"] for r in all_results) / total_tests if total_tests > 0 else 0.0\ntotal_ambiguous = sum(len(r[\"ambiguous_detections\"]) for r in all_results)\ntotal_disambiguations = sum(len(r[\"sense_disambiguations\"]) for r in all_results)\ntotal_explanations = sum(len(r[\"explanations\"]) for r in all_results)\n\nprint(f\"\\nüìä TRANSLATION QUALITY:\")\nprint(f\"   Total tests: {total_tests}\")\nprint(f\"   Successful: {successful_tests} ({successful_tests/total_tests*100:.1f}%)\")\nprint(f\"   Average similarity: {avg_similarity:.1f}%\")\n\nprint(f\"\\nüîç AMBIGUITY DETECTION:\")\nprint(f\"   Total ambiguous words detected: {total_ambiguous}\")\nprint(f\"   Average per sentence: {total_ambiguous/total_tests:.1f}\")\n\nprint(f\"\\nüí° SENSE DISAMBIGUATION:\")\nprint(f\"   Total disambiguations: {total_disambiguations}\")\nprint(f\"   Coverage: {total_disambiguations/total_ambiguous*100:.1f}%\" if total_ambiguous > 0 else \"   Coverage: N/A\")\n\nprint(f\"\\nüìñ EXPLANATIONS:\")\nprint(f\"   Total explanations: {total_explanations}\")\nprint(f\"   Average per sentence: {total_explanations/total_tests:.1f}\")\n\nhomograph_coverage = {}\nfor test_case in TEST_SENTENCES:\n    for ambig_word in test_case.get(\"ambiguous_words\", []):\n        if ambig_word not in homograph_coverage:\n            homograph_coverage[ambig_word] = {\"expected\": 0, \"detected\": 0}\n        homograph_coverage[ambig_word][\"expected\"] += 1\n\nfor result in all_results:\n    for detection in result[\"ambiguous_detections\"]:\n        word = detection[\"word\"]\n        if word in homograph_coverage:\n            homograph_coverage[word][\"detected\"] += 1\n\nprint(f\"\\nüéØ HOMOGRAPH DETECTION ACCURACY:\")\nfor word, stats in homograph_coverage.items():\n    detection_rate = stats[\"detected\"] / stats[\"expected\"] * 100 if stats[\"expected\"] > 0 else 0\n    marker = \"‚úÖ\" if detection_rate >= 80 else \"‚ö†Ô∏è\" if detection_rate >= 50 else \"‚ùå\"\n    print(f\"   {marker} {word}: {stats['detected']}/{stats['expected']} ({detection_rate:.0f}%)\")\n\nprint(f\"\\nüî¨ PROTOTYPE STATISTICS:\")\nif prototypes_data:\n    print(f\"   Total prototypes loaded: {len(prototypes_data)}\")\n    avg_senses = sum(p[\"num_senses\"] for p in prototypes_data.values()) / len(prototypes_data)\n    print(f\"   Average prototypes per token: {avg_senses:.1f}\")\n    multi_sense = sum(1 for p in prototypes_data.values() if p[\"num_senses\"] >= 2)\n    single_sense = sum(1 for p in prototypes_data.values() if p[\"num_senses\"] == 1)\n    print(f\"   Multi-sense tokens (‚â•2): {multi_sense}\")\n    print(f\"   Single-sense tokens (=1): {single_sense}\")\n\n    if multi_sense > 0:\n        print(f\"\\n   Sample multi-sense prototypes:\")\n        count = 0\n        for word, info in prototypes_data.items():\n            if info[\"num_senses\"] >= 2:\n                print(f\"      {word}: {info['num_senses']} senses\")\n                count += 1\n                if count >= 5:\n                    break\nelse:\n    print(f\"   ‚ö†Ô∏è  No prototypes loaded\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 12: TRANSLATION TEST COMPLETE\")\nprint(\"=\" * 80)\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n‚úÖ Execution completed successfully\")","metadata":{"id":"oDQGVh5w_oAx","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.446991Z","iopub.execute_input":"2026-02-16T03:06:31.447297Z","iopub.status.idle":"2026-02-16T03:06:31.509311Z","shell.execute_reply.started":"2026-02-16T03:06:31.447279Z","shell.execute_reply":"2026-02-16T03:06:31.508313Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCELL 12: TRANSLATION TEST WITH SENSE DISAMBIGUATION (BanglaT5)\n================================================================================\nDevice: cuda:0\nTranslation: bn ‚Üí en\nTask prefix: 'translate Bengali to English: '\n================================================================================\n\n[STEP 1] Loaded 13 test sentences\n\n================================================================================\n[STEP 2] Loading Trained Model...\n================================================================================\n\n‚ùå FAILED TO LOAD MODEL: [Errno 2] No such file or directory: '/content/model/'\n\nPlease ensure:\n  1. Cell 0-11 have been run\n  2. Training completed successfully\n  3. Model checkpoint exists at: /kaggle/working/tatn_final.pt\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_55/1691752395.py\", line 81, in <cell line: 0>\n    available_models = [f for f in os.listdir(\"/content/model/\") if f.endswith('.pt')]\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/content/model/'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1691752395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CHECKPOINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mavailable_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/model/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå Model not found: {MODEL_CHECKPOINT_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Available models in /content/model/:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/model/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/model/'","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# ==============================================================================\n# KAGGLE: DOWNLOAD MODEL FROM GOOGLE DRIVE\n# ==============================================================================\n\nimport os\nimport sys\n\nprint(\"=\"*80)\nprint(\"DOWNLOADING MODEL FROM GOOGLE DRIVE\")\nprint(\"=\"*80)\n\n# Install gdown for Google Drive downloads\ntry:\n    import gdown\n    print(\"‚úÖ gdown already installed\")\nexcept ImportError:\n    print(\"üì• Installing gdown...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gdown\"])\n    import gdown\n    print(\"‚úÖ gdown installed successfully\")\n\n# Google Drive file ID (extracted from your link)\nDRIVE_FILE_ID = \"1ydItwFyr5dnQH0vnPB63ycU2fe_sw0bR\"\nDRIVE_URL = f\"https://drive.google.com/uc?id={DRIVE_FILE_ID}\"\n\n# Download destination (Kaggle working directory)\nMODEL_DOWNLOAD_PATH = \"/kaggle/working/tatn_final.pt\"\n\n# Check if already downloaded\nif os.path.exists(MODEL_DOWNLOAD_PATH):\n    file_size = os.path.getsize(MODEL_DOWNLOAD_PATH) / (1024**2)\n    print(f\"‚úÖ Model already exists: {file_size:.2f} MB\")\n    print(f\"   Path: {MODEL_DOWNLOAD_PATH}\")\nelse:\n    print(f\"\\nüì• Downloading model from Google Drive...\")\n    print(f\"   File ID: {DRIVE_FILE_ID}\")\n    print(f\"   Destination: {MODEL_DOWNLOAD_PATH}\")\n    print(f\"\\n‚è≥ This may take 5-10 minutes for large files...\")\n    \n    try:\n        # Download file using gdown\n        gdown.download(DRIVE_URL, MODEL_DOWNLOAD_PATH, quiet=False)\n        \n        # Verify download\n        if os.path.exists(MODEL_DOWNLOAD_PATH):\n            file_size = os.path.getsize(MODEL_DOWNLOAD_PATH) / (1024**2)\n            print(f\"\\n‚úÖ DOWNLOAD COMPLETE!\")\n            print(f\"   Size: {file_size:.2f} MB\")\n            print(f\"   Path: {MODEL_DOWNLOAD_PATH}\")\n            \n            # Verify it's a valid PyTorch checkpoint\n            import torch\n            try:\n                print(f\"\\nüîç Verifying checkpoint integrity...\")\n                checkpoint = torch.load(MODEL_DOWNLOAD_PATH, map_location='cpu', weights_only=False)\n                \n                if 'model_state_dict' in checkpoint or 'model' in checkpoint:\n                    print(f\"‚úÖ Checkpoint is valid and loadable\")\n                    \n                    # Show checkpoint contents\n                    print(f\"\\nüì¶ Checkpoint contents:\")\n                    for key in checkpoint.keys():\n                        if isinstance(checkpoint[key], dict):\n                            print(f\"   - {key}: {len(checkpoint[key])} items\")\n                        else:\n                            print(f\"   - {key}: {type(checkpoint[key]).__name__}\")\n                else:\n                    print(f\"‚ö†Ô∏è Warning: Checkpoint missing model weights!\")\n                \n                del checkpoint\n                \n            except Exception as e:\n                print(f\"‚ö†Ô∏è Checkpoint verification failed: {e}\")\n                print(f\"   Will attempt to use anyway...\")\n                \n        else:\n            raise FileNotFoundError(\"Download completed but file not found!\")\n            \n    except Exception as e:\n        print(f\"\\n‚ùå DOWNLOAD FAILED: {e}\")\n        print(f\"\\nüí° Troubleshooting:\")\n        print(f\"   1. Check if file is publicly accessible\")\n        print(f\"   2. Try downloading manually and uploading to Kaggle dataset\")\n        print(f\"   3. Verify the Google Drive link is correct\")\n        raise\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL READY FOR EVALUATION\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.509834Z","iopub.status.idle":"2026-02-16T03:06:31.510098Z","shell.execute_reply.started":"2026-02-16T03:06:31.509984Z","shell.execute_reply":"2026-02-16T03:06:31.509997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 4: LOAD TRAINED TATN MODEL (MBART-50) - KAGGLE VERSION\n# ==============================================================================\n\n# ‚úÖ UPDATED PATH: Use downloaded model from /kaggle/working/\nMODEL_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\nprint(f\"\\n[SECTION 4] Loading Trained TATN Model (mBART-50)...\")\nprint(\"-\" * 80)\nprint(f\"Path: {MODEL_CHECKPOINT_PATH}\")\n\nif not os.path.exists(MODEL_CHECKPOINT_PATH):\n    raise FileNotFoundError(\n        f\"Model checkpoint not found: {MODEL_CHECKPOINT_PATH}\\n\"\n        f\"Did you run the download cell first?\"\n    )\n\ntry:\n    # Load checkpoint to CPU first to avoid OOM\n    print(f\"üìÇ Loading checkpoint to CPU...\")\n    checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n    print(f\"‚úÖ Checkpoint loaded to CPU\")\n\n    # CRITICAL FIX: Use MBart50Tokenizer for mBART-50 model\n    if \"tokenizer\" in checkpoint:\n        tokenizer = checkpoint[\"tokenizer\"]\n        print(f\"‚úÖ Tokenizer loaded from checkpoint\")\n    else:\n        from transformers import MBart50Tokenizer\n        tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\")\n        print(f\"‚úÖ MBart50Tokenizer loaded from pretrained\")\n\n    # Set language codes (correct for mBART-50)\n    tokenizer.src_lang = _SOURCE_LANGUAGE  # \"bn_IN\"\n    tokenizer.tgt_lang = _TARGET_LANGUAGE  # \"en_XX\"\n    print(f\"‚úÖ Language codes set: {_SOURCE_LANGUAGE} ‚Üí {_TARGET_LANGUAGE}\")\n\n    TATNModelClass = globals().get(\"MemoryOptimizedTATNWithExplanations\") or globals().get(\"TATNModelWithDSCDAndASBN\")\n    if TATNModelClass is None:\n        raise RuntimeError(\"TATN model class not found. Run Cell 6 first.\")\n\n    print(f\"üîß Initializing TATN model...\")\n    tatn_model = TATNModelClass(tokenizer)\n\n    if \"model\" in checkpoint:\n        model_state = checkpoint[\"model\"]\n    elif \"model_state_dict\" in checkpoint:\n        model_state = checkpoint[\"model_state_dict\"]\n    else:\n        raise ValueError(\"No model state found in checkpoint\")\n\n    print(f\"üîß Loading model weights (strict=False)...\")\n    tatn_model.load_state_dict(model_state, strict=False)\n\n    # Free checkpoint memory before moving to GPU\n    try:\n        del model_state\n    except Exception:\n        pass\n    if 'dscd_state' in checkpoint:\n        try:\n            del checkpoint['dscd_state']\n        except Exception:\n            pass\n    try:\n        del checkpoint\n    except Exception:\n        pass\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(f\"üîß Moving model to {_DEVICE}...\")\n    tatn_model.to(_DEVICE)\n    tatn_model.eval()\n\n    print(f\"‚úÖ TATN model loaded successfully\")\n    try:\n        print(f\"   Device: {next(tatn_model.parameters()).device}\")\n    except Exception:\n        pass\n\n    if torch.cuda.is_available():\n        try:\n            allocated = torch.cuda.memory_allocated(0) / 1024**3\n            print(f\"   GPU memory: {allocated:.2f} GB\")\n        except Exception:\n            pass\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load TATN model: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.511419Z","iopub.status.idle":"2026-02-16T03:06:31.511836Z","shell.execute_reply.started":"2026-02-16T03:06:31.511642Z","shell.execute_reply":"2026-02-16T03:06:31.511666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 3: LOAD NTREX DATASET (TEXT FILES) - KAGGLE VERSION\n# ==============================================================================\n\n# ‚úÖ KAGGLE PATHS: Choose one option based on where your dataset is\n\n# Option A: If dataset is uploaded as Kaggle dataset (RECOMMENDED)\n# BENGALI_FILE = \"/kaggle/input/ntrex-dataset/ntrex_ref_ben.txt\"\n# ENGLISH_FILE = \"/kaggle/input/ntrex-dataset/ntrex_src_eng.txt\"\n\n# Option B: If dataset is in your working directory\nBENGALI_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_ref_ben.txt\"\nENGLISH_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_src_eng.txt\"\n\n# Option C: If you need to download from Drive (add download cell before this)\n# BENGALI_FILE = \"/kaggle/working/ntrex_ref_ben.txt\"\n# ENGLISH_FILE = \"/kaggle/working/ntrex_src_eng.txt\"\n\nprint(f\"\\n[SECTION 3] Loading NTREX Dataset...\")\nprint(\"-\" * 80)\nprint(f\"Bengali file: {BENGALI_FILE}\")\nprint(f\"English file: {ENGLISH_FILE}\")\n\n# Check files exist\nif not os.path.exists(BENGALI_FILE):\n    raise FileNotFoundError(\n        f\"Bengali file not found: {BENGALI_FILE}\\n\"\n        f\"üí° Make sure to:\\n\"\n        f\"   1. Upload dataset as Kaggle dataset, OR\\n\"\n        f\"   2. Download files to /kaggle/working/ first\"\n    )\nif not os.path.exists(ENGLISH_FILE):\n    raise FileNotFoundError(\n        f\"English file not found: {ENGLISH_FILE}\\n\"\n        f\"üí° Make sure to:\\n\"\n        f\"   1. Upload dataset as Kaggle dataset, OR\\n\"\n        f\"   2. Download files to /kaggle/working/ first\"\n    )\n\ntry:\n    # Load Bengali sentences (these become SOURCE for bn‚Üíen)\n    with open(BENGALI_FILE, 'r', encoding='utf-8') as f:\n        bengali_sentences = [line.strip() for line in f if line.strip()]\n\n    # Load English sentences (these become REFERENCE for bn‚Üíen)\n    with open(ENGLISH_FILE, 'r', encoding='utf-8') as f:\n        english_sentences = [line.strip() for line in f if line.strip()]\n\n    print(f\"‚úÖ Loaded {len(bengali_sentences)} Bengali sentences\")\n    print(f\"‚úÖ Loaded {len(english_sentences)} English sentences\")\n\n    # Verify same length\n    if len(bengali_sentences) != len(english_sentences):\n        raise ValueError(\n            f\"Mismatch: {len(bengali_sentences)} Bengali vs {len(english_sentences)} English\"\n        )\n\n    # ASSIGN CORRECTLY FOR bn‚Üíen TRANSLATION\n    sources = bengali_sentences      # Bengali is SOURCE\n    references = english_sentences   # English is REFERENCE (target)\n\n    print(f\"\\n‚úÖ Dataset prepared for bn‚Üíen translation:\")\n    print(f\"   Source (Bengali): {len(sources)} sentences\")\n    print(f\"   Reference (English): {len(references)} sentences\")\n\n    print(f\"\\n   Sample 1:\")\n    print(f\"      SRC (bn): {sources[0][:80]}...\")\n    print(f\"      REF (en): {references[0][:80]}...\")\n\n    if len(sources) > 1:\n        print(f\"\\n   Sample 2:\")\n        print(f\"      SRC (bn): {sources[1][:80]}...\")\n        print(f\"      REF (en): {references[1][:80]}...\")\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load dataset: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.513168Z","iopub.status.idle":"2026-02-16T03:06:31.513510Z","shell.execute_reply.started":"2026-02-16T03:06:31.513340Z","shell.execute_reply":"2026-02-16T03:06:31.513358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 12: SAVE RESULTS - KAGGLE VERSION\n# ==============================================================================\nprint(f\"\\n[SECTION 12] Saving Results...\")\nprint(\"-\" * 80)\n\n# ‚úÖ KAGGLE: Save to working directory\nresults_dir = \"/kaggle/working/\"\nos.makedirs(results_dir, exist_ok=True)\n\n# Save summary with COMET\nsummary_file = os.path.join(results_dir, \"ntrex_evaluation_summary.csv\")\ntry:\n    summary_data = {\n        \"Model\": [\"TATN\"],\n        \"BLEU\": [tatn_bleu_score],\n        \"ChrF++\": [tatn_chrf_score],\n        \"COMET\": [tatn_comet_score],\n        \"Num_Samples\": [len(sources)],\n    }\n    summary_df = pd.DataFrame(summary_data)\n    summary_df.to_csv(summary_file, index=False)\n    print(f\"‚úÖ Summary saved: {summary_file}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Failed to save summary: {e}\")\n\n# Save detailed results with COMET segment scores\ndetailed_file = os.path.join(results_dir, \"ntrex_evaluation_detailed.csv\")\ntry:\n    detailed_data = {\n        \"source\": sources,\n        \"reference\": references,\n        \"tatn_translation\": tatn_translations,\n        \"comet_score\": tatn_comet_segment_scores,\n    }\n    detailed_df = pd.DataFrame(detailed_data)\n    detailed_df.to_csv(detailed_file, index=False)\n    print(f\"‚úÖ Detailed results saved: {detailed_file}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Failed to save detailed results: {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 13: EVALUATION COMPLETE\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä FINAL SCORES:\")\nprint(f\"   BLEU:   {tatn_bleu_score:.2f}\")\nprint(f\"   ChrF++: {tatn_chrf_score:.2f}\")\nprint(f\"   COMET:  {tatn_comet_score:.4f}\")\n\nprint(f\"\\n‚úÖ Results saved to:\")\nprint(f\"   - {summary_file}\")\nprint(f\"   - {detailed_file}\")\n\nprint(\"=\" * 80 + \"\\n\")\n\n# Final cleanup\ntry:\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\nexcept Exception:\n    pass\ngc.collect()\n\nprint(\"\\nüí° To download results:\")\nprint(\"   1. Go to Kaggle 'Output' tab\")\nprint(\"   2. Download the CSV files\")\nprint(\"   OR use: from IPython.display import FileLink\")\nprint(f\"          FileLink('{summary_file}')\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.514508Z","iopub.status.idle":"2026-02-16T03:06:31.514749Z","shell.execute_reply.started":"2026-02-16T03:06:31.514636Z","shell.execute_reply":"2026-02-16T03:06:31.514648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# FIX DEPENDENCY CONFLICTS (RUN BEFORE EVALUATION)\n# ==============================================================================\n\nimport warnings\nimport os\nimport sys\n\nprint(\"=\"*80)\nprint(\"FIXING DEPENDENCY CONFLICTS\")\nprint(\"=\"*80)\n\n# Suppress specific warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\n# Suppress threadpoolctl errors\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\nos.environ['MKL_NUM_THREADS'] = '1'\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['NUMEXPR_NUM_THREADS'] = '1'\n\nprint(\"‚úÖ Threading environment variables set\")\n\n# Fix numpy/scipy conflicts\ntry:\n    import numpy as np\n    print(f\"‚úÖ NumPy version: {np.__version__}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è NumPy warning: {e}\")\n\ntry:\n    import scipy\n    print(f\"‚úÖ SciPy version: {scipy.__version__}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è SciPy not critical: {e}\")\n\n# Suppress ctypes callback errors\nimport ctypes\nimport ctypes.util\n\n# Monkey-patch to suppress dl_iterate_phdr errors\noriginal_dlopen = ctypes.CDLL\ndef safe_dlopen(*args, **kwargs):\n    try:\n        return original_dlopen(*args, **kwargs)\n    except OSError:\n        pass  # Silently ignore library loading errors\nctypes.CDLL = safe_dlopen\n\nprint(\"‚úÖ Library loading errors suppressed\")\nprint(\"=\"*80)\nprint(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.515829Z","iopub.status.idle":"2026-02-16T03:06:31.516081Z","shell.execute_reply.started":"2026-02-16T03:06:31.515969Z","shell.execute_reply":"2026-02-16T03:06:31.515982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13: MEMORY CLEANUP + BLEU & CHRF++ & COMET EVALUATION (BANGLAT5)\n# ==============================================================================\n\nimport warnings\nimport os\nimport sys\n\nwarnings.filterwarnings('ignore')\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\nos.environ['MKL_NUM_THREADS'] = '1'\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport ctypes\n_original_excepthook = sys.excepthook\ndef exception_handler(exctype, value, tb):\n    if exctype == OSError and 'libscipy_openblas' in str(value):\n        pass\n    else:\n        _original_excepthook(exctype, value, tb)\nsys.excepthook = exception_handler\n\nimport time\nimport csv\nimport gc\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport torch\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 13: EVALUATION WITH MEMORY MANAGEMENT (BanglaT5)\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 1: MEMORY CLEANUP\n# ==============================================================================\nprint(\"\\n[SECTION 1] Memory Cleanup...\")\nprint(\"-\" * 80)\n\nif torch.cuda.is_available():\n    try:\n        initial_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        initial_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"üìä BEFORE CLEANUP:\")\n        print(f\"   Allocated: {initial_allocated:.2f} GB\")\n        print(f\"   Reserved: {initial_reserved:.2f} GB\")\n    except Exception:\n        pass\n\nvariables_to_delete = [\n    'model', 'tatn_model',\n    'tokenizer',\n    'optimizer', 'scheduler',\n    'train_dataloader', 'val_dataloader',\n    'checkpoint', 'model_state',\n    'training_args', 'trainer',\n    'dscd_outputs', 'asbn_outputs', 'trg_outputs',\n    'encoder_outputs', 'forward_outputs',\n    'prototypes_data', 'all_results',\n    'result', 'test_case',\n    'baseline_model', 'baseline_tokenizer', 'baseline_translations'\n]\n\ndeleted_count = 0\nfor var_name in variables_to_delete:\n    if var_name in globals():\n        try:\n            del globals()[var_name]\n            deleted_count += 1\n        except Exception:\n            pass\n\nprint(f\"‚úì Attempted to delete {deleted_count} variables\")\n\ngc.collect()\nprint(f\"‚úì Python garbage collection invoked\")\n\nif torch.cuda.is_available():\n    try:\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(f\"‚úì CUDA cache cleared\")\n        final_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        final_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"\\nüìä AFTER CLEANUP:\")\n        print(f\"   Allocated: {final_allocated:.2f} GB\")\n        print(f\"   Reserved: {final_reserved:.2f} GB\")\n        try:\n            print(f\"   Memory freed: {initial_allocated - final_allocated:.2f} GB allocated, {initial_reserved - final_reserved:.2f} GB reserved\")\n        except Exception:\n            pass\n    except Exception:\n        pass\n\nprint(\"\\n‚úÖ Memory cleanup complete - Ready for evaluation\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 2: SETUP AND IMPORTS\n# ==============================================================================\nprint(\"\\n[SECTION 2] Setup and Imports...\")\nprint(\"-\" * 80)\n\ntry:\n    import sacrebleu\n    print(f\"‚úÖ sacrebleu version: {sacrebleu.__version__}\")\nexcept Exception:\n    print(\"‚ö†Ô∏è  sacrebleu not available ‚Äî attempting install...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sacrebleu\"])\n    import sacrebleu\n    print(f\"‚úÖ sacrebleu version: {sacrebleu.__version__}\")\n\ntry:\n    from comet import download_model, load_from_checkpoint\n    print(f\"‚úÖ unbabel-comet already installed\")\nexcept Exception:\n    print(\"‚ö†Ô∏è  unbabel-comet not available ‚Äî installing...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unbabel-comet\"])\n    from comet import download_model, load_from_checkpoint\n    print(f\"‚úÖ unbabel-comet installed successfully\")\n\ntry:\n    _DEVICE = DEVICE if isinstance(DEVICE, torch.device) else torch.device(str(DEVICE)) if isinstance(DEVICE, str) else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\n    _MAX_LENGTH = int(MAX_LENGTH)\n    _EVAL_BATCH_SIZE = int(EVAL_BATCH_SIZE) if \"EVAL_BATCH_SIZE\" in globals() else 4\n    _EVAL_NUM_BEAMS = int(EVAL_NUM_BEAMS) if \"EVAL_NUM_BEAMS\" in globals() else 4\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _MAX_LENGTH = 64\n    _EVAL_BATCH_SIZE = 4\n    _EVAL_NUM_BEAMS = 4\n\ntry:\n    _TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    _TASK_PREFIX = \"translate Bengali to English: \"\n\nprint(f\"‚úÖ Configuration loaded\")\nprint(f\"   Device: {_DEVICE}\")\nprint(f\"   Direction: {_SOURCE_LANGUAGE} ‚Üí {_TARGET_LANGUAGE}\")\nprint(f\"   Task prefix: '{_TASK_PREFIX}'\")\nprint(f\"   Max length: {_MAX_LENGTH}\")\nprint(f\"   Batch size: {_EVAL_BATCH_SIZE}\")\nprint(f\"   Num beams: {_EVAL_NUM_BEAMS}\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 3: LOAD NTREX DATASET (TEXT FILES)\n# ==============================================================================\nBENGALI_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_ref_ben.txt\"\nENGLISH_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_src_eng.txt\"\n\nprint(f\"\\n[SECTION 3] Loading NTREX Dataset...\")\nprint(\"-\" * 80)\nprint(f\"Bengali file: {BENGALI_FILE}\")\nprint(f\"English file: {ENGLISH_FILE}\")\n\nif not os.path.exists(BENGALI_FILE):\n    raise FileNotFoundError(f\"Bengali file not found: {BENGALI_FILE}\")\nif not os.path.exists(ENGLISH_FILE):\n    raise FileNotFoundError(f\"English file not found: {ENGLISH_FILE}\")\n\ntry:\n    with open(BENGALI_FILE, 'r', encoding='utf-8') as f:\n        bengali_sentences = [line.strip() for line in f if line.strip()]\n\n    with open(ENGLISH_FILE, 'r', encoding='utf-8') as f:\n        english_sentences = [line.strip() for line in f if line.strip()]\n\n    print(f\"‚úÖ Loaded {len(bengali_sentences)} Bengali sentences\")\n    print(f\"‚úÖ Loaded {len(english_sentences)} English sentences\")\n\n    if len(bengali_sentences) != len(english_sentences):\n        raise ValueError(f\"Mismatch: {len(bengali_sentences)} Bengali vs {len(english_sentences)} English\")\n\n    sources = bengali_sentences\n    references = english_sentences\n\n    print(f\"\\n‚úÖ Dataset prepared for bn‚Üíen translation:\")\n    print(f\"   Source (Bengali): {len(sources)} sentences\")\n    print(f\"   Reference (English): {len(references)} sentences\")\n\n    print(f\"\\n   Sample 1:\")\n    print(f\"      SRC (bn): {sources[0][:80]}...\")\n    print(f\"      REF (en): {references[0][:80]}...\")\n\n    if len(sources) > 1:\n        print(f\"\\n   Sample 2:\")\n        print(f\"      SRC (bn): {sources[1][:80]}...\")\n        print(f\"      REF (en): {references[1][:80]}...\")\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load dataset: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# ==============================================================================\n# SECTION 4: LOAD TRAINED TATN MODEL (BANGLAT5)\n# ==============================================================================\nMODEL_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\nprint(f\"\\n[SECTION 4] Loading Trained TATN Model (BanglaT5)...\")\nprint(\"-\" * 80)\nprint(f\"Path: {MODEL_CHECKPOINT_PATH}\")\n\nif not os.path.exists(MODEL_CHECKPOINT_PATH):\n    raise FileNotFoundError(f\"Model checkpoint not found: {MODEL_CHECKPOINT_PATH}\")\n\ntry:\n    print(f\"üìÇ Loading checkpoint to CPU...\")\n    checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n    print(f\"‚úÖ Checkpoint loaded to CPU\")\n\n    if \"tokenizer\" in checkpoint:\n        tokenizer = checkpoint[\"tokenizer\"]\n        print(f\"‚úÖ Tokenizer loaded from checkpoint\")\n    else:\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n        print(f\"‚úÖ BanglaT5 tokenizer loaded from pretrained\")\n\n    TATNModelClass = globals().get(\"MemoryOptimizedTATNWithExplanations\")\n    if TATNModelClass is None:\n        raise RuntimeError(\"TATN model class not found. Run Cell 6 first.\")\n\n    print(f\"üîß Initializing TATN model...\")\n    tatn_model = TATNModelClass(tokenizer)\n\n    if \"model_state_dict\" in checkpoint:\n        model_state = checkpoint[\"model_state_dict\"]\n    elif \"model\" in checkpoint:\n        model_state = checkpoint[\"model\"]\n    else:\n        raise ValueError(\"No model state found in checkpoint\")\n\n    print(f\"üîß Loading model weights (strict=False)...\")\n    tatn_model.load_state_dict(model_state, strict=False)\n\n    try:\n        del model_state\n    except Exception:\n        pass\n    if 'dscd_state' in checkpoint:\n        try:\n            del checkpoint['dscd_state']\n        except Exception:\n            pass\n    try:\n        del checkpoint\n    except Exception:\n        pass\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(f\"üîß Moving model to {_DEVICE}...\")\n    tatn_model.to(_DEVICE)\n    tatn_model.eval()\n\n    print(f\"‚úÖ TATN model loaded successfully\")\n    try:\n        print(f\"   Device: {next(tatn_model.parameters()).device}\")\n    except Exception:\n        pass\n\n    if torch.cuda.is_available():\n        try:\n            allocated = torch.cuda.memory_allocated(0) / 1024**3\n            print(f\"   GPU memory: {allocated:.2f} GB\")\n        except Exception:\n            pass\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load TATN model: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# ==============================================================================\n# SECTION 6: TRANSLATION FUNCTION (TATN only - BanglaT5)\n# ==============================================================================\ndef translate_batch_tatn(\n    sentences: List[str],\n    model,\n    tokenizer,\n    max_length: int = 64,\n    num_beams: int = 4,\n) -> List[str]:\n    \"\"\"Translate batch using TATN model (BanglaT5)\"\"\"\n    try:\n        input_texts = [_TASK_PREFIX + sent for sent in sentences]\n        \n        inputs = tokenizer(\n            input_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length\n        )\n\n        input_ids = inputs[\"input_ids\"].to(_DEVICE)\n        attention_mask = inputs[\"attention_mask\"].to(_DEVICE)\n\n        with torch.no_grad():\n            generated = model.t5.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                no_repeat_ngram_size=3,\n                length_penalty=0.8,\n                repetition_penalty=1.2,\n            )\n\n            translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n            \n            translations = [\n                trans[0].upper() + trans[1:] if trans and len(trans) > 0 else trans\n                for trans in translations\n            ]\n\n            try:\n                del input_ids, attention_mask, generated\n            except Exception:\n                pass\n            try:\n                torch.cuda.empty_cache()\n            except Exception:\n                pass\n\n            return translations\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Batch translation failed: {e}\")\n        return [\"[ERROR]\"] * len(sentences)\n\n# ==============================================================================\n# SECTION 7: EVALUATE TATN MODEL\n# ==============================================================================\nprint(f\"\\n[SECTION 7] Evaluating TATN Model...\")\nprint(\"-\" * 80)\nprint(f\"Translating {len(sources)} samples...\")\n\ntatn_translations = []\nstart_time = time.time()\n\nfor i in range(0, len(sources), _EVAL_BATCH_SIZE):\n    batch_sources = sources[i:i + _EVAL_BATCH_SIZE]\n    batch_translations = translate_batch_tatn(\n        batch_sources,\n        tatn_model,\n        tokenizer,\n        max_length=_MAX_LENGTH,\n        num_beams=_EVAL_NUM_BEAMS\n    )\n    tatn_translations.extend(batch_translations)\n\n    if (i + _EVAL_BATCH_SIZE) % 200 == 0 or (i + _EVAL_BATCH_SIZE) >= len(sources):\n        elapsed = time.time() - start_time\n        processed = min(i + _EVAL_BATCH_SIZE, len(sources))\n        speed = processed / elapsed if elapsed > 0 else 0\n        eta = (len(sources) - processed) / speed if speed > 0 else 0\n\n        if torch.cuda.is_available():\n            try:\n                mem_gb = torch.cuda.memory_allocated(0) / 1024**3\n                print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                      f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min | GPU: {mem_gb:.2f}GB\")\n            except Exception:\n                print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                      f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min\")\n        else:\n            print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                  f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min\")\n\nelapsed_tatn = time.time() - start_time\n\nprint(f\"\\n‚úÖ TATN translation complete\")\nprint(f\"   Time: {elapsed_tatn:.1f}s ({elapsed_tatn/60:.2f} min)\")\nprint(f\"   Speed: {len(sources)/elapsed_tatn:.2f} samples/s\")\n\n# ==============================================================================\n# SECTION 8: COMPUTE BLEU & CHRF++ SCORES\n# ==============================================================================\nprint(f\"\\n[SECTION 8] Computing BLEU & ChrF++ Scores...\")\nprint(\"-\" * 80)\n\ntry:\n    tatn_bleu = sacrebleu.corpus_bleu(tatn_translations, [references])\n    tatn_chrf = sacrebleu.corpus_chrf(tatn_translations, [references])\n\n    tatn_bleu_score = tatn_bleu.score\n    tatn_chrf_score = tatn_chrf.score\n\n    print(f\"‚úÖ BLEU computed: {tatn_bleu_score:.2f}\")\n    print(f\"‚úÖ ChrF++ computed: {tatn_chrf_score:.2f}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  sacrebleu computation failed: {e}\")\n    tatn_bleu_score = 0.0\n    tatn_chrf_score = 0.0\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 9: COMPUTE COMET SCORE (OFFICIAL UNBABEL IMPLEMENTATION)\n# ==============================================================================\nprint(f\"\\n[SECTION 9] Computing COMET Score...\")\nprint(\"-\" * 80)\n\ntry:\n    print(f\"üì• Downloading COMET model: Unbabel/wmt22-comet-da...\")\n    comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n    print(f\"‚úÖ Model downloaded: {comet_model_path}\")\n\n    print(f\"üîß Loading COMET model...\")\n    comet_model = load_from_checkpoint(comet_model_path)\n    print(f\"‚úÖ COMET model loaded successfully\")\n\n    print(f\"üìã Preparing data for COMET evaluation...\")\n    comet_data = []\n    for src, mt, ref in zip(sources, tatn_translations, references):\n        comet_data.append({\n            \"src\": src,\n            \"mt\": mt,\n            \"ref\": ref\n        })\n    print(f\"‚úÖ Prepared {len(comet_data)} samples\")\n\n    print(f\"üöÄ Running COMET evaluation...\")\n    print(f\"   Batch size: 8\")\n    print(f\"   GPUs: {1 if torch.cuda.is_available() else 0}\")\n\n    comet_output = comet_model.predict(\n        comet_data,\n        batch_size=8,\n        gpus=1 if torch.cuda.is_available() else 0\n    )\n\n    tatn_comet_score = comet_output.system_score\n    tatn_comet_segment_scores = comet_output.scores\n\n    print(f\"\\n‚úÖ COMET evaluation complete\")\n    print(f\"   System score: {tatn_comet_score:.4f}\")\n    print(f\"   Segment scores: {len(tatn_comet_segment_scores)} samples\")\n    print(f\"   Score range: [{min(tatn_comet_segment_scores):.4f}, {max(tatn_comet_segment_scores):.4f}]\")\n\n    try:\n        del comet_model, comet_data, comet_output\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        print(f\"‚úÖ COMET model memory freed\")\n    except Exception:\n        pass\n\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  COMET evaluation failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    tatn_comet_score = 0.0\n    tatn_comet_segment_scores = [0.0] * len(sources)\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 10: FINAL SUMMARY\n# ==============================================================================\nprint(f\"\\n[SECTION 10] FINAL EVALUATION SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä TATN MODEL SCORES:\")\nprint(f\"   BLEU:   {tatn_bleu_score:.2f}\")\nprint(f\"   ChrF++: {tatn_chrf_score:.2f}\")\nprint(f\"   COMET:  {tatn_comet_score:.4f}\")\nprint(f\"\\n   Samples evaluated: {len(sources)}\")\nprint(f\"   Translation time: {elapsed_tatn/60:.2f} minutes\")\nprint(f\"   Speed: {len(sources)/elapsed_tatn:.2f} samples/second\")\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 11: SAMPLE TRANSLATIONS\n# ==============================================================================\nprint(f\"\\n[SECTION 11] Sample Translations\")\nprint(\"=\" * 80)\n\nnum_samples = min(5, len(sources))\nfor i in range(num_samples):\n    print(f\"\\n{'='*60}\")\n    print(f\"SAMPLE {i+1}/{num_samples}\")\n    print(f\"{'='*60}\")\n    print(f\"\\nüìù Source ({_SOURCE_LANGUAGE}):\")\n    print(f\"   {sources[i]}\")\n    print(f\"\\nüéØ Reference ({_TARGET_LANGUAGE}):\")\n    print(f\"   {references[i]}\")\n    print(f\"\\nü§ñ TATN Translation:\")\n    print(f\"   {tatn_translations[i]}\")\n    print(f\"\\nüìä COMET Segment Score: {tatn_comet_segment_scores[i]:.4f}\")\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 12: SAVE RESULTS\n# ==============================================================================\nprint(f\"\\n[SECTION 12] Saving Results...\")\nprint(\"-\" * 80)\n\nresults_dir = \"/kaggle/working/\"\nos.makedirs(results_dir, exist_ok=True)\n\nsummary_file = os.path.join(results_dir, \"ntrex_evaluation_summary.csv\")\nsummary_data = {\n    \"Model\": [\"TATN (BanglaT5)\"],\n    \"BLEU\": [tatn_bleu_score],\n    \"ChrF++\": [tatn_chrf_score],\n    \"COMET\": [tatn_comet_score],\n    \"Num_Samples\": [len(sources)],\n}\nsummary_df = pd.DataFrame(summary_data)\nsummary_df.to_csv(summary_file, index=False)\nprint(f\"‚úÖ Summary saved: {summary_file}\")\n\ndetailed_file = os.path.join(results_dir, \"ntrex_evaluation_detailed.csv\")\ndetailed_data = {\n    \"source\": sources,\n    \"reference\": references,\n    \"tatn_translation\": tatn_translations,\n    \"comet_score\": tatn_comet_segment_scores,\n}\ndetailed_df = pd.DataFrame(detailed_data)\ndetailed_df.to_csv(detailed_file, index=False)\nprint(f\"‚úÖ Detailed results saved: {detailed_file}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 13: EVALUATION COMPLETE\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä FINAL SCORES:\")\nprint(f\"   BLEU:   {tatn_bleu_score:.2f}\")\nprint(f\"   ChrF++: {tatn_chrf_score:.2f}\")\nprint(f\"   COMET:  {tatn_comet_score:.4f}\")\n\nprint(f\"\\n‚úÖ Results saved to:\")\nprint(f\"   - {summary_file}\")\nprint(f\"   - {detailed_file}\")\n\nprint(\"=\" * 80 + \"\\n\")\n\ntry:\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\nexcept Exception:\n    pass\ngc.collect()","metadata":{"id":"QDxrLqB0Vjps","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.517249Z","iopub.status.idle":"2026-02-16T03:06:31.517585Z","shell.execute_reply.started":"2026-02-16T03:06:31.517414Z","shell.execute_reply":"2026-02-16T03:06:31.517433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# FLORES EVALUATION WITH TRAINED BANGLAT5 MODEL\n# ================================================================================\n\nimport os\nimport time\nimport json\nimport torch\nimport gc\nfrom tqdm import tqdm\nimport sacrebleu\n\nprint(\"=\"*80)\nprint(\"FLORES EVALUATION - TATN BANGLAT5 MODEL\")\nprint(\"=\"*80)\n\n# ================================================================================\n# CONFIGURATION\n# ================================================================================\n\nCHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\nFLORES_BN_PATH = \"/kaggle/input/datasets/manaskumarmanna/flores-dataset/flores_bn.txt\"\nFLORES_EN_PATH = \"/kaggle/input/datasets/manaskumarmanna/flores-dataset/flores_en.txt\"\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntry:\n    TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    TASK_PREFIX = \"translate Bengali to English: \"\n\nprint(f\"üìÇ Checkpoint: {CHECKPOINT_PATH}\")\nprint(f\"üìÇ FLORES BN: {FLORES_BN_PATH}\")\nprint(f\"üìÇ FLORES EN: {FLORES_EN_PATH}\")\nprint(f\"üéØ Task prefix: '{TASK_PREFIX}'\")\nprint(f\"üñ•Ô∏è  Device: {DEVICE}\")\n\n# ================================================================================\n# AGGRESSIVE MEMORY CLEANUP\n# ================================================================================\n\nprint(\"\\nüßπ Memory cleanup...\")\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n    print(f\"  Reserved:  {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n\n# ================================================================================\n# LOAD MODEL\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING TRAINED MODEL\")\nprint(\"=\"*80)\n\nmodel_needs_loading = True\n\nif 'tatn_model' in globals() and tatn_model is not None:\n    print(\"‚ö†Ô∏è Found existing tatn_model in memory\")\n    try:\n        user_input = input(\"Use existing model? (y/n): \")\n        if user_input.lower() == 'y':\n            model_needs_loading = False\n            print(\"‚úÖ Using existing model\")\n        else:\n            print(\"üîÑ Will load from checkpoint\")\n            del tatn_model\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    except EOFError:\n        print(\"üîÑ Non-interactive mode, loading from checkpoint\")\n        try:\n            del tatn_model\n        except:\n            pass\n        gc.collect()\n\nif model_needs_loading:\n    if not os.path.exists(CHECKPOINT_PATH):\n        raise FileNotFoundError(f\"‚ùå Checkpoint not found: {CHECKPOINT_PATH}\")\n    \n    print(f\"üì• Loading checkpoint: {CHECKPOINT_PATH}\")\n    \n    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n    print(f\"‚úÖ Checkpoint loaded\")\n    print(f\"   Keys: {list(checkpoint.keys())[:5]}...\")\n    \n    from transformers import AutoTokenizer\n    \n    if 'tokenizer' not in globals() or tokenizer is None:\n        print(\"üì• Loading tokenizer...\")\n        if 'tokenizer' in checkpoint and checkpoint['tokenizer'] is not None:\n            tokenizer = checkpoint['tokenizer']\n            print(\"‚úÖ Tokenizer loaded from checkpoint\")\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n            print(\"‚úÖ Tokenizer loaded from HuggingFace\")\n    \n    print(\"üîß Initializing model architecture...\")\n    \n    if 'MemoryOptimizedTATNWithExplanations' in globals():\n        tatn_model = MemoryOptimizedTATNWithExplanations(tokenizer)\n        print(\"‚úÖ Using MemoryOptimizedTATNWithExplanations\")\n    else:\n        raise RuntimeError(\n            \"‚ùå Model class not found! Run Cell 6 (model definition) first.\"\n        )\n    \n    print(\"üîß Loading trained weights...\")\n    \n    if 'model_state_dict' in checkpoint:\n        state_dict = checkpoint['model_state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n    \n    missing, unexpected = tatn_model.load_state_dict(state_dict, strict=False)\n    \n    if missing:\n        print(f\"‚ö†Ô∏è Missing keys: {len(missing)}\")\n        if len(missing) <= 10:\n            for key in missing:\n                print(f\"   - {key}\")\n    if unexpected:\n        print(f\"‚ö†Ô∏è Unexpected keys: {len(unexpected)}\")\n        if len(unexpected) <= 10:\n            for key in unexpected:\n                print(f\"   - {key}\")\n    \n    print(f\"üîß Moving to {DEVICE}...\")\n    tatn_model.to(DEVICE)\n    \n    del checkpoint, state_dict\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    print(\"‚úÖ Model loaded successfully!\")\n\nelse:\n    if 'tokenizer' not in globals() or tokenizer is None:\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n\n# ================================================================================\n# VERIFY MODEL\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"VERIFYING MODEL\")\nprint(\"=\"*80)\n\nprint(f\"Model type: {type(tatn_model).__name__}\")\nprint(f\"Has t5: {hasattr(tatn_model, 't5')}\")\nprint(f\"Has dscd: {hasattr(tatn_model, 'dscd')}\")\nprint(f\"Has asbn: {hasattr(tatn_model, 'asbn')}\")\nprint(f\"Has trg: {hasattr(tatn_model, 'trg')}\")\n\ntatn_model.eval()\n\nprint(\"\\nüîß Freezing all parameters for inference...\")\nfor param in tatn_model.parameters():\n    param.requires_grad = False\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1024**3\n    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"\\nGPU Memory:\")\n    print(f\"  Allocated: {allocated:.2f} GB\")\n    print(f\"  Total:     {total:.2f} GB\")\n    print(f\"  Free:      {total - allocated:.2f} GB\")\n\nprint(\"\\nüß™ Testing model...\")\ntest_sent = \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ó‡¶æ‡¶® ‡¶ó‡¶æ‡¶á‡•§\"\ntest_input_text = TASK_PREFIX + test_sent\ntest_input = tokenizer(test_input_text, return_tensors='pt', truncation=True, max_length=128).to(DEVICE)\n\nwith torch.no_grad():\n    test_output = tatn_model.generate(\n        input_ids=test_input['input_ids'],\n        attention_mask=test_input['attention_mask'],\n        max_length=64,\n        num_beams=4,\n        early_stopping=True,\n        use_dscd=True,\n        use_asbn=False,\n        return_text=True\n    )\n\nif isinstance(test_output, str):\n    test_translation = test_output\nelif isinstance(test_output, torch.Tensor):\n    test_translation = tokenizer.decode(test_output[0], skip_special_tokens=True)\n    if test_translation and len(test_translation) > 0:\n        test_translation = test_translation[0].upper() + test_translation[1:]\nelse:\n    test_translation = str(test_output)\n\nprint(f\"  Input:  {test_sent}\")\nprint(f\"  Output: {test_translation}\")\nprint(\"‚úÖ Model working!\")\n\ndel test_input, test_output\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# ================================================================================\n# LOAD FLORES DATA\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING FLORES DATA\")\nprint(\"=\"*80)\n\nif not os.path.exists(FLORES_BN_PATH):\n    raise FileNotFoundError(f\"‚ùå Not found: {FLORES_BN_PATH}\")\nif not os.path.exists(FLORES_EN_PATH):\n    raise FileNotFoundError(f\"‚ùå Not found: {FLORES_EN_PATH}\")\n\nwith open(FLORES_BN_PATH, 'r', encoding='utf-8') as f:\n    bengali_sentences = [line.strip() for line in f if line.strip()]\n\nwith open(FLORES_EN_PATH, 'r', encoding='utf-8') as f:\n    english_references = [line.strip() for line in f if line.strip()]\n\nprint(f\"‚úÖ Loaded {len(bengali_sentences)} sentence pairs\")\nprint(f\"\\nSample:\")\nprint(f\"  BN: {bengali_sentences[0][:100]}...\")\nprint(f\"  EN: {english_references[0][:100]}...\")\n\n# ================================================================================\n# GENERATION PARAMETERS\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATION SETTINGS\")\nprint(\"=\"*80)\n\ngen_params = {\n    'max_length': 64,\n    'num_beams': 4,\n    'early_stopping': True,\n    'use_dscd': True,\n    'use_asbn': False,\n    'return_text': True\n}\n\nBATCH_SIZE = 1\n\nprint(f\"Generation params: {gen_params}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\n\n# ================================================================================\n# TRANSLATION\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRANSLATING FLORES-200\")\nprint(\"=\"*80)\n\npredictions = []\nstart_time = time.time()\n\nestimated_minutes = (len(bengali_sentences) * 8) / 60\nprint(f\"Estimated time: {estimated_minutes:.1f} minutes\\n\")\n\nfor idx, bn_sent in enumerate(tqdm(bengali_sentences, desc=\"Translating\")):\n    input_text = TASK_PREFIX + bn_sent\n    \n    inputs = tokenizer(\n        input_text,\n        return_tensors='pt',\n        truncation=True,\n        max_length=128,\n        padding=False\n    ).to(DEVICE)\n    \n    with torch.no_grad():\n        try:\n            outputs = tatn_model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs.get('attention_mask'),\n                **gen_params\n            )\n            \n            if isinstance(outputs, str):\n                translation = outputs\n            elif isinstance(outputs, torch.Tensor):\n                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                if translation and len(translation) > 0:\n                    translation = translation[0].upper() + translation[1:]\n            else:\n                translation = str(outputs)\n            \n            predictions.append(translation)\n            \n        except RuntimeError as e:\n            if 'out of memory' in str(e):\n                print(f\"\\n‚ö†Ô∏è OOM at sample {idx}, retrying with reduced beams...\")\n                del inputs, outputs\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=128).to(DEVICE)\n                with torch.no_grad():\n                    outputs = tatn_model.generate(\n                        input_ids=inputs['input_ids'],\n                        attention_mask=inputs.get('attention_mask'),\n                        num_beams=2,\n                        max_length=64,\n                        use_dscd=False,\n                        use_asbn=False,\n                        return_text=True\n                    )\n                \n                if isinstance(outputs, str):\n                    translation = outputs\n                elif isinstance(outputs, torch.Tensor):\n                    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                    if translation and len(translation) > 0:\n                        translation = translation[0].upper() + translation[1:]\n                else:\n                    translation = str(outputs)\n                \n                predictions.append(translation)\n            else:\n                raise e\n    \n    del inputs, outputs\n    \n    if (idx + 1) % 10 == 0:\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    if idx < 3:\n        print(f\"\\n{'‚îÄ'*60}\")\n        print(f\"Example {idx+1}:\")\n        print(f\"  SRC: {bn_sent[:70]}...\")\n        print(f\"  REF: {english_references[idx][:70]}...\")\n        print(f\"  TRN: {translation[:70]}...\")\n    \n    if (idx + 1) % 100 == 0:\n        elapsed = time.time() - start_time\n        speed = (idx + 1) / elapsed\n        remaining = (len(bengali_sentences) - idx - 1) / speed\n        print(f\"\\n   {idx+1}/{len(bengali_sentences)} | \"\n              f\"{speed:.2f} samples/sec | \"\n              f\"ETA: {remaining/60:.1f} min\")\n\nelapsed_time = time.time() - start_time\n\nprint(f\"\\n‚úÖ Translation complete!\")\nprint(f\"   Time: {elapsed_time/60:.1f} minutes\")\nprint(f\"   Speed: {len(bengali_sentences)/elapsed_time:.2f} samples/sec\")\n\n# ================================================================================\n# COMPUTE METRICS\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPUTING METRICS\")\nprint(\"=\"*80)\n\nbleu = sacrebleu.corpus_bleu(predictions, [english_references])\nchrf = sacrebleu.corpus_chrf(predictions, [english_references])\n\nprint(f\"‚úÖ BLEU:   {bleu.score:.2f}\")\nprint(f\"‚úÖ chrF++: {chrf.score:.2f}\")\n\n# ================================================================================\n# SAVE RESULTS\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*80)\n\ntraining_samples = 'custom'\ntry:\n    if 'checkpoint' in locals() and 'epochs_trained' in checkpoint:\n        training_samples = f\"{checkpoint['epochs_trained']} epochs\"\nexcept:\n    pass\n\nresults = {\n    'checkpoint': CHECKPOINT_PATH,\n    'model': 'BanglaT5 + TATN',\n    'dataset': 'FLORES-200 devtest',\n    'num_samples': len(bengali_sentences),\n    'training_samples': training_samples,\n    'batch_size': BATCH_SIZE,\n    'num_beams': gen_params['num_beams'],\n    'bleu': float(bleu.score),\n    'chrf': float(chrf.score),\n    'time_minutes': elapsed_time / 60,\n    'speed_samples_per_sec': len(bengali_sentences) / elapsed_time,\n    'generation_params': gen_params,\n    'task_prefix': TASK_PREFIX,\n}\n\nresults_path = '/kaggle/working/flores_results.json'\nwith open(results_path, 'w', encoding='utf-8') as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\npredictions_path = '/kaggle/working/flores_predictions.txt'\nwith open(predictions_path, 'w', encoding='utf-8') as f:\n    for pred in predictions:\n        f.write(pred + '\\n')\n\nprint(f\"‚úÖ Results: {results_path}\")\nprint(f\"‚úÖ Predictions: {predictions_path}\")\n\n# ================================================================================\n# FINAL SUMMARY\n# ================================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL RESULTS - BANGLAT5 + TATN\")\nprint(\"=\"*80)\n\nprint(f\"\"\"\nCheckpoint:  {CHECKPOINT_PATH}\nModel:       BanglaT5 + TATN (DSCD + ASBN + TRG)\nDataset:     FLORES-200 devtest\nSamples:     {len(bengali_sentences)}\nBatch size:  {BATCH_SIZE}\nNum beams:   {gen_params['num_beams']}\nTime:        {elapsed_time/60:.1f} minutes\n\nSCORES:\n  BLEU:      {bleu.score:.2f}\n  chrF++:    {chrf.score:.2f}\n\nREFERENCE BENCHMARKS:\n  BanglaT5 baseline:        ~18-20 BLEU\n  FLORES typical range:     15-25 BLEU (depends on training)\n  Your model:               {bleu.score:.2f} BLEU\n\"\"\")\n\nif bleu.score >= 25:\n    print(\"üéâ EXCELLENT! 25+ BLEU is competitive!\")\n    print(\"   Your TATN enhancements are working very well!\")\nelif bleu.score >= 22:\n    print(\"‚úÖ VERY GOOD! 22+ BLEU is strong performance!\")\n    print(\"   Clear improvement from DSCD/ASBN/TRG modules!\")\nelif bleu.score >= 20:\n    print(\"‚úÖ GOOD! 20+ BLEU shows solid translation quality!\")\nelif bleu.score >= 18:\n    print(\"üìä Decent performance, close to baseline BanglaT5.\")\nelse:\n    print(\"üìä Evaluation complete. Consider more training for improvement.\")\n\nprint(\"\\nüí° Expected performance:\")\nprint(\"   - BanglaT5 baseline: ~18-20 BLEU\")\nprint(\"   - With DSCD/ASBN/TRG: +2-5 BLEU improvement expected\")\nprint(\"   - Your BLEU: {:.2f}\".format(bleu.score))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION COMPLETE!\")\nprint(\"=\"*80)\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"id":"rmvRSPZHw6ru","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.518729Z","iopub.status.idle":"2026-02-16T03:06:31.519044Z","shell.execute_reply.started":"2026-02-16T03:06:31.518890Z","shell.execute_reply":"2026-02-16T03:06:31.518907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13: MEMORY CLEANUP + BERTSCORE EVALUATION (BANGLAT5)\n# ==============================================================================\n\nimport warnings\nimport os\nimport sys\n\nwarnings.filterwarnings('ignore')\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\nos.environ['MKL_NUM_THREADS'] = '1'\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport ctypes\n_original_excepthook = sys.excepthook\ndef exception_handler(exctype, value, tb):\n    if exctype == OSError and 'libscipy_openblas' in str(value):\n        pass\n    else:\n        _original_excepthook(exctype, value, tb)\nsys.excepthook = exception_handler\n\nimport time\nimport csv\nimport gc\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport torch\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 13: EVALUATION WITH MEMORY MANAGEMENT (BanglaT5)\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 1: MEMORY CLEANUP\n# ==============================================================================\nprint(\"\\n[SECTION 1] Memory Cleanup...\")\nprint(\"-\" * 80)\n\nif torch.cuda.is_available():\n    try:\n        initial_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        initial_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"üìä BEFORE CLEANUP:\")\n        print(f\"   Allocated: {initial_allocated:.2f} GB\")\n        print(f\"   Reserved: {initial_reserved:.2f} GB\")\n    except Exception:\n        pass\n\nvariables_to_delete = [\n    'model', 'tatn_model',\n    'tokenizer',\n    'optimizer', 'scheduler',\n    'train_dataloader', 'val_dataloader',\n    'checkpoint', 'model_state',\n    'training_args', 'trainer',\n    'dscd_outputs', 'asbn_outputs', 'trg_outputs',\n    'encoder_outputs', 'forward_outputs',\n    'prototypes_data', 'all_results',\n    'result', 'test_case',\n    'baseline_model', 'baseline_tokenizer', 'baseline_translations'\n]\n\ndeleted_count = 0\nfor var_name in variables_to_delete:\n    if var_name in globals():\n        try:\n            del globals()[var_name]\n            deleted_count += 1\n        except Exception:\n            pass\n\nprint(f\"‚úì Attempted to delete {deleted_count} variables\")\n\ngc.collect()\nprint(f\"‚úì Python garbage collection invoked\")\n\nif torch.cuda.is_available():\n    try:\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(f\"‚úì CUDA cache cleared\")\n        final_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        final_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"\\nüìä AFTER CLEANUP:\")\n        print(f\"   Allocated: {final_allocated:.2f} GB\")\n        print(f\"   Reserved: {final_reserved:.2f} GB\")\n        try:\n            print(f\"   Memory freed: {initial_allocated - final_allocated:.2f} GB allocated, {initial_reserved - final_reserved:.2f} GB reserved\")\n        except Exception:\n            pass\n    except Exception:\n        pass\n\nprint(\"\\n‚úÖ Memory cleanup complete - Ready for evaluation\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 2: SETUP AND IMPORTS\n# ==============================================================================\nprint(\"\\n[SECTION 2] Setup and Imports...\")\nprint(\"-\" * 80)\n\n# BERTScore import (install if missing)\ntry:\n    from bert_score import score as bert_score\n    try:\n        import bert_score as _bs_mod\n        bs_ver = getattr(_bs_mod, '__version__', 'unknown')\n        print(f\"‚úÖ bert-score available (version: {bs_ver})\")\n    except Exception:\n        print(f\"‚úÖ bert-score available\")\nexcept Exception:\n    print(\"‚ö†Ô∏è  bert-score not available ‚Äî installing (this may take a moment)...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bert-score\"])\n    from bert_score import score as bert_score\n    print(f\"‚úÖ bert-score installed successfully\")\n\ntry:\n    _DEVICE = DEVICE if isinstance(DEVICE, torch.device) else torch.device(str(DEVICE)) if isinstance(DEVICE, str) else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\n    _MAX_LENGTH = int(MAX_LENGTH)\n    _EVAL_BATCH_SIZE = int(EVAL_BATCH_SIZE) if \"EVAL_BATCH_SIZE\" in globals() else 4\n    _EVAL_NUM_BEAMS = int(EVAL_NUM_BEAMS) if \"EVAL_NUM_BEAMS\" in globals() else 4\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _MAX_LENGTH = 64\n    _EVAL_BATCH_SIZE = 4\n    _EVAL_NUM_BEAMS = 4\n\ntry:\n    _TASK_PREFIX = str(TASK_PREFIX)\nexcept (NameError, TypeError):\n    _TASK_PREFIX = \"translate Bengali to English: \"\n\nprint(f\"‚úÖ Configuration loaded\")\nprint(f\"   Device: {_DEVICE}\")\nprint(f\"   Direction: {_SOURCE_LANGUAGE} ‚Üí {_TARGET_LANGUAGE}\")\nprint(f\"   Task prefix: '{_TASK_PREFIX}'\")\nprint(f\"   Max length: {_MAX_LENGTH}\")\nprint(f\"   Batch size: {_EVAL_BATCH_SIZE}\")\nprint(f\"   Num beams: {_EVAL_NUM_BEAMS}\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 3: LOAD NTREX DATASET (TEXT FILES)\n# ==============================================================================\nBENGALI_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_ref_ben.txt\"\nENGLISH_FILE = \"/kaggle/input/datasets/manas00000003/paper-dataset/ntrex_src_eng.txt\"\n\nprint(f\"\\n[SECTION 3] Loading NTREX Dataset...\")\nprint(\"-\" * 80)\nprint(f\"Bengali file: {BENGALI_FILE}\")\nprint(f\"English file: {ENGLISH_FILE}\")\n\nif not os.path.exists(BENGALI_FILE):\n    raise FileNotFoundError(f\"Bengali file not found: {BENGALI_FILE}\")\nif not os.path.exists(ENGLISH_FILE):\n    raise FileNotFoundError(f\"English file not found: {ENGLISH_FILE}\")\n\ntry:\n    with open(BENGALI_FILE, 'r', encoding='utf-8') as f:\n        bengali_sentences = [line.strip() for line in f if line.strip()]\n\n    with open(ENGLISH_FILE, 'r', encoding='utf-8') as f:\n        english_sentences = [line.strip() for line in f if line.strip()]\n\n    print(f\"‚úÖ Loaded {len(bengali_sentences)} Bengali sentences\")\n    print(f\"‚úÖ Loaded {len(english_sentences)} English sentences\")\n\n    if len(bengali_sentences) != len(english_sentences):\n        raise ValueError(f\"Mismatch: {len(bengali_sentences)} Bengali vs {len(english_sentences)} English\")\n\n    sources = bengali_sentences\n    references = english_sentences\n\n    print(f\"\\n‚úÖ Dataset prepared for bn‚Üíen translation:\")\n    print(f\"   Source (Bengali): {len(sources)} sentences\")\n    print(f\"   Reference (English): {len(references)} sentences\")\n\n    print(f\"\\n   Sample 1:\")\n    print(f\"      SRC (bn): {sources[0][:80]}...\")\n    print(f\"      REF (en): {references[0][:80]}...\")\n\n    if len(sources) > 1:\n        print(f\"\\n   Sample 2:\")\n        print(f\"      SRC (bn): {sources[1][:80]}...\")\n        print(f\"      REF (en): {references[1][:80]}...\")\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load dataset: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# ==============================================================================\n# SECTION 4: LOAD TRAINED TATN MODEL (BANGLAT5)\n# ==============================================================================\nMODEL_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\nprint(f\"\\n[SECTION 4] Loading Trained TATN Model (BanglaT5)...\")\nprint(\"-\" * 80)\nprint(f\"Path: {MODEL_CHECKPOINT_PATH}\")\n\nif not os.path.exists(MODEL_CHECKPOINT_PATH):\n    raise FileNotFoundError(f\"Model checkpoint not found: {MODEL_CHECKPOINT_PATH}\")\n\ntry:\n    print(f\"üìÇ Loading checkpoint to CPU...\")\n    checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n    print(f\"‚úÖ Checkpoint loaded to CPU\")\n\n    if \"tokenizer\" in checkpoint:\n        tokenizer = checkpoint[\"tokenizer\"]\n        print(f\"‚úÖ Tokenizer loaded from checkpoint\")\n    else:\n        from transformers import AutoTokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n        print(f\"‚úÖ BanglaT5 tokenizer loaded from pretrained\")\n\n    TATNModelClass = globals().get(\"MemoryOptimizedTATNWithExplanations\")\n    if TATNModelClass is None:\n        raise RuntimeError(\"TATN model class not found. Run Cell 6 first.\")\n\n    print(f\"üîß Initializing TATN model...\")\n    tatn_model = TATNModelClass(tokenizer)\n\n    if \"model_state_dict\" in checkpoint:\n        model_state = checkpoint[\"model_state_dict\"]\n    elif \"model\" in checkpoint:\n        model_state = checkpoint[\"model\"]\n    else:\n        raise ValueError(\"No model state found in checkpoint\")\n\n    print(f\"üîß Loading model weights (strict=False)...\")\n    tatn_model.load_state_dict(model_state, strict=False)\n\n    try:\n        del model_state\n    except Exception:\n        pass\n    if 'dscd_state' in checkpoint:\n        try:\n            del checkpoint['dscd_state']\n        except Exception:\n            pass\n    try:\n        del checkpoint\n    except Exception:\n        pass\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(f\"üîß Moving model to {_DEVICE}...\")\n    tatn_model.to(_DEVICE)\n    tatn_model.eval()\n\n    print(f\"‚úÖ TATN model loaded successfully\")\n    try:\n        print(f\"   Device: {next(tatn_model.parameters()).device}\")\n    except Exception:\n        pass\n\n    if torch.cuda.is_available():\n        try:\n            allocated = torch.cuda.memory_allocated(0) / 1024**3\n            print(f\"   GPU memory: {allocated:.2f} GB\")\n        except Exception:\n            pass\n\n    print(\"=\" * 80)\n\nexcept Exception as e:\n    print(f\"‚ùå Failed to load TATN model: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# ==============================================================================\n# SECTION 5: TRANSLATION FUNCTION (TATN only - BanglaT5)\n# ==============================================================================\ndef translate_batch_tatn(\n    sentences: List[str],\n    model,\n    tokenizer,\n    max_length: int = 64,\n    num_beams: int = 4,\n) -> List[str]:\n    \"\"\"Translate batch using TATN model (BanglaT5)\"\"\"\n    try:\n        input_texts = [_TASK_PREFIX + sent for sent in sentences]\n        \n        inputs = tokenizer(\n            input_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length\n        )\n\n        input_ids = inputs[\"input_ids\"].to(_DEVICE)\n        attention_mask = inputs[\"attention_mask\"].to(_DEVICE)\n\n        with torch.no_grad():\n            generated = model.t5.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                no_repeat_ngram_size=3,\n                length_penalty=0.8,\n                repetition_penalty=1.2,\n            )\n\n            translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n            \n            translations = [\n                trans[0].upper() + trans[1:] if trans and len(trans) > 0 else trans\n                for trans in translations\n            ]\n\n            try:\n                del input_ids, attention_mask, generated\n            except Exception:\n                pass\n            try:\n                torch.cuda.empty_cache()\n            except Exception:\n                pass\n\n            return translations\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Batch translation failed: {e}\")\n        return [\"[ERROR]\"] * len(sentences)\n\n# ==============================================================================\n# SECTION 6: EVALUATE TATN MODEL\n# ==============================================================================\nprint(f\"\\n[SECTION 6] Evaluating TATN Model...\")\nprint(\"-\" * 80)\nprint(f\"Translating {len(sources)} samples...\")\n\ntatn_translations = []\nstart_time = time.time()\n\nfor i in range(0, len(sources), _EVAL_BATCH_SIZE):\n    batch_sources = sources[i:i + _EVAL_BATCH_SIZE]\n    batch_translations = translate_batch_tatn(\n        batch_sources,\n        tatn_model,\n        tokenizer,\n        max_length=_MAX_LENGTH,\n        num_beams=_EVAL_NUM_BEAMS\n    )\n    tatn_translations.extend(batch_translations)\n\n    if (i + _EVAL_BATCH_SIZE) % 200 == 0 or (i + _EVAL_BATCH_SIZE) >= len(sources):\n        elapsed = time.time() - start_time\n        processed = min(i + _EVAL_BATCH_SIZE, len(sources))\n        speed = processed / elapsed if elapsed > 0 else 0\n        eta = (len(sources) - processed) / speed if speed > 0 else 0\n\n        if torch.cuda.is_available():\n            try:\n                mem_gb = torch.cuda.memory_allocated(0) / 1024**3\n                print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                      f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min | GPU: {mem_gb:.2f}GB\")\n            except Exception:\n                print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                      f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min\")\n        else:\n            print(f\"   Progress: {processed}/{len(sources)} ({processed/len(sources)*100:.1f}%) | \"\n                  f\"Speed: {speed:.1f} samples/s | ETA: {eta/60:.1f} min\")\n\nelapsed_tatn = time.time() - start_time\n\nprint(f\"\\n‚úÖ TATN translation complete\")\nprint(f\"   Time: {elapsed_tatn:.1f}s ({elapsed_tatn/60:.2f} min)\")\nprint(f\"   Speed: {len(sources)/elapsed_tatn:.2f} samples/s\")\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 7: COMPUTE BERTSCORE\n# ==============================================================================\nprint(f\"\\n[SECTION 7] Computing BERTScore (Precision / Recall / F1)...\")\nprint(\"-\" * 80)\n\n_device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntatn_bertscore_precision = 0.0\ntatn_bertscore_recall = 0.0\ntatn_bertscore_f1 = 0.0\ntatn_bertscore_segment_f1 = [0.0] * len(sources)\n\ntry:\n    bs_batch_size = max(8, _EVAL_BATCH_SIZE * 4)\n    print(f\"üöÄ Running BERTScore: device={_device_str}, batch_size={bs_batch_size}, lang='en'\")\n\n    P, R, F = bert_score(\n        tatn_translations,\n        references,\n        lang='en',\n        rescale_with_baseline=True,\n        device=_device_str,\n        batch_size=bs_batch_size\n    )\n\n    P_np = P.cpu().numpy()\n    R_np = R.cpu().numpy()\n    F_np = F.cpu().numpy()\n\n    tatn_bertscore_precision = float(np.mean(P_np) * 100.0)\n    tatn_bertscore_recall = float(np.mean(R_np) * 100.0)\n    tatn_bertscore_f1 = float(np.mean(F_np) * 100.0)\n    tatn_bertscore_segment_f1 = (F_np * 100.0).tolist()\n\n    print(f\"‚úÖ BERTScore computed (F1 avg): {tatn_bertscore_f1:.2f}\")\n    print(f\"   Precision avg: {tatn_bertscore_precision:.2f}\")\n    print(f\"   Recall avg:    {tatn_bertscore_recall:.2f}\")\n    print(f\"   Segment scores: {len(tatn_bertscore_segment_f1)} samples\")\n\n    try:\n        del P, R, F, P_np, R_np, F_np\n    except Exception:\n        pass\n\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    except Exception:\n        pass\n\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  BERTScore computation failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    tatn_bertscore_precision = 0.0\n    tatn_bertscore_recall = 0.0\n    tatn_bertscore_f1 = 0.0\n    tatn_bertscore_segment_f1 = [0.0] * len(sources)\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 8: FINAL SUMMARY\n# ==============================================================================\nprint(f\"\\n[SECTION 8] FINAL EVALUATION SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä TATN MODEL SCORES:\")\nprint(f\"   BERTScore Precision: {tatn_bertscore_precision:.2f}\")\nprint(f\"   BERTScore Recall:    {tatn_bertscore_recall:.2f}\")\nprint(f\"   BERTScore F1:        {tatn_bertscore_f1:.2f}\")\nprint(f\"\\n   Samples evaluated: {len(sources)}\")\nprint(f\"   Translation time: {elapsed_tatn/60:.2f} minutes\")\nprint(f\"   Speed: {len(sources)/elapsed_tatn:.2f} samples/second\")\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 9: SAMPLE TRANSLATIONS\n# ==============================================================================\nprint(f\"\\n[SECTION 9] Sample Translations\")\nprint(\"=\" * 80)\n\nnum_samples = min(5, len(sources))\nfor i in range(num_samples):\n    print(f\"\\n{'='*60}\")\n    print(f\"SAMPLE {i+1}/{num_samples}\")\n    print(f\"{'='*60}\")\n    print(f\"\\nüìù Source ({_SOURCE_LANGUAGE}):\")\n    print(f\"   {sources[i]}\")\n    print(f\"\\nüéØ Reference ({_TARGET_LANGUAGE}):\")\n    print(f\"   {references[i]}\")\n    print(f\"\\nü§ñ TATN Translation:\")\n    print(f\"   {tatn_translations[i]}\")\n    bert_seg = tatn_bertscore_segment_f1[i] if i < len(tatn_bertscore_segment_f1) else 0.0\n    print(f\"\\nüìä BERTScore (F1) Segment: {bert_seg:.2f}\")\n\nprint(\"=\" * 80)\n\n# ==============================================================================\n# SECTION 10: SAVE RESULTS\n# ==============================================================================\nprint(f\"\\n[SECTION 10] Saving Results...\")\nprint(\"-\" * 80)\n\nresults_dir = \"/kaggle/working/\"\nos.makedirs(results_dir, exist_ok=True)\n\nsummary_file = os.path.join(results_dir, \"ntrex_evaluation_summary.csv\")\nsummary_data = {\n    \"Model\": [\"TATN (BanglaT5)\"],\n    \"BERTScore_Precision\": [tatn_bertscore_precision],\n    \"BERTScore_Recall\": [tatn_bertscore_recall],\n    \"BERTScore_F1\": [tatn_bertscore_f1],\n    \"Num_Samples\": [len(sources)],\n}\nsummary_df = pd.DataFrame(summary_data)\nsummary_df.to_csv(summary_file, index=False)\nprint(f\"‚úÖ Summary saved: {summary_file}\")\n\ndetailed_file = os.path.join(results_dir, \"ntrex_evaluation_detailed.csv\")\ndetailed_data = {\n    \"source\": sources,\n    \"reference\": references,\n    \"tatn_translation\": tatn_translations,\n    \"bertscore_f1\": tatn_bertscore_segment_f1,\n}\ndetailed_df = pd.DataFrame(detailed_data)\ndetailed_df.to_csv(detailed_file, index=False)\nprint(f\"‚úÖ Detailed results saved: {detailed_file}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CELL 13: EVALUATION COMPLETE\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä FINAL SCORES:\")\nprint(f\"   BERTScore Precision: {tatn_bertscore_precision:.2f}\")\nprint(f\"   BERTScore Recall:    {tatn_bertscore_recall:.2f}\")\nprint(f\"   BERTScore F1:        {tatn_bertscore_f1:.2f}\")\n\nprint(f\"\\n‚úÖ Results saved to:\")\nprint(f\"   - {summary_file}\")\nprint(f\"   - {detailed_file}\")\n\nprint(\"=\" * 80 + \"\\n\")\n\ntry:\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\nexcept Exception:\n    pass\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T03:06:31.520086Z","iopub.status.idle":"2026-02-16T03:06:31.520307Z","shell.execute_reply.started":"2026-02-16T03:06:31.520202Z","shell.execute_reply":"2026-02-16T03:06:31.520214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}