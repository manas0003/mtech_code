{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14014111,"sourceType":"datasetVersion","datasetId":8927824}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers sentence-transformers\n!pip install transformers==4.30.2 --no-deps\n!pip install \"tokenizers<0.14\" sacremoses\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"id":"W8IIWAEHH4Jy","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:01.071523Z","iopub.execute_input":"2026-01-07T14:57:01.071789Z","iopub.status.idle":"2026-01-07T14:57:34.552095Z","shell.execute_reply.started":"2026-01-07T14:57:01.071757Z","shell.execute_reply":"2026-01-07T14:57:34.551390Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.1\nUninstalling transformers-4.57.1:\n  Successfully uninstalled transformers-4.57.1\nFound existing installation: tokenizers 0.22.1\nUninstalling tokenizers-0.22.1:\n  Successfully uninstalled tokenizers-0.22.1\nFound existing installation: sentence-transformers 5.1.1\nUninstalling sentence-transformers-5.1.1:\n  Successfully uninstalled sentence-transformers-5.1.1\nCollecting transformers==4.30.2\n  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\nSuccessfully installed transformers-4.30.2\nCollecting tokenizers<0.14\n  Downloading tokenizers-0.13.3.tar.gz (314 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.9/314.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: tokenizers\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0mFailed to build tokenizers\n\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n\u001b[0mCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (4.30.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (0.23.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (2.0.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (1.15.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (3.9.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (0.2.1)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2025.11.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.6.2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->sentence-transformers==2.2.2) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.11.12)\nBuilding wheels for collected packages: sentence-transformers, tokenizers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=acc323a3a5ba4ead7b1c197ef61e8303cb4bcecaf5c99914e9f0fd42eeeeaf6b\n  Stored in directory: /root/.cache/pip/wheels/d9/3b/21/aa025e9c81a6cda4b8358756a756677b0969b4bc69be6dd5da\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0mSuccessfully built sentence-transformers\nFailed to build tokenizers\n\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n\u001b[0mCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:34.554264Z","iopub.execute_input":"2026-01-07T14:57:34.554510Z","iopub.status.idle":"2026-01-07T14:57:37.626968Z","shell.execute_reply.started":"2026-01-07T14:57:34.554483Z","shell.execute_reply":"2026-01-07T14:57:37.626315Z"}},"outputs":[{"name":"stdout","text":"4.30.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: TATN CONFIGURATION (BENGALI → ENGLISH) - FIXED & VERIFIED\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\n# ------------------------------------------------------------------------------\n# Optional dependencies\n# ------------------------------------------------------------------------------\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n    print(\"[WARN] pandas not available; CSV loading will fail\")\n\ntry:\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\n    _HAS_M2M_TOKENIZER = True\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer\n        _HAS_M2M_TOKENIZER = True\n    except Exception:\n        M2M100Tokenizer = None\n        _HAS_M2M_TOKENIZER = False\n        print(\"[WARN] M2M100Tokenizer not available\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# ------------------------------------------------------------------------------\n# Device / GPU configuration\n# ------------------------------------------------------------------------------\n\nNUM_GPUS = torch.cuda.device_count()\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    mode = \"Single GPU Mode\" if torch.cuda.is_available() else \"CPU Mode\"\n    print(f\"[Cell 0] {mode}\")\n\nprint(f\"[Cell 0] Device: {DEVICE} (visible GPUs: {NUM_GPUS})\")\n\n# ------------------------------------------------------------------------------\n# Dataset path (NOTE: CSV is en→bn but model trains bn→en; swap happens in Cell 2)\n# ------------------------------------------------------------------------------\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    \"/kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\"\n)\n\nif not os.path.exists(DATASET_CSV_PATH):\n    print(f\"[WARN] Dataset CSV not found at: {DATASET_CSV_PATH}\")\n    print(\"[WARN] Training will use fallback dataset if file is not accessible\")\nelse:\n    print(f\"[INFO] Dataset CSV found: {DATASET_CSV_PATH}\")\n    if _HAS_PANDAS:\n        try:\n            _test_df = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n            # Relaxed check: Accept if 'src' OR 'tgt' exists, logic handled in Cell 2\n            cols = _test_df.columns.tolist()\n            print(f\"[INFO] CSV validation passed (columns: {cols})\")\n            del _test_df\n        except Exception as e:\n            print(f\"[WARN] Could not validate CSV structure: {e}\")\n\n# ------------------------------------------------------------------------------\n# Core training hyperparameters\n# ------------------------------------------------------------------------------\n\nBATCH_SIZE = 100\nNUM_SAMPLES =30000\nMAX_LENGTH = 48\n\nLR_NMT = 2e-5\nLR_TRG = 1e-5\nLR_PHI = 1e-5\n\nEPOCHS = 1\nGRAD_CLIP_NORM = 1.0\nUSE_AMP = True\nPRINT_INTERVAL = 300\nSEED = 42\n\nACCUMULATION_STEPS = 16\n\n# ------------------------------------------------------------------------------\n# TRG / MC-dropout / data loader / memory\n# ------------------------------------------------------------------------------\n\nMC_DROPOUT_PASSES = 5\nTRG_EVIDENCE_K = 3\nMAX_SILVER_BUFFER = 50\n\nNUM_WORKERS = 2\nPIN_MEMORY = True\nPREFETCH_FACTOR = 2\nGRADIENT_CHECKPOINTING = True\n\n# ------------------------------------------------------------------------------\n# Debug flags (keep DSCD internal logs off by default to avoid spam)\n# ------------------------------------------------------------------------------\n\nDEBUG_DISCOVERY = False\nDEBUG_TIMING = True\nDEBUG_VERBOSE = False\n\n# ------------------------------------------------------------------------------\n# DSCD configuration (more permissive for multi-sense)\n# ------------------------------------------------------------------------------\n\nDSCD_BUFFER_SIZE = 50\nDSCD_MAX_PROTOS = 8\nDSCD_N_MIN = 2                 # allow minority sense clusters\nDSCD_DISPERSION_THRESHOLD = 0.15  # encourage cluster splitting\nDSCD_EMBED_DIM = 1024\nDSCD_TEMPERATURE = 0.7\nDSCD_DROPOUT = 0.1\nDSCD_AUGMENT_SCALE = 0.1\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_WARMUP_SAMPLES = 8000\n\nPERIODIC_DISCOVERY_FREQUENCY = 200\n_MAX_TOKENS_PER_DISCOVERY = 150\n\n# ------------------------------------------------------------------------------\n# ASBN / TRG configuration\n# ------------------------------------------------------------------------------\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = False\n\nENABLE_TRG_TRAINING = True\nENABLE_TRG_INFERENCE = True\n\nCLUSTERING_TIMEOUT = 5\nMEMORY_CLEANUP_FREQUENCY = 100\nVALIDATION_CHECK_INTERVAL = 200\nVERBOSE_LOGGING = False\n\n# ------------------------------------------------------------------------------\n# Checkpoint configuration\n# ------------------------------------------------------------------------------\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_SAVE_AFTER_TRAINING = True\nCHECKPOINT_FILENAME = \"tatn_final.pt\"\nCHECKPOINT_INTERVAL = 99999999\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = 25000\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\n\n# ------------------------------------------------------------------------------\n# TRG uncertainty / span thresholds\n# ------------------------------------------------------------------------------\n\nTAU_LOW = 0.15\nTAU_HIGH = 0.85\nTAU_ACCEPT = 0.8\n\nTRG_MAX_GEN_LEN = 16\nTRG_GEN_EMBED = 64\nTRG_GEN_HID = 64\n\nSPAN_THRESHOLD = 0.20\nUNCERTAINTY_THRESHOLD = 0.25\nTRG_TEMPERATURE = 1.0\n\n# ------------------------------------------------------------------------------\n# ASBN loss weights\n# ------------------------------------------------------------------------------\n\nASBN_HIDDEN_DIM = 64\nASBN_LAMBDA = 0.1\nASBN_DROPOUT = 0.1\n\nLAMBDA_ASBN = 0.05\nLAMBDA_DSCD = 0.15\n\n# ------------------------------------------------------------------------------\n# Domain labels / GRL schedule\n# ------------------------------------------------------------------------------\n\nTRAIN_DOMAIN = 0\nTEST_DOMAIN = 1\nUSE_DOMAIN_LABELS = True\n\nGRL_ALPHA_START = 0.0\nGRL_ALPHA_END = 1.0\nGRL_ALPHA_SCHEDULE = \"linear\"\nGRL_ALPHA_STEPS = (\n    NUM_SAMPLES // (BATCH_SIZE * ACCUMULATION_STEPS) * EPOCHS\n    if BATCH_SIZE * ACCUMULATION_STEPS > 0\n    else 10000\n)\n\n# ------------------------------------------------------------------------------\n# Language configuration\n# ------------------------------------------------------------------------------\n\nSOURCE_LANGUAGE = \"bn\"\nTARGET_LANGUAGE = \"en\"\n\nM2M100_BN_TOKEN_ID = 128025\nM2M100_EN_TOKEN_ID = 128022\n\n# ------------------------------------------------------------------------------\n# Reference homograph list (evaluation only; DSCD unsupervised)\n# ------------------------------------------------------------------------------\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = {\n    \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n    \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    \"দিন\", \"রাত\", \"জল\", \"বাড়ি\", \"পার্ক\", \"নদী\", \"বন\", \"ফুল\", \"গাছ\",\n    \"চোখ\", \"মুখ\", \"পা\", \"কান\", \"গলা\", \"নাক\", \"দাঁত\", \"কোমর\",\n    \"পড়া\", \"দেখা\", \"যাওয়া\", \"আসা\", \"খেলা\", \"লেখা\", \"বলা\", \"শোনা\",\n    \"চলা\", \"ধরা\", \"দেওয়া\", \"নেওয়া\",\n    \"সময়\", \"বছর\", \"মাস\", \"সাল\", \"ঘন্টা\", \"মুহূর্ত\",\n    \"গরম\", \"শীত\", \"বাতাস\", \"আগুন\", \"পাথর\", \"মাটি\",\n    \"ভাব\", \"রং\", \"আলো\", \"ছায়া\", \"শব্দ\", \"অর্থ\",\n}\n\nHOMOGRAPH_WATCHLIST_BN: Set[str] = set()\nHOMOGRAPH_WATCHLIST: Set[str] = set()\nUSE_WATCHLIST_PRIORITIZATION = False\nWATCHLIST_ONLY_FOR_TRG = False\n\n# ------------------------------------------------------------------------------\n# Normalization utilities\n# ------------------------------------------------------------------------------\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t)\n    t = t.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t).lower().strip()\n    return t\n\n# ------------------------------------------------------------------------------\n# CUDA helpers\n# ------------------------------------------------------------------------------\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(\n                    f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\"\n                )\n            except Exception:\n                print(f\"  GPU {i}: memory stats unavailable\")\n    else:\n        print(\"[GPU MONITOR] No CUDA devices available\")\n\n# ------------------------------------------------------------------------------\n# Checkpoint helpers\n# ------------------------------------------------------------------------------\n\ndef get_checkpoint_path() -> str:\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    if (\n        CHECKPOINT_INTERVAL < 99999999\n        and global_step >= CHECKPOINT_INTERVAL\n        and global_step % CHECKPOINT_INTERVAL == 0\n    ):\n        return True\n    return False\n\n# ------------------------------------------------------------------------------\n# Function timeout utility\n# ------------------------------------------------------------------------------\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\n# ------------------------------------------------------------------------------\n# Token utilities for DSCD / tokenizer\n# ------------------------------------------------------------------------------\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({SOURCE_LANGUAGE, TARGET_LANGUAGE})\n    return s\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        min_len = 2\n        if len(clean) < min_len:\n            result = False\n        else:\n            has_bengali_chars = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if \"\\u0980\" <= c <= \"\\u09FF\")\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    bengali_ratio = bengali_count / alphanum_count\n                    result = bengali_ratio >= 0.5\n\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n\n    return result\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    try:\n        encoded = tokenizer(\n            text,\n            return_offsets_mapping=True,\n            max_length=max_length,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        toks = tokenizer.convert_ids_to_tokens(encoded.get(\"input_ids\", []))\n        offsets = encoded.get(\"offset_mapping\", [(0, 0)] * len(toks))\n        return toks, offsets\n    except Exception:\n        return None, None\n\n# ------------------------------------------------------------------------------\n# Discovery timing helper used by DSCD\n# ------------------------------------------------------------------------------\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        return {\n            \"count\": len(self.discovery_times),\n            \"total\": total,\n            \"avg\": total / len(self.discovery_times),\n            \"max\": max(self.discovery_times),\n        }\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer # Alias for older DSCD code\n\n# ------------------------------------------------------------------------------\n# Seeding and CuDNN behaviour\n# ------------------------------------------------------------------------------\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\n# ------------------------------------------------------------------------------\n# Summary printout\n# ------------------------------------------------------------------------------\n\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nif USE_MULTI_GPU:\n    effective_batch *= NUM_GPUS\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TATN CONFIGURATION (Bengali to English)\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs)\")\nprint(f\"Dataset: {DATASET_CSV_PATH}\")\nprint(f\"Samples: {NUM_SAMPLES:,} | Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS} | AMP: {USE_AMP}\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer: {DSCD_BUFFER_SIZE} | n_min: {DSCD_N_MIN} | Max protos: {DSCD_MAX_PROTOS}\")\nprint(f\"  Dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  Periodic discovery: Every {PERIODIC_DISCOVERY_FREQUENCY} steps\")\nprint(f\"  Max tokens per discovery: {_MAX_TOKENS_PER_DISCOVERY}\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  MC Dropout passes: {MC_DROPOUT_PASSES} | TAU_LOW: {TAU_LOW}\")\nprint(f\"  SPAN_THRESHOLD: {SPAN_THRESHOLD} | TAU_HIGH: {TAU_HIGH}\")\nprint(f\"  Temperature: {TRG_TEMPERATURE}\")\nprint()\nprint(\"ASBN / Loss:\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN} | LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint(f\"  Domain labels: {USE_DOMAIN_LABELS} | GRL: {GRL_ALPHA_SCHEDULE}\")\nprint(f\"  GRL steps: {GRL_ALPHA_STEPS}\")\nprint()\nprint(\"Debug Flags:\")\nprint(f\"  Discovery logging: {DEBUG_DISCOVERY}\")\nprint(f\"  Timing monitoring: {DEBUG_TIMING}\")\nprint(f\"  Verbose mode: {DEBUG_VERBOSE}\")\nprint()\nprint(\"Validation:\")\nprint(f\"  Check interval: {VALIDATION_CHECK_INTERVAL} steps\")\nprint()\nprint(\"Language Tokens:\")\nprint(f\"  Bengali (bn): {M2M100_BN_TOKEN_ID}\")\nprint(f\"  English (en): {M2M100_EN_TOKEN_ID}\")\nprint()\nprint(\"Checkpoint:\")\nprint(f\"  Path: {get_checkpoint_path()}\")\nprint(f\"  Save strategy: Final only\")\nprint()\nprint(\"Discovery Mode:\")\nprint(\"  PURE UNSUPERVISED (no watchlist bias)\")\nprint(f\"  Reference list: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words (evaluation only)\")\nprint(\"  Watchlist prioritization: DISABLED\")\nprint(\"=\" * 80)\n\nif not _HAS_PANDAS:\n    print(\"[ERROR] pandas not available - CSV loading will fail!\")\nif not _HAS_M2M_TOKENIZER:\n    print(\"[ERROR] M2M100Tokenizer not available - tokenization will fail!\")\n\ntry:\n    test_file = os.path.join(CHECKPOINT_DIR, \".test_write\")\n    with open(test_file, \"w\") as f:\n        f.write(\"test\")\n    os.remove(test_file)\n    print(f\"Checkpoint directory writable: {CHECKPOINT_DIR}\")\nexcept Exception as e:\n    print(f\"Checkpoint directory not writable: {e}\")\n\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: Configuration loaded successfully\")\nprint(\"=\" * 80)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:37.627944Z","iopub.execute_input":"2026-01-07T14:57:37.628372Z","iopub.status.idle":"2026-01-07T14:57:43.385158Z","shell.execute_reply.started":"2026-01-07T14:57:37.628347Z","shell.execute_reply":"2026-01-07T14:57:43.384382Z"}},"outputs":[{"name":"stdout","text":"[Cell 0] Multi-GPU Mode: 2 GPUs available\n[Cell 0] Device: cuda:0 (visible GPUs: 2)\n[INFO] Dataset CSV found: /kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\n[INFO] CSV validation passed (columns: ['idx', 'src', 'tgt', 'word', 'sense'])\n\n================================================================================\nTATN CONFIGURATION (Bengali to English)\n================================================================================\nUser: manas0003\nDate: 2026-01-07 14:57:43 UTC\nMulti-GPU: ENABLED (2 GPUs)\nDataset: /kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\nSamples: 30,000 | Batch: 100 | Accum: 16\nEffective batch: 3200\nMax length: 48 | Epochs: 1 | AMP: True\n\nDSCD Config:\n  Buffer: 50 | n_min: 2 | Max protos: 8\n  Dispersion threshold: 0.15\n  Periodic discovery: Every 200 steps\n  Max tokens per discovery: 150\n\nTRG & Uncertainty:\n  MC Dropout passes: 5 | TAU_LOW: 0.15\n  SPAN_THRESHOLD: 0.2 | TAU_HIGH: 0.85\n  Temperature: 1.0\n\nASBN / Loss:\n  LAMBDA_ASBN: 0.05 | LAMBDA_DSCD: 0.15\n  Domain labels: True | GRL: linear\n  GRL steps: 18\n\nDebug Flags:\n  Discovery logging: False\n  Timing monitoring: True\n  Verbose mode: False\n\nValidation:\n  Check interval: 200 steps\n\nLanguage Tokens:\n  Bengali (bn): 128025\n  English (en): 128022\n\nCheckpoint:\n  Path: /kaggle/working/tatn_final.pt\n  Save strategy: Final only\n\nDiscovery Mode:\n  PURE UNSUPERVISED (no watchlist bias)\n  Reference list: 65 words (evaluation only)\n  Watchlist prioritization: DISABLED\n================================================================================\nCheckpoint directory writable: /kaggle/working/\n\n[GPU MONITOR] Checking 2 GPU(s):\n  GPU 0: 0.00GB allocated / 0.00GB reserved\n  GPU 1: 0.00GB allocated / 0.00GB reserved\n\n================================================================================\nCell 0: Configuration loaded successfully\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: TOKENIZER UTILITIES (BENGALI-FOCUSED)\n# ===========================================================================================\n\nimport threading\nfrom typing import Tuple, List, Dict, Optional, Set\nimport numpy as np\nimport torch\n\n# ------------------------------------------------------------------------------\n# Safe defaults based on MAX_LENGTH and global flags from Cell 0\n# ------------------------------------------------------------------------------\n\ntry:\n    if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0:\n        SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\n    else:\n        SAFE_OFFSET_MAX_LEN = 48\nexcept (NameError, ValueError, TypeError):\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _DEBUG_VERBOSE = DEBUG_VERBOSE\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = DEBUG_DISCOVERY\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n\n# ------------------------------------------------------------------------------\n# Special-token utilities\n# ------------------------------------------------------------------------------\n\ndef _special_token_cache_key(tokenizer) -> str:\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None)\n    if not name:\n        name = \"unknown_tokenizer\"\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens: Set[str] = set()\n        try:\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"all_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"additional_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\",\n                         \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            try:\n                stm = (\n                    getattr(tokenizer, \"special_tokens_map\", None)\n                    or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                )\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n        except Exception:\n            special_tokens = set()\n\n        special_tokens.update({\n            \"__bn__\", \"__en__\",\n            \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n        })\n\n        try:\n            vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n            special_tokens = {\n                tok\n                for tok in special_tokens\n                if tok in vocab or tok in {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n            }\n        except Exception:\n            pass\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\n# ------------------------------------------------------------------------------\n# Offset mapping normalization for fast / batch encodings\n# ------------------------------------------------------------------------------\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in arr[0]\n                        ]\n                        return enc\n                if isinstance(off, (list, tuple)):\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in off[0]\n                        ]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    try:\n        data = getattr(enc, \"data\", None)\n        if (\n            data\n            and isinstance(data, dict)\n            and \"offset_mapping\" in data\n            and data[\"offset_mapping\"] is not None\n        ):\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [\n                    (x[0], x[1])\n                    if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                    else (None, None)\n                    for x in om[0]\n                ]\n                return enc\n    except Exception:\n        pass\n\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif (\n                isinstance(input_ids, (list, tuple))\n                and len(input_ids) > 0\n                and isinstance(input_ids[0], (list, tuple))\n            ):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\n# ------------------------------------------------------------------------------\n# Safe tokenization with offsets (single sentence)\n# ------------------------------------------------------------------------------\n\ndef safe_offsets_tokenize(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n    include_special_tokens: bool = False,\n) -> dict:\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        if _DEBUG_VERBOSE:\n            print(\"[WARN] Failed to convert input to string, using empty string\")\n        text = \"\"\n\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    if is_fast:\n        try:\n            enc = tokenizer(\n                sample_text,\n                return_offsets_mapping=True,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=False,\n                max_length=eff_max,\n                add_special_tokens=include_special_tokens,\n            )\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            pass\n\n    try:\n        enc = tokenizer(\n            sample_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=eff_max,\n            add_special_tokens=include_special_tokens,\n        )\n    except Exception as e:\n        if _DEBUG_VERBOSE:\n            print(f\"[WARN] Tokenization failed: {e}, returning empty encoding\")\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\n            \"input_ids\": torch.tensor([[pad_id]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n        }\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n# ------------------------------------------------------------------------------\n# Word reconstruction from token offsets (used by DSCD to get surface words)\n# ------------------------------------------------------------------------------\n\ndef reconstruct_word_spans(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n) -> Tuple[Dict[int, Optional[str]], List[str]]:\n    global _LANGUAGE_WARNING_COUNT\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in text)\n    has_english = any(\"a\" <= c.lower() <= \"z\" for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (\n            sum(1 for c in text if \"\\u0980\" <= c <= \"\\u09FF\")\n            / max(1, len(text))\n            * 100.0\n        )\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(\n            f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), \"\n            f\"English: {has_english}\"\n        )\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    try:\n        encoded = safe_offsets_tokenize(\n            tokenizer, text, max_length=eff_max, include_special_tokens=False\n        )\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if isinstance(offsets, list) and len(offsets) > 0 and all(\n        isinstance(x, tuple) for x in offsets\n    ):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(\n        offsets[0], (list, tuple)\n    ):\n        offsets_list = [\n            (x[0], x[1])\n            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n            else (None, None)\n            for x in offsets[0]\n        ]\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, Optional[str]] = {}\n    words: List[str] = []\n\n    used_any_offset = any(\n        isinstance(o, tuple) and o[0] is not None and o[1] is not None\n        for o in offsets_list\n    )\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            if off_start is not None and off_end is not None:\n                if off_start < 0 or off_end < 0:\n                    if _DEBUG_VERBOSE:\n                        print(\n                            f\"[WARN] Negative offset detected: \"\n                            f\"({off_start}, {off_end}), skipping\"\n                        )\n                    off_start, off_end = None, None\n                else:\n                    off_start = max(0, min(off_start, text_len))\n                    off_end = max(off_start, min(off_end, text_len))\n\n            if off_start is None or off_end is None:\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                token_word_map[idx] = None\n                continue\n\n            if tok in special_tokens:\n                token_word_map[idx] = None\n                continue\n\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                if off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            try:\n                current_word = text[word_start:word_end].strip()\n                token_word_map[idx] = current_word if current_word else None\n            except Exception:\n                token_word_map[idx] = None\n\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    running_word = \"\"\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            token_word_map[i] = None\n            continue\n        clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            token_word_map[i] = None\n            continue\n\n        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n            current_parts = [clean]\n            running_word = clean\n        else:\n            current_parts.append(clean)\n            running_word = \"\".join(current_parts)\n            if len(running_word) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                current_parts = [clean]\n                running_word = clean\n\n        token_word_map[i] = running_word if running_word else None\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n\n        if tokens and word_list:\n            word_idx = 0\n\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    token_word_map[i] = None\n                    continue\n\n                if word_idx < len(word_list):\n                    current_word = word_list[word_idx]\n                    if clean in current_word or current_word.startswith(clean):\n                        token_word_map[i] = current_word\n                    else:\n                        word_idx = min(word_idx + 1, len(word_list) - 1)\n                        token_word_map[i] = word_list[word_idx]\n                else:\n                    token_word_map[i] = word_list[-1] if word_list else None\n\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\n# ------------------------------------------------------------------------------\n# Quick self-test helper\n# ------------------------------------------------------------------------------\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    sample_bn = \"কাল আমি বাজারে যাব।\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        enc_bn = safe_offsets_tokenize(\n            tokenizer, sample_bn, max_length=32, include_special_tokens=False\n        )\n        enc_len = (\n            int(enc_bn[\"input_ids\"].shape[-1])\n            if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn\n            else \"N/A\"\n        )\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(\n            any(\"\\u0980\" <= c <= \"\\u09FF\" for c in w) for w in words_bn\n        )\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(\n            any(\"a\" <= c.lower() <= \"z\" for c in w) for w in words_en\n        )\n        print(f\"  Contains English words: {has_english_words}\")\n\n        if has_bengali_words and not any(\n            \"a\" <= c.lower() <= \"z\" for c in \"\".join(words_bn)\n        ):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\n# ------------------------------------------------------------------------------\n# Backwards-compatible aliases for other cells (critical for DSCD & data loader)\n# ------------------------------------------------------------------------------\n\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\nisvalidtoken = is_valid_token = lambda token, special_tokens=None, tokenizer=None, language=\"bn\": \\\n    is_valid_token(token, special_tokens, tokenizer, language)  # type: ignore[name-defined]\n\nprint(\"Cell 1: Tokenizer utilities loaded\")","metadata":{"id":"WZE9PkHyH4J1","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:43.386308Z","iopub.execute_input":"2026-01-07T14:57:43.386631Z","iopub.status.idle":"2026-01-07T14:57:43.440210Z","shell.execute_reply.started":"2026-01-07T14:57:43.386609Z","shell.execute_reply":"2026-01-07T14:57:43.439451Z"}},"outputs":[{"name":"stdout","text":"Cell 1: Tokenizer utilities loaded\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (BENGALI → ENGLISH TASK)\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n    print(\"[CELL2] WARNING: SOURCE_LANGUAGE/TARGET_LANGUAGE not defined, using defaults bn/en\")\n\ntry:\n    _M2M_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\n    _M2M_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept NameError:\n    _M2M_BN_TOKEN_ID = 128025\n    _M2M_EN_TOKEN_ID = 128022\n    print(\"[CELL2] WARNING: M2M100 token IDs not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\n    _USE_DOMAIN_LABELS = bool(USE_DOMAIN_LABELS)\nexcept NameError:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = False\n    print(\"[CELL2] WARNING: Domain label config not found, disabling domain labels\")\n\n_has_normalize = (\"normalize_bengali\" in globals()) and (\"normalize_english\" in globals())\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n_BENGALI_CHAR_RE = re.compile(r\"[\\u0980-\\u09FF]\")\n\ndef is_bengali_text(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    try:\n        if dataset is not None and hasattr(dataset, \"_tokenizer_name_or_path\") and dataset._tokenizer_name_or_path:\n            try:\n                from transformers import M2M100Tokenizer\n                dataset.tokenizer = M2M100Tokenizer.from_pretrained(dataset._tokenizer_name_or_path)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n                if DEBUG_CELL2:\n                    print(f\"[CELL2-WORKER-{worker_id}] Tokenizer reloaded successfully\")\n            except Exception as e:\n                cell2_dbg(\"worker_tokenizer_reload\", f\"Worker {worker_id} tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] Tokenizer rebind failed in worker {worker_id}\")\n\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\ndef load_and_preprocess_optimized(\n    num_samples: Optional[int] = None,\n    split: str = \"train\",\n) -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    try:\n        print(\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        if df.empty:\n            print(\"[CELL2] ERROR: CSV file is empty\")\n            return _get_fallback_dataset()\n\n        if \"src\" not in df.columns or \"tgt\" not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing required columns. Found columns: {list(df.columns)}\")\n            print(\"[CELL2] Expected format: src (Bengali), tgt (English) OR src (English), tgt (Bengali)\")\n            return _get_fallback_dataset()\n\n        sample_src = str(df[\"src\"].iloc[0]) if len(df) > 0 else \"\"\n        sample_tgt = str(df[\"tgt\"].iloc[0]) if len(df) > 0 else \"\"\n\n        src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n        tgt_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_tgt))\n        src_is_english = bool(re.search(r\"[a-zA-Z]\", sample_src)) and not src_is_bengali\n        tgt_is_english = bool(re.search(r\"[a-zA-Z]\", sample_tgt)) and not tgt_is_bengali\n\n        if src_is_english and tgt_is_bengali:\n            print(\"[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn→en task.\")\n            df = df.rename(columns={\"src\": \"_temp_tgt\", \"tgt\": \"_temp_src\"})\n            df = df.rename(columns={\"_temp_src\": \"src\", \"_temp_tgt\": \"tgt\"})\n            sample_src = str(df[\"src\"].iloc[0]) if len(df) > 0 else \"\"\n            sample_tgt = str(df[\"tgt\"].iloc[0]) if len(df) > 0 else \"\"\n            src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n            tgt_is_english = bool(re.search(r\"[a-zA-Z]\", sample_tgt)) and not bool(_BENGALI_CHAR_RE.search(sample_tgt))\n            if not src_is_bengali or not tgt_is_english:\n                print(\"[CELL2] ERROR: Swap failed, after swap src is not Bengali or tgt is not English.\")\n                return _get_fallback_dataset()\n            else:\n                print(\"[CELL2] Swap successful: src=Bengali, tgt=English\")\n        elif not src_is_bengali or not tgt_is_english:\n            print(\"[CELL2] WARNING: After column check, src not Bengali or tgt not English. Proceeding but output may be incorrect.\")\n\n        df = df.head(num_samples)\n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n\n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n\n        for row_tuple in tqdm(df.itertuples(index=False), total=len(df), desc=\"Loading dataset\"):\n            try:\n                src_val = row_tuple.src\n                tgt_val = row_tuple.tgt\n                if pd.isna(src_val) or pd.isna(tgt_val):\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", \"NaN value detected\")\n                    continue\n                bn = str(src_val).strip()\n                en = str(tgt_val).strip()\n                if not bn or not en:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", \"Empty src/tgt field\")\n                    continue\n                if not is_bengali_text(bn):\n                    skipped += 1\n                    cell2_dbg(\"not_bengali_src\", \"src field not Bengali\")\n                    continue\n                if not re.search(r\"[a-zA-Z]\", en):\n                    skipped += 1\n                    cell2_dbg(\"not_english_tgt\", \"tgt field not English\")\n                    continue\n                max_words = max(20, _MAX_LENGTH // 2)\n                if len(bn.split()) > max_words or len(en.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", \"Text too long\")\n                    continue\n                if _has_normalize:\n                    bn_norm = normalize_bengali(bn)\n                    en_norm = normalize_english(en)\n                else:\n                    bn_norm = bn.strip()\n                    en_norm = en.lower().strip()\n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", \"Empty after normalization\")\n                    continue\n                pairs.append((bn_norm, en_norm))\n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception: {type(e).__name__}\")\n                continue\n\n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            print(\"[CELL2] Check that src column contains Bengali and tgt column contains English.\")\n            return _get_fallback_dataset()\n\n        return pairs\n\n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    print(\"[CELL2] Using fallback dataset (50 unique samples)\")\n    fallback_pairs = [\n        (\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\"),\n        (\"সে আমাকে পরে কল করবে।\", \"he will call me later.\"),\n        (\"আমরা প্রতিদিন তাজা ফল খাই।\", \"we eat fresh fruits every day.\"),\n        (\"তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\", \"his hard work has brought good results.\"),\n        (\"গাছে নতুন পাতাগুলো গজিয়েছে।\", \"new leaves have sprouted on the tree.\"),\n        (\"আমি বইয়ের পাতা উল্টাচ্ছি।\", \"i am turning the pages of the book.\"),\n        (\"কাল আমি বাজারে গিয়েছিলাম।\", \"yesterday i went to the market.\"),\n        (\"কাল আমি তোমার সাথে দেখা করব।\", \"tomorrow i will meet you.\"),\n        (\"তারা আকাশে উজ্জ্বল।\", \"the stars are bright in the sky.\"),\n        (\"তারা বাড়িতে নেই।\", \"they are not at home.\"),\n        (\"ব্যাংক নদীর ধারে ভেঙে গেছে।\", \"the bank by the river has collapsed.\"),\n        (\"আমি ব্যাংকে টাকা জমা দিয়েছি।\", \"i deposited money in the bank.\"),\n        (\"বার বার চেষ্টা করতে হবে।\", \"you have to try again and again.\"),\n        (\"আমি বার খুলে ভিতরে ঢুকলাম।\", \"i opened the bar and entered.\"),\n        (\"তার মাথা ব্যথা করছে।\", \"his head is hurting.\"),\n        (\"আমি মাথা নেড়ে সম্মতি দিলাম।\", \"i nodded my head in agreement.\"),\n        (\"সে হার মেনে নিয়েছে।\", \"he accepted defeat.\"),\n        (\"আমি গলায় সোনার হার পরেছি।\", \"i am wearing a gold necklace.\"),\n        (\"পানি খুব ঠান্ডা।\", \"the water is very cold.\"),\n        (\"আমি পানি খাচ্ছি।\", \"i am drinking water.\"),\n        (\"দল খেলায় জিতেছে।\", \"the team won the game.\"),\n        (\"আমি মাটি দল দিয়ে ফেললাম।\", \"i trampled the soil.\"),\n        (\"বাজার থেকে সবজি কিনলাম।\", \"i bought vegetables from the market.\"),\n        (\"বাজার অনেক ভিড় ছিল।\", \"the market was very crowded.\"),\n        (\"তার নাম আহমেদ।\", \"his name is ahmed.\"),\n        (\"নাম না করে কাজ করো।\", \"work without making a name.\"),\n        (\"কথা বলা বন্ধ করো।\", \"stop talking.\"),\n        (\"তার কথা শুনে ভালো লাগল।\", \"i felt good hearing his words.\"),\n        (\"বই পড়তে ভালো লাগে।\", \"i like reading books.\"),\n        (\"আমি একটি নতুন বই কিনেছি।\", \"i bought a new book.\"),\n        (\"ঘর পরিষ্কার করা হয়েছে।\", \"the house has been cleaned.\"),\n        (\"আমি ঘরে বসে আছি।\", \"i am sitting at home.\"),\n        (\"মন ভালো নেই।\", \"my mind is not good.\"),\n        (\"আমার মন চায় বেড়াতে যেতে।\", \"my mind wants to go for a walk.\"),\n        (\"হাত ধুয়ে নাও।\", \"wash your hands.\"),\n        (\"আমি তার হাত ধরলাম।\", \"i held his hand.\"),\n        (\"দিন কেটে যাচ্ছে।\", \"the day is passing by.\"),\n        (\"আজ কি দিন?\", \"what day is today?\"),\n        (\"রাত হয়ে এসেছে।\", \"night has come.\"),\n        (\"আমি রাত জেগে পড়েছি।\", \"i studied staying up at night.\"),\n        (\"জল খুব গরম।\", \"the water is very hot.\"),\n        (\"আমি জল দিয়ে গাছ সিঞ্চন করেছি।\", \"i watered the plants.\"),\n        (\"বাড়ি যাচ্ছি।\", \"i am going home.\"),\n        (\"আমার বাড়ি ঢাকায়।\", \"my house is in dhaka.\"),\n        (\"পার্কে অনেক মানুষ।\", \"there are many people in the park.\"),\n        (\"আমি প্রতিদিন পার্কে হাঁটি।\", \"i walk in the park every day.\"),\n        (\"নদী বইছে।\", \"the river is flowing.\"),\n        (\"আমি নদীর ধারে দাঁড়িয়ে আছি।\", \"i am standing by the river.\"),\n        (\"বন খুব সুন্দর।\", \"the forest is very beautiful.\"),\n        (\"আমি বন দেখতে গিয়েছিলাম।\", \"i went to see the forest.\"),\n    ]\n    if _has_normalize:\n        return [\n            (normalize_bengali(bn), normalize_english(en))\n            for bn, en in fallback_pairs\n        ]\n    else:\n        return [(bn.strip(), en.lower().strip()) for bn, en in fallback_pairs]\n\nclass MemoryEfficientDataset(Dataset):\n    def __init__(\n        self,\n        pairs: List[Tuple[str, str]],\n        tokenizer: Any = None,\n        max_length: Optional[int] = None,\n        split: str = \"train\",\n    ):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        self.split = split\n\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n\n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid\")\n\n        try:\n            if \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {\n                f\"__{_SOURCE_LANG}__\",\n                f\"__{_TARGET_LANG}__\",\n                \"</s>\",\n                \"<pad>\",\n                \"<s>\",\n                \"<unk>\",\n            }\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                try:\n                    if isinstance(enc[\"input_ids\"], torch.Tensor):\n                        input_ids = enc[\"input_ids\"].squeeze(0)\n                    else:\n                        input_ids = torch.tensor(enc[\"input_ids\"][0])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0])\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids))\n                if isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=False,\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception as e:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {e}\")\n\n            if not token_word_map and tokens:\n                try:\n                    word_buffer: List[Tuple[int, str]] = []\n                    \n                    for idx, tok in enumerate(tokens):\n                        if not isinstance(tok, str) or tok in self.special_tokens:\n                            continue\n                        \n                        clean = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                        if not clean:\n                            continue\n                        \n                        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n                            if word_buffer:\n                                full_word = \"\".join([part for _, part in word_buffer])\n                                for w_idx, _ in word_buffer:\n                                    token_word_map[w_idx] = full_word\n                            word_buffer = [(idx, clean)]\n                        else:\n                            word_buffer.append((idx, clean))\n                    \n                    if word_buffer:\n                        full_word = \"\".join([part for _, part in word_buffer])\n                        for w_idx, _ in word_buffer:\n                            token_word_map[w_idx] = full_word\n                            \n                except Exception as e:\n                    cell2_dbg(\"fallback_wm\", f\"Fallback word map failed: {e}\")\n\n            return input_ids, attention_mask, tokens, token_word_map\n\n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=False,\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            labels[labels == int(pad_id)] = -100\n            return labels\n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"আমি\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception:\n            pad_id = 1\n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label,\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx}\")\n                return self._make_safe_sample(\"oob\")\n\n            src, tgt = self.pairs[idx]\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            if DEBUG_CELL2 and idx < 3:\n                has_bengali = is_bengali_text(src)\n                has_english = any(\"a\" <= c.lower() <= \"z\" for c in src)\n                print(f\"[CELL2-GETITEM-{idx}] src sample: {src[:50]}\")\n                print(f\"[CELL2-GETITEM-{idx}] Bengali: {has_bengali}, English: {has_english}\")\n                if not has_bengali:\n                    print(f\"[CELL2] WARNING: src_text is NOT Bengali at idx={idx}!\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    t = tensor.view(-1).long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n    inputs, masks, labs, twmaps, srcs, toks, domains = [], [], [], [], [], [], []\n\n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", _TRAIN_DOMAIN)\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(domain)\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.full((len(inputs),), _TRAIN_DOMAIN, dtype=torch.long)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels,\n    }\n\ndef create_optimized_dataloader(\n    dataset: Dataset,\n    batch_size: Optional[int] = None,\n    shuffle: bool = True,\n    split: str = \"train\",\n) -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n\n    batch_size = int(batch_size)\n    original_batch_size = batch_size\n    adjusted = False\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        new_batch_size = (batch_size // _NUM_GPUS) * _NUM_GPUS\n        if new_batch_size == 0:\n            if DEBUG_CELL2:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original.\")\n        else:\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n\n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size {original_batch_size} to {batch_size} (DP-divisible, GPUs={_NUM_GPUS})\")\n\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\ndef aggregate_subwords_to_words(\n    subword_embeds: torch.Tensor,\n    tokens: List[str],\n    device: torch.device\n) -> Tuple[torch.Tensor, List[List[int]], List[str]]:\n    word_boundaries = []\n    word_strings = []\n    current_word_indices = []\n    current_word_parts = []\n    \n    for i, tok in enumerate(tokens):\n        if tok in {'__bn__', '__en__', '<s>', '</s>', '<pad>'}:\n            continue\n        \n        if tok.startswith('▁') or tok.startswith('Ġ'):\n            if current_word_indices:\n                word_boundaries.append(current_word_indices)\n                word_str = ''.join(current_word_parts)\n                word_strings.append(word_str)\n            \n            clean_tok = tok.replace('▁', '').replace('Ġ', '').strip()\n            current_word_indices = [i]\n            current_word_parts = [clean_tok]\n        else:\n            clean_tok = tok.replace('▁', '').replace('Ġ', '').strip()\n            if current_word_indices:\n                current_word_indices.append(i)\n                current_word_parts.append(clean_tok)\n            else:\n                current_word_indices = [i]\n                current_word_parts = [clean_tok]\n    \n    if current_word_indices:\n        word_boundaries.append(current_word_indices)\n        word_str = ''.join(current_word_parts)\n        word_strings.append(word_str)\n    \n    word_embeds_list = []\n    for indices in word_boundaries:\n        word_embed = subword_embeds[indices].mean(dim=0)\n        word_embeds_list.append(word_embed)\n    \n    if word_embeds_list:\n        word_embeds = torch.stack(word_embeds_list, dim=0)\n    else:\n        word_embeds = torch.zeros(1, subword_embeds.shape[-1], device=device)\n        word_boundaries = [[0]]\n        word_strings = ['']\n    \n    return word_embeds, word_boundaries, word_strings\n\ndef broadcast_word_to_subword(\n    word_outputs: Any,\n    word_boundaries: List[List[int]],\n    subword_len: int,\n    device: torch.device\n) -> Any:\n    if isinstance(word_outputs, list):\n        subword_out = [None] * subword_len\n        for word_idx, indices in enumerate(word_boundaries):\n            if word_idx < len(word_outputs):\n                word_val = word_outputs[word_idx]\n                for idx in indices:\n                    if idx < subword_len:\n                        subword_out[idx] = word_val\n        return subword_out\n    \n    elif isinstance(word_outputs, torch.Tensor):\n        subword_out = torch.zeros(subword_len, *word_outputs.shape[1:], device=device)\n        for word_idx, indices in enumerate(word_boundaries):\n            if word_idx < word_outputs.shape[0]:\n                word_val = word_outputs[word_idx]\n                for idx in indices:\n                    if idx < subword_len:\n                        subword_out[idx] = word_val\n        return subword_out\n    \n    return word_outputs\n\nprint(\"Cell 2: Memory-efficient data loading ready\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:43.441538Z","iopub.execute_input":"2026-01-07T14:57:43.442202Z","iopub.status.idle":"2026-01-07T14:57:43.525279Z","shell.execute_reply.started":"2026-01-07T14:57:43.442179Z","shell.execute_reply":"2026-01-07T14:57:43.524602Z"}},"outputs":[{"name":"stdout","text":"Cell 2: Memory-efficient data loading ready\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE (PURE UNSUPERVISED DISCOVERY) - FAST INFERENCE MODE ADDED\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\n\nPRINTINTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HASCLUSTERING = True\nexcept Exception:\n    HASCLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available\")\n\ntry:\n    from sklearn.cluster import KMeans\n    HASKMEANS = True\nexcept Exception:\n    HASKMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available\")\n\ntry:\n    DSCDMAXPROTOS = int(DSCD_MAX_PROTOS)\n    DSCDBUFFERSIZE = int(DSCD_BUFFER_SIZE)\n    DSCDNMIN = max(3, int(DSCD_N_MIN))\n    DSCDDISPERSIONTHRESHOLD = min(0.08, float(DSCD_DISPERSION_THRESHOLD))\n    VERBOSELOGGING = bool(VERBOSE_LOGGING)\n    DSCDENABLETRAININGCLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\nexcept (NameError, ValueError, TypeError):\n    DSCDMAXPROTOS = 8\n    DSCDBUFFERSIZE = 50\n    DSCDNMIN = 3\n    DSCDDISPERSIONTHRESHOLD = 0.08\n    VERBOSELOGGING = True\n    DSCDENABLETRAININGCLUSTERING = True\n    print(\"[CELL3] WARNING: Using default DSCD config\")\n\ntry:\n    DEBUGDISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    DEBUGDISCOVERY = False\n\ntry:\n    MAXTOKENSPERDISCOVERY = int(globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150))\nexcept Exception:\n    MAXTOKENSPERDISCOVERY = 150\n\ntry:\n    DSCDNEWSENSELAMBDA = float(globals().get(\"DSCD_NEW_SENSE_LAMBDA\", 1.5))\nexcept Exception:\n    DSCDNEWSENSELAMBDA = 1.5\n\ntry:\n    HOMOGRAPHREFERENCELISTBN = set(HOMOGRAPH_REFERENCE_LIST_BN)\n    print(f\"[CELL3] Loaded reference list for evaluation: {len(HOMOGRAPHREFERENCELISTBN)} words\")\nexcept (NameError, TypeError):\n    HOMOGRAPHREFERENCELISTBN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"}\n    print(\"[CELL3] Using default reference list\")\n\nDSCDMAXCLUSTERINGPOINTS = 500\nPUNCTSET = set(list(\".,!?-;:\"))\n\ndef normalizetokenkey(token: str) -> str:\n    token = \"\" if token is None else str(token)\n    token = unicodedata.normalize(\"NFKC\", token)\n    return token.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip().lower()\n\ndef iswordtoken(token: str, minletters: int = 2, minletterfraction: float = 0.6) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if token == \"\":\n        return False\n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith(\"L\"):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n    if total == 0:\n        return False\n    if letters < minletters:\n        return False\n    if letters / total < minletterfraction:\n        return False\n    return True\n\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embeddim: int, maxprotos: Optional[int] = None):\n        if maxprotos is None:\n            maxprotos = DSCDMAXPROTOS\n        self.embeddim = embeddim\n        self.maxprotos = int(maxprotos)\n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creationtime: List[float] = []\n        self.distances: List[float] = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n\n    def addprototype(self, vector: torch.Tensor, currenttime: Optional[float] = None, count: int = 1) -> None:\n        if currenttime is None:\n            currenttime = time.time()\n        v = vector.detach().cpu().clone()\n        if len(self.centroids) < self.maxprotos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creationtime.append(float(currenttime))\n        else:\n            minidx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            self.centroids[minidx] = v\n            self.counts[minidx] = int(count)\n            self.creationtime[minidx] = float(currenttime)\n\n    def updateprototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignmentdistance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.addprototype(vector, time.time(), count=1)\n            return\n        oldcentroid = self.centroids[idx]\n        newvector = vector.detach().cpu()\n        self.centroids[idx] = (1.0 - eta) * oldcentroid + eta * newvector\n        self.counts[idx] = int(self.counts[idx] + 1)\n        if assignmentdistance is not None:\n            self.updaterollingstats(float(assignmentdistance))\n\n    def updaterollingstats(self, d: float) -> None:\n        if not self.distances:\n            self.mu = float(d)\n            self.tau = 1e-6\n            self.distances.append(float(d))\n            return\n        prevmu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prevmu)\n        self.distances.append(float(d))\n        if len(self.distances) > 50:\n            self.distances.pop(0)\n\n    def getadaptivethreshold(self, lam: float = 1.0) -> float:\n        return float(self.mu + lam * self.tau)\n\n    def size(self) -> int:\n        return len(self.centroids)\n\n    def ensureconsistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            self.counts = self.counts[:n] if len(self.counts) > n else self.counts + [1] * (n - len(self.counts))\n        if len(self.creationtime) != n:\n            self.creationtime = self.creationtime[:n] if len(self.creationtime) > n else self.creationtime + [time.time()] * (n - len(self.creationtime))\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(\n        self,\n        embeddim: int,\n        tokenizer=None,\n        buffersize: Optional[int] = None,\n        maxprotos: Optional[int] = None,\n        nmin: Optional[int] = None,\n        dispersionthreshold: Optional[float] = None,\n        language: str = \"bn\",\n        enabletrainingclustering: Optional[bool] = None,\n        maxclusteringpoints: Optional[int] = None,\n        maxcandidatesperstep: int = 2,\n        dscdminletters: int = 2,\n        dscdminletterfraction: float = 0.6,\n    ):\n        super().__init__()\n        if buffersize is None:\n            buffersize = DSCDBUFFERSIZE\n        if maxprotos is None:\n            maxprotos = DSCDMAXPROTOS\n        if nmin is None:\n            nmin = DSCDNMIN\n        if dispersionthreshold is None:\n            dispersionthreshold = DSCDDISPERSIONTHRESHOLD\n        if maxclusteringpoints is None:\n            maxclusteringpoints = DSCDMAXCLUSTERINGPOINTS\n        if enabletrainingclustering is None:\n            enabletrainingclustering = DSCDENABLETRAININGCLUSTERING\n\n        self.embeddim = int(embeddim)\n        self.buffersize = int(buffersize)\n        self.maxprotos = int(maxprotos)\n        self.nmin = max(3, int(nmin))\n        self.dispersionthreshold = min(0.08, float(dispersionthreshold))\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscdminletters = int(dscdminletters)\n        self.dscdminletterfraction = float(dscdminletterfraction)\n\n        try:\n            if tokenizer is not None and \"get_special_tokens\" in globals():\n                self.specialtokens = get_special_tokens(tokenizer)\n            else:\n                self.specialtokens = set(getattr(tokenizer, \"all_special_tokens\", [])) if tokenizer is not None else set()\n        except Exception:\n            self.specialtokens = set()\n\n        self.dscdallowedtokens: Set[str] = set()\n        self.dscdignoredtokens: Set[str] = set()\n        self.dscdcachemaxsize = 10000\n\n        self.prototypestores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.buffers: Dict[str, deque] = {}\n        self.discoveredlog: List[Dict[str, Any]] = []\n        self.discoveredhomographs: Set[str] = set()\n\n        self.lastperiodiccheck = 0\n        self.cleanupcounter = 0\n\n        self.dispersioncache: Dict[str, float] = {}\n        self.dispersionlastupdated: Dict[str, float] = {}\n        self.dispersionlock = threading.Lock()\n\n        self.clusteringlock = threading.Lock()\n        self.bufferlock = threading.Lock()\n\n        from collections import deque as threaddeque\n        self.activethreads = threaddeque(maxlen=100)\n        self.threadlock = threading.Lock()\n\n        self.lastclustertime: Dict[str, float] = {}\n        self.clustercooldownseconds = 5.0\n        self.enabletrainingclustering = bool(enabletrainingclustering)\n\n        self.discoverycount = 0\n        self.discoverytimes: List[float] = []\n        self.clusteredtokens: Set[str] = set()\n        self.clusterstats: Dict[str, Dict[str, Any]] = {}\n\n        self.spanhead = nn.Sequential(\n            nn.Linear(self.embeddim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1),\n        )\n        self.sigmanet = nn.Sequential(\n            nn.Linear(self.embeddim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1),\n        )\n\n        self.gatew = nn.Parameter(torch.tensor(1.0))\n        self.gateb = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n\n        self.maxclusteringpoints = int(maxclusteringpoints)\n        self.maxcandidatesperstep = int(maxcandidatesperstep)\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n        state = super().state_dict(destination, prefix, keep_vars)\n        plainstores = {}\n        for token, store in self.prototypestores.items():\n            plainstores[token] = {\n                \"centroids\": [c.cpu().numpy() for c in store.centroids] if hasattr(store, \"centroids\") else [],\n                \"counts\": list(store.counts) if hasattr(store, \"counts\") else [],\n                \"creationtime\": list(store.creationtime) if hasattr(store, \"creationtime\") else [],\n                \"mu\": float(store.mu) if hasattr(store, \"mu\") else 0.0,\n                \"tau\": float(store.tau) if hasattr(store, \"tau\") else 0.0,\n                \"size\": int(store.size()) if hasattr(store, \"size\") else 0,\n            }\n        state[prefix + \"prototypestores\"] = plainstores\n        state[prefix + \"discoveredhomographs\"] = list(self.discoveredhomographs)\n        return state\n\n    def load_state_dict(self, state_dict, strict=True):\n        plainstores = state_dict.pop(\"prototypestores\", state_dict.pop(\"prototypestoresdata\", None))\n        discovered = state_dict.pop(\"discoveredhomographs\", [])\n        super().load_state_dict(state_dict, strict=strict)\n\n        if not plainstores:\n            print(\"[DSCD WARNING] Empty prototypestores in checkpoint\")\n            return\n\n        self.prototypestores = {}\n        self.discoveredhomographs = set(discovered)\n\n        for token, storedict in plainstores.items():\n            store = MemoryEfficientPrototypeStore(embeddim=self.embeddim, maxprotos=self.maxprotos)\n            centroidsdata = storedict.get(\"centroids\", [])\n            for c in centroidsdata:\n                if isinstance(c, torch.Tensor):\n                    store.centroids.append(c)\n                else:\n                    store.centroids.append(torch.tensor(c))\n            store.counts = storedict.get(\"counts\", [])\n            store.creationtime = storedict.get(\"creationtime\", [])\n            store.mu = storedict.get(\"mu\", 0.0)\n            store.tau = storedict.get(\"tau\", 0.0)\n            store.ensureconsistency()\n            self.prototypestores[token] = store\n\n        print(f\"[DSCD] Loaded {len(self.prototypestores)} tokens, {sum(s.size() for s in self.prototypestores.values())} prototypes\")\n\n    @staticmethod\n    def clean_token(token):\n        token = str(token)\n        token = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\")\n        for punct in [\"।\", \".\", \",\", \"!\", \"?\", \":\", \";\", \"-\"]:\n            token = token.replace(punct, \"\")\n        return token.strip()\n\n    def isvalidmultisense(self, token):\n        if token not in self.prototypestores:\n            return False\n        store = self.prototypestores[token]\n        totaloccurrences = sum(store.counts) if hasattr(store, \"counts\") else 0\n        minperproto = min(store.counts) if hasattr(store, \"counts\") and store.counts else 0\n        return store.size() >= 2 and totaloccurrences >= 10 and minperproto >= 2\n\n    def ismultisensestore(self, store: MemoryEfficientPrototypeStore) -> bool:\n        k = store.size()\n        if k < 2:\n            return False\n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(3, self.nmin))\n        if strong < 2:\n            return False\n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n            if len(cents) < 2:\n                return False\n            cents = np.stack(cents, axis=0)\n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            tri = dists[np.triu_indices(len(cents), k=1)]\n            if tri.size == 0:\n                return False\n            mindist = float(tri.min())\n            base = max(store.tau, 1e-3)\n            return mindist > base * DSCDNEWSENSELAMBDA\n        except Exception:\n            return True\n\n    def getdispersion(self, tokentype: str) -> float:\n        with self.dispersionlock:\n            if tokentype in self.dispersioncache:\n                try:\n                    lastupdate = self.dispersionlastupdated.get(tokentype, 0.0)\n                    if time.time() - lastupdate < 3600:\n                        return self.dispersioncache[tokentype]\n                except Exception:\n                    pass\n\n        with self.bufferlock:\n            if tokentype not in self.buffers or len(self.buffers[tokentype]) < 2:\n                return 0.0\n\n        try:\n            embeddings: List[np.ndarray] = []\n            for emb in self.buffers[tokentype]:\n                try:\n                    if isinstance(emb, torch.Tensor):\n                        embeddings.append(emb.cpu().numpy())\n                    else:\n                        embeddings.append(np.asarray(emb, dtype=np.float32))\n                except Exception:\n                    continue\n            if len(embeddings) < 2:\n                return 0.0\n            embeddingsnp = np.stack(embeddings, axis=0)\n            centroid = embeddingsnp.mean(axis=0)\n            distances = np.linalg.norm(embeddingsnp - centroid[None, :], axis=1)\n            dispersion = float(distances.std())\n            with self.dispersionlock:\n                self.dispersioncache[tokentype] = dispersion\n                self.dispersionlastupdated[tokentype] = time.time()\n            return dispersion\n        except Exception:\n            return 0.0\n\n    def shouldtracktoken(self, tokentext: str) -> bool:\n        if not tokentext or not isinstance(tokentext, str):\n            return False\n\n        if len(self.dscdallowedtokens) > self.dscdcachemaxsize:\n            self.dscdallowedtokens.clear()\n        if len(self.dscdignoredtokens) > self.dscdcachemaxsize:\n            self.dscdignoredtokens.clear()\n\n        if tokentext in self.dscdallowedtokens:\n            return True\n        if tokentext in self.dscdignoredtokens:\n            return False\n\n        if not getattr(self, \"training\", False):\n            if tokentext in self.prototypestores:\n                self.dscdallowedtokens.add(tokentext)\n                return True\n            clean = tokentext.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n            clean = unicodedata.normalize(\"NFKC\", clean)\n            if clean and clean in self.prototypestores:\n                self.dscdallowedtokens.add(tokentext)\n                return True\n\n        if tokentext in self.specialtokens:\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        clean = tokentext.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n        clean = unicodedata.normalize(\"NFKC\", clean)\n\n        if not clean:\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        if len(clean) < 2:\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        if not any(c.isalpha() for c in clean):\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        if clean.isdigit():\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        if all(c in PUNCTSET for c in clean):\n            self.dscdignoredtokens.add(tokentext)\n            return False\n\n        try:\n            bengaliblock = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if bengaliblock:\n                if len(clean) >= 2:\n                    self.dscdallowedtokens.add(tokentext)\n                    return True\n                self.dscdignoredtokens.add(tokentext)\n                return False\n        except Exception:\n            pass\n\n        if iswordtoken(clean, minletters=self.dscdminletters, minletterfraction=self.dscdminletterfraction):\n            self.dscdallowedtokens.add(tokentext)\n            return True\n\n        self.dscdignoredtokens.add(tokentext)\n        return False\n\n    def canonicaltokenkey(self, rawtoken: str, tokenwordmap: Optional[Dict[int, Optional[str]]], idx: int) -> Optional[str]:\n        canonical: Optional[str] = None\n        try:\n            if tokenwordmap and isinstance(tokenwordmap, dict) and idx in tokenwordmap and tokenwordmap[idx]:\n                canonical = str(tokenwordmap[idx]).strip()\n        except Exception:\n            canonical = None\n\n        if canonical:\n            canonical = unicodedata.normalize(\"NFKC\", canonical).strip().lower()\n            if canonical:\n                return canonical\n\n        cleaned = self.clean_token(rawtoken)\n        cleaned = unicodedata.normalize(\"NFKC\", cleaned)\n        cleaned = cleaned.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip().lower()\n        if cleaned:\n            return cleaned\n\n        return None\n\n    def cleanupthreads(self) -> None:\n        try:\n            with self.threadlock:\n                alive = [th for th in list(self.activethreads) if th.is_alive()]\n                self.activethreads.clear()\n                self.activethreads.extend(alive)\n        except Exception:\n            pass\n\n    def cleanupmemory(self) -> None:\n        try:\n            for tokentype, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffersize * 1.5):\n                    while len(buffer) > self.buffersize:\n                        buffer.popleft()\n            try:\n                now = time.time()\n                expired = [k for k, v in self.dispersionlastupdated.items() if now - v > 3600]\n                for k in expired:\n                    self.dispersioncache.pop(k, None)\n                    self.dispersionlastupdated.pop(k, None)\n            except Exception:\n                pass\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    def forward(\n        self,\n        tokenembeddings,\n        tokentypes=None,\n        trainmode: bool = True,\n        tokenwordmap=None,\n        hall=None,\n        inputids=None,\n        attentionmask=None,\n        fast_inference: bool = False,\n    ):\n        if tokenembeddings is None and hall is not None:\n            tokenembeddings = hall\n        if tokenembeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires tokenembeddings or hall\")\n\n        batch_size, seq_len, embed_dim = tokenembeddings.shape\n        device = tokenembeddings.device\n\n        if fast_inference:\n            return {\n                \"haugmented\": tokenembeddings.detach().clone(),\n                \"protoprobs\": [\n                    [torch.tensor([1.0], device=device, dtype=torch.float32) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"spanpreds\": [\n                    [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"protoassignments\": [\n                    torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)\n                ],\n            }\n\n        if inputids is not None and tokentypes is None:\n            batchsize, seqlen = inputids.shape\n            tokentypes = []\n            for b in range(batchsize):\n                if self.tokenizer is not None:\n                    try:\n                        tokentypes.append(self.tokenizer.convert_ids_to_tokens(inputids[b].tolist()))\n                    except Exception:\n                        tokentypes.append([f\"tok{i}\" for i in range(seqlen)])\n                else:\n                    tokentypes.append([f\"tok{i}\" for i in range(seqlen)])\n\n        self.cleanupcounter += 1\n        if self.cleanupcounter % 50 == 0:\n            self.cleanupcounter = 0\n            self.cleanupmemory()\n            self.cleanupthreads()\n\n        batchsize = int(tokenembeddings.size(0))\n        seqlen = int(tokenembeddings.size(1))\n\n        alloutputs: Dict[str, List[Any]] = {\n            \"protoassignments\": [],\n            \"protoprobs\": [],\n            \"uncertainties\": [],\n            \"spanpreds\": [],\n            \"gates\": [],\n            \"haugmented\": [],\n        }\n\n        for b in range(batchsize):\n            wordmap = tokenwordmap[b] if tokenwordmap and len(tokenwordmap) > b else None\n            batchoutputs = self.processsequence(\n                tokenembeddings[b],\n                tokentypes[b] if tokentypes and len(tokentypes) > b else [f\"tok{i}\" for i in range(seqlen)],\n                device,\n                wordmap=wordmap,\n                trainmode=trainmode,\n            )\n            for k in alloutputs:\n                alloutputs[k].append(batchoutputs[k])\n\n        try:\n            hauglist: List[torch.Tensor] = []\n            maxseqlen = seqlen\n            for b in range(batchsize):\n                hbatchlist = alloutputs[\"haugmented\"][b]\n                if len(hbatchlist) > 0 and isinstance(hbatchlist[0], torch.Tensor):\n                    hbatch = torch.stack(hbatchlist, dim=0)\n                    if hbatch.size(0) < maxseqlen:\n                        pad = maxseqlen - hbatch.size(0)\n                        hbatch = F.pad(hbatch, (0, 0, 0, pad), value=0)\n                    elif hbatch.size(0) > maxseqlen:\n                        hbatch = hbatch[:maxseqlen]\n                else:\n                    hbatch = torch.zeros(maxseqlen, self.embeddim, device=device)\n                hauglist.append(hbatch)\n            alloutputs[\"haugmented\"] = torch.stack(hauglist, dim=0)\n        except Exception:\n            alloutputs[\"haugmented\"] = tokenembeddings\n\n        try:\n            protoassigntensor = []\n            for row in alloutputs[\"protoassignments\"]:\n                try:\n                    stacked = torch.stack([x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in row], dim=0)\n                    protoassigntensor.append(stacked)\n                except Exception:\n                    protoassigntensor.append(torch.tensor([int(x) if not isinstance(x, torch.Tensor) else int(x.item()) for x in row], dtype=torch.long))\n            alloutputs[\"protoassignments\"] = protoassigntensor\n        except Exception:\n            pass\n\n        return alloutputs\n\n    def processsequence(\n        self,\n        tokenembeddings: torch.Tensor,\n        tokentypes: List[Any],\n        device: torch.device,\n        wordmap: Optional[Dict[int, Optional[str]]] = None,\n        trainmode: bool = True,\n    ) -> Dict[str, List[Any]]:\n        seqlen = int(tokenembeddings.size(0))\n        outputs: Dict[str, List[Any]] = {\n            \"protoassignments\": [],\n            \"protoprobs\": [],\n            \"uncertainties\": [],\n            \"spanpreds\": [],\n            \"gates\": [],\n            \"haugmented\": [],\n        }\n\n        for j in range(seqlen):\n            rawtok = tokentypes[j] if j < len(tokentypes) else f\"tok{j}\"\n            if not isinstance(rawtok, str):\n                rawtok = str(rawtok) if rawtok is not None else f\"tok{j}\"\n\n            tokenkey = self.canonicaltokenkey(rawtok, wordmap, j)\n            hj = tokenembeddings[j]\n\n            if not tokenkey:\n                outputs[\"protoassignments\"].append(torch.tensor(-1))\n                outputs[\"protoprobs\"].append([])\n                outputs[\"uncertainties\"].append(0.0)\n                outputs[\"spanpreds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"haugmented\"].append(hj)\n                continue\n\n            tokenkey = unicodedata.normalize(\"NFKC\", tokenkey).strip().lower()\n\n            if not self.shouldtracktoken(tokenkey):\n                outputs[\"protoassignments\"].append(torch.tensor(-1))\n                outputs[\"protoprobs\"].append([])\n                outputs[\"uncertainties\"].append(0.0)\n                outputs[\"spanpreds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"haugmented\"].append(hj)\n                continue\n\n            with self.bufferlock:\n                if tokenkey not in self.buffers:\n                    self.buffers[tokenkey] = deque(maxlen=self.buffersize)\n                    self.prototypestores[tokenkey] = MemoryEfficientPrototypeStore(self.embeddim, self.maxprotos)\n\n                try:\n                    self.buffers[tokenkey].append(hj.detach().clone().cpu())\n                except Exception:\n                    try:\n                        self.buffers[tokenkey].append(hj.cpu())\n                    except Exception:\n                        pass\n\n                bufferlen = len(self.buffers[tokenkey])\n\n            try:\n                if self.enabletrainingclustering and bufferlen >= max(self.nmin, 4):\n                    now = time.time()\n                    lastt = self.lastclustertime.get(tokenkey, 0.0)\n                    if now - lastt > self.clustercooldownseconds:\n                        self.lastclustertime[tokenkey] = now\n\n                        def bgcluster(tok: str = tokenkey) -> None:\n                            try:\n                                with self.clusteringlock:\n                                    self.clusterbuffertoprototypeshierarchical(tok)\n                            except Exception:\n                                pass\n\n                        th = threading.Thread(target=bgcluster, daemon=True)\n                        th.start()\n                        with self.threadlock:\n                            self.activethreads.append(th)\n            except Exception:\n                pass\n\n            store = self.prototypestores[tokenkey]\n\n            centroidssnapshot: Optional[List[torch.Tensor]] = None\n            with self.clusteringlock:\n                try:\n                    if hasattr(store, \"centroids\") and len(store.centroids) > 0:\n                        centroidssnapshot = []\n                        for c in store.centroids:\n                            try:\n                                if isinstance(c, torch.Tensor):\n                                    centroidssnapshot.append(c.clone().cpu())\n                                else:\n                                    centroidssnapshot.append(torch.from_numpy(np.asarray(c, dtype=np.float32)).cpu())\n                            except Exception:\n                                continue\n                        if not centroidssnapshot:\n                            centroidssnapshot = None\n                except Exception:\n                    centroidssnapshot = None\n\n            assignment = -1\n            problist: List[float] = []\n            uncertainty = 0.0\n            spanpred = 0.0\n            gateval = 0.0\n            haug = hj\n\n            if centroidssnapshot and len(centroidssnapshot) >= 1:\n                try:\n                    try:\n                        hcpu = hj.detach().cpu().numpy()\n                    except Exception:\n                        hcpu = hj.cpu().numpy()\n                    try:\n                        centsnp = np.stack([c.numpy() for c in centroidssnapshot], axis=0)\n                    except Exception:\n                        centsnp = np.stack([np.asarray(c, dtype=np.float32) for c in centroidssnapshot], axis=0)\n\n                    distsnp = np.linalg.norm(centsnp - hcpu[None, :], axis=1)\n                    if distsnp.size > 0:\n                        mindist = float(distsnp.min())\n                        minidx = int(np.argmin(distsnp))\n                        maxdist = float(distsnp.max())\n\n                        spanrange = maxdist - mindist\n                        ratio = spanrange / (maxdist + 1e-8) if maxdist > 0 else 0.0\n                        if ratio > 0.25:\n                            spanpred = float(min(1.0, max(0.0, 0.5 * (ratio - 0.25))))\n                        else:\n                            spanpred = float(min(1.0, max(0.0, ratio)))\n\n                        try:\n                            store.updaterollingstats(mindist)\n                        except Exception:\n                            pass\n\n                        try:\n                            disttensor = torch.from_numpy(distsnp).to(device)\n                            probstensor = F.softmax(-disttensor, dim=0)\n                            problist = probstensor.tolist()\n                            entropy = -torch.sum(probstensor * torch.log(probstensor + 1e-10))\n                            maxentropy = np.log(len(distsnp))\n                            uncertainty = float(entropy.item() / maxentropy) if maxentropy > 0 else 0.0\n                        except Exception:\n                            exps = np.exp(-distsnp - np.max(-distsnp)) if distsnp.size > 0 else np.array([])\n                            if exps.size > 0:\n                                probs = exps / (exps.sum() + 1e-12)\n                                problist = probs.tolist()\n                                entropyval = -np.sum(probs * np.log(probs + 1e-10))\n                                maxentropy = np.log(len(distsnp))\n                                uncertainty = float(entropyval / maxentropy) if maxentropy > 0 else 0.0\n\n                        try:\n                            gateval = float(torch.sigmoid(self.gatew * torch.norm(hj) - self.gateb).item())\n                        except Exception:\n                            gateval = 0.5\n\n                        if gateval > 0.3:\n                            assignment = minidx\n\n                        try:\n                            if store.size() < self.maxprotos and mindist > store.getadaptivethreshold(DSCDNEWSENSELAMBDA):\n                                store.addprototype(hj, time.time(), count=1)\n                                assignment = store.size() - 1\n                                centroidssnapshot.append(hj.detach().cpu())\n                        except Exception:\n                            pass\n\n                        if assignment >= 0 and assignment < len(centroidssnapshot):\n                            centroidt = centroidssnapshot[assignment]\n                            try:\n                                if device != torch.device(\"cpu\"):\n                                    centroidt = centroidt.to(device)\n                            except Exception:\n                                pass\n                            haug = hj + 0.1 * (centroidt - hj)\n                except Exception as e:\n                    if DEBUGDISCOVERY:\n                        print(f\"[DSCD] Assignment error for {tokenkey}: {str(e)[:200]}\")\n\n            outputs[\"protoassignments\"].append(torch.tensor(assignment))\n            outputs[\"protoprobs\"].append(problist)\n            outputs[\"uncertainties\"].append(uncertainty)\n            outputs[\"spanpreds\"].append(spanpred)\n            outputs[\"gates\"].append(gateval)\n            outputs[\"haugmented\"].append(haug)\n\n        return outputs\n\n    def clusterbuffertoprototypeshierarchical(self, tokentype: str) -> bool:\n        try:\n            if not self.shouldtracktoken(tokentype):\n                if DEBUGDISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping non-word token {tokentype}\")\n                return False\n\n            with self.bufferlock:\n                if tokentype not in self.buffers:\n                    return False\n                bufsnapshot = [e.clone() if isinstance(e, torch.Tensor) else e for e in self.buffers[tokentype]]\n\n            if len(bufsnapshot) < self.nmin:\n                if DEBUGDISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype}: buffer len {len(bufsnapshot)} < nmin {self.nmin}\")\n                return False\n\n            emblist: List[np.ndarray] = []\n            for e in bufsnapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        try:\n                            emblist.append(e.numpy())\n                        except Exception:\n                            emblist.append(e.cpu().numpy())\n                    else:\n                        emblist.append(np.asarray(e, dtype=np.float32))\n                except Exception:\n                    continue\n\n            if len(emblist) == 0:\n                return False\n\n            if len(emblist) > self.maxclusteringpoints:\n                idxs = np.random.choice(len(emblist), size=self.maxclusteringpoints, replace=False)\n                newembeddings = np.stack([emblist[i] for i in idxs], axis=0)\n            else:\n                newembeddings = np.stack(emblist, axis=0)\n\n            if newembeddings.shape[0] < 2:\n                return False\n\n            norms = np.linalg.norm(newembeddings, axis=1)\n            if np.all(norms < 1e-6):\n                if DEBUGDISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype}: all zero vectors, skipping\")\n                return False\n\n            store = self.prototypestores[tokentype]\n            protosadded = 0\n\n            if HASCLUSTERING:\n                try:\n                    condensed = pdist(newembeddings, metric=\"euclidean\")\n                    if condensed.size == 0:\n                        return False\n                    Z = linkage(condensed, method=\"average\")\n                    \n                    median_dist = float(np.median(condensed)) if condensed.size > 0 else 0.5\n                    absolutethreshold = max(self.dispersionthreshold, median_dist * 0.5)\n                    \n                    clusters = fcluster(Z, t=absolutethreshold, criterion=\"distance\") - 1\n                    if clusters.size == 0:\n                        return False\n\n                    maxc = int(clusters.max()) if clusters.size > 0 else 0\n                    newcentroids: List[torch.Tensor] = []\n                    newcounts: List[int] = []\n                    newtimes: List[float] = []\n\n                    for cid in range(maxc + 1):\n                        mask = clusters == cid\n                        clustersize = int(mask.sum())\n                        if clustersize >= self.nmin:\n                            centroid = newembeddings[mask].mean(axis=0).astype(np.float32)\n                            centroidtensor = torch.from_numpy(centroid)\n                            newcentroids.append(centroidtensor)\n                            newcounts.append(clustersize)\n                            newtimes.append(time.time())\n                            protosadded += 1\n\n                    if len(newcentroids) > self.maxprotos:\n                        sortedindices = np.argsort(newcounts)[::-1][: self.maxprotos]\n                        newcentroids = [newcentroids[i] for i in sortedindices]\n                        newcounts = [newcounts[i] for i in sortedindices]\n                        newtimes = [newtimes[i] for i in sortedindices]\n\n                    if protosadded > 0:\n                        store.centroids = newcentroids\n                        store.counts = newcounts\n                        store.creationtime = newtimes\n                        store.labels = torch.tensor(clusters)\n                        return store.size() > 0\n                except Exception as e:\n                    if DEBUGDISCOVERY:\n                        print(f\"[DSCD-CLUSTER] Hierarchical failed for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n\n            if protosadded == 0 and HASKMEANS:\n                try:\n                    embeddings = newembeddings\n                    lenembeddings = int(embeddings.shape[0])\n                    mink = 1\n                    maxk = min(self.maxprotos, max(1, lenembeddings // max(self.nmin, 1)))\n                    if maxk < mink:\n                        maxk = mink\n\n                    if lenembeddings >= 20:\n                        kguess = min(maxk, max(2, int(np.sqrt(lenembeddings) / 2)))\n                    elif lenembeddings >= 10:\n                        kguess = min(maxk, 2)\n                    else:\n                        kguess = 1\n\n                    kguess = max(mink, min(kguess, lenembeddings))\n                    if kguess > 1 and lenembeddings >= kguess:\n                        km = KMeans(n_clusters=kguess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n                        newcentroids = []\n                        newcounts = []\n                        newtimes = []\n                        for c in range(kguess):\n                            mask = labels == c\n                            clustersize = int(mask.sum())\n                            if clustersize >= self.nmin:\n                                centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                newcentroids.append(torch.from_numpy(centroid))\n                                newcounts.append(clustersize)\n                                newtimes.append(time.time())\n                                protosadded += 1\n                        if len(newcentroids) > 0:\n                            store.centroids = newcentroids\n                            store.counts = newcounts\n                            store.creationtime = newtimes\n                            store.labels = torch.tensor(labels)\n                    return store.size() > 0\n                except Exception as e:\n                    if DEBUGDISCOVERY:\n                        print(f\"[DSCD-CLUSTER] KMeans failed for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n\n            return store.size() > 0\n        except Exception as e:\n            if DEBUGDISCOVERY:\n                print(f\"[DSCD-ERROR] Clustering error for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n            return False\n\n    def periodic_discovery_check(self, current_step: int, frequency: int) -> None:\n        try:\n            if current_step - self.lastperiodiccheck < frequency:\n                return\n            \n            self.lastperiodiccheck = current_step\n            \n            if DEBUGDISCOVERY:\n                print(f\"\\n[DSCD] Periodic discovery check at step {current_step}\")\n            \n            self.discover_homographs()\n            \n        except Exception as e:\n            if DEBUGDISCOVERY:\n                print(f\"[DSCD] Periodic check failed: {e}\")\n\n    def discover_homographs(self) -> Dict[str, Any]:\n        try:\n            if DEBUGDISCOVERY:\n                print(f\"\\n[DSCD] Running discovery...\")\n            \n            discovered_count = 0\n            total_candidates = 0\n            \n            with self.clusteringlock:\n                for tokentype, store in list(self.prototypestores.items()):\n                    total_candidates += 1\n                    \n                    if self.ismultisensestore(store):\n                        clean_token = normalizetokenkey(tokentype)\n                        if clean_token and clean_token not in self.discoveredhomographs:\n                            self.discoveredhomographs.add(clean_token)\n                            discovered_count += 1\n                            \n                            if DEBUGDISCOVERY:\n                                print(f\"[DSCD] Discovered: {clean_token} ({store.size()} senses)\")\n            \n            discovery_result = {\n                'discovered': discovered_count,\n                'candidates': total_candidates,\n                'total_homographs': len(self.discoveredhomographs),\n                'timestamp': time.time()\n            }\n            \n            self.discoveredlog.append(discovery_result)\n            self.discoverycount += 1\n            self.discoverytimes.append(time.time())\n            \n            if DEBUGDISCOVERY:\n                print(f\"[DSCD] Discovery: {discovered_count}/{total_candidates} homographs discovered\")\n                print(f\"[DSCD] Total homographs: {len(self.discoveredhomographs)}\")\n            \n            return discovery_result\n            \n        except Exception as e:\n            if DEBUGDISCOVERY:\n                print(f\"[DSCD] discover_homographs failed: {e}\")\n            return {'discovered': 0, 'candidates': 0, 'total_homographs': 0}\n\n    def get_discovered_homographs(self) -> Set[str]:\n        try:\n            with self.clusteringlock:\n                return self.discoveredhomographs.copy()\n        except Exception:\n            return set()\n\n    def validate_prototypes(self) -> Dict[str, Any]:\n        try:\n            total_tokens = 0\n            total_prototypes = 0\n            multi_sense_tokens = 0\n            strong_multi_sense = 0\n            \n            with self.clusteringlock:\n                for tokentype, store in self.prototypestores.items():\n                    total_tokens += 1\n                    protos = store.size()\n                    total_prototypes += protos\n                    \n                    if protos >= 2:\n                        multi_sense_tokens += 1\n                        \n                        if self.ismultisensestore(store):\n                            strong_multi_sense += 1\n            \n            quality_score = strong_multi_sense / max(1, multi_sense_tokens) if multi_sense_tokens > 0 else 0.0\n            \n            return {\n                'quality_score': quality_score,\n                'multi_sense_tokens': multi_sense_tokens,\n                'strong_multi_sense_tokens': strong_multi_sense,\n                'total_prototypes': total_prototypes,\n                'total_tokens': total_tokens,\n            }\n        except Exception:\n            return {\n                'quality_score': 0.0,\n                'multi_sense_tokens': 0,\n                'strong_multi_sense_tokens': 0,\n                'total_prototypes': 0,\n                'total_tokens': 0,\n            }\n\n    def printclusterssummary(self) -> None:\n        try:\n            items: List[Tuple[str, int, int, float, float, int]] = []\n            for token, store in self.prototypestores.items():\n                try:\n                    protosamplecount = sum(getattr(store, \"counts\", []) or [])\n                except Exception:\n                    protosamplecount = 0\n                bufferlen = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                totalcount = protosamplecount if protosamplecount > 0 else bufferlen\n                protos = store.size()\n                mu = getattr(store, \"mu\", 0.0)\n                tau = getattr(store, \"tau\", 0.0)\n                items.append((token, totalcount, protos, mu, tau, bufferlen))\n\n            items.sort(key=lambda x: x[1], reverse=True)\n            top5 = items[:5]\n\n            if VERBOSELOGGING:\n                print(\"[CLUSTER] Top 5 clusters\")\n                print(\"-\" * 100)\n                print(f\"{'Rank':<6} {'Token':<18} {'Count':<12} {'Protos':<8} {'BufLen':<8} {'mu':<15} {'tau':<15}\")\n                print(\"-\" * 100)\n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(top5, 1):\n                    tokstr = str(tok)[:18]\n                    print(f\"{rank:<6} {tokstr:<18} {cnt:<12} {prot:<8} {buflen:<8} {mu:<15.6f} {tau:<15.6f}\")\n                print(\"-\" * 100)\n        except Exception:\n            pass\n\n    def getexplanations(self, thresholdspan: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for tokentype, store in self.prototypestores.items():\n            if store.size() >= 2:\n                expl.append({\"token\": str(tokentype), \"protos\": store.size()})\n        return expl\n\nprint(\"=\" * 80)\nprint(\"Cell 3 DSCD Ready: FAST INFERENCE MODE ADDED\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\" - Max Protos: {DSCDMAXPROTOS}\")\nprint(f\" - Buffer Size: {DSCDBUFFERSIZE}\")\nprint(f\" - N Min: {DSCDNMIN} (enforced minimum: 3)\")\nprint(f\" - Dispersion Threshold: {DSCDDISPERSIONTHRESHOLD} (enforced maximum: 0.08)\")\nprint(f\" - Fast inference mode: ENABLED\")\nprint(\"=\" * 80)\n","metadata":{"id":"L25pcKUPH4J2","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:43.526363Z","iopub.execute_input":"2026-01-07T14:57:43.526581Z","iopub.status.idle":"2026-01-07T14:57:44.872901Z","shell.execute_reply.started":"2026-01-07T14:57:43.526562Z","shell.execute_reply":"2026-01-07T14:57:44.872177Z"}},"outputs":[{"name":"stdout","text":"[CELL3] Loaded reference list for evaluation: 65 words\n================================================================================\nCell 3 DSCD Ready: FAST INFERENCE MODE ADDED\n================================================================================\nConfiguration:\n - Max Protos: 8\n - Buffer Size: 50\n - N Min: 3 (enforced minimum: 3)\n - Dispersion Threshold: 0.08 (enforced maximum: 0.08)\n - Fast inference mode: ENABLED\n================================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - FIXED & ROBUST\n# ==============================================================================\n\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.0\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_should_track_token = \"should_track_token\" in globals()\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = float(alpha)\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.alpha * grad_output, None\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    return GradientReversalFunction.apply(x, alpha)\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 1000,\n        encoder_grl_scale: float = 0.5,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = int(embed_dim)\n\n        self.bn_source = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n        self.bn_target = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n        self.freq_threshold = float(freq_threshold)\n        self.uncertainty_threshold = float(uncertainty_threshold)\n        self.gate_threshold = float(gate_threshold)\n        self.warmup_steps = int(warmup_steps)\n        self.current_step = 0\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 0.5, \"xl\": 0.8, \"domain\": 1.0}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = float(encoder_grl_scale)\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[ASBN-INIT] Initialized MemoryEfficientASBNModule:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - warmup_steps: {self.warmup_steps}\")\n            print(f\"  - encoder_grl_scale: {self.encoder_grl_scale}\")\n            print(f\"  - GRL_ALPHA_STEPS: {_GRL_ALPHA_STEPS}\")\n            print(f\"  - thresholds: freq={self.freq_threshold}, uncert={self.uncertainty_threshold}, gate={self.gate_threshold}\")\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            ratio = _GRL_ALPHA_END / max(1e-8, _GRL_ALPHA_START if _GRL_ALPHA_START > 0 else 1e-3)\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n        return float(alpha)\n\n    def get_asbn_stats(self) -> Dict[str, float]:\n        return self.get_detailed_stats()\n\n    def get_detailed_stats(self) -> Dict[str, float]:\n        if self.stats[\"num_updates\"] > 0:\n            n = float(self.stats[\"num_updates\"])\n            return {\n                \"domain_loss\": self.stats[\"domain_loss\"] / n,\n                \"domain_accuracy\": self.stats[\"domain_accuracy\"] / n,\n                \"source_accuracy\": self.stats[\"source_accuracy\"] / n,\n                \"target_accuracy\": self.stats[\"target_accuracy\"] / n,\n                \"asbn_loss\": self.stats[\"asbn_loss\"] / n,\n                \"num_updates\": self.stats[\"num_updates\"],\n            }\n        return {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def reset_stats(self) -> None:\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def critic_parameters(self):\n        return (\n            list(self.d_domain.parameters())\n            + list(self.d_freq.parameters())\n            + list(self.d_ctx.parameters())\n            + list(self.d_xl.parameters())\n        )\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (\n                self.d_domain,\n                self.d_freq,\n                self.d_ctx,\n                self.d_xl,\n                self.bn_source,\n                self.bn_target,\n            ):\n                try:\n                    p = next(mod.parameters())\n                    if p.device != device:\n                        mod.to(device)\n                except StopIteration:\n                    mod.to(device)\n                except Exception:\n                    pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] Device migration failed:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    print(\"[ASBN] Device migration failed\")\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n            if isinstance(proto_probs, torch.Tensor):\n                if proto_probs.dim() == 3:\n                    B, T, K = proto_probs.shape\n                    p = proto_probs.detach().to(device)\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    pmax[:b_max, :t_max] = p[:b_max, :t_max].max(dim=2)[0]\n                    return pmax\n                if proto_probs.dim() == 2:\n                    p = proto_probs.detach().to(device)\n                    if batch_size >= 1:\n                        t_max = min(seq_len, p.size(0))\n                        pmax[0, :t_max] = p[:t_max].max(dim=1)[0]\n                        return pmax\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            t_max = min(seq_len, row.size(0))\n                            pmax[b, :t_max] = row[:t_max].max(dim=1)[0].to(device)\n                        elif isinstance(row, (list, tuple)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        pmax[b, t] = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    pmax[0, t] = float(val.max().item())\n                                else:\n                                    pmax[0, t] = float(np.max(np.asarray(val, dtype=np.float32)))\n                            except Exception:\n                                pmax[0, t] = 0.5\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] parse_proto_probs exception: {e}\")\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device,\n                            default: float = 0.0) -> torch.Tensor:\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n            if isinstance(mat, torch.Tensor):\n                if mat.dim() == 3:\n                    B, T, D = mat.shape\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    if D == 1:\n                        out[:b_max, :t_max] = mat[:b_max, :t_max, 0].to(device)\n                    else:\n                        out[:b_max, :t_max] = mat[:b_max, :t_max, 0].to(device)\n                elif mat.dim() == 2:\n                    if mat.size(0) == batch_size:\n                        t_max = min(seq_len, mat.size(1))\n                        out[:, :t_max] = mat[:, :t_max].to(device)\n                    elif batch_size == 1:\n                        t_max = min(seq_len, mat.size(0))\n                        out[0, :t_max] = mat[:t_max].to(device)\n                elif mat.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, mat.size(0))\n                    out[0, :t_max] = mat[:t_max].to(device)\n            elif isinstance(mat, (list, tuple)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                            t_max = min(seq_len, row.size(0))\n                            for t in range(t_max):\n                                out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                                except Exception:\n                                    out[b, t] = float(default)\n                elif batch_size == 1:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            out[0, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                        except Exception:\n                            out[0, t] = float(default)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    pass\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor,\n                                    gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        base = float(self.lambda_base.get(lambda_type, 0.2))\n        \n        lam = base * (1.0 - pmax + 1e-3) * (uncertainty + 1e-3) * (gate + 1e-3)\n        \n        lam = torch.clamp(lam, min=1e-4, max=float(self.lambda_max))\n        \n        lam = torch.where(torch.isfinite(lam), lam, torch.zeros_like(lam))\n        return lam\n\n    def forward(self, h: torch.Tensor, domain_labels: Optional[torch.Tensor] = None, \n                device: Optional[torch.device] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            return h, torch.tensor(0.0, device=dev)\n        \n        B, T, H = h.size()\n        if device is None:\n            device = h.device\n        \n        if domain_labels is not None:\n            if domain_labels.dim() == 0:\n                domain_labels = domain_labels.unsqueeze(0)\n            if domain_labels.size(0) == 1 and B > 1:\n                domain_labels = domain_labels.expand(B)\n            elif domain_labels.size(0) != B:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[ASBN] Domain label size mismatch: {domain_labels.size(0)} vs batch {B}, using first label\")\n                domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n        \n        if domain_labels is not None and self.training:\n            try:\n                self._ensure_discriminators_on_device(device)\n                h_flat = h.view(B * T, H)\n                domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n                source_mask = domain_expanded == 0\n                target_mask = domain_expanded == 1\n                \n                h_normalized = h_flat.clone()\n                \n                source_count = int(source_mask.sum().item())\n                target_count = int(target_mask.sum().item())\n                \n                if source_count >= 2:\n                    h_normalized[source_mask] = self.bn_source(h_flat[source_mask])\n                elif source_count == 1:\n                    h_normalized[source_mask] = h_flat[source_mask]\n                \n                if target_count >= 2:\n                    h_normalized[target_mask] = self.bn_target(h_flat[target_mask])\n                elif target_count == 1:\n                    h_normalized[target_mask] = h_flat[target_mask]\n                \n                h_out = h_normalized.view(B, T, H)\n                if _DEBUG_DISCOVERY:\n                    print(f\"[ASBN] Applied BN: {source_count} source, {target_count} target tokens\")\n                return h_out, torch.tensor(0.0, device=device)\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] BN failed: {e}\")\n                return h, torch.tensor(0.0, device=device)\n        else:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            return h, torch.tensor(0.0, device=dev)\n\n    def forward_with_grl_simplified(self, h: torch.Tensor, proto_probs: Any, uncertainties: Any, gates: Any,\n                                   token_word_map: Optional[List[Dict[int, str]]] = None,\n                                   domain_labels: Optional[torch.Tensor] = None,\n                                   global_step: Optional[int] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        if global_step is not None:\n            self.current_step = int(global_step)\n        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n        if self.current_step < self.warmup_steps:\n            if _DEBUG_DISCOVERY and self.current_step % 100 == 0:\n                print(f\"[ASBN] Warmup: {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        device = h.device\n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n        B, T, H = h.size()\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n        batch_indices = torch.arange(B, device=device).unsqueeze(1).expand(B, T)\n        if token_word_map and isinstance(token_word_map, (list, tuple)) and len(token_word_map) > 0:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b]\n                    if not isinstance(wm, dict):\n                        continue\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                tracked = True\n                                if _has_should_track_token:\n                                    tracked = bool(globals()[\"should_track_token\"](token_str))\n                                elif _has_is_valid_token:\n                                    tracked = bool(is_valid_token(token_str, self.special_tokens, self.tokenizer, language=self.language))\n                                if not tracked:\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    try:\n                        print(\"[ASBN] Token filtering failed:\", traceback.format_exc().splitlines()[-1])\n                    except Exception:\n                        pass\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        batch_idx = batch_indices.view(-1)[sel_idx]\n        if sel_idx.numel() == 0:\n            if _DEBUG_DISCOVERY:\n                print(\"[ASBN] No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n        grl_alpha = self.get_grl_alpha(global_step)\n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n        freq_logits = self.d_freq(freq_input_grl)\n        ctx_logits = self.d_ctx(ctx_input_grl)\n        xl_logits = self.d_xl(xl_input_grl)\n        freq_label = (pmax_flat > self.freq_threshold).long().to(device)\n        ctx_label = (U_flat < self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat > self.gate_threshold).long().to(device)\n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        mean_weighted = torch.mean(weighted)\n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n        if domain_labels is not None:\n            try:\n                if domain_labels.dim() == 0:\n                    domain_labels = domain_labels.unsqueeze(0)\n                if domain_labels.size(0) == 1 and B > 1:\n                    domain_labels = domain_labels.expand(B)\n                elif domain_labels.size(0) != B:\n                    domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n                domain_flat = domain_labels[batch_idx]\n                domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                domain_logits = self.d_domain(domain_input)\n                domain_loss = F.cross_entropy(domain_logits, domain_flat)\n                with torch.no_grad():\n                    domain_preds = torch.argmax(domain_logits, dim=1)\n                    domain_accuracy = (domain_preds == domain_flat).float().mean()\n                    source_mask = domain_flat == 0\n                    target_mask = domain_flat == 1\n                    if source_mask.any():\n                        source_acc = ((domain_preds[source_mask] == domain_flat[source_mask]).float().mean())\n                        self.stats[\"source_accuracy\"] += float(source_acc.item())\n                    if target_mask.any():\n                        target_acc = ((domain_preds[target_mask] == domain_flat[target_mask]).float().mean())\n                        self.stats[\"target_accuracy\"] += float(target_acc.item())\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] Domain loss failed: {e}\")\n        encoder_loss = self.encoder_grl_scale * (mean_weighted + domain_loss)\n        try:\n            with torch.no_grad():\n                self.stats[\"domain_loss\"] += float(domain_loss.item())\n                self.stats[\"domain_accuracy\"] += float(domain_accuracy.item())\n                self.stats[\"asbn_loss\"] += float(encoder_loss.item())\n                self.stats[\"num_updates\"] += 1\n                if self.stats[\"num_updates\"] >= self.stats_reset_interval:\n                    if _DEBUG_DISCOVERY:\n                        stats = self.get_detailed_stats()\n                        print(f\"\\n[ASBN-STATS] After {stats['num_updates']} updates:\")\n                        print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                        print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                        print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                        print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                        print(f\"  ASBN loss: {stats['asbn_loss']:.4f}\")\n                    self.reset_stats()\n        except Exception:\n            pass\n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            print(f\"\\n[ASBN-STEP-{self.current_step}]\")\n            print(f\"  GRL alpha: {grl_alpha:.3f}\")\n            print(f\"  Encoder loss: {encoder_loss.item():.4f}\")\n            print(f\"  Domain loss: {domain_loss.item():.4f}\")\n            print(f\"  Domain acc: {domain_accuracy.item():.2%}\")\n        return encoder_loss, mean_weighted, domain_loss, domain_accuracy\n\n    def state_dict(self, *args, **kwargs):\n        state = super().state_dict(*args, **kwargs)\n        state['current_step'] = self.current_step\n        state['stats'] = self.stats.copy()\n        return state\n\n    def load_state_dict(self, state_dict, strict=True):\n        self.current_step = state_dict.pop('current_step', 0)\n        self.stats = state_dict.pop('stats', {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        })\n        return super().load_state_dict(state_dict, strict=strict)\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[ASBN-TEST] Testing ASBN module\")\n        print(\"=\" * 60)\n        try:\n            try:\n                device = next(self.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cpu\")\n            h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n            domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n            h_out, _ = self.forward(h, domain_labels, device=device)\n            assert h_out.shape == h.shape, \"Forward output shape mismatch\"\n            print(\"  forward() passed\")\n            proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n            uncertainties = torch.rand(batch_size, seq_len, device=device)\n            gates = torch.rand(batch_size, seq_len, device=device)\n            self.train()\n            self.current_step = self.warmup_steps + 1\n            enc_loss, adv_loss, dom_loss, dom_acc = self.forward_with_grl_simplified(\n                h,\n                proto_probs,\n                uncertainties,\n                gates,\n                domain_labels=domain_labels,\n                global_step=self.current_step,\n            )\n            assert enc_loss.item() >= 0.0, \"Encoder loss negative\"\n            assert 0.0 <= dom_acc.item() <= 1.0, \"Domain accuracy out of range\"\n            print(\"  forward_with_grl_simplified() passed\")\n            alpha = self.get_grl_alpha(self.current_step)\n            assert 0.0 <= alpha <= max(_GRL_ALPHA_START, _GRL_ALPHA_END) * 1.1, \"GRL alpha out of range\"\n            print(f\"  get_grl_alpha() passed (alpha={alpha:.3f})\")\n            stats = self.get_detailed_stats()\n            assert \"domain_loss\" in stats, \"Missing domain_loss in stats\"\n            print(\"  Statistics tracking passed\")\n            state = self.state_dict()\n            assert 'current_step' in state, \"Missing current_step in state_dict\"\n            print(\"  state_dict() passed\")\n            print(\"\\nAll ASBN tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n        except Exception as e:\n            print(f\"\\nASBN test failed: {e}\")\n            traceback.print_exc()\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN Ready (dynamic GRL, DSCD-aware) - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\n","metadata":{"id":"XrNq18UsH4J3","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:44.875039Z","iopub.execute_input":"2026-01-07T14:57:44.875421Z","iopub.status.idle":"2026-01-07T14:57:44.947215Z","shell.execute_reply.started":"2026-01-07T14:57:44.875398Z","shell.execute_reply":"2026-01-07T14:57:44.946298Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 4: ASBN Ready (dynamic GRL, DSCD-aware) - ALL FIXES APPLIED\n================================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG MODULE (TRANSLATION RATIONALE GENERATION) - COMPLETE FIXES\n# ==============================================================================\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque, defaultdict\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport threading\nimport time\n\ntry:\n    TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\nexcept (NameError, ValueError, TypeError):\n    TRG_EVIDENCE_K = 3\n\ntry:\n    TRG_GEN_EMBED = int(TRG_GEN_EMBED)\nexcept (NameError, ValueError, TypeError):\n    TRG_GEN_EMBED = 64\n\ntry:\n    MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\nexcept (NameError, ValueError, TypeError):\n    MAX_SILVER_BUFFER = 50\n\ntry:\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    VERBOSE_LOGGING = False\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    DEBUG_DISCOVERY = False\n\ntry:\n    DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept NameError:\n    DEBUG_TIMING = False\n\ntry:\n    ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    ENABLE_TRG_INFERENCE = True\n\ntry:\n    SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    TRG_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    TRG_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    TRG_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    TRG_SPAN_THRESHOLD = 0.20\n\ntry:\n    TAU_HIGH = float(TAU_HIGH)\nexcept (NameError, ValueError, TypeError):\n    TAU_HIGH = 0.85\n\ntry:\n    TAU_LOW = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    TAU_LOW = 0.15\n\ntry:\n    TAU_ACCEPT = float(TAU_ACCEPT)\nexcept (NameError, ValueError, TypeError):\n    TAU_ACCEPT = 0.80\n\ntry:\n    TRG_TEMPERATURE = float(TRG_TEMPERATURE)\nexcept (NameError, ValueError, TypeError):\n    TRG_TEMPERATURE = 1.0\n\ntry:\n    MAX_EXPLANATIONS_PER_SENTENCE = int(MAX_EXPLANATIONS_PER_SENTENCE) if \"MAX_EXPLANATIONS_PER_SENTENCE\" in globals() else 10\nexcept Exception:\n    MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\nTRG_PUNCT_SET = set(\".,!?-;:|\")\n\ndef fallback_is_valid_token(token: str, special_tokens: set, tokenizer=None, language: str = \"bn\") -> bool:\n    if token is None:\n        return False\n    if not isinstance(token, str):\n        try:\n            token = str(token)\n        except Exception:\n            return False\n    token = token.strip()\n    if not token:\n        return False\n    if token in special_tokens:\n        return False\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n    if len(clean) < 2:\n        return False\n    if not any(c.isalpha() for c in clean):\n        return False\n    if all(c in TRG_PUNCT_SET for c in clean):\n        return False\n    if clean.isdigit():\n        return False\n    return True\n\ndef is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    if not isinstance(raw_token, str):\n        return False\n    try:\n        if token_word_map is not None and isinstance(token_word_map, dict):\n            if idx in token_word_map:\n                w = token_word_map[idx]\n                if isinstance(w, str) and w.strip():\n                    return True\n\n        if raw_token.startswith(\" \") or raw_token.startswith(\"Ġ\") or raw_token.startswith(\"▁\"):\n            return True\n\n        clean = raw_token.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n        if len(clean) < 2:\n            return False\n        if all(ch in \".,!?-;:\" for ch in clean):\n            return False\n        if token_word_map is None and any(c.isalpha() for c in clean):\n            return True\n        return False\n    except Exception:\n        return False\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": \"Chose sense '{sense}' with high confidence ({confidence:.1%}) based on evidence: {evidence}. Pattern matches learned data. {alternatives_text}\",\n            \"medium_confidence\": \"Selected sense '{sense}' with moderate confidence ({confidence:.1%}). Evidence: {evidence}. Some uncertainty. {alternatives_text}\",\n            \"low_confidence\": \"Uncertain: chose sense '{sense}' ({confidence:.1%}). Evidence: {evidence}. {alternatives_text} Review recommended.\",\n            \"fallback\": \"Token '{token}' analyzed. Context: {evidence}.\",\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n\n        token = str(evidence.get(\"token\", \"unknown\")).replace(\"▁\", \"\").replace(\"##\", \"\")\n\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = \", \".join([str(tok).replace(\"▁\", \"\").replace(\"##\", \"\") for tok in evidence_tokens[:TRG_EVIDENCE_K]]) or \"limited context\"\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)}.\"\n\n        if confidence >= TAU_ACCEPT:\n            template_key = \"high_confidence\"\n        elif confidence <= TRG_UNCERTAINTY_THRESHOLD:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(template_key, self.explanation_templates[\"fallback\"])\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token {token} - {sense_name} ({confidence:.1%}).\"\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor,\n    ) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n\n        seq_len = len(tgt_preds) if isinstance(tgt_preds, list) else int(tgt_preds.size(0))\n        if span_end > seq_len:\n            return None\n        if span_start >= span_end:\n            return None\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n        if token_idx >= seq_len:\n            return None\n\n        try:\n            evidence_tokens: List[str] = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        evidence_tokens.append(f\"token_{i}\")\n            return evidence_tokens if evidence_tokens else None\n        except Exception:\n            return None\n\n    def _get_key(self, dscd_outputs: Dict, *names: str):\n        if not isinstance(dscd_outputs, dict):\n            return None\n        for k in names:\n            if k in dscd_outputs and dscd_outputs[k] is not None:\n                return dscd_outputs[k]\n        return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n    ) -> Dict:\n        if not isinstance(tokens, list):\n            return self.create_fallback_evidence(token_idx, [])\n\n        if not isinstance(token_idx, int):\n            return self.create_fallback_evidence(0, tokens)\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return self.create_fallback_evidence(max(0, min(token_idx, len(tokens) - 1)), tokens)\n\n        raw_token = tokens[token_idx]\n\n        if token_word_map and token_idx in token_word_map and token_word_map[token_idx]:\n            canonical_word = str(token_word_map[token_idx]).strip()\n            if _has_is_valid_token:\n                try:\n                    is_valid = is_valid_token(canonical_word, self.special_tokens, self.tokenizer, language=self.language)\n                except Exception:\n                    is_valid = fallback_is_valid_token(canonical_word, self.special_tokens, self.tokenizer, self.language)\n            else:\n                is_valid = fallback_is_valid_token(canonical_word, self.special_tokens, self.tokenizer, self.language)\n        else:\n            if _has_is_valid_token:\n                try:\n                    is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n                except Exception:\n                    is_valid = fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n            else:\n                is_valid = fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n\n        if not is_valid:\n            return self.create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self.safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self.safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self.safe_extract_gate(token_idx, dscd_outputs)\n            span = self.safe_extract_span(token_idx, dscd_outputs)\n\n            evidence_tokens: Optional[List[str]] = None\n            if decoder_attention is not None and isinstance(decoder_attention, torch.Tensor):\n                try:\n                    if decoder_attention.dim() == 4:\n                        if decoder_attention.size(0) == 1 and decoder_attention.size(1) == 1:\n                            attn_avg = decoder_attention.mean(dim=(0, 1))\n                        elif decoder_attention.size(0) == 1:\n                            attn_avg = decoder_attention.mean(dim=1).squeeze(0)\n                        else:\n                            attn_avg = decoder_attention.mean(dim=0)\n\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n\n                    elif decoder_attention.dim() == 3:\n                        attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n\n                    elif decoder_attention.dim() == 2:\n                        if token_idx < decoder_attention.size(0):\n                            vec = decoder_attention[token_idx]\n                        else:\n                            vec = decoder_attention.reshape(-1)\n\n                    elif decoder_attention.dim() == 1:\n                        vec = decoder_attention\n                    else:\n                        vec = None\n\n                    if vec is not None and vec.numel() > 0:\n                        k = min(5, int(vec.size(0)))\n                        topk_indices = torch.topk(vec, k).indices.detach().cpu().numpy()\n                        evidence_tokens = []\n                        for i in topk_indices:\n                            ii = int(i)\n                            is_same_word = False\n                            if token_word_map and token_idx in token_word_map:\n                                if ii in token_word_map and token_word_map[ii] == token_word_map[token_idx]:\n                                    is_same_word = True\n                            elif ii == token_idx:\n                                is_same_word = True\n\n                            if ii < len(tokens) and not is_same_word:\n                                evidence_tokens.append(tokens[ii])\n                except Exception:\n                    evidence_tokens = None\n\n            if evidence_tokens is None:\n                evidence_tokens = self.extract_context_window(token_idx, tokens, token_word_map)\n\n            seen: Dict[str, bool] = {}\n            dedup_evidence: List[str] = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen[t] = True\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:TRG_EVIDENCE_K]\n\n            top_senses = self.compute_sense_alternatives_fast(proto_probs, temperature=TRG_TEMPERATURE)\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            if token_word_map and token_idx in token_word_map and isinstance(token_word_map[token_idx], str) and token_word_map[token_idx].strip():\n                token_value = token_word_map[token_idx]\n            else:\n                token_value = raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n        except Exception as e:\n            if VERBOSE_LOGGING or DEBUG_DISCOVERY:\n                print(f\"[TRG] Evidence error {token_idx}: {e}\")\n            return self.create_fallback_evidence(token_idx, tokens)\n\n    def extract_context_window(self, token_idx: int, tokens: List[str], token_word_map: Optional[dict]) -> List[str]:\n        context_window = 2\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens: List[str] = []\n\n        current_word_id = None\n        if token_word_map:\n            current_word_id = token_word_map.get(token_idx)\n\n        for i in range(start_idx, end_idx):\n            if i == token_idx:\n                continue\n\n            if current_word_id and token_word_map and token_word_map.get(i) == current_word_id:\n                continue\n\n            if i >= len(tokens):\n                continue\n\n            r_tok = tokens[i]\n            clean_token = str(r_tok).replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n\n            if not is_word_start(r_tok, token_word_map, i):\n                if token_word_map is None and len(clean_token) > 2 and any(c.isalpha() for c in clean_token):\n                    pass\n                else:\n                    continue\n\n            if _has_is_valid_token:\n                try:\n                    ok = is_valid_token(r_tok, self.special_tokens, self.tokenizer, language=self.language)\n                except Exception:\n                    ok = fallback_is_valid_token(r_tok, self.special_tokens, self.tokenizer, self.language)\n            else:\n                ok = fallback_is_valid_token(r_tok, self.special_tokens, self.tokenizer, self.language)\n\n            if ok and len(clean_token) > 0:\n                if token_word_map and isinstance(token_word_map.get(i, \"\"), str) and token_word_map[i].strip():\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    evidence_tokens.append(clean_token)\n        return evidence_tokens\n\n    def safe_extract_proto_probs(self, token_idx: int, dscd_outputs: Dict) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            pp_all = self._get_key(dscd_outputs, \"proto_probs\", \"protoprobs\")\n            if pp_all is None:\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            if isinstance(pp_all, list):\n                if len(pp_all) == 0:\n                    return torch.tensor([1.0], dtype=torch.float32)\n\n                row0 = pp_all[0]\n\n                if isinstance(row0, list):\n                    if token_idx < len(row0):\n                        v = row0[token_idx]\n                        if isinstance(v, torch.Tensor):\n                            vv = v.detach().cpu().float().flatten()\n                            return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n                        if isinstance(v, (list, tuple, np.ndarray)):\n                            vv = torch.as_tensor(v, dtype=torch.float32).flatten()\n                            return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n                        return torch.tensor([float(v)], dtype=torch.float32)\n\n                if isinstance(row0, torch.Tensor):\n                    t = row0\n                    if t.ndim == 2 and token_idx < t.shape[0]:\n                        vv = t[token_idx].detach().cpu().float().flatten()\n                        return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n                    vv = t.detach().cpu().float().flatten()\n                    return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n\n                if isinstance(pp_all, list) and token_idx < len(pp_all):\n                    v = pp_all[token_idx]\n                    if isinstance(v, torch.Tensor):\n                        vv = v.detach().cpu().float().flatten()\n                        return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n\n            if isinstance(pp_all, torch.Tensor):\n                t = pp_all\n                if t.ndim == 3 and t.size(0) > 0 and token_idx < t.size(1):\n                    vv = t[0, token_idx].detach().cpu().float().flatten()\n                    return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n                if t.ndim == 2 and token_idx < t.size(0):\n                    vv = t[token_idx].detach().cpu().float().flatten()\n                    return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n                vv = t.detach().cpu().float().flatten()\n                return vv if vv.numel() > 0 else torch.tensor([1.0], dtype=torch.float32)\n\n        except Exception:\n            pass\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def safe_extract_uncertainty(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n            U_all = self._get_key(dscd_outputs, \"uncertainties\")\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n        except Exception:\n            pass\n        return 0.5\n\n    def safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n            G_all = self._get_key(dscd_outputs, \"gates\")\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n        except Exception:\n            pass\n        return 0.0\n\n    def safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n            S_all = self._get_key(dscd_outputs, \"span_preds\", \"spanpreds\")\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    elif row.ndim == 1 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    else:\n                        return 0.0\n                elif isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    span_val = float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n                else:\n                    return 0.0\n\n                if span_val < 0.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or current_time - self.last_warning_time > 60.0:\n                        if VERBOSE_LOGGING or DEBUG_DISCOVERY:\n                            print(f\"[TRG] Negative span {span_val:.3f} -> 0.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 0.0\n                if span_val > 1.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or current_time - self.last_warning_time > 60.0:\n                        if VERBOSE_LOGGING or DEBUG_DISCOVERY:\n                            print(f\"[TRG] Span {span_val:.3f} > 1.0 -> 1.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 1.0\n                return span_val\n        except Exception:\n            pass\n        return 0.0\n\n    def compute_span(self, sense_probs) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n\n            if len(probs) < 2:\n                return 0.0\n\n            sorted_probs = sorted([float(p) for p in probs], reverse=True)\n            span = float(sorted_probs[0] - float(sorted_probs[1]))\n            return max(0.0, min(1.0, span))\n        except Exception:\n            return 0.0\n\n    def compute_sense_alternatives_fast(self, proto_probs: torch.Tensor, temperature: float = 1.0) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n\n            probs = proto_probs.flatten().float()\n            probs = torch.clamp(probs, min=1e-10, max=1.0)\n\n            if temperature != 1.0 and probs.numel() > 1:\n                log_probs = torch.log(probs)\n                scaled_log_probs = log_probs / float(temperature)\n                probs = torch.softmax(scaled_log_probs, dim=0)\n\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                topk = min(3, int(indices.numel()))\n                return [(f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item())) for i in range(topk)]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def create_fallback_evidence(self, token_idx: int, tokens: List[str]) -> Dict:\n        if isinstance(tokens, list) and 0 <= token_idx < len(tokens):\n            token = tokens[token_idx]\n        else:\n            token = \"UNK\"\n\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"get_discovered_homographs\"):\n                    homograph_tokens = set(self.dscd_module.get_discovered_homographs())\n                elif hasattr(self.dscd_module, \"prototypestores\"):\n                    for token, store in self.dscd_module.prototypestores.items():\n                        try:\n                            sz = int(store.size()) if hasattr(store, \"size\") and callable(store.size) else 0\n                        except Exception:\n                            try:\n                                cents = getattr(store, \"centroids\", None)\n                                sz = int(len(cents)) if cents is not None else 0\n                            except Exception:\n                                sz = 0\n                        if sz >= 2:\n                            clean = str(token).replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n                            homograph_tokens.add(clean)\n        except Exception:\n            pass\n        return homograph_tokens\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(self, embed_dim: Optional[int] = None, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(TRG_GEN_EMBED)\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n\n        if dscd_module is None:\n            if VERBOSE_LOGGING or DEBUG_DISCOVERY:\n                print(\"[TRG] No DSCD module - homograph detection disabled\")\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(tokenizer, language=language, dscd_module=dscd_module)\n\n        self.silver_buffer = deque(maxlen=int(MAX_SILVER_BUFFER))\n        self.silver_lock = threading.Lock()\n\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"explanations_generated\": 0,\n            \"high_confidence_explanations\": 0,\n            \"low_confidence_explanations\": 0,\n            \"empty_evidence_count\": 0,\n            \"total_evidence_tokens\": 0,\n            \"tokens_filtered_wordstart\": 0,\n            \"tokens_filtered_validity\": 0,\n            \"tokens_filtered_ambiguity\": 0,\n            \"dscd_homographs_explained\": 0,\n        }\n        self.stats_lock = threading.Lock()\n\n        if VERBOSE_LOGGING or DEBUG_DISCOVERY:\n            print(\"[TRG] Initialized\")\n            print(f\"  - Uncertainty: {TRG_UNCERTAINTY_THRESHOLD:.2f}\")\n            print(f\"  - Span: {TRG_SPAN_THRESHOLD:.2f}\")\n            print(f\"  - Temperature: {TRG_TEMPERATURE:.2f}\")\n            print(\"  - Mode: DATA-DRIVEN\")\n\n    def update_stats(self, evidence: Dict, is_dscd_homograph: bool = False) -> None:\n        with self.stats_lock:\n            self.stats[\"explanations_generated\"] += 1\n            if is_dscd_homograph:\n                self.stats[\"dscd_homographs_explained\"] += 1\n\n            if not evidence.get(\"evidence_tokens\"):\n                self.stats[\"empty_evidence_count\"] += 1\n            else:\n                self.stats[\"total_evidence_tokens\"] += len(evidence[\"evidence_tokens\"])\n\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= TAU_ACCEPT:\n                self.stats[\"high_confidence_explanations\"] += 1\n            elif confidence <= TRG_UNCERTAINTY_THRESHOLD:\n                self.stats[\"low_confidence_explanations\"] += 1\n\n            if self.stats[\"explanations_generated\"] % self.stats_reset_interval == 0:\n                if DEBUG_DISCOVERY:\n                    current_stats = self.get_statistics()\n                    print(f\"[TRG-STATS] After {self.stats['explanations_generated']}\")\n                    print(f\"  High conf: {current_stats['high_confidence_rate']:.2%}\")\n                    print(f\"  DSCD: {current_stats['dscd_homograph_rate']:.2%}\")\n                self.reset_statistics()\n\n    def add_to_silver_buffer(self, evidence: Dict, explanation: str, tokens: List[str]) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n            with self.silver_lock:\n                self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        is_dscd_homograph: bool = False,\n    ) -> Tuple[str, Dict]:\n        if self.training or not ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n        else:\n            is_valid = fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n\n        if not is_valid and not is_word_start(raw_token, token_word_map, token_idx):\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx, tokens, dscd_outputs,\n                token_word_map=token_word_map,\n                decoder_attention=decoder_attention\n            )\n\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self.update_stats(evidence, is_dscd_homograph=is_dscd_homograph)\n            self.add_to_silver_buffer(evidence, explanation_text, tokens)\n\n            return explanation_text, evidence\n        except Exception:\n            return \"\", {}\n\n    @staticmethod\n    def _tolist_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n        try:\n            if isinstance(x, torch.Tensor):\n                if x.ndim == 0:\n                    return [float(x.item())]\n                if x.ndim == 1:\n                    return [float(v.item()) for v in x]\n                if x.ndim == 2:\n                    return [float(v.item()) for v in x[0]]\n                return [float(v.item()) for v in x.flatten()]\n            if isinstance(x, (list, tuple)):\n                out = []\n                for v in x:\n                    if isinstance(v, torch.Tensor):\n                        if v.ndim == 0:\n                            out.append(float(v.item()))\n                        elif v.numel() == 1:\n                            out.append(float(v.flatten()[0].item()))\n                        else:\n                            out.append(0.0)\n                    elif isinstance(v, (int, float, np.number)):\n                        out.append(float(v))\n                    else:\n                        try:\n                            out.append(float(v))\n                        except Exception:\n                            out.append(0.0)\n                return out\n            if isinstance(x, (int, float, np.number)):\n                return [float(x)]\n            return [float(x)]\n        except Exception:\n            return []\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        uncertainty_threshold: Optional[float] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        max_explanations: int = MAX_EXPLANATIONS_PER_SENTENCE,\n    ) -> List[Dict]:\n\n        if self.training or not ENABLE_TRG_INFERENCE:\n            return []\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(TRG_UNCERTAINTY_THRESHOLD)\n\n        strict_uncertainty = max(TRG_UNCERTAINTY_THRESHOLD, uncertainty_threshold)\n\n        explanations: List[Dict] = []\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\")\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all is None:\n                S_all = dscd_outputs.get(\"spanpreds\", None)\n\n            if not U_all or not U_all[0]:\n                return explanations\n\n            U = self._tolist_helper(U_all[0])\n            if S_all and S_all[0]:\n                S = self._tolist_helper(S_all[0])\n            else:\n                S = [0.0] * len(U)\n\n            if len(S) < len(U):\n                S.extend([0.0] * (len(U) - len(S)))\n\n            if not U:\n                return explanations\n\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n\n            word_groups = defaultdict(list)\n            for idx, tok in enumerate(tokens):\n                if idx >= len(U):\n                    break\n\n                if token_word_map and idx in token_word_map and token_word_map[idx]:\n                    word_key = token_word_map[idx]\n                else:\n                    word_key = f\"RAW_{idx}_{tok}\"\n\n                word_groups[word_key].append({\n                    \"idx\": idx,\n                    \"tok\": tok,\n                    \"u\": float(U[idx]),\n                    \"s\": float(S[idx])\n                })\n\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n\n            for word_key, group in word_groups.items():\n                max_u = max(g[\"u\"] for g in group)\n                max_s = max(g[\"s\"] for g in group)\n\n                best_subword = max(group, key=lambda x: x[\"u\"] + x[\"s\"])\n                idx = best_subword[\"idx\"]\n\n                clean_tok = str(word_key).replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(clean_tok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        valid = fallback_is_valid_token(clean_tok, self.special_tokens, self.tokenizer, self.language)\n                else:\n                    valid = fallback_is_valid_token(clean_tok, self.special_tokens, self.tokenizer, self.language)\n\n                if not valid:\n                    with self.stats_lock:\n                        self.stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                in_dscd = clean_tok in dscd_homographs\n\n                if in_dscd:\n                    priority = 1\n                elif max_s > TRG_SPAN_THRESHOLD:\n                    priority = 2\n                elif max_u > strict_uncertainty:\n                    priority = 3\n                else:\n                    with self.stats_lock:\n                        self.stats[\"tokens_filtered_ambiguity\"] += 1\n                    continue\n\n                candidates.append((idx, max_u, max_s, clean_tok, priority, idx))\n\n            if not candidates:\n                return explanations\n\n            candidates.sort(key=lambda t: (t[4], -t[2], -t[1], t[5]))\n\n            for token_idx, u, s, clean_tok, priority, _ in candidates[:max_explanations]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx, tokens, dscd_outputs,\n                        token_word_map=token_word_map,\n                        decoder_attention=decoder_attention,\n                        is_dscd_homograph=(priority == 1)\n                    )\n\n                    if explanation_text and evidence:\n                        display_token = clean_tok\n\n                        explanations.append({\n                            \"token_idx\": token_idx,\n                            \"token\": display_token,\n                            \"explanation\": explanation_text,\n                            \"uncertainty\": u,\n                            \"span\": s,\n                            \"dscd_discovered\": (priority == 1),\n                            \"priority\": priority\n                        })\n                except Exception:\n                    continue\n\n        except Exception:\n            pass\n\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self.stats_lock:\n            total = max(self.stats[\"explanations_generated\"], 1)\n            if self.stats[\"explanations_generated\"] > 0:\n                avg_evidence_tokens = self.stats[\"total_evidence_tokens\"] / total\n            else:\n                avg_evidence_tokens = 0.0\n\n            return {\n                **self.stats.copy(),\n                \"high_confidence_rate\": self.stats[\"high_confidence_explanations\"] / total,\n                \"low_confidence_rate\": self.stats[\"low_confidence_explanations\"] / total,\n                \"empty_evidence_rate\": self.stats[\"empty_evidence_count\"] / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.stats[\"dscd_homographs_explained\"] / total\n            }\n\n    def reset_statistics(self) -> None:\n        with self.stats_lock:\n            self.stats = {\n                \"explanations_generated\": 0,\n                \"high_confidence_explanations\": 0,\n                \"low_confidence_explanations\": 0,\n                \"empty_evidence_count\": 0,\n                \"total_evidence_tokens\": 0,\n                \"tokens_filtered_wordstart\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n                \"dscd_homographs_explained\": 0,\n            }\n\n    def clear_silver_buffer(self) -> None:\n        with self.silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        print(\"=\" * 60)\n        print(\"[TRG-TEST] Testing\")\n        print(\"=\" * 60)\n\n        if not ENABLE_TRG_INFERENCE:\n            print(\"[TRG] inference disabled, enabling for test...\")\n\n        try:\n            tokens = [\"_\", \"Kal\", \"er\", \"_\", \"Pa\", \"ta\"]\n            dscd_outputs = {\n                \"proto_probs\": [[torch.tensor([0.6, 0.4])] for _ in tokens],\n                \"uncertainties\": [[0.1], [0.5], [0.2], [0.1], [0.05], [0.0]],\n                \"span_preds\": [[0.05], [0.3], [0.1], [0.05], [0.0], [0.0]],\n                \"gates\": [[0.2], [0.8], [0.3], [0.2], [0.0], [0.0]]\n            }\n            token_word_map = {\n                0: \"_\",\n                1: \"Kaler\",\n                2: \"Kaler\",\n                3: \"_\",\n                4: \"Pata\",\n                5: \"Pata\"\n            }\n\n            self.eval()\n            explanations = self.process_sentence_for_explanations(\n                tokens=tokens,\n                dscd_outputs=dscd_outputs,\n                token_word_map=token_word_map,\n                max_explanations=3\n            )\n\n            print(f\"  Generated {len(explanations)} explanations\")\n            if len(explanations) > 0:\n                for i, expl in enumerate(explanations, 1):\n                    print(f\"  {i}. {expl['token']}: {expl['explanation'][:50]}... (u={expl['uncertainty']:.2f})\")\n\n            found_kaler = any(e[\"token\"] == \"Kaler\" for e in explanations)\n            if found_kaler:\n                print(\"  [SUCCESS] Found aggregated word 'Kaler'\")\n            else:\n                print(\"  [WARN] Did not find 'Kaler' (check thresholds)\")\n\n            stats = self.get_statistics()\n            print(f\"  Stats: {stats['explanations_generated']} total\")\n            self.reset_statistics()\n            stats_after = self.get_statistics()\n            assert stats_after[\"explanations_generated\"] == 0\n            print(\"  Reset OK\")\n            print(\"  All tests passed\")\n            print(\"=\" * 60)\n            return True\n        except Exception as e:\n            print(f\"  [FAIL] failed: {e}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n            print(\"=\" * 60)\n            return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Ready (DATA-DRIVEN, SUBWORD-AWARE) - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\n","metadata":{"id":"svk-wKO7H4J3","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:44.948672Z","iopub.execute_input":"2026-01-07T14:57:44.949233Z","iopub.status.idle":"2026-01-07T14:57:45.067450Z","shell.execute_reply.started":"2026-01-07T14:57:44.949167Z","shell.execute_reply":"2026-01-07T14:57:45.066692Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 5: TRG Ready (DATA-DRIVEN, SUBWORD-AWARE) - ALL FIXES APPLIED\n================================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: MEMORY-OPTIMIZED TATN MODEL (FAST INFERENCE MODE ADDED)\n# ==============================================================================\nfrom typing import List, Dict, Optional, Any, Tuple\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\nimport unicodedata\n\ntry:\n    SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    SOURCE_LANGUAGE = \"bn\"\n    TARGET_LANGUAGE = \"en\"\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return int(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return float(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return bool(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\nDSCD_BUFFER_SIZE = _get_int_global(\"DSCD_BUFFER_SIZE\", 50)\nDSCD_MAX_PROTOS = _get_int_global(\"DSCD_MAX_PROTOS\", 8)\nDSCD_N_MIN = _get_int_global(\"DSCD_N_MIN\", 2)\nDSCD_DISPERSION_THRESHOLD = _get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.15)\n\nENABLE_ASBN_TRAINING = _get_bool_global(\"ENABLE_ASBN_TRAINING\", True)\nENABLE_TRG_INFERENCE = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\nMEMORY_CLEANUP_FREQUENCY = _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 2000)\n\nNUM_GPUS = _get_int_global(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 1)\nUSE_GC = _get_bool_global(\"GRADIENT_CHECKPOINTING\", False)\nDSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\"DSCD_ENABLE_TRAINING_CLUSTERING\", True)\n\nLAMBDA_ASBN = _get_float_global(\"LAMBDA_ASBN\", 0.05)\nLAMBDA_DSCD = _get_float_global(\"LAMBDA_DSCD\", 0.15)\n\nVERBOSE_LOGGING = _get_bool_global(\"VERBOSE_LOGGING\", False)\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    DEBUG_DISCOVERY = False\n\ntry:\n    DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    DEBUG_TIMING = False\n\nPERIODIC_DISCOVERY_FREQUENCY = _get_int_global(\"PERIODIC_DISCOVERY_FREQUENCY\", 200)\nVALIDATION_CHECK_INTERVAL = _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 200)\n\nSPAN_THRESHOLD = _get_float_global(\"SPAN_THRESHOLD\", 0.20)\nUNCERTAINTY_THRESHOLD = _get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.15)\n\nTRG_UNCERTAINTY_THRESHOLD = _get_float_global(\"TRG_UNCERTAINTY_THRESHOLD\", _get_float_global(\"TAU_LOW\", 0.15))\nTAU_LOW = _get_float_global(\"TAU_LOW\", 0.15)\n\nTRAIN_DOMAIN = _get_int_global(\"TRAIN_DOMAIN\", 0)\nTEST_DOMAIN = _get_int_global(\"TEST_DOMAIN\", 1)\nUSE_DOMAIN_LABELS = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_BN_TOKEN_ID = 128025\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        return enc_output[0]\n    return None\n\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int,\n) -> Dict[str, Any]:\n    defaults = {\n        \"h_augmented\": torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32),\n        \"proto_probs\": [\n            [torch.tensor([1.0], device=device, dtype=torch.float32) for _ in range(seq_len)]\n            for _ in range(batch_size)\n        ],\n        \"uncertainties\": [\n            [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n            for _ in range(batch_size)\n        ],\n        \"gates\": [\n            [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n            for _ in range(batch_size)\n        ],\n        \"span_preds\": [\n            [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n            for _ in range(batch_size)\n        ],\n        \"proto_assignments\": [\n            torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)\n        ],\n    }\n    \n    if not isinstance(raw, dict):\n        return defaults\n    \n    out = defaults.copy()\n    \n    try:\n        haug_key = None\n        if \"haugmented\" in raw and raw[\"haugmented\"] is not None:\n            haug_key = \"haugmented\"\n        elif \"h_augmented\" in raw and raw[\"h_augmented\"] is not None:\n            haug_key = \"h_augmented\"\n        \n        if haug_key:\n            h = raw[haug_key]\n            if isinstance(h, torch.Tensor) and h.shape == (batch_size, seq_len, embed_dim):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = h.to(device).reshape(batch_size, seq_len, embed_dim)\n                except Exception:\n                    pass\n    except Exception:\n        pass\n    \n    for list_key in (\"proto_probs\", \"uncertainties\", \"gates\", \"span_preds\"):\n        list_key_alt = list_key.replace(\"_\", \"\")\n        actual_key = list_key if list_key in raw else list_key_alt\n        \n        if actual_key in raw and raw[actual_key] is not None:\n            try:\n                val = raw[actual_key]\n                if isinstance(val, list) and len(val) == batch_size:\n                    safe_batch = []\n                    for b_row in val:\n                        if isinstance(b_row, list):\n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < len(b_row):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            vv = v.to(device)\n                                            if list_key == \"proto_probs\":\n                                                vv = vv.float().flatten()\n                                                if vv.numel() == 0:\n                                                    vv = torch.tensor([1.0], device=device, dtype=torch.float32)\n                                            else:\n                                                if vv.numel() != 1:\n                                                    vv = vv.float().mean()\n                                                vv = vv.float()\n                                            safe_row.append(vv)\n                                        else:\n                                            if list_key == \"proto_probs\":\n                                                safe_row.append(\n                                                    torch.as_tensor(v, device=device, dtype=torch.float32).flatten()\n                                                )\n                                            else:\n                                                safe_row.append(torch.tensor(float(v), device=device, dtype=torch.float32))\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(torch.tensor([1.0], device=device, dtype=torch.float32))\n                                        else:\n                                            safe_row.append(torch.tensor(0.0, device=device, dtype=torch.float32))\n                                except Exception:\n                                    if list_key == \"proto_probs\":\n                                        safe_row.append(torch.tensor([1.0], device=device, dtype=torch.float32))\n                                    else:\n                                        safe_row.append(torch.tensor(0.0, device=device, dtype=torch.float32))\n                            safe_batch.append(safe_row)\n                        else:\n                            if list_key == \"proto_probs\":\n                                safe_batch.append(\n                                    [torch.tensor([1.0], device=device, dtype=torch.float32) for _ in range(seq_len)]\n                                )\n                            else:\n                                safe_batch.append(\n                                    [torch.tensor(0.0, device=device, dtype=torch.float32) for _ in range(seq_len)]\n                                )\n                    out[list_key] = safe_batch\n            except Exception:\n                pass\n    \n    try:\n        pa_key = None\n        if \"protoassignments\" in raw and raw[\"protoassignments\"] is not None:\n            pa_key = \"protoassignments\"\n        elif \"proto_assignments\" in raw and raw[\"proto_assignments\"] is not None:\n            pa_key = \"proto_assignments\"\n        \n        if pa_key:\n            pa = raw[pa_key]\n            if isinstance(pa, list) and len(pa) == batch_size:\n                safe_pa = []\n                for b_row in pa:\n                    try:\n                        if isinstance(b_row, torch.Tensor):\n                            t = b_row.to(device).long()\n                            if t.numel() != seq_len:\n                                t = torch.zeros(seq_len, dtype=torch.long, device=device)\n                            safe_pa.append(t)\n                        else:\n                            t = torch.tensor(b_row, dtype=torch.long, device=device)\n                            if t.numel() != seq_len:\n                                t = torch.zeros(seq_len, dtype=torch.long, device=device)\n                            safe_pa.append(t)\n                    except Exception:\n                        safe_pa.append(torch.zeros(seq_len, dtype=torch.long, device=device))\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n    \n    return out\n\n\ndef aggregate_subwords_to_words(\n    subword_embeddings: torch.Tensor,\n    token_to_word_map: Dict[int, int],\n    num_words: int,\n    device: torch.device,\n) -> torch.Tensor:\n    embed_dim = subword_embeddings.size(-1)\n    word_embeddings = torch.zeros(num_words, embed_dim, device=device, dtype=subword_embeddings.dtype)\n    word_counts = torch.zeros(num_words, device=device, dtype=torch.int32)\n    \n    for token_idx, word_idx in token_to_word_map.items():\n        if 0 <= token_idx < subword_embeddings.size(0) and 0 <= word_idx < num_words:\n            word_embeddings[word_idx] += subword_embeddings[token_idx]\n            word_counts[word_idx] += 1\n    \n    for w_idx in range(num_words):\n        if word_counts[w_idx] > 0:\n            word_embeddings[w_idx] /= word_counts[w_idx]\n    \n    return word_embeddings\n\n\ndef broadcast_word_to_subword(\n    word_level_outputs: Dict[str, Any],\n    token_to_word_map: Dict[int, int],\n    seq_len: int,\n    device: torch.device,\n) -> Dict[str, Any]:\n    subword_outputs = {\n        \"haugmented\": [],\n        \"protoprobs\": [],\n        \"uncertainties\": [],\n        \"gates\": [],\n        \"spanpreds\": [],\n        \"protoassignments\": [],\n    }\n    \n    h_words = word_level_outputs.get(\"haugmented\", [])\n    proto_probs_words = word_level_outputs.get(\"protoprobs\", [])\n    uncertainties_words = word_level_outputs.get(\"uncertainties\", [])\n    gates_words = word_level_outputs.get(\"gates\", [])\n    span_preds_words = word_level_outputs.get(\"spanpreds\", [])\n    proto_assign_words = word_level_outputs.get(\"protoassignments\", [])\n    \n    for token_idx in range(seq_len):\n        word_idx = token_to_word_map.get(token_idx, -1)\n        \n        if word_idx >= 0:\n            if isinstance(h_words, list) and word_idx < len(h_words):\n                subword_outputs[\"haugmented\"].append(h_words[word_idx])\n            elif isinstance(h_words, torch.Tensor) and word_idx < h_words.size(0):\n                subword_outputs[\"haugmented\"].append(h_words[word_idx])\n            else:\n                subword_outputs[\"haugmented\"].append(torch.zeros(word_level_outputs.get(\"embed_dim\", 1024), device=device))\n            \n            if isinstance(proto_probs_words, list) and word_idx < len(proto_probs_words):\n                subword_outputs[\"protoprobs\"].append(proto_probs_words[word_idx])\n            else:\n                subword_outputs[\"protoprobs\"].append(torch.tensor([1.0], device=device, dtype=torch.float32))\n            \n            if isinstance(uncertainties_words, list) and word_idx < len(uncertainties_words):\n                subword_outputs[\"uncertainties\"].append(uncertainties_words[word_idx])\n            else:\n                subword_outputs[\"uncertainties\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            \n            if isinstance(gates_words, list) and word_idx < len(gates_words):\n                subword_outputs[\"gates\"].append(gates_words[word_idx])\n            else:\n                subword_outputs[\"gates\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            \n            if isinstance(span_preds_words, list) and word_idx < len(span_preds_words):\n                subword_outputs[\"spanpreds\"].append(span_preds_words[word_idx])\n            else:\n                subword_outputs[\"spanpreds\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            \n            if isinstance(proto_assign_words, list) and word_idx < len(proto_assign_words):\n                subword_outputs[\"protoassignments\"].append(proto_assign_words[word_idx])\n            elif isinstance(proto_assign_words, torch.Tensor) and word_idx < proto_assign_words.size(0):\n                subword_outputs[\"protoassignments\"].append(proto_assign_words[word_idx])\n            else:\n                subword_outputs[\"protoassignments\"].append(torch.tensor(-1))\n        else:\n            embed_dim = word_level_outputs.get(\"embed_dim\", 1024)\n            subword_outputs[\"haugmented\"].append(torch.zeros(embed_dim, device=device))\n            subword_outputs[\"protoprobs\"].append(torch.tensor([1.0], device=device, dtype=torch.float32))\n            subword_outputs[\"uncertainties\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            subword_outputs[\"gates\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            subword_outputs[\"spanpreds\"].append(torch.tensor(0.0, device=device, dtype=torch.float32))\n            subword_outputs[\"protoassignments\"].append(torch.tensor(-1))\n    \n    return subword_outputs\n\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        \n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n        \n        self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n            \"facebook/m2m100_418M\",\n            torch_dtype=torch.float32,\n            use_cache=False,\n        )\n        try:\n            self.mbart.config.use_cache = False\n        except Exception:\n            pass\n        \n        try:\n            en_token_id = None\n            bn_token_id = None\n            \n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                for code in (TARGET_LANGUAGE, \"en\", \"en_XX\", \"eng\"):\n                    try:\n                        v = self.tokenizer.get_lang_id(code)\n                        if v is not None:\n                            en_token_id = int(v)\n                            break\n                    except Exception:\n                        continue\n                for code in (SOURCE_LANGUAGE, \"bn\", \"bn_IN\", \"ben\"):\n                    try:\n                        v = self.tokenizer.get_lang_id(code)\n                        if v is not None:\n                            bn_token_id = int(v)\n                            break\n                    except Exception:\n                        continue\n            if en_token_id is None and hasattr(self.tokenizer, \"lang_code_to_id\"):\n                try:\n                    en_token_id = int(self.tokenizer.lang_code_to_id.get(TARGET_LANGUAGE, M2M100_EN_TOKEN_ID))\n                except Exception:\n                    en_token_id = None\n            if bn_token_id is None and hasattr(self.tokenizer, \"lang_code_to_id\"):\n                try:\n                    bn_token_id = int(self.tokenizer.lang_code_to_id.get(SOURCE_LANGUAGE, M2M100_BN_TOKEN_ID))\n                except Exception:\n                    bn_token_id = None\n            \n            if en_token_id is None:\n                en_token_id = M2M100_EN_TOKEN_ID\n            if bn_token_id is None:\n                bn_token_id = M2M100_BN_TOKEN_ID\n            \n            self.mbart.config.forced_bos_token_id = int(en_token_id)\n            self.mbart.config.decoder_start_token_id = int(en_token_id)\n            self.en_token_id = int(en_token_id)\n            self.bn_token_id = int(bn_token_id)\n            \n            if DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Language tokens: BN={bn_token_id}, EN={en_token_id}\")\n        \n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Failed to set language tokens: {e}\")\n            self.en_token_id = M2M100_EN_TOKEN_ID\n            self.bn_token_id = M2M100_BN_TOKEN_ID\n        \n        try:\n            if USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n        \n        embed_dim = int(getattr(self.mbart.config, \"d_model\", 1024))\n        \n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            try:\n                self.dscd = dscd_cls(\n                    embeddim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffersize=DSCD_BUFFER_SIZE,\n                    maxprotos=DSCD_MAX_PROTOS,\n                    nmin=max(3, DSCD_N_MIN),\n                    dispersionthreshold=min(0.08, DSCD_DISPERSION_THRESHOLD),\n                    language=SOURCE_LANGUAGE,\n                    enabletrainingclustering=DSCD_ENABLE_TRAINING_CLUSTERING,\n                    maxclusteringpoints=500,\n                    maxcandidatesperstep=1,\n                )\n            except Exception as e:\n                raise RuntimeError(f\"Failed to instantiate MemoryEfficientDSCDOnline: {e}\")\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found in globals()\")\n        \n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(embed_dim, tokenizer, language=SOURCE_LANGUAGE)\n            except Exception:\n                class _StubASBN(nn.Module):\n                    def forward(self, h, domain_labels=None):\n                        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                        return h, torch.tensor(0.0, device=dev)\n                    \n                    def forward_with_grl_simplified(self, h, *args, **kwargs):\n                        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                        zero = torch.tensor(0.0, device=dev)\n                        return zero, zero, zero, zero\n                self.asbn = _StubASBN()\n        else:\n            class _StubASBN(nn.Module):\n                def forward(self, h, domain_labels=None):\n                    dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                    return h, torch.tensor(0.0, device=dev)\n                \n                def forward_with_grl_simplified(self, h, *args, **kwargs):\n                    dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                    zero = torch.tensor(0.0, device=dev)\n                    return zero, zero, zero, zero\n            self.asbn = _StubASBN()\n        \n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg_system = trg_cls(\n                    embed_dim,\n                    tokenizer,\n                    language=SOURCE_LANGUAGE,\n                    dscd_module=self.dscd,\n                )\n            except Exception:\n                class _StubTRG:\n                    def process_sentence_for_explanations(\n                        self,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=None,\n                        uncertainty_threshold=0.1,\n                        decoder_attention=None,\n                    ):\n                        return []\n                self.trg_system = _StubTRG()\n        else:\n            class _StubTRG:\n                def process_sentence_for_explanations(\n                    self,\n                    tokens,\n                    dscd_outputs,\n                    token_word_map=None,\n                    uncertainty_threshold=0.1,\n                    decoder_attention=None,\n                ):\n                    return []\n            self.trg_system = _StubTRG()\n        \n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(\"=\" * 80)\n            print(\"[TATN-INIT] Initialized MemoryOptimizedTATNWithExplanations:\")\n            print(f\"  - Embed dim: {embed_dim}\")\n            print(f\"  - Discovery frequency: {PERIODIC_DISCOVERY_FREQUENCY}\")\n            print(f\"  - Validation interval: {VALIDATION_CHECK_INTERVAL}\")\n            print(f\"  - Lambda ASBN: {LAMBDA_ASBN}\")\n            print(f\"  - Lambda DSCD: {LAMBDA_DSCD}\")\n            print(f\"  - DSCD nmin: {max(3, DSCD_N_MIN)} (enforced)\")\n            print(f\"  - DSCD dispersion: {min(0.08, DSCD_DISPERSION_THRESHOLD)} (enforced)\")\n            print(\"=\" * 80)\n    \n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(proto_probs_list, gates_list=None, min_gate: float = 0.0) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n        \n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n        \n        if dev is None:\n            return torch.tensor(0.0)\n        \n        total = torch.tensor(0.0, device=dev)\n        count = 0\n        \n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (isinstance(gates_list, list) and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                \n                if isinstance(gl, list) and j < len(gl):\n                    try:\n                        gv = gl[j]\n                        if isinstance(gv, torch.Tensor):\n                            gv = float(gv.detach().float().mean().item())\n                        else:\n                            gv = float(gv)\n                        if gv < min_gate:\n                            continue\n                    except Exception:\n                        pass\n                \n                try:\n                    p = torch.clamp(probs.to(dev).float().flatten(), 1e-8, 1.0)\n                    H = -torch.sum(p * torch.log(p))\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n        \n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n    \n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> Tuple[List[Dict[int, int]], List[List[str]]]:\n        token_to_word_maps_batch: List[Dict[int, int]] = []\n        word_lists_batch: List[List[str]] = []\n        \n        if not _has_reconstruct_word_spans:\n            if DEBUG_DISCOVERY:\n                print(\"[TATN-WORDMAP] reconstruct_word_spans() not available - using fallback\")\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    token_to_word_map: Dict[int, int] = {}\n                    word_list: List[str] = []\n                    current_word = \"\"\n                    word_idx = 0\n                    \n                    for i, tok in enumerate(tokens):\n                        clean = (\n                            tok.replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .replace(\"##\", \"\")\n                            .replace(\"@@\", \"\")\n                            .strip()\n                            .lower()\n                        )\n                        \n                        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n                            if current_word:\n                                word_list.append(current_word)\n                                word_idx += 1\n                            current_word = clean\n                            token_to_word_map[i] = word_idx\n                        else:\n                            current_word += clean\n                            token_to_word_map[i] = word_idx\n                    \n                    if current_word:\n                        word_list.append(current_word)\n                    \n                    if word_list:\n                        token_to_word_maps_batch.append(token_to_word_map)\n                        word_lists_batch.append(word_list)\n                    else:\n                        token_to_word_maps_batch.append({i: 0 for i in range(min(5, seq_len))})\n                        word_lists_batch.append([f\"tok{i}\" for i in range(min(5, seq_len))])\n                except Exception:\n                    token_to_word_maps_batch.append({i: 0 for i in range(min(5, seq_len))})\n                    word_lists_batch.append([f\"tok{i}\" for i in range(min(5, seq_len))])\n            return token_to_word_maps_batch, word_lists_batch\n        \n        if DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructing word maps for {batch_size} samples...\")\n        \n        for b in range(batch_size):\n            try:\n                if src_texts and b < len(src_texts) and isinstance(src_texts[b], str) and src_texts[b].strip():\n                    orig_text = src_texts[b]\n                else:\n                    try:\n                        orig_text = self.tokenizer.decode(input_ids[b], skip_special_tokens=True)\n                    except Exception:\n                        orig_text = \"\"\n                \n                if not orig_text.strip():\n                    token_to_word_maps_batch.append({i: 0 for i in range(min(5, seq_len))})\n                    word_lists_batch.append([f\"tok{i}\" for i in range(min(5, seq_len))])\n                    continue\n                \n                wm, words = reconstruct_word_spans(self.tokenizer, orig_text, max_length=seq_len)\n                \n                if not isinstance(wm, dict):\n                    wm = {}\n                if not isinstance(words, list):\n                    words = []\n                \n                token_to_word_map: Dict[int, int] = {}\n                word_list: List[str] = []\n                \n                for word in words:\n                    if isinstance(word, str) and word.strip():\n                        clean_word = unicodedata.normalize(\"NFKC\", word.strip().lower())\n                        if clean_word:\n                            word_list.append(clean_word)\n                \n                for token_idx, word_str in wm.items():\n                    if isinstance(word_str, str) and word_str.strip():\n                        clean_word = unicodedata.normalize(\"NFKC\", word_str.strip().lower())\n                        if clean_word and clean_word in word_list:\n                            word_idx = word_list.index(clean_word)\n                            token_to_word_map[token_idx] = word_idx\n                \n                if word_list and token_to_word_map:\n                    token_to_word_maps_batch.append(token_to_word_map)\n                    word_lists_batch.append(word_list)\n                else:\n                    token_to_word_maps_batch.append({i: 0 for i in range(min(5, seq_len))})\n                    word_lists_batch.append([f\"tok{i}\" for i in range(min(5, seq_len))])\n                \n                if DEBUG_DISCOVERY and b == 0:\n                    print(f\"[TATN-WORDMAP] Sample 0: {len(word_list)} words, {len(token_to_word_map)} mapped tokens\")\n            \n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN-WORDMAP] Reconstruction failed for sample {b}: {e}\")\n                token_to_word_maps_batch.append({i: 0 for i in range(min(5, seq_len))})\n                word_lists_batch.append([f\"tok{i}\" for i in range(min(5, seq_len))])\n        \n        total_words = sum(len(wl) for wl in word_lists_batch)\n        if DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructed {total_words} words across {batch_size} samples\")\n        \n        return token_to_word_maps_batch, word_lists_batch\n    \n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None,\n    ) -> Optional[torch.Tensor]:\n        if not USE_DOMAIN_LABELS:\n            return None\n        \n        try:\n            if self.training:\n                return torch.full((batch_size,), TRAIN_DOMAIN, dtype=torch.long, device=device)\n            else:\n                return torch.full((batch_size,), TEST_DOMAIN, dtype=torch.long, device=device)\n        except Exception:\n            return None\n    \n    @staticmethod\n    def _safe_take_key_static(\n        dscd_struct: Dict[str, Any],\n        key: str,\n        b_index: int,\n        seq_len: int,\n        device: torch.device,\n    ):\n        if key == \"proto_probs\":\n            out = [torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)]\n        else:\n            out = [torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)]\n        \n        try:\n            val = dscd_struct.get(key, None)\n            if val is None:\n                return out\n            \n            if key == \"proto_probs\":\n                if isinstance(val, list) and len(val) > b_index:\n                    row = val[b_index]\n                    if isinstance(row, list):\n                        for t in range(min(seq_len, len(row))):\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                vv = v.to(device).float().flatten()\n                                out[t] = vv if vv.numel() > 0 else torch.tensor([1.0], device=device, dtype=torch.float32)\n                            else:\n                                try:\n                                    vv = torch.as_tensor(v, dtype=torch.float32, device=device).flatten()\n                                    out[t] = vv if vv.numel() > 0 else torch.tensor([1.0], device=device, dtype=torch.float32)\n                                except Exception:\n                                    pass\n                return out\n            \n            if isinstance(val, list) and len(val) > b_index:\n                row = val[b_index]\n                if isinstance(row, list):\n                    for t in range(min(seq_len, len(row))):\n                        v = row[t]\n                        try:\n                            if isinstance(v, torch.Tensor):\n                                out[t] = torch.tensor(float(v.detach().float().mean().item()), device=device)\n                            else:\n                                out[t] = torch.tensor(float(v), device=device)\n                        except Exception:\n                            pass\n                elif isinstance(row, torch.Tensor):\n                    if row.dim() == 1:\n                        for t in range(min(seq_len, int(row.size(0)))):\n                            try:\n                                out[t] = torch.tensor(float(row[t].item()), device=device)\n                            except Exception:\n                                pass\n                return out\n            \n            if isinstance(val, torch.Tensor):\n                if val.dim() >= 2 and int(val.size(0)) > b_index:\n                    for t in range(min(seq_len, int(val.size(1)))):\n                        try:\n                            v = val[b_index, t]\n                            if isinstance(v, torch.Tensor):\n                                if v.numel() == 1:\n                                    out[t] = torch.tensor(float(v.item()), device=device)\n                                else:\n                                    out[t] = v.to(device).float().mean()\n                            else:\n                                out[t] = torch.tensor(float(v), device=device)\n                        except Exception:\n                            pass\n        except Exception:\n            pass\n        \n        return out\n    \n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = True,\n        track_stats: bool = False,\n        fast_inference: bool = False,\n    ):\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n        \n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(f\"Expected 2D tensors, got {input_ids.shape}, {attention_mask.shape}\")\n        \n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n        \n        if (\n            torch.cuda.is_available()\n            and MEMORY_CLEANUP_FREQUENCY > 0\n            and current_step % MEMORY_CLEANUP_FREQUENCY == 0\n        ):\n            for i in range(min(NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            if gc.isenabled():\n                gc.collect()\n        \n        if self.training and DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if (current_step - self.last_discovery_step) >= PERIODIC_DISCOVERY_FREQUENCY:\n                try:\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(\"\\n\" + \"=\" * 80)\n                        print(f\"[TATN] PERIODIC DISCOVERY @ step {current_step}\")\n                        print(\"=\" * 80)\n                    \n                    start_time = time.time()\n                    if hasattr(self.dscd, 'periodic_discovery_check'):\n                        self.dscd.periodic_discovery_check(current_step, PERIODIC_DISCOVERY_FREQUENCY)\n                    elapsed = time.time() - start_time\n                    self.last_discovery_step = current_step\n                    \n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        num_tokens = len(self.dscd.prototypestores) if hasattr(self.dscd, 'prototypestores') else 0\n                        total_protos = sum(s.size() for s in self.dscd.prototypestores.values()) if hasattr(self.dscd, 'prototypestores') else 0\n                        num_homographs = len(self.dscd.discoveredhomographs) if hasattr(self.dscd, 'discoveredhomographs') else 0\n                        \n                        print(f\"[TATN] Discovery completed in {elapsed:.2f}s\")\n                        print(f\"[TATN]   Tokens: {num_tokens}\")\n                        print(f\"[TATN]   Prototypes: {total_protos}\")\n                        print(f\"[TATN]   Homographs: {num_homographs}\")\n                        print(\"=\" * 80 + \"\\n\")\n                except Exception as e:\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(f\"[TATN] Discovery failed: {e}\")\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n        \n        if not self.training and VALIDATION_CHECK_INTERVAL > 0:\n            if (current_step - self.last_validation_step) >= VALIDATION_CHECK_INTERVAL:\n                try:\n                    if DEBUG_DISCOVERY:\n                        print(f\"\\n[TATN-VALIDATION] Step {current_step}\")\n                        num_tokens = len(self.dscd.prototypestores) if hasattr(self.dscd, 'prototypestores') else 0\n                        total_protos = sum(s.size() for s in self.dscd.prototypestores.values()) if hasattr(self.dscd, 'prototypestores') else 0\n                        num_homographs = len(self.dscd.discoveredhomographs) if hasattr(self.dscd, 'discoveredhomographs') else 0\n                        print(f\"  - Tokens: {num_tokens}\")\n                        print(f\"  - Prototypes: {total_protos}\")\n                        print(f\"  - Homographs: {num_homographs}\")\n                    self.last_validation_step = current_step\n                except Exception:\n                    pass\n        \n        enc_outputs = None\n        try:\n            enc_outputs = self.mbart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        except Exception:\n            try:\n                enc_outputs = self.mbart.get_encoder()(input_ids=input_ids, attention_mask=attention_mask)\n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN] Encoder failed: {e}\")\n                enc_outputs = None\n        \n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            try:\n                emb = self.mbart.get_input_embeddings()(input_ids).to(device)\n                h = emb\n            except Exception:\n                h = torch.zeros(\n                    batch_size,\n                    seq_len,\n                    int(getattr(self.mbart.config, \"d_model\", 1024)),\n                    device=device,\n                )\n        \n        embed_dim = int(h.size(-1))\n        training_mode = labels is not None and self.training\n        \n        token_to_word_maps_batch, word_lists_batch = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n        \n        domain_labels = self._extract_domain_labels(batch_size, device, src_texts)\n        \n        if use_dscd and not fast_inference:\n            try:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN-DSCD] Processing {batch_size} samples with word-level aggregation...\")\n                \n                batch_word_embeddings = []\n                for b in range(batch_size):\n                    token_to_word_map_b = token_to_word_maps_batch[b]\n                    num_words = len(word_lists_batch[b])\n                    word_emb = aggregate_subwords_to_words(h[b], token_to_word_map_b, num_words, device)\n                    batch_word_embeddings.append(word_emb)\n                \n                max_word_len = max(w.size(0) for w in batch_word_embeddings)\n                h_words_padded = torch.zeros(batch_size, max_word_len, embed_dim, device=device)\n                for b, w_emb in enumerate(batch_word_embeddings):\n                    h_words_padded[b, :w_emb.size(0)] = w_emb\n                \n                tokentypes_batch = word_lists_batch\n                \n                raw_dscd = self.dscd.forward(\n                    tokenembeddings=h_words_padded,\n                    tokentypes=tokentypes_batch,\n                    trainmode=self.training,\n                    tokenwordmap=None,\n                    inputids=None,\n                    attentionmask=None,\n                    fast_inference=False,\n                )\n                \n                dscd_subword_batch = []\n                for b in range(batch_size):\n                    word_level_out = {\n                        \"haugmented\": raw_dscd.get(\"haugmented\", [])[b] if isinstance(raw_dscd.get(\"haugmented\"), (list, tuple)) else (raw_dscd.get(\"haugmented\")[b] if isinstance(raw_dscd.get(\"haugmented\"), torch.Tensor) else []),\n                        \"protoprobs\": raw_dscd.get(\"protoprobs\", [])[b] if isinstance(raw_dscd.get(\"protoprobs\"), list) else [],\n                        \"uncertainties\": raw_dscd.get(\"uncertainties\", [])[b] if isinstance(raw_dscd.get(\"uncertainties\"), list) else [],\n                        \"gates\": raw_dscd.get(\"gates\", [])[b] if isinstance(raw_dscd.get(\"gates\"), list) else [],\n                        \"spanpreds\": raw_dscd.get(\"spanpreds\", [])[b] if isinstance(raw_dscd.get(\"spanpreds\"), list) else [],\n                        \"protoassignments\": raw_dscd.get(\"protoassignments\", [])[b] if isinstance(raw_dscd.get(\"protoassignments\"), list) else torch.zeros(0),\n                        \"embed_dim\": embed_dim,\n                    }\n                    \n                    subword_out = broadcast_word_to_subword(\n                        word_level_out,\n                        token_to_word_maps_batch[b],\n                        seq_len,\n                        device\n                    )\n                    dscd_subword_batch.append(subword_out)\n                \n                combined_raw_dscd = {\n                    \"haugmented\": [],\n                    \"protoprobs\": [],\n                    \"uncertainties\": [],\n                    \"gates\": [],\n                    \"spanpreds\": [],\n                    \"protoassignments\": [],\n                }\n                \n                for b in range(batch_size):\n                    if isinstance(dscd_subword_batch[b][\"haugmented\"], list):\n                        combined_raw_dscd[\"haugmented\"].append(torch.stack(dscd_subword_batch[b][\"haugmented\"]))\n                    else:\n                        combined_raw_dscd[\"haugmented\"].append(torch.zeros(seq_len, embed_dim, device=device))\n                    \n                    combined_raw_dscd[\"protoprobs\"].append(dscd_subword_batch[b][\"protoprobs\"])\n                    combined_raw_dscd[\"uncertainties\"].append(dscd_subword_batch[b][\"uncertainties\"])\n                    combined_raw_dscd[\"gates\"].append(dscd_subword_batch[b][\"gates\"])\n                    combined_raw_dscd[\"spanpreds\"].append(dscd_subword_batch[b][\"spanpreds\"])\n                    combined_raw_dscd[\"protoassignments\"].append(dscd_subword_batch[b][\"protoassignments\"])\n                \n                try:\n                    combined_raw_dscd[\"haugmented\"] = torch.stack(combined_raw_dscd[\"haugmented\"])\n                except Exception:\n                    combined_raw_dscd[\"haugmented\"] = h.clone()\n                \n                raw_dscd = combined_raw_dscd\n                \n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN-DSCD] Word-level processing complete\")\n            \n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD word-level processing failed: {e}\")\n                    traceback.print_exc()\n                raw_dscd = {\n                    \"haugmented\": h.detach().clone(),\n                    \"protoprobs\": [\n                        [torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)]\n                        for _ in range(batch_size)\n                    ],\n                    \"uncertainties\": [\n                        [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                        for _ in range(batch_size)\n                    ],\n                    \"gates\": [\n                        [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                        for _ in range(batch_size)\n                    ],\n                    \"spanpreds\": [\n                        [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                        for _ in range(batch_size)\n                    ],\n                    \"protoassignments\": [\n                        torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)\n                    ],\n                }\n        else:\n            raw_dscd = {\n                \"haugmented\": h.detach().clone(),\n                \"protoprobs\": [\n                    [torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"spanpreds\": [\n                    [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                    for _ in range(batch_size)\n                ],\n                \"protoassignments\": [\n                    torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)\n                ],\n            }\n        \n        dscd = _normalize_dscd_outputs(raw_dscd, batch_size, seq_len, device, embed_dim)\n        \n        h_aug = dscd.get(\"h_augmented\", h)\n        \n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN] h_augmented shape mismatch (expected {h.shape}, got {getattr(h_aug, 'shape', None)})\"\n                )\n            h_aug = h\n        \n        if use_asbn and domain_labels is not None:\n            try:\n                h_aug, _ = self.asbn.forward(h_aug, domain_labels=domain_labels)\n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN] ASBN forward (BN) failed: {e}\")\n        \n        try:\n            enc_for_decoder = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=(getattr(enc_outputs, \"hidden_states\", None) if enc_outputs else None),\n                attentions=(getattr(enc_outputs, \"attentions\", None) if enc_outputs else None),\n            )\n        except Exception:\n            enc_for_decoder = (h_aug,)\n        \n        if training_mode:\n            try:\n                if labels is not None:\n                    decoder_input_ids = labels.clone()\n                    decoder_input_ids = torch.where(\n                        decoder_input_ids == -100,\n                        torch.full_like(decoder_input_ids, self.tokenizer.pad_token_id),\n                        decoder_input_ids,\n                    )\n                    \n                    bos_column = torch.full(\n                        (batch_size, 1),\n                        int(self.mbart.config.decoder_start_token_id),\n                        dtype=torch.long,\n                        device=device,\n                    )\n                    decoder_input_ids = torch.cat([bos_column, decoder_input_ids[:, :-1]], dim=1)\n                    decoder_attention_mask = (decoder_input_ids != self.tokenizer.pad_token_id).long()\n                else:\n                    decoder_input_ids = None\n                    decoder_attention_mask = None\n                \n                seq_outputs = self.mbart(\n                    input_ids=None,\n                    attention_mask=attention_mask,\n                    encoder_outputs=enc_for_decoder,\n                    decoder_input_ids=decoder_input_ids,\n                    decoder_attention_mask=decoder_attention_mask,\n                    labels=labels,\n                    use_cache=False,\n                    return_dict=True,\n                )\n                translation_loss = getattr(seq_outputs, \"loss\", None)\n                if translation_loss is None:\n                    translation_loss = torch.tensor(0.0, device=device)\n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN] Decoder forward failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                translation_loss = torch.tensor(0.0, device=device)\n            \n            if use_asbn:\n                try:\n                    asbn_ret = self.asbn.forward_with_grl_simplified(\n                        h_aug,\n                        dscd.get(\"proto_probs\", None),\n                        dscd.get(\"uncertainties\", None),\n                        dscd.get(\"gates\", None),\n                        token_word_map=token_to_word_maps_batch,\n                        domain_labels=domain_labels,\n                        global_step=current_step,\n                    )\n                    if isinstance(asbn_ret, (tuple, list)):\n                        asbn_loss = asbn_ret[0]\n                    else:\n                        asbn_loss = asbn_ret\n                    \n                    if not isinstance(asbn_loss, torch.Tensor):\n                        asbn_loss = torch.tensor(float(asbn_loss), device=device)\n                    else:\n                        asbn_loss = asbn_loss.to(device)\n                    \n                    if not torch.isfinite(asbn_loss):\n                        asbn_loss = torch.tensor(0.0, device=device)\n                    asbn_loss = torch.clamp(asbn_loss, 0.0, 10.0)\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[TATN] ASBN forward failed: {e}\")\n                    asbn_loss = torch.tensor(0.0, device=device)\n            else:\n                asbn_loss = torch.tensor(0.0, device=device)\n            \n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(\n                    dscd.get(\"proto_probs\", []),\n                    gates_list=dscd.get(\"gates\", []),\n                    min_gate=0.0,\n                )\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(float(dscd_reg), device=device)\n                else:\n                    dscd_reg = dscd_reg.to(device)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device)\n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD reg failed: {e}\")\n                dscd_reg = torch.tensor(0.0, device=device)\n            \n            total_loss = translation_loss + LAMBDA_ASBN * asbn_loss + LAMBDA_DSCD * dscd_reg\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n            \n            if not torch.isfinite(total_loss):\n                if DEBUG_DISCOVERY:\n                    print(\"[TATN] NaN/Inf detected in total_loss - using translation_loss only\")\n                total_loss = translation_loss if torch.isfinite(translation_loss) else torch.tensor(1.0, device=device)\n            \n            try:\n                del enc_outputs, h, raw_dscd\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return total_loss\n        \n        explanations_list: List[List[Dict[str, Any]]] = []\n        \n        if (not self.training) and ENABLE_TRG_INFERENCE:\n            if DEBUG_DISCOVERY:\n                print(f\"\\n[TATN-INFERENCE] Starting TRG for {batch_size} samples\")\n            \n            tokens_batch: List[List[str]] = []\n            \n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    if hasattr(self.tokenizer, \"convert_ids_to_tokens\"):\n                        toks = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    else:\n                        toks = []\n                    if not toks:\n                        toks = [\"UNK\"] * seq_len\n                    elif len(toks) < seq_len:\n                        toks = toks + [\"\"] * (seq_len - len(toks))\n                    elif len(toks) > seq_len:\n                        toks = toks[:seq_len]\n                except Exception:\n                    toks = [\"UNK\"] * seq_len\n                tokens_batch.append(toks)\n            \n            decoder_attention = None\n            \n            try:\n                total_explanations = 0\n                \n                for b in range(batch_size):\n                    per_sent = {\n                        \"proto_probs\": self._safe_take_key_static(dscd, \"proto_probs\", b, seq_len, device),\n                        \"uncertainties\": self._safe_take_key_static(dscd, \"uncertainties\", b, seq_len, device),\n                        \"gates\": self._safe_take_key_static(dscd, \"gates\", b, seq_len, device),\n                        \"span_preds\": self._safe_take_key_static(dscd, \"span_preds\", b, seq_len, device),\n                    }\n                    \n                    try:\n                        word_map_for_trg = {}\n                        for tok_idx, word_idx in token_to_word_maps_batch[b].items():\n                            if word_idx < len(word_lists_batch[b]):\n                                word_map_for_trg[tok_idx] = word_lists_batch[b][word_idx]\n                        \n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=word_map_for_trg,\n                            uncertainty_threshold=TRG_UNCERTAINTY_THRESHOLD,\n                            decoder_attention=decoder_attention,\n                        )\n                        batch_exps = exps if isinstance(exps, list) else []\n                        explanations_list.append(batch_exps)\n                        total_explanations += len(batch_exps)\n                        \n                        if DEBUG_DISCOVERY and b < 2:\n                            print(f\"[TATN-INFERENCE] Sample {b}: {len(batch_exps)} explanations\")\n                    \n                    except Exception as e:\n                        if DEBUG_DISCOVERY:\n                            print(f\"[TATN-INFERENCE] TRG failed for sample {b}: {e}\")\n                        explanations_list.append([])\n                \n                if DEBUG_DISCOVERY:\n                    print(f\"\\n[TATN-INFERENCE] Total explanations: {total_explanations}\")\n                    if total_explanations == 0:\n                        print(\"[TATN-INFERENCE] NO EXPLANATIONS GENERATED\")\n            \n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"[TATN-INFERENCE] TRG generation failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                explanations_list = [[] for _ in range(batch_size)]\n        else:\n            explanations_list = [[] for _ in range(batch_size)]\n        \n        outputs = {\n            \"encoder_outputs\": enc_outputs,\n            \"dscd_outputs\": dscd,\n            \"sense_augmented_embeddings\": h_aug,\n            \"explanations\": explanations_list,\n            \"asbn_loss\": torch.tensor(0.0, device=device),\n            \"ambiguity_signals\": {\n                \"span\": dscd.get(\"span_preds\", []),\n                \"uncertainty\": dscd.get(\"uncertainties\", []),\n                \"confidence\": [\n                    [\n                        1.0\n                        - (\n                            float(u)\n                            if isinstance(u, (float, int))\n                            else (float(u.item()) if isinstance(u, torch.Tensor) else 1.0)\n                        )\n                        for u in row\n                    ]\n                    for row in dscd.get(\"uncertainties\", [])\n                ],\n                \"proto_probs\": dscd.get(\"proto_probs\", []),\n            },\n        }\n        \n        try:\n            del h, raw_dscd\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        return outputs\n    \n    def generate(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        max_length: int = 128,\n        num_beams: int = 5,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> torch.Tensor:\n        try:\n            enc_outputs = self.mbart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n            \n            enc_wrapped = BaseModelOutput(\n                last_hidden_state=(enc_outputs.last_hidden_state if hasattr(enc_outputs, \"last_hidden_state\") else enc_outputs[0]),\n                hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                attentions=getattr(enc_outputs, \"attentions\", None),\n            )\n            \n            return self.mbart.generate(\n                input_ids=None,\n                attention_mask=attention_mask,\n                encoder_outputs=enc_wrapped,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=early_stopping,\n                forced_bos_token_id=int(self.mbart.config.forced_bos_token_id),\n                repetition_penalty=1.2,\n                no_repeat_ngram_size=3,\n                length_penalty=1.0,\n                do_sample=False,\n                **kwargs,\n            )\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[TATN-GENERATE] Failed: {e}\")\n            raise\n    \n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n        )\n    \n    def get_component_stats(self) -> Dict[str, Any]:\n        stats: Dict[str, Any] = {\n            \"global_step\": self.global_step,\n            \"last_discovery_step\": self.last_discovery_step,\n            \"last_validation_step\": self.last_validation_step,\n        }\n        \n        try:\n            if hasattr(self.dscd, 'prototypestores'):\n                num_tokens = len(self.dscd.prototypestores)\n                total_protos = sum(s.size() for s in self.dscd.prototypestores.values())\n                num_homographs = len(self.dscd.discoveredhomographs) if hasattr(self.dscd, 'discoveredhomographs') else 0\n                \n                stats[\"dscd\"] = {\n                    \"total_tokens\": num_tokens,\n                    \"total_prototypes\": total_protos,\n                    \"num_homographs\": num_homographs,\n                }\n        except Exception:\n            pass\n        \n        try:\n            if hasattr(self.asbn, \"get_detailed_stats\"):\n                stats[\"asbn\"] = self.asbn.get_detailed_stats()\n        except Exception:\n            pass\n        \n        try:\n            if hasattr(self.trg_system, \"get_statistics\"):\n                stats[\"trg\"] = self.trg_system.get_statistics()\n        except Exception:\n            pass\n        \n        return stats\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 6: MemoryOptimizedTATNWithExplanations Ready\")\nprint(\"=\" * 80)\nprint(\"NEW FEATURES:\")\nprint(\"  ✓ Word-level DSCD aggregation\")\nprint(\"  ✓ Subword broadcasting\")\nprint(\"  ✓ Case-insensitive word keys\")\nprint(\"  ✓ Enforced nmin=3, dispersion=0.08\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"KZbMDpIYH4J4","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:45.069166Z","iopub.execute_input":"2026-01-07T14:57:45.069961Z","iopub.status.idle":"2026-01-07T14:57:46.195913Z","shell.execute_reply.started":"2026-01-07T14:57:45.069928Z","shell.execute_reply":"2026-01-07T14:57:46.195057Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 6: MemoryOptimizedTATNWithExplanations Ready\n================================================================================\nNEW FEATURES:\n  ✓ Word-level DSCD aggregation\n  ✓ Subword broadcasting\n  ✓ Case-insensitive word keys\n  ✓ Enforced nmin=3, dispersion=0.08\n================================================================================\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP (PURE UNSUPERVISED) - FIXED\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept (NameError, ValueError, TypeError):\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept (NameError, ValueError, TypeError):\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept (NameError, ValueError, TypeError):\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept (NameError, ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept (NameError, TypeError):\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept (NameError, ValueError, TypeError):\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _PERIODIC_DISCOVERY_FREQUENCY = 3000\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\ndef _resolve_dscd_stores(dscd):\n    if dscd is None:\n        return {}\n    for attr_name in (\"prototypestores\", \"prototype_stores\"):\n        stores = getattr(dscd, attr_name, None)\n        if isinstance(stores, dict):\n            return stores\n        if stores is not None:\n            try:\n                return dict(stores)\n            except Exception:\n                pass\n    return {}\n\ndef _resolve_dscd_lock(dscd):\n    if dscd is None:\n        return None\n    for name in (\"bufferlock\", \"buffer_lock\", \"clusteringlock\", \"clustering_lock\"):\n        lock = getattr(dscd, name, None)\n        if lock is not None:\n            return lock\n    return None\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            return dscd.get_discovered_homographs()\n\n        homographs = set()\n        stores = _resolve_dscd_stores(dscd)\n        lock = _resolve_dscd_lock(dscd)\n\n        if lock:\n            with lock:\n                for token, store in stores.items():\n                    try:\n                        size_val = store.size() if hasattr(store, 'size') else len(getattr(store, 'centroids', []))\n                        if size_val >= 2:\n                            clean_token = str(token).replace('▁', '').replace('Ġ', '').replace('##', '').strip().lower()\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            for token, store in stores.items():\n                try:\n                    size_val = store.size() if hasattr(store, 'size') else len(getattr(store, 'centroids', []))\n                    if size_val >= 2:\n                        clean_token = str(token).replace('▁', '').replace('Ġ', '').replace('##', '').strip().lower()\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\ndef synchronous_dscd_clustering(model: torch.nn.Module, force: bool = True) -> Dict[str, Any]:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        \n        if dscd is None:\n            return {'success': False, 'reason': 'no_dscd'}\n        \n        stores = _resolve_dscd_stores(dscd)\n        lock = _resolve_dscd_lock(dscd)\n        \n        if not stores:\n            return {'success': True, 'clustered': 0, 'reason': 'no_stores'}\n        \n        cluster_count = 0\n        failed_count = 0\n        \n        tokens_to_cluster = list(stores.keys())\n        \n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[SYNC-CLUSTER] Clustering {len(tokens_to_cluster)} tokens...\")\n        \n        for token in tokens_to_cluster:\n            try:\n                if hasattr(dscd, 'clusterbuffertoprototypeshierarchical'):\n                    if lock:\n                        with lock:\n                            success = dscd.clusterbuffertoprototypeshierarchical(token)\n                    else:\n                        success = dscd.clusterbuffertoprototypeshierarchical(token)\n                    \n                    if success:\n                        cluster_count += 1\n                    else:\n                        failed_count += 1\n                else:\n                    failed_count += 1\n            except Exception:\n                failed_count += 1\n                continue\n        \n        discovery_result = {}\n        if hasattr(dscd, 'discover_homographs'):\n            try:\n                if lock:\n                    with lock:\n                        discovery_result = dscd.discover_homographs()\n                else:\n                    discovery_result = dscd.discover_homographs()\n            except Exception:\n                pass\n        \n        result = {\n            'success': True,\n            'clustered': cluster_count,\n            'failed': failed_count,\n            'total_tokens': len(tokens_to_cluster),\n            'discovered': discovery_result.get('discovered', 0),\n            'total_homographs': discovery_result.get('total_homographs', 0),\n        }\n        \n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[SYNC-CLUSTER] Results: {cluster_count} clustered, {failed_count} failed\")\n            print(f\"[SYNC-CLUSTER] Discovery: {result.get('discovered', 0)} new homographs\")\n            print(f\"[SYNC-CLUSTER] Total homographs: {result.get('total_homographs', 0)}\")\n        \n        return result\n        \n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[SYNC-CLUSTER] Error: {type(e).__name__}: {str(e)[:200]}\")\n        return {'success': False, 'reason': str(e)[:200]}\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module,\n    tokenizer,\n    epoch: int,\n    global_step: int,\n    source_lang: str,\n    target_lang: str,\n    max_length: int,\n    device: torch.device\n) -> Dict[str, Any]:\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n\n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n\n    if not isinstance(device, torch.device):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dscd_homographs = _get_dscd_homographs(model)\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[VALIDATION] DSCD discovered homographs: {len(dscd_homographs)}\")\n        if dscd_homographs:\n            print(f\"[VALIDATION] Sample: {list(dscd_homographs)[:10]}\")\n\n    validation_results = {\n        'epoch': epoch,\n        'step': global_step,\n        'translations_success': 0,\n        'translations_failed': 0,\n        'explanations_generated': 0,\n        'dscd_homographs_explained': 0,\n        'reference_homographs_explained': 0,\n        'avg_explanation_confidence': 0.0,\n        'dscd_quality_score': 0.0,\n        'dscd_multi_sense_tokens': 0,\n        'dscd_total_prototypes': 0,\n        'asbn_domain_loss': 0.0,\n        'asbn_domain_accuracy': 0.0,\n        'asbn_source_accuracy': 0.0,\n        'asbn_target_accuracy': 0.0,\n        'trg_total_explanations': 0,\n        'validation_completed': False,\n    }\n\n    try:\n        core_model.eval()\n\n        val_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল=tap/call\"),\n            (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল=tomorrow/yesterday\"),\n            (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা=leaf/page\"),\n            (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক=bank/embankment\"),\n            (\"আমি ভালো আছি।\", \"I am fine\", \"No ambiguity\"),\n            (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"No ambiguity\"),\n            (\"এটা আমার বই।\", \"This is my book\", \"No ambiguity\"),\n            (\"আজ আবহাওয়া ভালো।\", \"Weather is good today\", \"No ambiguity\"),\n            (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল=fruit/result\"),\n            (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা=head/top\"),\n        ]\n\n        print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n        print(\"-\" * 80)\n\n        confidences = []\n        dscd_homograph_words_detected = set()\n        reference_homograph_words_detected = set()\n\n        mbart_obj = None\n        try:\n            mbart_obj = getattr(core_model, \"mbart\", None)\n        except Exception:\n            mbart_obj = None\n\n        try:\n            try:\n                tokenizer.src_lang = source_lang\n            except Exception:\n                pass\n\n            forced_id = None\n            try:\n                if hasattr(tokenizer, \"get_lang_id\"):\n                    for code in (target_lang, \"en_XX\", \"en\", \"eng\"):\n                        try:\n                            lid = tokenizer.get_lang_id(code)\n                            if lid is not None:\n                                forced_id = int(lid)\n                                break\n                        except Exception:\n                            continue\n                elif hasattr(tokenizer, \"lang_code_to_id\"):\n                    forced_id = tokenizer.lang_code_to_id.get(target_lang, None)\n                    if forced_id is not None:\n                        forced_id = int(forced_id)\n            except Exception:\n                forced_id = None\n\n            if forced_id is None:\n                try:\n                    forced_id = int(globals().get('M2M100_EN_TOKEN_ID', 128022))\n                except Exception:\n                    forced_id = 128022\n\n            orig_use_cache = None\n            try:\n                if mbart_obj is not None and hasattr(mbart_obj.config, \"use_cache\"):\n                    orig_use_cache = mbart_obj.config.use_cache\n                    mbart_obj.config.use_cache = True\n            except Exception:\n                orig_use_cache = None\n\n            for idx, (src, expected, note) in enumerate(val_sentences, 1):\n                try:\n                    enc = tokenizer(\n                        src,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_length,\n                        add_special_tokens=True\n                    )\n                    enc = {\n                        k: (\n                            v.to(device, non_blocking=True)\n                            if isinstance(v, torch.Tensor)\n                            else v\n                        )\n                        for k, v in enc.items()\n                    }\n\n                    if forced_id is not None:\n                        try:\n                            if mbart_obj is not None:\n                                mbart_obj.config.forced_bos_token_id = int(forced_id)\n                                mbart_obj.config.decoder_start_token_id = int(forced_id)\n                        except Exception:\n                            pass\n\n                    out_ids = None\n                    try:\n                        gen_src = mbart_obj if mbart_obj is not None else core_model\n                        if hasattr(gen_src, \"generate\"):\n                            out_ids = gen_src.generate(\n                                enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                max_length=max_length,\n                                num_beams=4,\n                                do_sample=False,\n                                early_stopping=True,\n                                pad_token_id=int(\n                                    getattr(tokenizer, \"pad_token_id\", 1)\n                                ),\n                                forced_bos_token_id=int(forced_id)\n                                if forced_id is not None\n                                else None,\n                                repetition_penalty=1.2,\n                                no_repeat_ngram_size=3,\n                                length_penalty=1.0,\n                            )\n                    except AttributeError:\n                        if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                            print(\n                                \"[VALIDATION] Warning: generation raised AttributeError (protobuf incompatibility).\"\n                            )\n                            _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                        out_ids = None\n                    except Exception as e:\n                        print(\n                            f\"[VALIDATION] Generation error: {type(e).__name__}: {str(e)[:200]}\"\n                        )\n                        out_ids = None\n\n                    translation = \"\"\n                    if out_ids is not None and (\n                        (isinstance(out_ids, torch.Tensor) and out_ids.numel() > 0)\n                        or (\n                            isinstance(out_ids, (list, tuple))\n                            and len(out_ids) > 0\n                        )\n                    ):\n                        try:\n                            if isinstance(out_ids, (list, tuple)):\n                                translation = tokenizer.batch_decode(\n                                    out_ids, skip_special_tokens=True\n                                )[0] if out_ids else \"\"\n                            else:\n                                translation = (\n                                    tokenizer.decode(\n                                        out_ids[0], skip_special_tokens=True\n                                    )\n                                    if out_ids.size(0) > 0\n                                    else \"\"\n                                )\n                        except AttributeError:\n                            if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                                print(\n                                    \"[VALIDATION] Warning: decode raised AttributeError (protobuf).\"\n                                )\n                                _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                            translation = \"\"\n                        except Exception as e:\n                            print(\n                                f\"[VALIDATION] Decode error: {type(e).__name__}: {str(e)[:200]}\"\n                            )\n                            translation = \"\"\n                    else:\n                        translation = \"\"\n\n                    if translation:\n                        validation_results['translations_success'] += 1\n                    else:\n                        validation_results['translations_failed'] += 1\n                        print(\n                            f\"  {idx:2d}. Translation failed: {note[:30]:30s}\"\n                        )\n                        continue\n\n                    explanation_status = \"\"\n                    try:\n                        if 'translate_with_explanations' in globals():\n                            res = translate_with_explanations(\n                                model, tokenizer, src\n                            )\n                            exps = res.get('explanations', [])\n                            validation_results['explanations_generated'] += len(\n                                exps\n                            )\n\n                            if exps:\n                                explanation_status = f\"{len(exps)} expl\"\n                                for exp in exps:\n                                    try:\n                                        conf = exp.get('confidence', 0.5)\n                                        confidences.append(float(conf))\n\n                                        word = exp.get('ambiguous_word', '').strip()\n                                        clean_word = (\n                                            word.replace('▁', '')\n                                            .replace('Ġ', '')\n                                            .lower()\n                                        )\n\n                                        if clean_word in dscd_homographs:\n                                            validation_results[\n                                                'dscd_homographs_explained'\n                                            ] += 1\n                                            dscd_homograph_words_detected.add(\n                                                clean_word\n                                            )\n\n                                        if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                                            validation_results[\n                                                'reference_homographs_explained'\n                                            ] += 1\n                                            reference_homograph_words_detected.add(\n                                                clean_word\n                                            )\n                                    except Exception:\n                                        pass\n                            else:\n                                explanation_status = \"no expl\"\n                        else:\n                            explanation_status = \"unavailable\"\n                    except Exception as e:\n                        explanation_status = f\"error: {type(e).__name__}\"\n\n                    print(\n                        f\"  {idx:2d}. {explanation_status:15s} \"\n                        f\"{note[:30]:30s} -> {translation[:200]}\"\n                    )\n                    del enc\n                    if out_ids is not None:\n                        del out_ids\n\n                except Exception as e:\n                    validation_results['translations_failed'] += 1\n                    print(\n                        f\"  {idx:2d}. ERROR: {note[:30]:30s} -> {type(e).__name__}\"\n                    )\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n        finally:\n            try:\n                if mbart_obj is not None and orig_use_cache is not None:\n                    mbart_obj.config.use_cache = orig_use_cache\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                try:\n                    torch.cuda.synchronize()\n                except Exception:\n                    pass\n\n            clear_all_gpu_caches()\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n        try:\n            dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n            if dscd and hasattr(dscd, 'validate_prototypes'):\n                lock = _resolve_dscd_lock(dscd)\n\n                if lock:\n                    with lock:\n                        quality_results = dscd.validate_prototypes()\n                else:\n                    quality_results = dscd.validate_prototypes()\n\n                validation_results['dscd_quality_score'] = quality_results.get(\n                    'quality_score', 0.0\n                )\n                validation_results['dscd_multi_sense_tokens'] = quality_results.get(\n                    'multi_sense_tokens', 0\n                )\n                validation_results['dscd_total_prototypes'] = quality_results.get(\n                    'total_prototypes', 0\n                )\n                print(\n                    f\"  - Quality Score: {validation_results['dscd_quality_score']:.1%}\"\n                )\n                print(\n                    f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\"\n                )\n                print(\n                    f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\"\n                )\n            else:\n                print(\"  - Validation not available\")\n        except Exception as e:\n            print(f\"  - Validation failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] ASBN Training Statistics:\")\n        try:\n            asbn = core_model.asbn if hasattr(core_model, 'asbn') else None\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                asbn_stats = asbn.get_detailed_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get(\n                    'domain_loss', 0.0\n                )\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get(\n                    'domain_accuracy', 0.0\n                )\n                validation_results['asbn_source_accuracy'] = asbn_stats.get(\n                    'source_accuracy', 0.0\n                )\n                validation_results['asbn_target_accuracy'] = asbn_stats.get(\n                    'target_accuracy', 0.0\n                )\n                print(\n                    f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\"\n                )\n                print(\n                    f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n                )\n                print(\n                    f\"  - Source Accuracy: {validation_results['asbn_source_accuracy']:.2%}\"\n                )\n                print(\n                    f\"  - Target Accuracy: {validation_results['asbn_target_accuracy']:.2%}\"\n                )\n            elif asbn and hasattr(asbn, 'get_asbn_stats'):\n                asbn_stats = asbn.get_asbn_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get(\n                    'domain_loss', 0.0\n                )\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get(\n                    'domain_accuracy', 0.0\n                )\n                print(\n                    f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\"\n                )\n                print(\n                    f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n                )\n            else:\n                print(\"  - ASBN statistics not available\")\n        except Exception as e:\n            print(f\"  - ASBN stats retrieval failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] TRG Explanation Statistics:\")\n        try:\n            trg = core_model.trg_system if hasattr(core_model, 'trg_system') else None\n            if trg and hasattr(trg, 'get_statistics'):\n                trg_stats = trg.get_statistics()\n                validation_results['trg_total_explanations'] = trg_stats.get(\n                    'explanations_generated', 0\n                )\n                print(\n                    f\"  - Total explanations: {validation_results['trg_total_explanations']}\"\n                )\n                print(\n                    f\"  - High confidence rate: {trg_stats.get('high_confidence_rate', 0):.1%}\"\n                )\n                print(\n                    f\"  - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\"\n                )\n            else:\n                print(\"  - TRG statistics not available\")\n        except Exception as e:\n            print(f\"  - TRG stats retrieval failed: {type(e).__name__}\")\n\n        if confidences:\n            validation_results['avg_explanation_confidence'] = sum(\n                confidences\n            ) / len(confidences)\n\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(\n            f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\"\n        )\n        print(\n            f\"  - Explanations generated: {validation_results['explanations_generated']}\"\n        )\n        print(\n            f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\"\n        )\n        print(\n            f\"  - DSCD homographs explained: {validation_results['dscd_homographs_explained']}\"\n        )\n        print(\n            f\"  - Reference homographs explained: {validation_results['reference_homographs_explained']}\"\n        )\n\n        if dscd_homograph_words_detected:\n            print(\n                f\"  - DSCD homographs detected: {', '.join(sorted(dscd_homograph_words_detected))}\"\n            )\n\n        print(\n            f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\"\n        )\n        print(\n            f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\"\n        )\n        print(\n            f\"  - ASBN Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n        )\n\n        warnings = []\n        if validation_results['translations_failed'] > len(val_sentences) // 2:\n            warnings.append(\"High translation failure rate\")\n        if validation_results['explanations_generated'] == 0:\n            warnings.append(\"No explanations generated\")\n        if validation_results['dscd_quality_score'] < 0.3:\n            warnings.append(\"Low DSCD quality score\")\n        if validation_results['dscd_multi_sense_tokens'] < 10:\n            warnings.append(\"Very few multi-sense tokens\")\n\n        if warnings:\n            print(\"\\n[VALIDATION] Health Warnings:\")\n            for w in warnings:\n                print(f\"  - {w}\")\n        else:\n            print(\"\\n[VALIDATION] All systems healthy\")\n\n        validation_results['validation_completed'] = True\n\n    except Exception as e:\n        print(\n            f\"\\n[VALIDATION] Critical error: {type(e).__name__}: {str(e)[:200]}\"\n        )\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n        validation_results['validation_completed'] = False\n\n    finally:\n        if was_training:\n            core_model.train()\n        clear_all_gpu_caches()\n\n    print(\"=\" * 80 + \"\\n\")\n    return validation_results\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return 0\n\n        stores = _resolve_dscd_stores(dscd)\n        lock = _resolve_dscd_lock(dscd)\n\n        if lock:\n            with lock:\n                return len(stores)\n        else:\n            return len(stores)\n\n    except Exception:\n        return 0\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        return getattr(core, 'dscd', None)\n    except Exception:\n        return None\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n\n    try:\n        dscd_homographs = _get_dscd_homographs(model)\n        items = []\n        homograph_items = []\n\n        stores = _resolve_dscd_stores(dscd)\n        lock = _resolve_dscd_lock(dscd)\n\n        if lock:\n            with lock:\n                stores_snapshot = list(stores.items())\n        else:\n            stores_snapshot = list(stores.items())\n\n        for token, store in stores_snapshot:\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                protos = store.size() if hasattr(store, \"size\") else len(\n                    getattr(store, \"centroids\", [])\n                )\n                clean_token = (\n                    str(token).replace('▁', '').replace('Ġ', '').strip().lower()\n                )\n                is_homograph = clean_token in dscd_homographs\n                buffers = getattr(dscd, 'buffers', {})\n                item = (\n                    token,\n                    total_count,\n                    protos,\n                    len(buffers.get(token, [])),\n                    is_homograph,\n                )\n                items.append(item)\n                if is_homograph:\n                    homograph_items.append(item)\n            except Exception:\n                continue\n\n        items.sort(key=lambda x: x[1], reverse=True)\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen, is_homo) in enumerate(\n                items[:top_n], 1\n            ):\n                marker = \"HOMO\" if is_homo else \"    \"\n                print(\n                    f\"{marker} {i:2d}. {str(tok)[:20]:20s} \"\n                    f\"samples={cnt:4d} protos={prot} buf={buflen}\"\n                )\n            if homograph_items:\n                print(\n                    f\"[CLUSTER-DBG] DSCD-discovered homographs: {len(homograph_items)}\"\n                )\n                for tok, cnt, prot, buflen, _ in homograph_items[:5]:\n                    print(\n                        f\"  HOMO {str(tok)[:20]:20s} samples={cnt:4d} protos={prot}\"\n                    )\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}\")\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return\n\n        if hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\n                    f\"[DISCOVERY-STATUS] Step {global_step}: {total_discovered} discovery events\"\n                )\n\n                recent = (\n                    dscd.discovered_log[-3:]\n                    if len(dscd.discovered_log) >= 3\n                    else dscd.discovered_log\n                )\n                for entry in recent:\n                    discovered = entry.get('discovered', 0)\n                    candidates = entry.get('candidates', 0)\n                    print(\n                        f\"  - {discovered}/{candidates} homographs discovered\"\n                    )\n        else:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\n                    f\"[DISCOVERY-STATUS] No discoveries yet at step {global_step}\"\n                )\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[DISCOVERY-STATUS] Error: {e}\")\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True,\n    device: Optional[torch.device] = None\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n    if device is None:\n        device = _DEVICE\n\n    print(\n        f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, \"\n        f\"accum_steps={accumulation_steps}\"\n    )\n    print(\n        f\"[TRAIN] Validation: \"\n        f\"{'enabled' if enable_validation and validate_every > 0 else 'disabled'}\"\n    )\n    print(\n        f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {device}\"\n    )\n    print(\n        f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\"\n    )\n    print(\n        \"[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt \"\n        \"after all epochs\"\n    )\n\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"trg_explanation_history\": [],\n        \"synchronous_clustering_results\": [],\n    }\n\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n\n        print(f\"\\n{'='*80}\")\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(f\"{'='*80}\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'reset_statistics'):\n                try:\n                    trg.reset_statistics()\n                    print(f\"[TRAIN] TRG statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] TRG stats reset failed: {e}\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'reset_stats'):\n                try:\n                    asbn.reset_stats()\n                    print(f\"[TRAIN] ASBN statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] ASBN stats reset failed: {e}\")\n\n        try:\n            optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n\n        progress = None\n        try:\n            progress = tqdm(\n                train_loader,\n                desc=f\"Epoch {epoch}/{epochs}\",\n                dynamic_ncols=True,\n            )\n\n            for batch_idx, batch in enumerate(progress):\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n\n                if (\n                    _DEBUG_DISCOVERY or _VERBOSE_LOGGING\n                ) and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    print(\n                        f\"\\n[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} \"\n                        f\"GlobalStep {global_step}\"\n                    )\n                    _check_discovery_status(model, global_step)\n\n                if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                    if global_step % _PERIODIC_DISCOVERY_FREQUENCY == 0:\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            dscd = getattr(core, 'dscd', None)\n                            if dscd and hasattr(dscd, 'periodic_discovery_check'):\n                                print(f\"\\n[TRAIN] Triggering periodic discovery at step {global_step}...\")\n                                dscd.periodic_discovery_check(global_step, _PERIODIC_DISCOVERY_FREQUENCY)\n                        except Exception as e:\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                print(f\"[TRAIN] Periodic discovery failed: {e}\")\n\n                if (\n                    enable_validation\n                    and validate_every\n                    and validate_every > 0\n                    and (global_step % validate_every == 0)\n                ):\n                    if accumulated_steps == 0:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n\n                        val_result = comprehensive_epoch_validation(\n                            model,\n                            tokenizer,\n                            epoch,\n                            global_step,\n                            _SOURCE_LANGUAGE,\n                            _TARGET_LANGUAGE,\n                            _MAX_LENGTH,\n                            device,\n                        )\n\n                        if val_result:\n                            training_stats['epoch_validations'].append(\n                                val_result\n                            )\n                    else:\n                        pending_validation = True\n\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    continue\n\n                try:\n                    input_ids = batch[\"input_ids\"]\n                    attention_mask = batch[\"attention_mask\"]\n                    labels = batch[\"labels\"]\n\n                    domain_labels = batch.get(\"domain_labels\", None)\n                    if domain_labels is not None:\n                        if not isinstance(domain_labels, torch.Tensor):\n                            domain_labels = None\n                        elif domain_labels.dim() == 0:\n                            domain_labels = domain_labels.unsqueeze(0)\n\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        bsz = int(input_ids.size(0))\n                        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            continue\n                        if keep != bsz:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n                            if domain_labels is not None:\n                                domain_labels = domain_labels[:keep]\n\n                    input_ids = input_ids.to(device, non_blocking=True)\n                    attention_mask = attention_mask.to(\n                        device, non_blocking=True\n                    )\n                    labels = labels.to(device, non_blocking=True)\n\n                    if domain_labels is not None:\n                        domain_labels = domain_labels.to(\n                            device, non_blocking=True\n                        )\n\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        continue\n\n                    forward_kwargs = {\n                        \"input_ids\": input_ids,\n                        \"attention_mask\": attention_mask,\n                        \"labels\": labels,\n                        \"src_texts\": batch.get(\"src_text\", None),\n                        \"token_word_map\": batch.get(\"token_word_map\", None),\n                        \"track_stats\": True,\n                    }\n\n                    amp_ctx = get_amp_ctx()\n                    with amp_ctx:\n                        forward_out = model(**forward_kwargs)\n\n                        if isinstance(forward_out, torch.Tensor):\n                            loss_tensor = forward_out\n                        elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                            loss_tensor = forward_out[\"loss\"]\n                        else:\n                            if isinstance(forward_out, (list, tuple)) and len(\n                                forward_out\n                            ) > 0 and isinstance(\n                                forward_out[0], torch.Tensor\n                            ):\n                                loss_tensor = forward_out[0]\n                            else:\n                                raise RuntimeError(\n                                    \"Model forward did not return a recognizable loss tensor\"\n                                )\n\n                        if not isinstance(loss_tensor, torch.Tensor):\n                            loss_tensor = torch.tensor(\n                                float(loss_tensor), device=device\n                            )\n                        else:\n                            loss_tensor = loss_tensor.to(device)\n\n                        if loss_tensor.numel() > 1:\n                            loss_val = float(loss_tensor.mean().item())\n                            loss_tensor = loss_tensor.mean()\n                        else:\n                            loss_val = float(loss_tensor.item())\n\n                        last_forward_loss = loss_val\n                        epoch_losses.append(loss_val)\n                        training_stats[\"total_loss\"].append(loss_val)\n\n                    loss_scaled = loss_tensor / max(1, accumulation_steps)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n\n                    if scaler.is_enabled():\n                        scaler.scale(loss_scaled).backward()\n                    else:\n                        loss_scaled.backward()\n\n                    accumulated_steps += 1\n\n                    if accumulated_steps >= accumulation_steps:\n                        try:\n                            if scaler.is_enabled():\n                                scaler.unscale_(optimizer)\n                                torch.nn.utils.clip_grad_norm_(\n                                    model.parameters(), _GRAD_CLIP_NORM\n                                )\n                                scaler.step(optimizer)\n                                scaler.update()\n                            else:\n                                torch.nn.utils.clip_grad_norm_(\n                                    model.parameters(), _GRAD_CLIP_NORM\n                                )\n                                optimizer.step()\n                            optimizer.zero_grad(set_to_none=True)\n                            training_stats[\"optimizer_updates\"] += 1\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"[OOM] OOM at step {global_step}\")\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    p.grad = None\n                                clear_all_gpu_caches()\n                                accumulated_steps = 0\n                                continue\n                            else:\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n                                print(\n                                    f\"[ERROR] Runtime error during optimizer step: {type(e).__name__}\"\n                                )\n                        except Exception as e:\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n                            print(\n                                f\"[ERROR] Exception during optimizer step: {type(e).__name__}\"\n                            )\n                        finally:\n                            accumulated_steps = 0\n                            if pending_validation:\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n\n                                val_result = comprehensive_epoch_validation(\n                                    model,\n                                    tokenizer,\n                                    epoch,\n                                    global_step,\n                                    _SOURCE_LANGUAGE,\n                                    _TARGET_LANGUAGE,\n                                    _MAX_LENGTH,\n                                    device,\n                                )\n\n                                if val_result:\n                                    training_stats['epoch_validations'].append(\n                                        val_result\n                                    )\n\n                                pending_validation = False\n\n                    if global_step % DEBUG_PRINT_INTERVAL == 0:\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cluster_count = _get_cluster_count(model)\n                        print(\n                            f\"[TRAIN-DEBUG] step={global_step} \"\n                            f\"loss={last_forward_loss:.4f} clusters={cluster_count}\"\n                        )\n                        _print_top_clusters(model, top_n=5)\n\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"[OOM] Caught OOM at step {global_step}\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            p.grad = None\n                        clear_all_gpu_caches()\n                        accumulated_steps = 0\n                        continue\n                    else:\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        print(\n                            f\"[RUNTIME] RuntimeError at step {global_step}: {type(e).__name__}\"\n                        )\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        continue\n                except Exception as e:\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    print(\n                        f\"[EXCEPTION] Exception at step {global_step}: {type(e).__name__}\"\n                    )\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    continue\n\n                processed_batches = (\n                    training_stats[\"batches_processed\"]\n                    - training_stats[\"skipped_batches\"]\n                )\n                expected_updates = max(\n                    1,\n                    math.floor(\n                        processed_batches / max(1, accumulation_steps)\n                    ),\n                )\n                success_rate = (\n                    100.0\n                    * training_stats[\"optimizer_updates\"]\n                    / expected_updates\n                    if expected_updates > 0\n                    else 0.0\n                )\n                cluster_count = _get_cluster_count(model)\n\n                next_disc_str = \"N/A\"\n                try:\n                    if (\n                        _PERIODIC_DISCOVERY_FREQUENCY\n                        and _PERIODIC_DISCOVERY_FREQUENCY > 0\n                    ):\n                        steps_to_next = (\n                            _PERIODIC_DISCOVERY_FREQUENCY\n                            - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                        )\n                        next_disc_str = f\"next_disc_in={steps_to_next}\"\n                except Exception:\n                    next_disc_str = \"next_disc_err\"\n\n                progress.set_postfix_str(\n                    f\"fwd_loss={last_forward_loss:.4f} \"\n                    f\"bwd_loss={last_backward_loss:.4f} \"\n                    f\"rate={success_rate:.1f}% \"\n                    f\"clusters={cluster_count} {next_disc_str}\"\n                )\n\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n\n        if accumulated_steps > 0:\n            try:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), _GRAD_CLIP_NORM\n                    )\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), _GRAD_CLIP_NORM\n                    )\n                    optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(\n                    f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}\"\n                )\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = (\n            training_stats[\"batches_processed\"]\n            - training_stats[\"skipped_batches\"]\n        )\n        expected_updates = max(\n            1,\n            math.floor(processed_batches / max(1, accumulation_steps)),\n        )\n        success_rate = (\n            100.0\n            * training_stats[\"optimizer_updates\"]\n            / expected_updates\n            if expected_updates > 0\n            else 0.0\n        )\n        cluster_count = _get_cluster_count(model)\n\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"  Duration (min): {epoch_duration_min:.2f}\")\n        print(f\"  Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(\n            f\"  Batches: processed={processed_batches}, \"\n            f\"skipped={training_stats['skipped_batches']}\"\n        )\n        print(f\"  Success rate: {success_rate:.1f}%\")\n        print(f\"  Clustered tokens: {cluster_count}\")\n        print(f\"  Avg epoch loss: {avg_epoch_loss:.6f}\")\n        if skip_reasons:\n            print(\"  Skip reasons:\")\n            for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"    - {k}: {v}\")\n        print(\"=\" * 80)\n\n        try:\n            print(f\"\\n[TRAIN] Running synchronous DSCD clustering after epoch {epoch}...\")\n            clustering_result = synchronous_dscd_clustering(model, force=True)\n            training_stats['synchronous_clustering_results'].append(clustering_result)\n            \n            if clustering_result.get('success', False):\n                print(f\"[TRAIN] Clustered {clustering_result.get('clustered', 0)} tokens\")\n                print(f\"[TRAIN] Discovered {clustering_result.get('discovered', 0)} new homographs\")\n                print(f\"[TRAIN] Total homographs: {clustering_result.get('total_homographs', 0)}\")\n            else:\n                print(f\"[TRAIN] Clustering incomplete: {clustering_result.get('reason', 'unknown')}\")\n        \n        except Exception as e:\n            print(f\"[TRAIN] Epoch-end clustering failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        try:\n            print(\n                f\"\\n[TRAIN] Running comprehensive validation after epoch {epoch}...\"\n            )\n\n            try:\n                optimizer.zero_grad(set_to_none=True)\n            except Exception:\n                pass\n\n            validation_results = comprehensive_epoch_validation(\n                model=model,\n                tokenizer=tokenizer,\n                epoch=epoch,\n                global_step=global_step,\n                source_lang=_SOURCE_LANGUAGE,\n                target_lang=_TARGET_LANGUAGE,\n                max_length=_MAX_LENGTH,\n                device=device,\n            )\n\n            if validation_results and validation_results.get(\n                'validation_completed', False\n            ):\n                training_stats['epoch_validations'].append(\n                    validation_results\n                )\n                training_stats['dscd_quality_history'].append(\n                    validation_results.get('dscd_quality_score', 0.0)\n                )\n                training_stats['asbn_domain_accuracy_history'].append(\n                    validation_results.get('asbn_domain_accuracy', 0.0)\n                )\n                training_stats['trg_explanation_history'].append(\n                    validation_results.get('trg_total_explanations', 0)\n                )\n\n                try:\n                    dscd = (\n                        model.module.dscd\n                        if hasattr(model, 'module')\n                        else getattr(model, 'dscd', None)\n                    )\n\n                    if dscd is not None:\n                        stores = _resolve_dscd_stores(dscd)\n                        lock = _resolve_dscd_lock(dscd)\n\n                        if lock:\n                            with lock:\n                                total_tokens = len(stores)\n                        else:\n                            total_tokens = len(stores)\n\n                        multi_sense = validation_results.get(\n                            'dscd_multi_sense_tokens', 0\n                        )\n                        ratio = (\n                            multi_sense / total_tokens\n                            if total_tokens > 0\n                            else 0.0\n                        )\n                        training_stats['multi_sense_ratio_history'].append(\n                            ratio\n                        )\n                    else:\n                        training_stats['multi_sense_ratio_history'].append(\n                            0.0\n                        )\n                except Exception:\n                    training_stats['multi_sense_ratio_history'].append(0.0)\n            else:\n                print(\"[TRAIN] Validation incomplete\")\n\n        except Exception as e:\n            print(f\"[TRAIN] Epoch validation failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    print(f\"\\n{'='*80}\")\n    print(\"TRAINING COMPLETE - RUNNING FINAL CLUSTERING\")\n    print(f\"{'='*80}\")\n\n    try:\n        print(\"\\n[TRAIN] Running final synchronous DSCD clustering...\")\n        final_clustering_result = synchronous_dscd_clustering(model, force=True)\n        training_stats['final_clustering_result'] = final_clustering_result\n        \n        if final_clustering_result.get('success', False):\n            print(f\"[TRAIN] Final clustering: {final_clustering_result.get('clustered', 0)} tokens\")\n            print(f\"[TRAIN] Final discovery: {final_clustering_result.get('discovered', 0)} new homographs\")\n            print(f\"[TRAIN] Total homographs: {final_clustering_result.get('total_homographs', 0)}\")\n        else:\n            print(f\"[TRAIN] Final clustering incomplete: {final_clustering_result.get('reason', 'unknown')}\")\n    \n    except Exception as e:\n        print(f\"[TRAIN] Final clustering failed: {type(e).__name__}\")\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    print(f\"\\n{'='*80}\")\n    print(\"SAVING FINAL CHECKPOINT\")\n    print(f\"{'='*80}\")\n\n    try:\n        checkpoint_path = Path(\"/kaggle/working/tatn_final.pt\")\n\n        core_model = model.module if hasattr(model, 'module') else model\n\n        dscd_state = {}\n        try:\n            if hasattr(core_model, 'dscd'):\n                try:\n                    dscd_state = core_model.dscd.state_dict()\n                except Exception:\n                    dscd_state = {}\n        except Exception:\n            dscd_state = {}\n\n        checkpoint_data = {\n            'epochs_trained': epochs,\n            'global_steps': global_step,\n            'final_train_loss': training_stats['epoch_losses'][-1]\n            if training_stats['epoch_losses']\n            else 0.0,\n            'model_state_dict': core_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scaler_state_dict': scaler.state_dict()\n            if scaler is not None\n            else None,\n            'training_stats': training_stats,\n            'dscd_state': dscd_state,\n            'config': {\n                'SPAN_THRESHOLD': globals().get('SPAN_THRESHOLD', 0.20),\n                'TAU_LOW': globals().get('TAU_LOW', 0.15),\n                'LAMBDA_ASBN': globals().get('LAMBDA_ASBN', 0.05),\n                'LAMBDA_DSCD': globals().get('LAMBDA_DSCD', 0.15),\n                'TRG_TEMPERATURE': globals().get('TRG_TEMPERATURE', 1.0),\n                'PERIODIC_DISCOVERY_FREQUENCY': _PERIODIC_DISCOVERY_FREQUENCY,\n                'NUM_EPOCHS': epochs,\n                'BATCH_SIZE': _BATCH_SIZE,\n                'LEARNING_RATE': optimizer.param_groups[0]['lr']\n                if optimizer and optimizer.param_groups\n                else 0.0,\n            },\n        }\n\n        torch.save(checkpoint_data, checkpoint_path)\n\n        file_size_mb = checkpoint_path.stat().st_size / (1024**2)\n\n        print(\"\\nFINAL CHECKPOINT SAVED\")\n        print(f\"   Path: {checkpoint_path}\")\n        print(f\"   Size: {file_size_mb:.2f} MB\")\n        print(f\"   Epochs trained: {epochs}\")\n        print(f\"   Global steps: {global_step}\")\n        print(\n            f\"   Final train loss: \"\n            f\"{training_stats['epoch_losses'][-1] if training_stats['epoch_losses'] else 0.0:.4f}\"\n        )\n        print(f\"{'='*80}\\n\")\n\n    except Exception as e:\n        print(f\"FINAL CHECKPOINT SAVE FAILED: {type(e).__name__}\")\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRAINING COMPLETED - FINAL SUMMARY\")\n    print(\"=\" * 80)\n\n    processed_batches = (\n        training_stats[\"batches_processed\"]\n        - training_stats[\"skipped_batches\"]\n    )\n    expected_updates = max(\n        1,\n        math.floor(processed_batches / max(1, accumulation_steps)),\n    )\n    success_rate = (\n        100.0\n        * training_stats[\"optimizer_updates\"]\n        / expected_updates\n        if expected_updates > 0\n        else 0.0\n    )\n\n    print(f\"[TRAIN] Success Rate: {success_rate:.1f}%\")\n    print(f\"[TRAIN] Total Steps: {global_step}\")\n    print(\n        f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\"\n    )\n\n    if training_stats['dscd_quality_history']:\n        print(\"\\n[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats['dscd_quality_history'], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n\n    if training_stats['asbn_domain_accuracy_history']:\n        print(\"\\n[TRAIN] ASBN Domain Accuracy Trend:\")\n        for i, acc in enumerate(\n            training_stats['asbn_domain_accuracy_history'], 1\n        ):\n            print(f\"  Epoch {i}: {acc:.1%}\")\n\n    if training_stats['trg_explanation_history']:\n        print(\"\\n[TRAIN] TRG Explanation Count Trend:\")\n        for i, count in enumerate(\n            training_stats['trg_explanation_history'], 1\n        ):\n            print(f\"  Epoch {i}: {count} explanations\")\n\n    if training_stats.get('synchronous_clustering_results'):\n        print(\"\\n[TRAIN] Synchronous Clustering Results:\")\n        for i, result in enumerate(training_stats['synchronous_clustering_results'], 1):\n            if result.get('success'):\n                print(f\"  Epoch {i}: {result.get('clustered', 0)} tokens, {result.get('discovered', 0)} homographs\")\n\n    print(\"=\" * 80)\n    return model\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training loop ready (PURE UNSUPERVISED) - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\nprint(\"NEW FEATURES:\")\nprint(\"  ✓ Synchronous clustering after each epoch\")\nprint(\"  ✓ Forced discovery before validation\")\nprint(\"  ✓ Final clustering before checkpoint save\")\nprint(\"=\" * 80)\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:46.197530Z","iopub.execute_input":"2026-01-07T14:57:46.197983Z","iopub.status.idle":"2026-01-07T14:57:46.499036Z","shell.execute_reply.started":"2026-01-07T14:57:46.197956Z","shell.execute_reply":"2026-01-07T14:57:46.498374Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 7: Training loop ready (PURE UNSUPERVISED) - ALL FIXES APPLIED\n================================================================================\nNEW FEATURES:\n  ✓ Synchronous clustering after each epoch\n  ✓ Forced discovery before validation\n  ✓ Final clustering before checkpoint save\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: INFERENCE & EVALUATION PIPELINE - COMPLETE FIXED VERSION\n# ==============================================================================\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nimport unicodedata\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\nimport threading\nimport gc\nfrom transformers.modeling_outputs import BaseModelOutput\n\ntry:\n    SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    SOURCE_LANGUAGE = \"bn\"\n    TARGET_LANGUAGE = \"en\"\n\ntry:\n    MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    MAX_LENGTH = 48\n\ntry:\n    DEVICE = DEVICE\nexcept (NameError, TypeError):\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    VERBOSE_LOGGING = False\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    DEBUG_DISCOVERY = False\n\ntry:\n    DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    DEBUG_TIMING = False\n\ntry:\n    USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    REAL_AMB_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    REAL_AMB_SPAN_THRESHOLD = 0.05\n\ntry:\n    REAL_AMB_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    REAL_AMB_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST = set(w.lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    HOMOGRAPH_REFERENCE_LIST = set(w.lower() for w in HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_BN_TOKEN_ID = 128025\n\nSUBWORD_PUNCT_SET = set(\".,!?-;:|\")\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\ndef build_token_word_map_with_cell2(\n    text: str,\n    tokenizer,\n    max_length: int = None\n) -> Tuple[Dict[int, int], List[str]]:\n    if max_length is None:\n        max_length = MAX_LENGTH\n    \n    try:\n        if _has_reconstruct_word_spans:\n            token_to_word_map, word_list = reconstruct_word_spans(\n                tokenizer,\n                text,\n                max_length=max_length\n            )\n            \n            word_list_lower = [w.lower() for w in word_list]\n            \n            return token_to_word_map, word_list_lower\n        else:\n            return build_token_word_map_fallback(text, tokenizer, max_length)\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] build_token_word_map_with_cell2 error: {e}\")\n        return build_token_word_map_fallback(text, tokenizer, max_length)\n\ndef build_token_word_map_fallback(\n    text: str,\n    tokenizer,\n    max_length: int = None\n) -> Tuple[Dict[int, int], List[str]]:\n    if max_length is None:\n        max_length = MAX_LENGTH\n    \n    try:\n        text = str(text).strip()\n        if not text:\n            return {}, []\n        \n        try:\n            toks = tokenizer.tokenize(text)\n        except Exception:\n            return {}, []\n        \n        if len(toks) > max_length:\n            toks = toks[:max_length]\n        \n        token_to_word_idx: Dict[int, int] = {}\n        word_list: List[str] = []\n        current_word_parts: List[str] = []\n        current_word_idx = 0\n        \n        for token_idx, tok in enumerate(toks):\n            t = str(tok)\n            clean = t.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\"@@\", \"\").strip().lower()\n            \n            if not clean:\n                continue\n            \n            if t.startswith(\"▁\") or t.startswith(\"Ġ\"):\n                if current_word_parts:\n                    complete_word = \"\".join(current_word_parts)\n                    if complete_word:\n                        word_list.append(complete_word)\n                        current_word_idx = len(word_list) - 1\n                    current_word_parts = []\n                \n                current_word_parts = [clean]\n                token_to_word_idx[token_idx] = len(word_list)\n            else:\n                current_word_parts.append(clean)\n                token_to_word_idx[token_idx] = len(word_list)\n        \n        if current_word_parts:\n            complete_word = \"\".join(current_word_parts)\n            if complete_word:\n                word_list.append(complete_word)\n        \n        return token_to_word_idx, word_list\n    \n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] build_token_word_map_fallback error: {e}\")\n        return {}, []\n\ndef get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, \"get_discovered_homographs\"):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        def _store_size(store) -> int:\n            try:\n                if hasattr(store, \"size\") and callable(store.size):\n                    return int(store.size())\n            except Exception:\n                pass\n            try:\n                cents = getattr(store, \"centroids\", None)\n                return int(len(cents)) if cents is not None else 0\n            except Exception:\n                return 0\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, \"bufferlock\"):\n            lock = dscd.bufferlock\n        elif hasattr(dscd, \"clusteringlock\"):\n            lock = dscd.clusteringlock\n\n        if lock:\n            with lock:\n                items = list(getattr(dscd, \"prototypestores\", {}).items())\n        else:\n            items = list(getattr(dscd, \"prototypestores\", {}).items())\n\n        for token, store in items:\n            try:\n                if _store_size(store) >= 2:\n                    clean_token = (\n                        str(token)\n                        .replace(\"▁\", \"\")\n                        .replace(\"##\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .strip()\n                        .lower()\n                    )\n                    homographs.add(clean_token)\n            except Exception:\n                continue\n\n        return homographs\n    except Exception:\n        return set()\n\ndef synchronous_clustering_for_warmup(model: torch.nn.Module) -> Dict[str, Any]:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        \n        if dscd is None:\n            return {'success': False, 'reason': 'no_dscd'}\n        \n        lock = None\n        if hasattr(dscd, \"bufferlock\"):\n            lock = dscd.bufferlock\n        elif hasattr(dscd, \"clusteringlock\"):\n            lock = dscd.clusteringlock\n        \n        if lock:\n            with lock:\n                stores_list = list(getattr(dscd, \"prototypestores\", {}).keys())\n        else:\n            stores_list = list(getattr(dscd, \"prototypestores\", {}).keys())\n        \n        if not stores_list:\n            return {'success': True, 'clustered': 0, 'reason': 'no_stores'}\n        \n        cluster_count = 0\n        failed_count = 0\n        \n        for token in stores_list:\n            try:\n                if hasattr(dscd, 'clusterbuffertoprototypeshierarchical'):\n                    if lock:\n                        with lock:\n                            success = dscd.clusterbuffertoprototypeshierarchical(token)\n                    else:\n                        success = dscd.clusterbuffertoprototypeshierarchical(token)\n                    \n                    if success:\n                        cluster_count += 1\n                    else:\n                        failed_count += 1\n                else:\n                    failed_count += 1\n            except Exception:\n                failed_count += 1\n                continue\n        \n        discovery_result = {}\n        if hasattr(dscd, 'discover_homographs'):\n            try:\n                if lock:\n                    with lock:\n                        discovery_result = dscd.discover_homographs()\n                else:\n                    discovery_result = dscd.discover_homographs()\n            except Exception:\n                pass\n        \n        result = {\n            'success': True,\n            'clustered': cluster_count,\n            'failed': failed_count,\n            'total_tokens': len(stores_list),\n            'discovered': discovery_result.get('discovered', 0),\n            'total_homographs': discovery_result.get('total_homographs', 0),\n        }\n        \n        return result\n        \n    except Exception as e:\n        return {'success': False, 'reason': str(e)[:200]}\n\nclass InferenceStatistics:\n    def __init__(self):\n        self.lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self.lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.avg_span = 0.0\n            self.avg_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        with self.lock:\n            self.total_inferences += 1\n            if result.get(\"translation\") and result[\"translation\"] != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get(\"explanations\", [])\n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                try:\n                    conf = exp.get(\"confidence\", 0.5)\n                    self.total_confidence += float(conf)\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf <= 0.4:\n                        self.low_confidence_explanations += 1\n\n                    word = str(exp.get(\"ambiguous_word\", exp.get(\"token\", \"\"))).strip()\n                    clean_word = word.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").lower().strip()\n                    if not clean_word:\n                        continue\n\n                    self.token_counts[clean_word] += 1\n                    self.token_confidences[clean_word].append(float(conf))\n\n                    if dscd_homographs and clean_word in dscd_homographs:\n                        self.dscd_homographs_explained.add(clean_word)\n\n                    if clean_word in HOMOGRAPH_REFERENCE_LIST:\n                        self.reference_homographs_explained.add(clean_word)\n\n                    self.avg_span += float(exp.get(\"span\", 0.0))\n                    self.avg_uncertainty += float(exp.get(\"uncertainty\", 0.0))\n                except Exception:\n                    pass\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self.lock:\n            total_exp = max(self.total_explanations, 1)\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n\n            return {\n                \"total_inferences\": self.total_inferences,\n                \"successful_translations\": self.successful_translations,\n                \"failed_translations\": self.failed_translations,\n                \"success_rate\": self.successful_translations / max(self.total_inferences, 1),\n                \"total_explanations\": self.total_explanations,\n                \"explanations_per_inference\": self.total_explanations / max(self.total_inferences, 1),\n                \"high_confidence_rate\": self.high_confidence_explanations / total_exp,\n                \"low_confidence_rate\": self.low_confidence_explanations / total_exp,\n                \"avg_confidence\": self.total_confidence / total_exp,\n                \"avg_span\": self.avg_span / total_exp,\n                \"avg_uncertainty\": self.avg_uncertainty / total_exp,\n                \"dscd_homographs_explained\": list(self.dscd_homographs_explained),\n                \"reference_homographs_explained\": list(self.reference_homographs_explained),\n                \"dscd_empty_warnings\": self.dscd_empty_warnings,\n                \"unique_tokens_explained\": unique_tokens,\n                \"diversity_ratio\": diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2%}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n\n        if summary[\"dscd_homographs_explained\"]:\n            print(f\"{len(summary['dscd_homographs_explained'])} DSCD homographs explained\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'][:10])}...\")\n\n        if summary[\"reference_homographs_explained\"]:\n            print(f\"{len(summary['reference_homographs_explained'])} Reference homographs explained\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'][:10])}...\")\n\n        if summary[\"dscd_empty_warnings\"] > 0:\n            print(f\"DSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80)\n\nINFERENCE_STATS = InferenceStatistics()\n\ndef to_device_batch(enc: Any, device: torch.device):\n    try:\n        if hasattr(enc, \"to\"):\n            return enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(enc, dict):\n        out = {}\n        for k, v in enc.items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                elif isinstance(v, dict):\n                    out[k] = to_device_batch(v, device)\n                elif isinstance(v, (list, tuple)):\n                    out[k] = [t.to(device) if isinstance(t, torch.Tensor) else t for t in v]\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n    return enc\n\ndef extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    if raw_out is None:\n        return {}\n\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"explanations\" in raw_out or \"protoprobs\" in raw_out:\n            return raw_out\n        for key in [\"dscd_outputs\", \"dscd\", \"dscdout\"]:\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n        return raw_out\n\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return extract_dscd_outputs(item)\n    return {}\n\ndef get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    if not dscd:\n        return []\n\n    expl = dscd.get(\"explanations\", None)\n    if expl is None:\n        for alt in [\"explanations_per_sentence\", \"trg_explanations\", \"exps\"]:\n            if alt in dscd:\n                expl = dscd[alt]\n                break\n\n    if expl is None:\n        return []\n\n    if isinstance(expl, list):\n        if len(expl) > 0 and isinstance(expl[0], dict):\n            return [expl]\n        if len(expl) > 0 and isinstance(expl[0], list):\n            return expl\n\n    return []\n\ndef is_subword_token(token: str) -> bool:\n    if not token or len(token.strip()) == 0:\n        return True\n    token = token.strip()\n    if token.startswith(\"##\") or token.startswith(\"▁\") or token.startswith(\"Ġ\") or token.startswith(\"_\"):\n        return True\n    if len(token) < 2:\n        return True\n    if len(token) == 1 and (token in SUBWORD_PUNCT_SET or token.isdigit()):\n        return True\n    return False\n\ndef should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    try:\n        token = expl.get(\"ambiguous_word\", expl.get(\"token\", \"\"))\n        span = float(expl.get(\"span\", 0.0))\n        uncertainty = float(expl.get(\"uncertainty\", 0.0))\n\n        if is_subword_token(str(token)):\n            return True\n\n        if span < span_th and uncertainty < u_th:\n            return True\n\n        return False\n    except Exception:\n        return True\n\ndef force_english_bos(tokenizer, mbart_model) -> int:\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in [TARGET_LANGUAGE, \"en_XX\", \"en\", \"eng\"]:\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = int(lid)\n                        break\n                except Exception:\n                    continue\n        \n        if forced_id is None and hasattr(tokenizer, \"lang_code_to_id\"):\n            try:\n                forced_id = tokenizer.lang_code_to_id.get(TARGET_LANGUAGE, None)\n                if forced_id is not None:\n                    forced_id = int(forced_id)\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    if forced_id is None:\n        forced_id = M2M100_EN_TOKEN_ID\n\n    if forced_id is not None and hasattr(mbart_model, \"config\"):\n        try:\n            mbart_model.config.forced_bos_token_id = int(forced_id)\n            mbart_model.config.decoder_start_token_id = int(forced_id)\n        except Exception:\n            if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                print(\"[INF] Could not set forced BOS on mbart config\")\n\n    return forced_id\n\ndef safe_generate(mbart, input_ids=None, encoder_outputs=None, attention_mask=None, max_length=64, num_beams=2, **kwargs):\n    try:\n        if encoder_outputs is not None:\n            return mbart.generate(\n                input_ids=None,\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                do_sample=False,\n                early_stopping=True,\n                num_return_sequences=1,\n                repetition_penalty=1.2,\n                no_repeat_ngram_size=3,\n                length_penalty=1.0,\n                **kwargs\n            )\n        else:\n            return mbart.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                do_sample=False,\n                early_stopping=True,\n                num_return_sequences=1,\n                repetition_penalty=1.2,\n                no_repeat_ngram_size=3,\n                length_penalty=1.0,\n                **kwargs\n            )\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            if DEBUG_DISCOVERY:\n                print(\"[INF] OOM during generation, reducing beam size...\")\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            if encoder_outputs is not None:\n                return mbart.generate(\n                    input_ids=None,\n                    encoder_outputs=encoder_outputs,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    do_sample=False,\n                    early_stopping=True,\n                    num_return_sequences=1,\n                    repetition_penalty=1.2,\n                    no_repeat_ngram_size=3,\n                    length_penalty=1.0,\n                    **kwargs\n                )\n            else:\n                return mbart.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    do_sample=False,\n                    early_stopping=True,\n                    num_return_sequences=1,\n                    repetition_penalty=1.2,\n                    no_repeat_ngram_size=3,\n                    length_penalty=1.0,\n                    **kwargs\n                )\n        else:\n            raise\n\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True\n) -> Dict[str, Any]:\n\n    device = DEVICE if device is None else device\n    span_th = REAL_AMB_SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = REAL_AMB_UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n        print(f\"[INF] Starting inference\")\n        print(f\"[INF] Input: {input_sentence[:60]}...\")\n        print(f\"[INF] Thresholds: span={span_th:.2f}, uncertainty={u_th:.2f}\")\n\n    cleanup_vars = []\n    dscd = None\n    mbart = None\n    encoder_hidden = None\n    encoder_hidden_adjusted = None\n    dscd_homographs = get_dscd_homographs(model)\n\n    try:\n        try:\n            tokenizer.src_lang = SOURCE_LANGUAGE\n        except Exception:\n            pass\n\n        token_to_word_idx_map, word_list = build_token_word_map_with_cell2(\n            input_sentence,\n            tokenizer,\n            max_length=MAX_LENGTH\n        )\n        \n        token_word_map_batch = [token_to_word_idx_map]\n\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=MAX_LENGTH,\n            add_special_tokens=True\n        )\n        enc = to_device_batch(enc, device)\n        cleanup_vars.append(enc)\n\n        model.eval()\n        core = model.module if USE_MULTI_GPU and hasattr(model, \"module\") else model\n        src_texts = [input_sentence]\n        dscd_validated = False\n\n        try:\n            dscd = core.dscd if hasattr(core, \"dscd\") else None\n            if dscd:\n                lock = None\n                if hasattr(dscd, \"bufferlock\"):\n                    lock = dscd.bufferlock\n                elif hasattr(dscd, \"clusteringlock\"):\n                    lock = dscd.clusteringlock\n\n                if lock:\n                    with lock:\n                        num_stores = len(dscd.prototypestores)\n                        multisense = sum(\n                            1 for store in dscd.prototypestores.values()\n                            if hasattr(store, \"centroids\") and len(store.centroids) >= 2\n                        )\n                else:\n                    num_stores = len(dscd.prototypestores)\n                    multisense = sum(\n                        1 for store in dscd.prototypestores.values()\n                        if hasattr(store, \"centroids\") and len(store.centroids) >= 2\n                    )\n\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD state: {num_stores} tokens, {multisense} multi-sense, {len(dscd_homographs)} discovered\")\n\n                if num_stores == 0:\n                    print(\"[INF] WARNING: DSCD prototype stores are EMPTY\")\n                    if track_stats:\n                        INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[INF] DSCD validation failed: {e}\")\n\n        with torch.inference_mode():\n            raw_dscd_out: Dict[str, Any] = {}\n\n            try:\n                if not hasattr(core, \"mbart\"):\n                    raise RuntimeError(\"Model backend missing .mbart\")\n\n                mbart = core.mbart\n                \n                orig_cache = getattr(mbart.config, \"use_cache\", None) if hasattr(mbart, \"config\") else None\n                if hasattr(mbart, \"config\"):\n                    try:\n                        mbart.config.use_cache = False\n                    except Exception:\n                        pass\n                \n                encoder_outputs_raw = mbart.model.encoder(\n                    input_ids=enc.get(\"input_ids\"),\n                    attention_mask=enc.get(\"attention_mask\")\n                )\n                cleanup_vars.append(encoder_outputs_raw)\n\n                if hasattr(encoder_outputs_raw, \"last_hidden_state\"):\n                    encoder_hidden = encoder_outputs_raw.last_hidden_state\n                elif isinstance(encoder_outputs_raw, tuple):\n                    encoder_hidden = encoder_outputs_raw[0]\n                else:\n                    encoder_hidden = encoder_outputs_raw\n                \n                cleanup_vars.append(encoder_hidden)\n\n                if not isinstance(encoder_hidden, torch.Tensor) or encoder_hidden.dim() != 3:\n                    raise RuntimeError(\n                        f\"Invalid encoder hidden: type={type(encoder_hidden)}, \"\n                        f\"shape={encoder_hidden.shape if isinstance(encoder_hidden, torch.Tensor) else 'NA'}\"\n                    )\n\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Encoder hidden: {encoder_hidden.shape}\")\n\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                            token_word_map=token_word_map_batch\n                        )\n                    except TypeError:\n                        raw_dscd_out = core.forward_with_explanations(\n                            enc.get(\"input_ids\"),\n                            enc.get(\"attention_mask\"),\n                            src_texts\n                        )\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] forward_with_explanations not found, using forward\")\n                    out = core.forward(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=src_texts,\n                        labels=None\n                    )\n                    if isinstance(out, dict):\n                        raw_dscd_out = extract_dscd_outputs(out)\n\n                dscd_out = extract_dscd_outputs(raw_dscd_out)\n                \n                if isinstance(raw_dscd_out, dict) and \"sense_augmented_embeddings\" in raw_dscd_out:\n                    encoder_hidden_adjusted = raw_dscd_out[\"sense_augmented_embeddings\"]\n                elif \"h_augmented\" in dscd_out:\n                    encoder_hidden_adjusted = dscd_out[\"h_augmented\"]\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n                \n                cleanup_vars.append(encoder_hidden_adjusted)\n\n                if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    if encoder_hidden_adjusted.shape != encoder_hidden.shape:\n                        if DEBUG_DISCOVERY:\n                            print(\"[INF] Shape mismatch, using original\")\n                        encoder_hidden_adjusted = encoder_hidden\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n\n                if DEBUG_DISCOVERY:\n                    print(\"[INF] DSCD forward completed\")\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD forward error: {e}\")\n                raw_dscd_out = {}\n                encoder_hidden_adjusted = encoder_hidden if encoder_hidden is not None else None\n\n            if mbart is None:\n                raise RuntimeError(\"mbart is not available for generation\")\n\n            forced_id = force_english_bos(tokenizer, mbart)\n\n            if hasattr(mbart, \"config\"):\n                try:\n                    mbart.config.use_cache = True\n                except Exception:\n                    pass\n\n            try:\n                if DEBUG_DISCOVERY:\n                    print(\"[INF] Generating translation...\")\n\n                if encoder_hidden_adjusted is not None and isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    encoder_hidden_adjusted = encoder_hidden_adjusted.to(device)\n                    encoder_outputs_for_decoder = BaseModelOutput(\n                        last_hidden_state=encoder_hidden_adjusted,\n                        hidden_states=getattr(encoder_outputs_raw, \"hidden_states\", None) if encoder_outputs_raw else None,\n                        attentions=getattr(encoder_outputs_raw, \"attentions\", None) if encoder_outputs_raw else None\n                    )\n\n                    generated = safe_generate(\n                        mbart,\n                        encoder_outputs=encoder_outputs_for_decoder,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(MAX_LENGTH, 64),\n                        num_beams=2,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", 1),\n                        forced_bos_token_id=forced_id\n                    )\n                else:\n                    generated = safe_generate(\n                        mbart,\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(MAX_LENGTH, 64),\n                        num_beams=2,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", 1),\n                        forced_bos_token_id=forced_id\n                    )\n\n                cleanup_vars.append(generated)\n                translation = tokenizer.decode(generated[0], skip_special_tokens=True) if generated is not None and len(generated) > 0 else \"\"\n\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Translation: {translation[:60]}...\")\n\n            finally:\n                if hasattr(mbart, \"config\") and orig_cache is not None:\n                    try:\n                        mbart.config.use_cache = orig_cache\n                    except Exception:\n                        pass\n\n        if DEBUG_DISCOVERY:\n            print(\"[INF] Extracting explanations...\")\n\n        dscd_out = extract_dscd_outputs(raw_dscd_out)\n        explanations_list = get_explanations_list(dscd_out)\n        sentence_explanations = explanations_list[0] if isinstance(explanations_list, list) and len(explanations_list) > 0 else []\n\n        if DEBUG_DISCOVERY:\n            print(f\"[INF] Raw explanations: {len(sentence_explanations)}\")\n\n        def is_real_ambiguity(e: Dict[str, Any]) -> bool:\n            try:\n                s = float(e.get(\"span\", 0.0))\n                u = float(e.get(\"uncertainty\", 0.0))\n                return s >= span_th or u >= u_th\n            except Exception:\n                return False\n\n        real_amb_count = 0\n        out_explanations: List[Dict[str, Any]] = []\n        filtered_count = 0\n\n        cleaned_explanations = []\n        for ex in sentence_explanations:\n            try:\n                word = ex.get(\"ambiguous_word\", ex.get(\"token\", \"\"))\n                if isinstance(word, str):\n                    clean_word = word.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n                    if clean_word and ex.get(\"ambiguous_word\", None) is not None and clean_word != ex.get(\"ambiguous_word\"):\n                        ex[\"ambiguous_word\"] = clean_word\n                    elif clean_word and ex.get(\"ambiguous_word\", None) is None:\n                        ex[\"ambiguous_word\"] = clean_word\n                cleaned_explanations.append(ex)\n            except Exception:\n                cleaned_explanations.append(ex)\n\n        sentence_explanations = cleaned_explanations\n\n        quality_metrics = {\n            \"total_raw_explanations\": len(sentence_explanations) if isinstance(sentence_explanations, list) else 0,\n            \"filtered_explanations\": 0,\n            \"high_confidence_count\": 0,\n            \"low_confidence_count\": 0,\n            \"avg_confidence\": 0.0,\n            \"avg_span\": 0.0,\n            \"avg_uncertainty\": 0.0\n        }\n\n        confidences: List[float] = []\n        spans: List[float] = []\n        uncertainties: List[float] = []\n\n        if isinstance(sentence_explanations, list):\n            for ex in sentence_explanations:\n                try:\n                    if should_filter_explanation(ex, span_th, u_th):\n                        filtered_count += 1\n                        continue\n\n                    is_real = is_real_ambiguity(ex)\n                    if is_real:\n                        real_amb_count += 1\n\n                    confidence = ex.get(\"confidence\", None)\n                    if confidence is None:\n                        s = float(ex.get(\"span\", 0.0))\n                        u = float(ex.get(\"uncertainty\", 0.0))\n                        confidence = max(s, u)\n                    confidence = float(confidence)\n\n                    confidences.append(confidence)\n                    spans.append(float(ex.get(\"span\", 0.0)))\n                    uncertainties.append(float(ex.get(\"uncertainty\", 0.0)))\n\n                    if confidence >= 0.65:\n                        quality_metrics[\"high_confidence_count\"] += 1\n                    elif confidence <= 0.4:\n                        quality_metrics[\"low_confidence_count\"] += 1\n\n                    out_explanations.append({\n                        \"ambiguous_word\": ex.get(\"ambiguous_word\", ex.get(\"token\", \"NA\")),\n                        \"position\": ex.get(\"position\", ex.get(\"token_idx\", \"NA\")),\n                        \"explanation\": ex.get(\"explanation\", \"\") or ex.get(\"explain\", \"\") or \"\",\n                        \"uncertainty\": float(ex.get(\"uncertainty\", 0.0)),\n                        \"span\": float(ex.get(\"span\", 0.0)),\n                        \"confidence\": confidence,\n                        \"is_real_amb\": bool(is_real)\n                    })\n                except Exception:\n                    continue\n\n        quality_metrics[\"filtered_explanations\"] = filtered_count\n\n        if confidences:\n            quality_metrics[\"avg_confidence\"] = sum(confidences) / len(confidences)\n            quality_metrics[\"avg_span\"] = sum(spans) / len(spans)\n            quality_metrics[\"avg_uncertainty\"] = sum(uncertainties) / len(uncertainties)\n\n        if DEBUG_DISCOVERY:\n            print(f\"[INF] Final {len(out_explanations)} explanations (filtered {filtered_count})\")\n\n        result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": translation,\n            \"ambiguous_words_detected\": int(real_amb_count),\n            \"explanations\": out_explanations,\n            \"quality_metrics\": quality_metrics,\n            \"dscd_validated\": dscd_validated\n        }\n\n        if track_stats:\n            INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n        return result\n\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__} {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": str(e)[:200]\n        }\n\n        if track_stats:\n            INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"আমি কল বন্ধ করেছি\",\n            \"আগামীকাল আমি একটি বই কিনব\",\n            \"পাতা পড়ে গেছে\"\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations\")\n    print(\"=\" * 80)\n\n    INFERENCE_STATS.reset()\n\n    for s in sentences:\n        print(f\"\\n> {s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n        print(f\"Translation: {res.get('translation', '')}\")\n        print(f\"Ambiguous words detected: {res.get('ambiguous_words_detected', 0)}\")\n\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(\n                f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, \"\n                f\"high={quality.get('high_confidence_count', 0)}, \"\n                f\"low={quality.get('low_confidence_count', 0)}\"\n            )\n\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(f\"  {idx}. {ex['ambiguous_word']} (pos={ex['position']}, conf={ex.get('confidence', 0):.3f})\")\n                print(f\"     {ex.get('explanation', '')[:200]}\")\n        else:\n            print(\"  No explanations.\")\n\n    print(\"=\" * 80)\n    INFERENCE_STATS.print_summary()\n\ndef dscd_discovery_warmup(\n    model,\n    tokenizer,\n    num_sents: int = 8000,\n    batch_size: int = 64,\n    max_len: Optional[int] = None\n):\n    if max_len is None:\n        max_len = MAX_LENGTH\n\n    core = model.module if USE_MULTI_GPU and hasattr(model, \"module\") else model\n\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"WARMUP: Model has no dscd component\")\n            return\n\n        print(\"=\" * 80)\n        print(\"WARMUP: Starting DSCD discovery warmup\")\n        print(\"=\" * 80)\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_nmin = getattr(dscd, \"n_min\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"n_min\"):\n                dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for bn, _ in pairs[:num_sents]]\n            else:\n                base = [\n                    \"আমি কল বন্ধ করেছি\",\n                    \"আগামীকাল আমি একটি বই কিনব\",\n                    \"পাতা পড়ে গেছে\",\n                    \"তিনি ব্যাংকে গিয়েছেন\",\n                    \"নদীর ধারে বসে আছি\"\n                ]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n        except Exception:\n            texts = []\n\n        processed = 0\n        core.eval()\n        print(f\"WARMUP: Processing {len(texts)} sentences (batch={batch_size})...\")\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i:i+batch_size]\n                try:\n                    enc = tokenizer(\n                        batch,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_len,\n                        add_special_tokens=False\n                    )\n                    enc = to_device_batch(enc, DEVICE)\n\n                    if hasattr(core, \"forward_with_explanations\"):\n                        core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch\n                        )\n                    else:\n                        core.mbart.model.encoder(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\")\n                        )\n\n                    processed += len(batch)\n\n                    current_time = time.time()\n                    if (i + batch_size) % 10 == 0 or (current_time - last_print) >= 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n                        print(\n                            f\"WARMUP: {processed}/{len(texts)} \"\n                            f\"({processed/len(texts)*100:.1%}) @ {rate:.1f} sents/s \"\n                            f\"ETA: {eta:.0f}s\"\n                        )\n                        last_print = current_time\n\n                    del enc\n                except Exception as e:\n                    print(f\"WARMUP: Batch {i}/{batch_size} failed: {str(e)[:100]}\")\n                    continue\n\n        total_time = time.time() - start_time\n        print(f\"WARMUP: Completed in {total_time:.1f}s ({processed/total_time:.1f} sents/s)\")\n        print(\"-\" * 80)\n\n        print(\"WARMUP: Running synchronous clustering...\")\n        clustering_result = synchronous_clustering_for_warmup(model)\n        \n        if clustering_result.get('success'):\n            print(f\"WARMUP: Clustered {clustering_result.get('clustered', 0)} tokens\")\n            print(f\"WARMUP: Discovered {clustering_result.get('discovered', 0)} new homographs\")\n            print(f\"WARMUP: Total homographs: {clustering_result.get('total_homographs', 0)}\")\n        else:\n            print(f\"WARMUP: Clustering incomplete: {clustering_result.get('reason', 'unknown')}\")\n\n        print(\"-\" * 80)\n\n        try:\n            lock = None\n            if hasattr(dscd, \"bufferlock\"):\n                lock = dscd.bufferlock\n            elif hasattr(dscd, \"clusteringlock\"):\n                lock = dscd.clusteringlock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototypestores)\n            else:\n                stores = dict(dscd.prototypestores)\n\n            num_types = len(stores)\n            total_protos = sum(store.size() for store in stores.values()) if stores else 0\n            multi = sum(1 for store in stores.values() if store.size() >= 2) if stores else 0\n\n            print(\"WARMUP: Summary\")\n            print(f\"  - Token types: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n\n            dscd_homographs = get_dscd_homographs(model)\n            print(f\"\\nWARMUP: Discovered Homographs: {len(dscd_homographs)}\")\n            if dscd_homographs:\n                print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n            reference_found = dscd_homographs.intersection(HOMOGRAPH_REFERENCE_LIST)\n            print(f\"\\nWARMUP: Reference List Comparison\")\n            print(f\"  - Reference list: {len(HOMOGRAPH_REFERENCE_LIST)} words\")\n            print(f\"  - Found in DSCD: {len(reference_found)}\")\n            print(f\"  - Coverage: {len(reference_found)/len(HOMOGRAPH_REFERENCE_LIST):.1%}\")\n\n            if num_types == 0:\n                print(\"WARMUP: CRITICAL - NO PROTOTYPES CREATED\")\n            elif len(reference_found) / len(HOMOGRAPH_REFERENCE_LIST) < 0.2:\n                print(\"WARMUP: WARNING - <20% reference coverage\")\n            else:\n                print(\"WARMUP: SUCCESS\")\n\n        except Exception as e:\n            print(f\"WARMUP: Validation failed: {e}\")\n\n    finally:\n        try:\n            if dscd is not None:\n                if hasattr(dscd, \"enable_training_clustering\"):\n                    dscd.enable_training_clustering = orig_enable\n                if hasattr(dscd, \"n_min\") and orig_nmin is not None:\n                    dscd.n_min = orig_nmin\n                if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                    dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    print(\"=\" * 80)\n\ndef load_checkpoint_for_resume(\n    model: torch.nn.Module,\n    optimizer,\n    checkpoint_path: str\n) -> Tuple[bool, int, int, float]:\n    if not os.path.exists(checkpoint_path):\n        print(f\"CHECKPOINT: Not found: {checkpoint_path}\")\n        return False, 0, 0, 0.0\n\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n    except Exception as e:\n        print(f\"CHECKPOINT: Load failed: {e}\")\n        return False, 0, 0, 0.0\n\n    core = model.module if USE_MULTI_GPU and hasattr(model, \"module\") else model\n    state = ckpt.get(\"model_state_dict\", ckpt)\n\n    try:\n        core.load_state_dict(state, strict=False)\n    except Exception as e:\n        print(f\"CHECKPOINT: model.load_state_dict failed: {e}\")\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                core.load_state_dict(new_state, strict=False)\n        except Exception:\n            pass\n\n    try:\n        if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n    except Exception as e:\n        print(f\"CHECKPOINT: optimizer.load_state_dict failed: {e}\")\n\n    try:\n        if \"dscd_state\" in ckpt and ckpt[\"dscd_state\"]:\n            dscd_state = ckpt[\"dscd_state\"]\n            print(\"CHECKPOINT: Restoring DSCD...\")\n            dscd = core.dscd if hasattr(core, \"dscd\") else None\n            if dscd and hasattr(dscd, \"load_state_dict\"):\n                lock = None\n                if hasattr(dscd, \"bufferlock\"):\n                    lock = dscd.bufferlock\n                elif hasattr(dscd, \"clusteringlock\"):\n                    lock = dscd.clusteringlock\n\n                if lock:\n                    with lock:\n                        dscd.load_state_dict(dscd_state)\n                else:\n                    dscd.load_state_dict(dscd_state)\n\n                num_tokens = len(dscd.prototypestores)\n                total_protos = sum(store.size() for store in dscd.prototypestores.values())\n                multisense = sum(1 for store in dscd.prototypestores.values() if store.size() >= 2)\n\n                print(f\"CHECKPOINT: DSCD restored\")\n                print(f\"  - Tokens: {num_tokens}\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multisense}\")\n\n                if num_tokens == 0:\n                    print(\"CHECKPOINT: WARNING - DSCD state empty - consider running warmup\")\n            else:\n                print(\"CHECKPOINT: Model has no dscd.load_state_dict\")\n        else:\n            print(\"CHECKPOINT: No DSCD state in checkpoint\")\n    except Exception as e:\n        print(f\"CHECKPOINT: DSCD restore failed: {e}\")\n\n    epoch = int(ckpt.get(\"epochs_trained\", ckpt.get(\"epoch\", 0)))\n    step = int(ckpt.get(\"global_steps\", ckpt.get(\"global_step\", ckpt.get(\"step\", 0))))\n    avg_loss = float(ckpt.get(\"final_train_loss\", ckpt.get(\"avg_epoch_loss\", ckpt.get(\"avg_loss\", 0.0))))\n\n    print(f\"CHECKPOINT: Loaded (epoch={epoch}, step={step}, loss={avg_loss:.6f})\")\n    return True, epoch, step, avg_loss\n\nprint(\"=\" * 80)\nprint(\"Cell 8: Inference pipeline ready - COMPLETE FIXED VERSION\")\nprint(\"=\" * 80)\nprint(\"KEY FIXES:\")\nprint(\"  ✓ Word map format aligned with Cell 6 TATN (Dict[int, int] + List[str])\")\nprint(\"  ✓ Uses Cell 2 reconstruct_word_spans() when available\")\nprint(\"  ✓ Synchronous clustering added to warmup\")\nprint(\"  ✓ Proper word-level aggregation support\")\nprint(\"=\" * 80)\n","metadata":{"id":"7Dxg7ck0H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:46.500301Z","iopub.execute_input":"2026-01-07T14:57:46.500537Z","iopub.status.idle":"2026-01-07T14:57:46.608339Z","shell.execute_reply.started":"2026-01-07T14:57:46.500515Z","shell.execute_reply":"2026-01-07T14:57:46.607644Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 8: Inference pipeline ready - COMPLETE FIXED VERSION\n================================================================================\nKEY FIXES:\n  ✓ Word map format aligned with Cell 6 TATN (Dict[int, int] + List[str])\n  ✓ Uses Cell 2 reconstruct_word_spans() when available\n  ✓ Synchronous clustering added to warmup\n  ✓ Proper word-level aggregation support\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION (WITH BLEU/COMET) - COMPLETE FIXED\n# ==============================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nimport gc\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.05\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n_SACREBLEU_AVAILABLE = False\n_COMET_AVAILABLE = False\n\ntry:\n    import sacrebleu\n    _SACREBLEU_AVAILABLE = True\nexcept ImportError:\n    pass\n\ntry:\n    from comet import download_model, load_from_checkpoint\n    _COMET_AVAILABLE = True\nexcept ImportError:\n    pass\n\ndef _resolve_dscd(model: torch.nn.Module):\n    core = model.module if hasattr(model, \"module\") else model\n    dscd = getattr(core, \"dscd\", None)\n    return core, dscd\n\ndef _resolve_dscd_lock(dscd):\n    if dscd is None:\n        return None\n    for name in (\"bufferlock\", \"buffer_lock\", \"clusteringlock\", \"clustering_lock\"):\n        lock = getattr(dscd, name, None)\n        if lock is not None:\n            return lock\n    return None\n\ndef _resolve_prototype_stores(dscd) -> Dict[Any, Any]:\n    if dscd is None:\n        return {}\n    for name in (\"prototypestores\", \"prototype_stores\"):\n        stores = getattr(dscd, name, None)\n        if isinstance(stores, dict):\n            return stores\n        if stores is not None:\n            try:\n                return dict(stores)\n            except Exception:\n                pass\n    return {}\n\ndef _store_size(store) -> int:\n    if store is None:\n        return 0\n    try:\n        size_attr = getattr(store, \"size\", None)\n        if callable(size_attr):\n            return int(size_attr())\n        if isinstance(size_attr, int):\n            return int(size_attr)\n    except Exception:\n        pass\n    try:\n        cents = getattr(store, \"centroids\", None)\n        return int(len(cents)) if cents is not None else 0\n    except Exception:\n        return 0\n\ndef _clean_token_for_set(x: Any) -> str:\n    return (\n        str(x)\n        .replace(\" \", \"\")\n        .replace(\"Ġ\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"▁\", \"\")\n        .strip()\n        .lower()\n    )\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        _, dscd = _resolve_dscd(model)\n        if dscd is None:\n            return 0\n\n        lock = _resolve_dscd_lock(dscd)\n        if lock:\n            with lock:\n                stores = _resolve_prototype_stores(dscd)\n                return len(stores)\n        else:\n            stores = _resolve_prototype_stores(dscd)\n            return len(stores)\n    except Exception:\n        return 0\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        _, dscd = _resolve_dscd(model)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, \"get_discovered_homographs\"):\n            try:\n                return set(_clean_token_for_set(w) for w in dscd.get_discovered_homographs())\n            except Exception:\n                pass\n\n        homographs = set()\n        lock = _resolve_dscd_lock(dscd)\n\n        if lock:\n            with lock:\n                prototype_stores = _resolve_prototype_stores(dscd)\n                items = list(prototype_stores.items())\n        else:\n            prototype_stores = _resolve_prototype_stores(dscd)\n            items = list(prototype_stores.items())\n\n        for token, store in items:\n            try:\n                if _store_size(store) >= 2:\n                    homographs.add(_clean_token_for_set(token))\n            except Exception:\n                continue\n\n        return homographs\n    except Exception:\n        return set()\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        _, dscd = _resolve_dscd(model)\n        if dscd is None:\n            return\n\n        lock = _resolve_dscd_lock(dscd)\n        if lock:\n            with lock:\n                prototype_stores = dict(_resolve_prototype_stores(dscd))\n        else:\n            prototype_stores = dict(_resolve_prototype_stores(dscd))\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = len(getattr(store, \"centroids\", []))\n            except Exception:\n                n_protos = 0\n            cluster_info.append({\n                \"token\": token,\n                \"count\": total_count,\n                \"protos\": n_protos,\n                \"mu\": getattr(store, \"mu\", 0.0),\n                \"tau\": getattr(store, \"tau\", 0.0)\n            })\n\n        cluster_info.sort(key=lambda x: x[\"count\"], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info[\"token\"])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n\n        print(\"-\" * 90)\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n\ndef compute_bleu_score(predictions: List[str], references: List[str]) -> float:\n    if not _SACREBLEU_AVAILABLE:\n        print(\"[BLEU] sacrebleu not available, install: pip install sacrebleu\")\n        return 0.0\n    \n    if not predictions or not references or len(predictions) != len(references):\n        return 0.0\n    \n    try:\n        predictions_clean = [p.strip() if p else \"\" for p in predictions]\n        references_clean = [[r.strip() if r else \"\"] for r in references]\n        \n        bleu = sacrebleu.corpus_bleu(predictions_clean, references_clean)\n        return float(bleu.score)\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[BLEU] Calculation error: {e}\")\n        return 0.0\n\ndef compute_comet_score(\n    sources: List[str],\n    predictions: List[str],\n    references: List[str],\n    model_name: str = \"Unbabel/wmt22-comet-da\"\n) -> float:\n    if not _COMET_AVAILABLE:\n        print(\"[COMET] comet-ml not available, install: pip install unbabel-comet\")\n        return 0.0\n    \n    if not sources or not predictions or not references:\n        return 0.0\n    \n    if len(sources) != len(predictions) or len(sources) != len(references):\n        return 0.0\n    \n    try:\n        model_path = download_model(model_name)\n        model = load_from_checkpoint(model_path)\n        \n        data = [\n            {\"src\": src, \"mt\": pred, \"ref\": ref}\n            for src, pred, ref in zip(sources, predictions, references)\n        ]\n        \n        output = model.predict(data, batch_size=8, gpus=1 if torch.cuda.is_available() else 0)\n        return float(output.system_score)\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[COMET] Calculation error: {e}\")\n        return 0.0\n\ndef batch_translate_with_explanations(\n    model,\n    tokenizer,\n    test_pairs: List[Tuple[str, str]],\n    batch_size: int = 32,\n    max_samples: Optional[int] = None,\n    compute_bleu: bool = True,\n    compute_comet: bool = False\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"BATCH TRANSLATION WITH EXPLANATIONS\")\n    print(\"=\" * 80)\n    \n    if max_samples and len(test_pairs) > max_samples:\n        test_pairs = test_pairs[:max_samples]\n    \n    sources = [src for src, _ in test_pairs]\n    references = [ref for _, ref in test_pairs]\n    \n    predictions = []\n    all_explanations = []\n    total_ambiguous = 0\n    \n    print(f\"[BATCH] Translating {len(sources)} sentences (batch={batch_size})...\")\n    start_time = time.time()\n    \n    model.eval()\n    \n    for i in range(0, len(sources), batch_size):\n        batch_sources = sources[i:i+batch_size]\n        \n        for src in batch_sources:\n            try:\n                if \"translate_with_explanations\" in globals():\n                    result = translate_with_explanations(\n                        model,\n                        tokenizer,\n                        src,\n                        device=_DEVICE,\n                        span_threshold=_SPAN_THRESHOLD,\n                        uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                        track_stats=True\n                    )\n                    \n                    predictions.append(result.get(\"translation\", \"\"))\n                    all_explanations.append(result.get(\"explanations\", []))\n                    total_ambiguous += result.get(\"ambiguous_words_detected\", 0)\n                else:\n                    predictions.append(\"\")\n                    all_explanations.append([])\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[BATCH] Translation error: {e}\")\n                predictions.append(\"\")\n                all_explanations.append([])\n        \n        if (i + batch_size) % 100 == 0:\n            elapsed = time.time() - start_time\n            rate = len(predictions) / elapsed if elapsed > 0 else 0\n            print(f\"[BATCH] {len(predictions)}/{len(sources)} @ {rate:.1f} sents/s\")\n    \n    elapsed = time.time() - start_time\n    print(f\"[BATCH] Completed in {elapsed:.1f}s ({len(sources)/elapsed:.1f} sents/s)\")\n    \n    bleu_score = 0.0\n    if compute_bleu and _SACREBLEU_AVAILABLE:\n        print(\"[BATCH] Computing BLEU...\")\n        bleu_score = compute_bleu_score(predictions, references)\n        print(f\"[BATCH] BLEU: {bleu_score:.2f}\")\n    \n    comet_score = 0.0\n    if compute_comet and _COMET_AVAILABLE:\n        print(\"[BATCH] Computing COMET...\")\n        comet_score = compute_comet_score(sources, predictions, references)\n        print(f\"[BATCH] COMET: {comet_score:.4f}\")\n    \n    total_explanations = sum(len(exps) for exps in all_explanations)\n    \n    inference_stats_summary = {}\n    if \"INFERENCE_STATS\" in globals():\n        try:\n            inference_stats_summary = INFERENCE_STATS.get_summary()\n        except Exception:\n            pass\n    \n    print(\"=\" * 80)\n    \n    return {\n        \"sources\": sources,\n        \"predictions\": predictions,\n        \"references\": references,\n        \"bleu_score\": bleu_score,\n        \"comet_score\": comet_score,\n        \"total_explanations\": total_explanations,\n        \"total_ambiguous\": total_ambiguous,\n        \"inference_stats\": inference_stats_summary,\n        \"elapsed_time\": elapsed,\n    }\n\ndef evaluate_on_test_set(\n    model,\n    tokenizer,\n    test_size: int = 1000,\n    batch_size: int = 32,\n    compute_bleu: bool = True,\n    compute_comet: bool = False\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EVALUATE ON TEST SET\")\n    print(\"=\" * 80)\n    \n    test_pairs = []\n    \n    try:\n        if \"load_and_preprocess_optimized\" in globals():\n            print(f\"[EVAL] Loading {test_size} test pairs from dataset...\")\n            all_pairs = load_and_preprocess_optimized(test_size * 2)\n            test_pairs = all_pairs[-test_size:]\n            print(f\"[EVAL] Loaded {len(test_pairs)} pairs\")\n        else:\n            print(\"[EVAL] load_and_preprocess_optimized not available\")\n            return {\"error\": \"dataset_loader_not_available\"}\n    except Exception as e:\n        print(f\"[EVAL] Failed to load test set: {e}\")\n        return {\"error\": str(e)}\n    \n    if not test_pairs:\n        print(\"[EVAL] No test pairs loaded\")\n        return {\"error\": \"no_test_pairs\"}\n    \n    result = batch_translate_with_explanations(\n        model,\n        tokenizer,\n        test_pairs,\n        batch_size=batch_size,\n        max_samples=test_size,\n        compute_bleu=compute_bleu,\n        compute_comet=compute_comet\n    )\n    \n    print(\"\\n[EVAL] Test Set Evaluation Complete\")\n    print(f\"  BLEU: {result.get('bleu_score', 0):.2f}\")\n    if compute_comet:\n        print(f\"  COMET: {result.get('comet_score', 0):.4f}\")\n    print(f\"  Explanations: {result.get('total_explanations', 0)}\")\n    print(f\"  Ambiguous: {result.get('total_ambiguous', 0)}\")\n    print(\"=\" * 80)\n    \n    return result\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Pure Data-Driven)\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\", [\"কল\"]),\n        (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\", [\"কাল\"]),\n        (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\", [\"পাতা\"]),\n        (\"তিনি ব্যাংকে গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\", [\"ব্যাংক\"]),\n        (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল = fruit/result\", [\"ফল\"]),\n        (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা = head/top\", [\"মাথা\"]),\n        (\"কল থেকে কল এসেছে।\", \"A call came from the tap\", \"Multiple কল\", [\"কল\"]),\n        (\"কালকে কাল মেঘ দেখা গেছে।\", \"Yesterday black clouds were seen\", \"Multiple কাল\", [\"কাল\"]),\n        (\"আজ ভাল আবহাওয়া।\", \"Weather is good today\", \"Simple\", []),\n        (\"আমি ভালো আছি।\", \"I am fine\", \"Simple\", []),\n        (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Simple\", []),\n        (\"এটা আমার বই।\", \"This is my book\", \"Simple\", []),\n        (\"তিনি ব্যাংকে কাজ করেন এবং ব্যাংকে বসে থাকেন।\",\n         \"He works at the bank and sits on the embankment\",\n         \"Long with multiple\", [\"ব্যাংক\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    quality_metrics = {\n        \"total_confidence\": 0.0,\n        \"confidence_samples\": 0,\n        \"high_confidence_count\": 0,\n        \"medium_confidence_count\": 0,\n        \"low_confidence_count\": 0,\n        \"confidences\": [],\n        \"spans\": [],\n        \"uncertainties\": [],\n    }\n\n    homograph_tracking = {\n        \"test_expected_homographs\": set(),\n        \"dscd_discovered_homographs\": set(),\n        \"explained_homographs\": set(),\n        \"homograph_explanations\": defaultdict(list),\n    }\n\n    error_tracking = {\n        \"translation_failures\": 0,\n        \"dscd_failures\": 0,\n        \"trg_failures\": 0,\n        \"timeout_errors\": 0,\n        \"oom_errors\": 0,\n        \"other_errors\": 0,\n        \"error_details\": [],\n        \"per_test_status\": [],\n    }\n\n    timing_metrics = {\n        \"total_time\": 0.0,\n        \"per_test_times\": [],\n        \"avg_test_time\": 0.0,\n    }\n\n    discovery_validated = False\n    try:\n        _, dscd = _resolve_dscd(core_model)\n        disc_log = getattr(dscd, \"discoveredlog\", None)\n        if not disc_log:\n            disc_log = getattr(dscd, \"discovered_log\", None)\n\n        if dscd and isinstance(disc_log, list) and disc_log:\n            discovery_validated = True\n            last_discovery = disc_log[-1] if isinstance(disc_log[-1], dict) else {}\n            discovered = last_discovery.get(\"discovered\", 0)\n            candidates = last_discovery.get(\"candidates\", 0)\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] No discovery log found\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] Discovery validation failed: {e}\")\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn and hasattr(asbn, \"get_detailed_stats\"):\n            asbn_stats = asbn.get_detailed_stats()\n        elif asbn and hasattr(asbn, \"get_asbn_stats\"):\n            asbn_stats = asbn.get_asbn_stats()\n\n        if asbn_stats and _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN: domain_acc={asbn_stats.get('domain_accuracy', 0):.2%}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN stats failed: {e}\")\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg_system\", None)\n        if trg and hasattr(trg, \"get_statistics\"):\n            trg_stats = trg.get_statistics()\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] TRG: {trg_stats.get('explanations_generated', 0)} total\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] TRG stats failed: {e}\")\n\n    homograph_tracking[\"dscd_discovered_homographs\"] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking[\"dscd_discovered_homographs\"] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            _, dscd = _resolve_dscd(core_model)\n            if dscd is not None:\n                lock = _resolve_dscd_lock(dscd)\n\n                if lock:\n                    with lock:\n                        stores = _resolve_prototype_stores(dscd)\n                        store_count = len(stores) if stores else 0\n                else:\n                    stores = _resolve_prototype_stores(dscd)\n                    store_count = len(stores) if stores else 0\n\n                if store_count == 0 and \"dscd_discovery_warmup\" in globals():\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(model, tokenizer, num_sents=4000, batch_size=64)\n                        homograph_tracking[\"dscd_discovered_homographs\"] = _get_dscd_homographs(core_model)\n                        \n                        if lock:\n                            with lock:\n                                stores_after = _resolve_prototype_stores(dscd)\n                                store_count_after = len(stores_after) if stores_after else 0\n                        else:\n                            stores_after = _resolve_prototype_stores(dscd)\n                            store_count_after = len(stores_after) if stores_after else 0\n                        \n                        if store_count_after == 0:\n                            print(\"[EVAL] WARNING: Warmup completed but DSCD stores still empty\")\n                        else:\n                            print(f\"[EVAL] Warmup completed: {store_count_after} stores created\")\n                    except Exception as e:\n                        print(f\"[EVAL] Warmup failed: {e}\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    def _compute_similarity(pred: str, expected: str) -> float:\n        try:\n            pred_words = set(pred.lower().split())\n            exp_words = set(expected.lower().split())\n            if not pred_words and not exp_words:\n                return 1.0\n            if not pred_words or not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            union = len(pred_words | exp_words)\n            return overlap / union if union > 0 else 0.0\n        except Exception:\n            return 0.0\n\n    for _, _, _, expected_homos in test_sentences:\n        homograph_tracking[\"test_expected_homographs\"].update([h.lower() for h in expected_homos])\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            \"test_id\": idx,\n            \"success\": False,\n            \"translation_ok\": False,\n            \"explanations_count\": 0,\n            \"error\": None,\n        }\n\n        try:\n            if \"translate_with_explanations\" not in globals():\n                print(\"[EVAL] translate_with_explanations not available\")\n                error_tracking[\"other_errors\"] += 1\n                test_status[\"error\"] = \"function_not_available\"\n                error_tracking[\"per_test_status\"].append(test_status)\n                continue\n\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                device=_DEVICE,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                track_stats=True\n            )\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n\n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0))\n                    u_val = float(expl.get(\"uncertainty\", 0.0))\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n\n                    marker = f\"[S>{_SPAN_THRESHOLD:.2f}]\" if span_val > _SPAN_THRESHOLD else \"          \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics[\"confidences\"].append(conf_val)\n                    quality_metrics[\"spans\"].append(span_val)\n                    quality_metrics[\"uncertainties\"].append(u_val)\n                    quality_metrics[\"total_confidence\"] = quality_metrics.get(\"total_confidence\", 0.0) + conf_val\n                    quality_metrics[\"confidence_samples\"] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics[\"high_confidence_count\"] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics[\"medium_confidence_count\"] += 1\n                    else:\n                        quality_metrics[\"low_confidence_count\"] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = _clean_token_for_set(word)\n                    homograph_tracking[\"explained_homographs\"].add(clean_word)\n                    homograph_tracking[\"homograph_explanations\"][clean_word].append({\n                        \"sentence\": src_text,\n                        \"confidence\": conf_val,\n                        \"span\": span_val,\n                        \"uncertainty\": u_val,\n                    })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status[\"explanations_count\"] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            if translation and translation.strip() and translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            ):\n                successful_translations += 1\n                test_status[\"translation_ok\"] = True\n                test_status[\"success\"] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking[\"translation_failures\"] += 1\n                test_status[\"error\"] = \"translation_failed\"\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking[\"oom_errors\"] += 1\n                test_status[\"error\"] = \"oom\"\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking[\"timeout_errors\"] += 1\n                test_status[\"error\"] = \"timeout\"\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking[\"other_errors\"] += 1\n                test_status[\"error\"] = \"runtime\"\n            error_tracking[\"error_details\"].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking[\"other_errors\"] += 1\n            test_status[\"error\"] = type(e).__name__\n            error_tracking[\"error_details\"].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking[\"per_test_status\"].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics[\"per_test_times\"].append(test_time)\n\n        print(\"-\" * 60)\n\n    timing_metrics[\"total_time\"] = time.time() - eval_start\n    if timing_metrics[\"per_test_times\"]:\n        timing_metrics[\"avg_test_time\"] = (\n            sum(timing_metrics[\"per_test_times\"]) / len(timing_metrics[\"per_test_times\"])\n        )\n\n    if quality_metrics[\"confidence_samples\"] > 0:\n        quality_metrics[\"avg_confidence\"] = (\n            quality_metrics[\"total_confidence\"] / quality_metrics[\"confidence_samples\"]\n        )\n        quality_metrics[\"avg_span\"] = (\n            sum(quality_metrics[\"spans\"]) / len(quality_metrics[\"spans\"])\n            if quality_metrics[\"spans\"]\n            else 0.0\n        )\n        quality_metrics[\"avg_uncertainty\"] = (\n            sum(quality_metrics[\"uncertainties\"]) / len(quality_metrics[\"uncertainties\"])\n            if quality_metrics[\"uncertainties\"]\n            else 0.0\n        )\n\n        if quality_metrics[\"confidences\"]:\n            sorted_conf = sorted(quality_metrics[\"confidences\"])\n            quality_metrics[\"confidence_p25\"] = sorted_conf[len(sorted_conf) // 4]\n            quality_metrics[\"confidence_p50\"] = sorted_conf[len(sorted_conf) // 2]\n            quality_metrics[\"confidence_p75\"] = sorted_conf[3 * len(sorted_conf) // 4]\n    else:\n        quality_metrics[\"avg_confidence\"] = 0.0\n        quality_metrics[\"avg_span\"] = 0.0\n        quality_metrics[\"avg_uncertainty\"] = 0.0\n\n    explained_from_dscd = homograph_tracking[\"explained_homographs\"].intersection(\n        homograph_tracking[\"dscd_discovered_homographs\"]\n    )\n\n    test_expected_discovered = homograph_tracking[\"test_expected_homographs\"].intersection(\n        homograph_tracking[\"dscd_discovered_homographs\"]\n    )\n\n    reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n        homograph_tracking[\"dscd_discovered_homographs\"]\n    )\n\n    homograph_tracking[\"explained_from_dscd_rate\"] = (\n        len(explained_from_dscd) / len(homograph_tracking[\"dscd_discovered_homographs\"])\n        if homograph_tracking[\"dscd_discovered_homographs\"]\n        else 0.0\n    )\n    homograph_tracking[\"test_expected_discovery_rate\"] = (\n        len(test_expected_discovered) / len(homograph_tracking[\"test_expected_homographs\"])\n        if homograph_tracking[\"test_expected_homographs\"]\n        else 0.0\n    )\n    homograph_tracking[\"reference_discovery_rate\"] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        _, dscd = _resolve_dscd(core_model)\n        if dscd is not None:\n            lock = _resolve_dscd_lock(dscd)\n            if lock:\n                with lock:\n                    stores = dict(_resolve_prototype_stores(dscd))\n            else:\n                stores = dict(_resolve_prototype_stores(dscd))\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for _, store in stores.items():\n                sz = 0\n                try:\n                    sz = _store_size(store)\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n            }\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] DSCD stats failed: {e}\")\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n\n    if \"confidence_p50\" in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: \"\n            f\"{quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking[\"explained_homographs\"]:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking[\"explained_homographs\"])[:10]:\n            exps = homograph_tracking[\"homograph_explanations\"].get(homo, [])\n            count = len(exps)\n            avg_conf = sum(e[\"confidence\"] for e in exps) / len(exps) if exps else 0.0\n            in_dscd = \"[D]\" if homo in homograph_tracking[\"dscd_discovered_homographs\"] else \"   \"\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\")\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats[\"total_words\"] > 0:\n        print(\n            f\"  Multi-sense ratio: \"\n            f\"{dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        print(f\"  Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n        if \"source_accuracy\" in asbn_stats:\n            print(f\"  Source accuracy: {asbn_stats['source_accuracy']:.2%}\")\n            print(f\"  Target accuracy: {asbn_stats['target_accuracy']:.2%}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        print(f\"  Total explanations: {trg_stats.get('explanations_generated', 0)}\")\n        print(f\"  High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum([\n        error_tracking[\"translation_failures\"],\n        error_tracking[\"dscd_failures\"],\n        error_tracking[\"trg_failures\"],\n        error_tracking[\"timeout_errors\"],\n        error_tracking[\"oom_errors\"],\n        error_tracking[\"other_errors\"],\n    ])\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = float(baseline_metrics.get(\"success_rate_pct\", 0))\n            current_success = (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n\n            baseline_expl = int(baseline_metrics.get(\"total_explanations\", 0))\n            expl_delta = total_explanations - baseline_expl\n\n            baseline_quality_dict = baseline_metrics.get(\"quality_metrics\", {})\n            baseline_quality = float(baseline_quality_dict.get(\"avg_confidence\", 0)) if isinstance(baseline_quality_dict, dict) else 0.0\n            quality_delta = quality_metrics[\"avg_confidence\"] - baseline_quality\n\n            print(f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics['avg_confidence']:.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n\n            if \"homograph_tracking\" in baseline_metrics and isinstance(baseline_metrics[\"homograph_tracking\"], dict):\n                baseline_homo_dict = baseline_metrics[\"homograph_tracking\"]\n                baseline_homo_rate = float(baseline_homo_dict.get(\"explained_from_dscd_rate\", 0))\n\n                homo_delta = (homograph_tracking[\"explained_from_dscd_rate\"] - baseline_homo_rate)\n                print(\n                    f\"  Explanation rate: \"\n                    f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                    f\"({homo_delta:+.1%})\"\n                )\n        except Exception as e:\n            print(f\"  Comparison failed: {e}\")\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"No explanations generated\")\n    if dscd_stats[\"total_words\"] < 100:\n        warnings.append(\"Very few prototypes (<100)\")\n    if quality_metrics[\"low_confidence_count\"] > quality_metrics[\"high_confidence_count\"]:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking[\"explained_from_dscd_rate\"] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    if asbn_stats and asbn_stats.get(\"domain_accuracy\", 0) < 0.5:\n        warnings.append(\"ASBN domain accuracy <50%\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] All systems nominal\")\n\n    print(\"=\" * 80)\n\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n    }\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\" * 60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model,\n            tokenizer,\n            run_warmup=False,\n            compare_baseline=False\n        )\n\n        assert \"total_tests\" in result\n        assert \"quality_metrics\" in result\n        assert \"homograph_tracking\" in result\n\n        print(\"Evaluation pipeline test passed\")\n        print(\"=\" * 60 + \"\\n\")\n        return True\n\n    except Exception as e:\n        print(f\"Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\" * 60 + \"\\n\")\n        return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Testing & evaluation ready - COMPLETE FIXED VERSION\")\nprint(\"=\" * 80)\nprint(\"KEY FIXES:\")\nprint(\"  ✓ Added batch_translate_with_explanations() for large test sets\")\nprint(\"  ✓ Added compute_bleu_score() with sacrebleu\")\nprint(\"  ✓ Added compute_comet_score() with comet-ml\")\nprint(\"  ✓ Added evaluate_on_test_set() for dataset evaluation\")\nprint(\"  ✓ Fixed _compute_similarity() to proper Jaccard similarity\")\nprint(\"  ✓ Fixed typo in test sentence (বন্ধেছি → বন্ধ করেছি)\")\nprint(\"  ✓ Changed track_stats=True for Cell 8 integration\")\nprint(\"  ✓ Import detection for sacrebleu and comet-ml\")\nprint()\nprint(\"Available functions:\")\nprint(\"  - comprehensive_post_training_testing() - 13 curated tests\")\nprint(\"  - batch_translate_with_explanations() - Batch inference\")\nprint(\"  - evaluate_on_test_set() - Full test set with BLEU/COMET\")\nprint(\"  - compute_bleu_score() - BLEU calculation\")\nprint(\"  - compute_comet_score() - COMET calculation\")\nprint()\nprint(f\"Evaluation metrics:\")\nprint(f\"  - BLEU: {'Available' if _SACREBLEU_AVAILABLE else 'Not installed (pip install sacrebleu)'}\")\nprint(f\"  - COMET: {'Available' if _COMET_AVAILABLE else 'Not installed (pip install unbabel-comet)'}\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"8uL574F8H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:46.609418Z","iopub.execute_input":"2026-01-07T14:57:46.609657Z","iopub.status.idle":"2026-01-07T14:57:46.789280Z","shell.execute_reply.started":"2026-01-07T14:57:46.609634Z","shell.execute_reply":"2026-01-07T14:57:46.788561Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 9: Testing & evaluation ready - COMPLETE FIXED VERSION\n================================================================================\nKEY FIXES:\n  ✓ Added batch_translate_with_explanations() for large test sets\n  ✓ Added compute_bleu_score() with sacrebleu\n  ✓ Added compute_comet_score() with comet-ml\n  ✓ Added evaluate_on_test_set() for dataset evaluation\n  ✓ Fixed _compute_similarity() to proper Jaccard similarity\n  ✓ Fixed typo in test sentence (বন্ধেছি → বন্ধ করেছি)\n  ✓ Changed track_stats=True for Cell 8 integration\n  ✓ Import detection for sacrebleu and comet-ml\n\nAvailable functions:\n  - comprehensive_post_training_testing() - 13 curated tests\n  - batch_translate_with_explanations() - Batch inference\n  - evaluate_on_test_set() - Full test set with BLEU/COMET\n  - compute_bleu_score() - BLEU calculation\n  - compute_comet_score() - COMET calculation\n\nEvaluation metrics:\n  - BLEU: Available\n  - COMET: Not installed (pip install unbabel-comet)\n  - Span threshold: 0.2\n  - Uncertainty threshold: 0.15\n  - Reference list: 65 words\n================================================================================\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE - COMPLETE FIXED VERSION\n# ==============================================================================\n\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Optional, Dict, Any\nimport gc\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef g(name, default):\n    return globals().get(name, default)\n\nUSE_MULTI_GPU = bool(g(\"USE_MULTI_GPU\", False))\nNUM_GPUS = int(g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\nDEVICE = g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nSOURCE_LANGUAGE = str(g(\"SOURCE_LANGUAGE\", \"bn\"))\nTARGET_LANGUAGE = str(g(\"TARGET_LANGUAGE\", \"en\"))\nNUM_SAMPLES = int(g(\"NUM_SAMPLES\", 30000))\nMAX_LENGTH = int(g(\"MAX_LENGTH\", 48))\nBATCH_SIZE = int(g(\"BATCH_SIZE\", 100))\nEPOCHS = int(g(\"EPOCHS\", 1))\nACCUMULATION_STEPS = int(g(\"ACCUMULATION_STEPS\", 16))\nLR_NMT = float(g(\"LR_NMT\", 2e-5))\nLR_PHI = float(g(\"LR_PHI\", 1e-5))\nLR_TRG = float(g(\"LR_TRG\", 1e-5))\nENABLE_ASBN_TRAINING = bool(g(\"ENABLE_ASBN_TRAINING\", True))\nENABLE_TRG_TRAINING = bool(g(\"ENABLE_TRG_TRAINING\", True))\nVALIDATION_CHECK_INTERVAL = int(g(\"VALIDATION_CHECK_INTERVAL\", 200))\nPERIODIC_DISCOVERY_FREQUENCY = int(g(\"PERIODIC_DISCOVERY_FREQUENCY\", 200))\nDSCD_WARMUP_SAMPLES = int(g(\"DSCD_WARMUP_SAMPLES\", 8000))\nHOMOGRAPH_REFERENCE_LIST_BN = set(g(\"HOMOGRAPH_REFERENCE_LIST_BN\", {\"কল\", \"কাল\", \"পাতা\"}))\nHOMOGRAPH_REFERENCE_LIST = HOMOGRAPH_REFERENCE_LIST_BN\nFREEZE_ENCODER = bool(g(\"FREEZE_ENCODER\", False))\nDEBUG_TIMING = bool(g(\"DEBUG_TIMING\", True))\nVERBOSE_LOGGING = bool(g(\"VERBOSE_LOGGING\", False))\nSPAN_THRESHOLD = float(g(\"SPAN_THRESHOLD\", 0.20))\nUNCERTAINTY_THRESHOLD = float(g(\"TAU_LOW\", 0.15))\nUSE_AMP = bool(g(\"USE_AMP\", True))\nGRAD_CLIP_NORM = float(g(\"GRAD_CLIP_NORM\", 1.0))\nCHECKPOINT_SAVE_AFTER_TRAINING = bool(g(\"CHECKPOINT_SAVE_AFTER_TRAINING\", True))\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"tatn_final.pt\")\n\ndef safe_clear_gpu_caches():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\ndef safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import M2M100Tokenizer\n        tok = M2M100Tokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n        \n        required = [\"encode\", \"decode\", \"convert_ids_to_tokens\", \"__call__\"]\n        for method in required:\n            if not hasattr(tok, method):\n                raise RuntimeError(f\"Tokenizer missing {method}\")\n        \n        return tok\n    except Exception as e:\n        print(f\"[TOKENIZER] Load failed: {e}\")\n        raise\n\ndef resolve_dscd_stores(dscd):\n    if dscd is None:\n        return {}\n    \n    for attrname in [\"prototype_stores\", \"_prototype_stores\"]:\n        stores = getattr(dscd, attrname, None)\n        if isinstance(stores, dict):\n            return stores\n        if stores is not None:\n            try:\n                return dict(stores)\n            except Exception:\n                pass\n    \n    return {}\n\ndef resolve_dscd_lock(dscd):\n    if dscd is None:\n        return None\n    \n    for name in [\"buffer_lock\", \"_buffer_lock\", \"clustering_lock\", \"_clustering_lock\"]:\n        lock = getattr(dscd, name, None)\n        if lock is not None:\n            return lock\n    \n    return None\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    \n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        \n        safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    \n    return True\n\ndef warmup_model(\n    model,\n    tokenizer,\n    num_samples: int = 100,\n    batch_size: int = 16,\n    max_length: int = 48,\n) -> bool:\n    print(\"=\" * 80)\n    print(\"[WARMUP] Starting fast model warmup...\")\n    print(\"=\" * 80)\n    \n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        was_training = getattr(core, \"training\", False)\n        core.eval()\n        \n        start_time = time.time()\n        \n        with torch.no_grad():\n            dummy_input_ids = torch.randint(\n                0,\n                getattr(tokenizer, \"vocab_size\", 50000),\n                (batch_size, max_length),\n                dtype=torch.long,\n            ).to(DEVICE)\n            \n            dummy_attention_mask = torch.ones(\n                (batch_size, max_length),\n                dtype=torch.long,\n            ).to(DEVICE)\n            \n            print(f\"[WARMUP] Running fast inference pass (batch={batch_size}, seqlen={max_length})...\")\n            \n            core.forward(\n                input_ids=dummy_input_ids,\n                attention_mask=dummy_attention_mask,\n                use_dscd=True,\n                use_asbn=False,\n                fast_inference=True,\n            )\n        \n        elapsed = time.time() - start_time\n        print(f\"[WARMUP] Completed in {elapsed:.2f}s\")\n        \n        if was_training:\n            core.train()\n        \n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        print(\"[WARMUP] Model warmup successful\")\n        print(\"=\" * 80)\n        return True\n        \n    except Exception as e:\n        print(f\"[WARMUP] Failed: {e}\")\n        if VERBOSE_LOGGING:\n            traceback.print_exc()\n        return False\n\ndef dscd_discovery_warmup(\n    model,\n    tokenizer,\n    num_sents: int = 4000,\n    batch_size: int = 64,\n    max_len: int = 48,\n    timeout_per_batch: float = 30.0,\n) -> bool:\n    print(\"=\" * 80)\n    print(\"[WARMUP] Starting DSCD discovery warmup\")\n    print(\"=\" * 80)\n    \n    try:\n        if \"load_and_preprocess_optimized\" in globals():\n            pairs = globals()[\"load_and_preprocess_optimized\"](num_sents)\n        else:\n            raise RuntimeError(\"load_and_preprocess_optimized not found\")\n        \n        dataset = globals()[\"MemoryEfficientDataset\"](pairs, tokenizer, max_length=max_len)\n        \n        collate_fn = globals().get(\"safe_collate\", None)\n        \n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=collate_fn,\n        )\n        \n        print(f\"[WARMUP] Processing {num_sents} sentences (batch={batch_size})...\")\n        \n        core = model.module if hasattr(model, \"module\") else model\n        was_training = getattr(core, \"training\", False)\n        core.eval()\n        \n        dscd = getattr(core, \"dscd\", None)\n        original_clustering_flag = None\n        \n        if dscd and hasattr(dscd, \"enable_training_clustering\"):\n            original_clustering_flag = dscd.enable_training_clustering\n            dscd.enable_training_clustering = False\n            print(\"[WARMUP] Clustering DISABLED during warmup\")\n        \n        processed = 0\n        skipped = 0\n        start_time = time.time()\n        last_print_time = start_time\n        \n        with torch.no_grad():\n            for batch_idx, batch in enumerate(dataloader):\n                if batch is None:\n                    continue\n                \n                batch_start = time.time()\n                \n                try:\n                    input_ids = batch[\"input_ids\"].to(DEVICE)\n                    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n                    src_texts = batch.get(\"src_texts\", None)\n                    \n                    core.forward(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        src_texts=src_texts,\n                        labels=None,\n                        use_dscd=True,\n                        use_asbn=False,\n                        fast_inference=False,\n                    )\n                    \n                    processed += int(input_ids.size(0))\n                    \n                    batch_elapsed = time.time() - batch_start\n                    if batch_elapsed > timeout_per_batch:\n                        print(f\"\\n[WARMUP] Batch {batch_idx} timeout ({batch_elapsed:.1f}s) - stopping warmup\")\n                        break\n                    \n                    current_time = time.time()\n                    if (current_time - last_print_time >= 5.0) or (processed % (batch_size * 5) == 0):\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (num_sents - processed) / rate if rate > 0 else 0\n                        progress = (processed / num_sents) * 100.0\n                        print(\n                            f\"\\r[WARMUP] {processed}/{num_sents} ({progress:.1f}%) | \"\n                            f\"{rate:.1f} sent/s | ETA {eta:.0f}s\",\n                            end=\"\",\n                        )\n                        last_print_time = current_time\n                \n                except KeyboardInterrupt:\n                    print(\"\\n[WARMUP] Interrupted by user\")\n                    break\n                \n                except Exception as e:\n                    skipped += batch_size\n                    if VERBOSE_LOGGING or skipped > 100:\n                        print(f\"\\n[WARMUP] Batch {batch_idx} failed: {e}\")\n                    \n                    if skipped > 500:\n                        print(f\"\\n[WARMUP] Too many failures ({skipped}) - aborting\")\n                        break\n                    \n                    continue\n        \n        print()\n        \n        if dscd and original_clustering_flag is not None:\n            dscd.enable_training_clustering = original_clustering_flag\n            print(\"[WARMUP] Clustering flag restored\")\n        \n        if was_training:\n            core.train()\n        \n        print(\"[WARMUP] Skipping discovery check to avoid stalls\")\n        \n        if dscd and hasattr(dscd, \"prototype_stores\"):\n            prototype_stores = resolve_dscd_stores(dscd)\n            lock = resolve_dscd_lock(dscd)\n            \n            if lock:\n                with lock:\n                    stores = dict(prototype_stores)\n            else:\n                stores = dict(prototype_stores)\n            \n            def store_size(s):\n                try:\n                    if callable(getattr(s, \"size\", None)):\n                        return int(s.size())\n                    return int(getattr(s, \"size\", 0))\n                except Exception:\n                    return 0\n            \n            num_tokens = len(stores)\n            total_protos = sum(store_size(s) for s in stores.values())\n            multi_sense = sum(1 for s in stores.values() if store_size(s) >= 2)\n            \n            print(f\"[WARMUP] Complete - Processed {processed}/{num_sents}\")\n            print(f\"  - Tokens: {num_tokens}\")\n            print(f\"  - Prototypes: {total_protos}\")\n            print(f\"  - Multi-sense: {multi_sense}\")\n            print(f\"  - Skipped: {skipped}\")\n            \n            return num_tokens > 0\n        \n        print(f\"[WARMUP] Completed {processed}/{num_sents} sentences\")\n        return processed > 0\n        \n    except Exception as e:\n        print(f\"[WARMUP] Critical failure: {e}\")\n        if VERBOSE_LOGGING:\n            traceback.print_exc()\n        return False\n    \n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\ndef main_pipeline() -> Tuple[object, object]:\n    print(\"=\" * 80)\n    print(\"TATN MAIN PIPELINE - COMPLETE INTEGRATION\")\n    print(\"=\" * 80)\n    print(f\"Configuration:\")\n    print(f\"  - Span threshold: {SPAN_THRESHOLD}\")\n    print(f\"  - Uncertainty threshold: {UNCERTAINTY_THRESHOLD}\")\n    print(f\"  - Discovery frequency: {PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - Epochs: {EPOCHS}\")\n    print(f\"  - Batch size: {BATCH_SIZE}\")\n    print(f\"  - Accumulation steps: {ACCUMULATION_STEPS}\")\n    print(f\"  - Device: {DEVICE}\")\n    print(\"=\" * 80)\n    \n    required_functions = [\"MemoryEfficientDataset\", \"MemoryOptimizedTATNWithExplanations\", \"train_memory_efficient_tatn\"]\n    missing = [fn for fn in required_functions if fn not in globals()]\n    if missing:\n        print(f\"[PIPELINE] ERROR: Missing critical functions: {missing}\")\n        print(\"[PIPELINE] Please run all previous cells (1-9) first\")\n        raise RuntimeError(f\"Missing required functions: {missing}\")\n    \n    optional_functions = [\"comprehensive_post_training_testing\"]\n    missing_optional = [fn for fn in optional_functions if fn not in globals()]\n    if missing_optional:\n        print(f\"[PIPELINE] Warning: Missing optional functions: {missing_optional}\")\n        print(\"[PIPELINE] Some evaluation features will be skipped\")\n    \n    pipeline_start = time.time()\n    \n    if DEBUG_TIMING:\n        phase_start = time.time()\n    \n    initialize_environment()\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    print(\"[PHASE 1] Loading tokenizer...\")\n    tokenizer = safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    \n    try:\n        tokenizer.src_lang = SOURCE_LANGUAGE\n    except Exception:\n        pass\n    \n    try:\n        if not hasattr(tokenizer, \"pad_token_id\") or tokenizer.pad_token_id is None:\n            if hasattr(tokenizer, \"add_special_tokens\"):\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n    except Exception:\n        pass\n    \n    vocab_size = getattr(tokenizer, \"vocab_size\", None)\n    if vocab_size is None:\n        try:\n            vocab_size = len(tokenizer)\n        except Exception:\n            vocab_size = \"unknown\"\n    \n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size})\")\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    print(f\"[PHASE 2] Loading data ({NUM_SAMPLES} samples)...\")\n    \n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = globals()[\"load_and_preprocess_optimized\"](NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n    \n    try:\n        dataset = globals()[\"MemoryEfficientDataset\"](pairs, tokenizer, max_length=MAX_LENGTH)\n    except Exception as e:\n        print(f\"[PHASE 2] Dataset creation failed: {e}\")\n        raise RuntimeError(f\"Failed to create dataset: {e}\")\n    \n    collate_fn = globals().get(\"safe_collate\", None)\n    \n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = globals()[\"create_optimized_dataloader\"](dataset, batch_size=BATCH_SIZE, shuffle=True)\n        except Exception as e:\n            print(f\"[PHASE 2] create_optimized_dataloader failed: {e}, using DataLoader\")\n            dataloader_kwargs = {\n                \"batch_size\": BATCH_SIZE,\n                \"shuffle\": True,\n                \"num_workers\": 0,\n                \"pin_memory\": torch.cuda.is_available(),\n            }\n            if collate_fn is not None:\n                dataloader_kwargs[\"collate_fn\"] = collate_fn\n            train_loader = DataLoader(dataset, **dataloader_kwargs)\n    else:\n        dataloader_kwargs = {\n            \"batch_size\": BATCH_SIZE,\n            \"shuffle\": True,\n            \"num_workers\": 0,\n            \"pin_memory\": torch.cuda.is_available(),\n        }\n        if collate_fn is not None:\n            dataloader_kwargs[\"collate_fn\"] = collate_fn\n        else:\n            print(\"[PHASE 2] Warning: safe_collate not found, using default collation\")\n        train_loader = DataLoader(dataset, **dataloader_kwargs)\n    \n    try:\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {len(train_loader)} batches\")\n    except Exception:\n        print(\"[PHASE 2] Dataset loaded\")\n    \n    del pairs\n    safe_clear_gpu_caches()\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    print(\"[PHASE 3] Initializing model...\")\n    model_core = globals()[\"MemoryOptimizedTATNWithExplanations\"](tokenizer)\n    \n    if USE_MULTI_GPU and NUM_GPUS > 1:\n        device_ids = list(range(NUM_GPUS))\n        print(f\"[PHASE 3] Using DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        model = model_core\n    \n    model = model.to(DEVICE)\n    core_model = model.module if hasattr(model, \"module\") else model\n    \n    try:\n        mbart = getattr(core_model, \"mbart\", None)\n        if mbart and hasattr(mbart, \"resize_token_embeddings\"):\n            try:\n                current_size = mbart.get_input_embeddings().num_embeddings\n                if isinstance(vocab_size, int):\n                    target_size = vocab_size\n                else:\n                    target_size = current_size\n                \n                if current_size != target_size:\n                    mbart.resize_token_embeddings(target_size)\n                    print(f\"[PHASE 3] Resized embeddings: {current_size} -> {target_size}\")\n            except Exception:\n                pass\n    except Exception:\n        pass\n    \n    if FREEZE_ENCODER:\n        try:\n            for p in core_model.mbart.model.encoder.parameters():\n                p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n    \n    print(f\"[PHASE 3] Model initialized\")\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    print(\"[PHASE 3.5] Initial model warmup...\")\n    \n    try:\n        warmup_success = warmup_model(model, tokenizer, num_samples=100, batch_size=16, max_length=MAX_LENGTH)\n        if warmup_success:\n            print(\"[PHASE 3.5] Initial warmup successful\")\n        else:\n            print(\"[PHASE 3.5] Initial warmup completed with issues\")\n    except Exception as e:\n        print(f\"[PHASE 3.5] Initial warmup failed: {e}\")\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Initial warmup: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    print(f\"[PHASE 4] Training for {EPOCHS} epoch(s)...\")\n    print(\"[PHASE 4] Creating optimizers...\")\n    \n    try:\n        nmt_params = []\n        phi_params = []\n        \n        if hasattr(core_model, \"mbart\") and core_model.mbart is not None:\n            nmt_params.extend([p for p in core_model.mbart.parameters() if p.requires_grad])\n        \n        if hasattr(core_model, \"dscd\") and core_model.dscd is not None:\n            phi_params.extend([p for p in core_model.dscd.parameters() if p.requires_grad])\n        \n        if hasattr(core_model, \"asbn\") and core_model.asbn is not None:\n            phi_params.extend([p for p in core_model.asbn.parameters() if p.requires_grad])\n        \n        if hasattr(core_model, \"trg_system\") and core_model.trg_system is not None:\n            phi_params.extend([p for p in core_model.trg_system.parameters() if p.requires_grad])\n        \n        if not nmt_params:\n            nmt_params = [p for p in model.parameters() if p.requires_grad]\n        \n        optimizer = torch.optim.AdamW(nmt_params, lr=LR_NMT, weight_decay=0.01)\n        print(f\"[PHASE 4] Created optimizer with {len(nmt_params)} parameters (lr={LR_NMT})\")\n        \n        phi_optimizer = None\n        if phi_params:\n            phi_optimizer = torch.optim.AdamW(phi_params, lr=LR_PHI, weight_decay=0.01)\n            print(f\"[PHASE 4] Created phi_optimizer with {len(phi_params)} parameters (lr={LR_PHI})\")\n        else:\n            print(\"[PHASE 4] No phi parameters found, phi_optimizer=None\")\n        \n    except Exception as e:\n        print(f\"[PHASE 4] Optimizer creation failed: {e}\")\n        if VERBOSE_LOGGING:\n            traceback.print_exc()\n        raise\n    \n    try:\n        train_fn = globals()[\"train_memory_efficient_tatn\"]\n        \n        trained_model = train_fn(\n            model=model,\n            tokenizer=tokenizer,\n            train_loader=train_loader,\n            optimizer=optimizer,\n            phi_optimizer=phi_optimizer,\n            epochs=EPOCHS,\n            accumulation_steps=ACCUMULATION_STEPS,\n            validate_every=VALIDATION_CHECK_INTERVAL,\n            enable_validation=True,\n            device=DEVICE,\n        )\n        \n        print(\"[PHASE 4] Training completed successfully\")\n        \n    except Exception as e:\n        print(f\"[PHASE 4] Training failed: {e}\")\n        if VERBOSE_LOGGING:\n            traceback.print_exc()\n        trained_model = model\n    \n    if DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    \n    if CHECKPOINT_SAVE_AFTER_TRAINING:\n        print(\"[PHASE 5] Saving checkpoint...\")\n        \n        try:\n            core_model_to_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            \n            checkpoint = {\n                \"model_state_dict\": core_model_to_save.state_dict(),\n                \"epoch\": EPOCHS,\n                \"config\": {\n                    \"source_lang\": SOURCE_LANGUAGE,\n                    \"target_lang\": TARGET_LANGUAGE,\n                    \"num_samples\": NUM_SAMPLES,\n                    \"batch_size\": BATCH_SIZE,\n                    \"max_length\": MAX_LENGTH,\n                }\n            }\n            \n            if hasattr(core_model_to_save, \"dscd\") and hasattr(core_model_to_save.dscd, \"state_dict\"):\n                try:\n                    checkpoint[\"dscd_state\"] = core_model_to_save.dscd.state_dict()\n                    print(\"[PHASE 5] DSCD state included in checkpoint\")\n                except Exception:\n                    pass\n            \n            torch.save(checkpoint, CHECKPOINT_PATH)\n            \n            size_mb = os.path.getsize(CHECKPOINT_PATH) / (1024**2)\n            print(f\"[PHASE 5] Checkpoint saved: {CHECKPOINT_PATH} ({size_mb:.1f} MB)\")\n            \n        except Exception as e:\n            print(f\"[PHASE 5] Checkpoint save failed: {e}\")\n        \n        if DEBUG_TIMING:\n            print(f\"[TIMING] Checkpoint save: {time.time() - phase_start:.2f}s\")\n            phase_start = time.time()\n    \n    if \"comprehensive_post_training_testing\" in globals():\n        print(\"[PHASE 6] Running evaluation...\")\n        \n        try:\n            eval_fn = globals()[\"comprehensive_post_training_testing\"]\n            results = eval_fn(trained_model, tokenizer, run_warmup=False)\n            \n            print(f\"[PHASE 6] Evaluation completed\")\n            print(f\"  - Success rate: {results.get('success_rate_pct', 0):.1f}%\")\n            print(f\"  - Total explanations: {results.get('total_explanations', 0)}\")\n            \n        except Exception as e:\n            print(f\"[PHASE 6] Evaluation failed: {e}\")\n            if VERBOSE_LOGGING:\n                traceback.print_exc()\n        \n        if DEBUG_TIMING:\n            print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n    else:\n        print(\"[PHASE 6] Evaluation skipped (function not available)\")\n    \n    pipeline_duration = time.time() - pipeline_start\n    \n    print(\"=\" * 80)\n    print(f\"PIPELINE COMPLETED IN {pipeline_duration:.1f}s\")\n    print(\"=\" * 80)\n    \n    safe_clear_gpu_caches()\n    \n    return trained_model, tokenizer\n\nprint(\"=\" * 80)\nprint(\"Cell 10: Main pipeline ready - COMPLETE VERSION\")\nprint(\"=\" * 80)\n","metadata":{"id":"kEux2BVXH4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:46.790451Z","iopub.execute_input":"2026-01-07T14:57:46.790883Z","iopub.status.idle":"2026-01-07T14:57:46.845873Z","shell.execute_reply.started":"2026-01-07T14:57:46.790858Z","shell.execute_reply":"2026-01-07T14:57:46.845242Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 10: Main pipeline ready - COMPLETE VERSION\n================================================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER (FINAL) - FIXED\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\n\ntry:\n    _NUM_SAMPLES = int(globals().get('NUM_SAMPLES', 30000))\n    _EPOCHS = int(globals().get('EPOCHS', 2))\n    _BATCH_SIZE = int(globals().get('BATCH_SIZE', 4))\n    _ACCUMULATION_STEPS = int(globals().get('ACCUMULATION_STEPS', 16))\n    \n    # FIX: Robust device loading\n    raw_device = globals().get('DEVICE', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        _DEVICE = torch.device(str(raw_device))\n\n    _ENABLE_ASBN_TRAINING = bool(globals().get('ENABLE_ASBN_TRAINING', True))\n    _ENABLE_TRG_INFERENCE = bool(globals().get('ENABLE_TRG_INFERENCE', True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(globals().get('PERIODIC_DISCOVERY_FREQUENCY', 3000))\n    _VERBOSE_LOGGING = bool(globals().get('VERBOSE_LOGGING', False))\n    _DEBUG_DISCOVERY = bool(globals().get('DEBUG_DISCOVERY', False))\n    _DEBUG_TIMING = bool(globals().get('DEBUG_TIMING', False))\n    _NUM_GPUS = int(globals().get('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(globals().get('USE_MULTI_GPU', _NUM_GPUS > 1))\n    _SPAN_THRESHOLD = float(globals().get('SPAN_THRESHOLD', 0.20))\n    _TAU_LOW = float(globals().get('TAU_LOW', 0.15))\n    \n    raw_list = globals().get('HOMOGRAPH_REFERENCE_LIST_BN', [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_list)\n    cell0_loaded = 'NUM_SAMPLES' in globals()\n    \nexcept (NameError, TypeError, ValueError) as e:\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 3000\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.20\n    _TAU_LOW = 0.15\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}\n    cell0_loaded = False\n    print(\"[EXEC] Using fallback configuration (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\ndef _format_duration(seconds: float) -> str:\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}min\"\n    else:\n        return f\"{seconds/3600:.2f}hr\"\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, default)\n        if result is default:\n            return default\n    return result\n\ndef _get_dscd_homographs(model):\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n\n        if dscd and hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        if dscd and hasattr(dscd, 'prototype_stores'):\n            homographs = set()\n\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            # FIX: Safe locking\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            for token, store in stores.items():\n                try:\n                    if hasattr(store, 'size') and callable(getattr(store, 'size', None)):\n                        size_ok = store.size() >= 2\n                    else:\n                        size_val = getattr(store, 'size', None)\n                        if isinstance(size_val, int):\n                            size_ok = size_val >= 2\n                        else:\n                            size_ok = False\n                except Exception:\n                    size_ok = False\n\n                if size_ok:\n                    clean = str(token).replace(' ', '').replace('Ġ', '').replace('##', '').strip().lower()\n                    homographs.add(clean)\n            return homographs\n    except Exception:\n        pass\n    return set()\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN - COMPLETE EXECUTION\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  Uncertainty threshold: {_TAU_LOW}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"  Batch per GPU: {per_gpu}\")\n\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    if 'main_pipeline' not in globals():\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n\n            if _DEBUG_TIMING:\n                print(\"   Expected: ~15-45 min (config dependent)\")\n\n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n\n            if \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n\n                print(\"\\nFixes:\")\n                print(\"   1. Reduce BATCH_SIZE (try 2-4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10k-20k)\")\n                print(\"   3. Increase ACCUMULATION_STEPS (32-64)\")\n\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024**2)\n                print(f\"  File: {_CHECKPOINT_PATH}\")\n                print(f\"  Size: {size_mb:.1f} MB\")\n\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n\n                has_model = 'model_state_dict' in ckpt and len(ckpt['model_state_dict']) > 0\n                has_dscd = 'dscd_state' in ckpt and len(ckpt.get('dscd_state', {})) > 0\n\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n\n                if has_dscd:\n                    num_tokens = len(ckpt['dscd_state'].get('prototype_stores_data', {})) # Changed to match DSCD format\n                    print(f\"  Tokens: {num_tokens}\")\n\n                    if num_tokens > 0:\n                        checkpoint_valid = True\n                        print(\"  Status: VALID\")\n                    else:\n                        print(\"  Status: EMPTY DSCD\")\n                else:\n                    print(\"  Status: MISSING DSCD\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n\n        except Exception as e:\n            print(f\"  Validation failed: {e}\")\n\n        print(\"\\n[COMPONENTS]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd = getattr(core, 'dscd', None)\n            if dscd and hasattr(dscd, 'get_prototype_summary'):\n                try:\n                    dscd_stats = dscd.get_prototype_summary()\n                    print(\"  DSCD:\")\n                    print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                    print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                    print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                except Exception:\n                    pass\n\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                try:\n                    asbn_stats = asbn.get_detailed_stats()\n                    print(\"  ASBN:\")\n                    print(f\"    - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n                    if 'source_accuracy' in asbn_stats:\n                        print(f\"    - Source: {asbn_stats['source_accuracy']:.2%}\")\n                        print(f\"    - Target: {asbn_stats['target_accuracy']:.2%}\")\n                except Exception:\n                    pass\n\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'get_statistics'):\n                try:\n                    trg_stats = trg.get_statistics()\n                    print(\"  TRG:\")\n                    print(f\"    - Explanations: {trg_stats.get('explanations_generated', 0)}\")\n                    print(f\"    - High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                    print(f\"    - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"  Stats failed: {e}\")\n\n        print(\"\\n[METRICS]\")\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n\n                training_stats = ckpt.get('training_stats', {})\n                if training_stats:\n                    total_loss = training_stats.get('total_loss', [])\n                    updates = training_stats.get('optimizer_updates', 0)\n\n                    print(\"  Training:\")\n                    print(f\"    - Updates: {updates}\")\n                    if total_loss:\n                        if len(total_loss) >= 100:\n                            final = sum(total_loss[-100:]) / len(total_loss[-100:])\n                        else:\n                            final = sum(total_loss) / len(total_loss)\n                        print(f\"    - Final loss: {final:.6f}\")\n\n                eval_results = ckpt.get('eval_results', {})\n                baseline = ckpt.get('baseline_metrics', {})\n\n                if eval_results:\n                    final_success = eval_results.get('success_rate_pct', 0)\n                    total_expl = eval_results.get('total_explanations', 0)\n\n                    print(\"  Evaluation:\")\n                    if baseline:\n                        baseline_success = baseline.get('success_rate_pct', 0)\n                        improvement = final_success - baseline_success\n                        print(f\"    - Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n                        print(f\"    - Improvement: {improvement:+.1f}%\")\n                    else:\n                        print(f\"    - Success: {final_success:.1f}%\")\n\n                    print(f\"    - Explanations: {total_expl}\")\n\n                    quality = eval_results.get('quality_metrics', {})\n                    if quality:\n                        print(f\"    - Avg confidence: {quality.get('avg_confidence', 0):.3f}\")\n\n        except Exception as e:\n            print(f\"  Metrics failed: {e}\")\n\n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing disambiguation on ambiguous sentences...\")\n        print(\"-\" * 80)\n\n        _safe_cleanup()\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n        test_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"কল (tap/call)\"),\n            (\"কাল আমি বই কিনব।\", \"কাল (tomorrow/yesterday)\"),\n            (\"পাতা ঝরে পড়েছে।\", \"পাতা (leaf/page)\"),\n        ]\n\n        inference_times = []\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"translate_with_explanations not available\")\n                print(\"   -> Run Cell 8 before Cell 11\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}.  {desc}\")\n                        print(f\"   Input: {sentence}\")\n\n                        inf_start = time.time()\n                        res = translate_with_explanations(trained_model, tokenizer, sentence)\n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n\n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations', []) or []\n\n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs:\n                                for exp in exs:\n                                    word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                    clean = str(word).replace(' ', '').replace('Ġ', '').strip().lower()\n\n                                    if clean in dscd_homographs:\n                                        dscd_homographs_detected.add(clean)\n\n                                    try:\n                                        conf = float(exp.get('confidence', 0.5))\n                                        span = float(exp.get('span', 0.0))\n                                        u = float(exp.get('uncertainty', 0.0))\n                                        print(f\"   -> '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                    except Exception:\n                                        print(f\"   -> '{word}': (no metrics)\")\n\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n\n                        _safe_cleanup()\n\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__}\")\n                        inference_failed += 1\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n\n                if inference_times:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg per sentence\")\n\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                else:\n                    print(\"No DSCD homographs detected\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries (run warmup)\")\n                    else:\n                        print(f\"   -> Check TRG thresholds (span={_SPAN_THRESHOLD}, u={_TAU_LOW})\")\n\n        except Exception as e:\n            print(f\"Validation failed: {e}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n[SYSTEM TEST]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd_ok = hasattr(core, 'dscd') and hasattr(core.dscd, 'forward')\n            asbn_ok = hasattr(core, 'asbn') and hasattr(core.asbn, 'forward')\n            trg_ok = hasattr(core, 'trg_system') and hasattr(core.trg_system, 'process_sentence_for_explanations')\n            mbart_ok = hasattr(core, 'mbart') and hasattr(core.mbart, 'generate')\n\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n            print(f\"    - TRG: {'OK' if trg_ok else 'MISSING'}\")\n            print(f\"    - M2M100: {'OK' if mbart_ok else 'MISSING'}\")\n\n            all_ok = dscd_ok and asbn_ok and trg_ok and mbart_ok\n\n            if all_ok:\n                print(\"  All components operational\")\n            else:\n                print(\"  Some components missing\")\n\n        except Exception as e:\n            print(f\"  Test failed: {e}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n\n        print(\"\\n1. Single translation:\")\n        print(\"   result = translate_with_explanations(trained_model, tokenizer, 'আমি কল বন্ধ করেছি।')\")\n\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(\"       res = translate_with_explanations(trained_model, tokenizer, sent)\")\n\n        print(\"\\n3. Load checkpoint:\")\n        print(\"   ckpt = torch.load('/kaggle/working/tatn_final.pt')\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if not checkpoint_valid:\n            print(\"\\nCheckpoint needs verification - re-run Cell 10 if needed\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n\n        print(\"\\n[DIAGNOSTICS]\")\n\n        components = {\n            'Cell 0': 'NUM_SAMPLES' in globals(),\n            'Cell 1': 'reconstruct_word_spans' in globals(),\n            'Cell 2': 'MemoryEfficientDataset' in globals(),\n            'Cell 3': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8': 'translate_with_explanations' in globals(),\n            'Cell 9': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10': 'main_pipeline' in globals(),\n        }\n\n        all_present = True\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status} {comp}\")\n            if not present:\n                all_present = False\n\n        print(\"\\n[RECOVERY]\")\n\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"  ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"  Then RESTART kernel and re-run Cells 0-11\")\n\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory in Cell 0:\")\n            print(\"  BATCH_SIZE = 2\")\n            print(\"  NUM_SAMPLES = 15000\")\n            print(\"  ACCUMULATION_STEPS = 32\")\n            print(\"  Then re-run Cells 0-11\")\n\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0:\")\n            print(\"  VERBOSE_LOGGING = True\")\n            print(\"  DEBUG_DISCOVERY = True\")\n            print(\"  Then re-run Cell 11 for details\")\n\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check checkpoint exists:\")\n            print(f\"  os.path.exists('{_CHECKPOINT_PATH}')\")\n            print(\"  If yes, can load and skip training\")\n            print(\"  If no, re-run Cell 11\")\n\n        else:\n            print(\"\\n-> General steps:\")\n            print(\"  1.  Enable DEBUG in Cell 0\")\n            print(\"  2. Re-run Cells 0-11\")\n            print(\"  3. Check GPU: torch.cuda.is_available()\")\n            print(\"  4.  Verify data loaded\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS\")\n        if 'checkpoint_valid' in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID\")\n        else:\n            print(\"Checkpoint: CHECK NEEDED\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n\n    print(\"=\" * 80)\n\n    _safe_cleanup()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution wrapper ready (FINAL) - FIXED\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"9n4Hrn1wH4J6","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:57:46.846957Z","iopub.execute_input":"2026-01-07T14:57:46.847192Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nMEMORY-OPTIMIZED TATN - COMPLETE EXECUTION\n================================================================================\nUser: manas0003\nStarted: 2026-01-07 14:57:46 UTC\n\n[CONFIGURATION]\n  Cell 0 status: Loaded\n  Samples: 30000\n  Epochs: 1\n  Batch Size: 100\n  Accumulation: 16\n  Device: cuda:0\n  Multi-GPU: ENABLED (2 GPUs)\n  Span threshold: 0.2\n  Uncertainty threshold: 0.15\n  Discovery frequency: 200\n  Batch per GPU: 50\n  ASBN: Enabled\n  TRG: Enabled\n  Debug: Disabled\n================================================================================\n\nStarting pipeline...\n   Expected: ~15-45 min (config dependent)\n================================================================================\nTATN MAIN PIPELINE - COMPLETE INTEGRATION\n================================================================================\nConfiguration:\n  - Span threshold: 0.2\n  - Uncertainty threshold: 0.15\n  - Discovery frequency: 200\n  - Epochs: 1\n  - Batch size: 100\n  - Accumulation steps: 16\n  - Device: cuda:0\n================================================================================\n[PIPELINE] Initializing environment...\n[PIPELINE] GPUs: 2\n  GPU 0: Tesla T4 (14.7 GB)\n  GPU 1: Tesla T4 (14.7 GB)\n[TIMING] Initialization: 0.35s\n[PHASE 1] Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897aadcb1a714bb5a08528f69e27eec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6fa9e50a62844cb847aeaa1659e0b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d506c6fc6fe4fc192d31b02c60d0a1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017db55f89cd44b7abf1eb3b99620438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be637ffb435f466eaa9098fa3f97bd7a"}},"metadata":{}},{"name":"stdout","text":"[PHASE 1] Tokenizer loaded (vocab: 128104)\n[TIMING] Tokenizer: 1.42s\n[PHASE 2] Loading data (30000 samples)...\n[CELL2] Loading up to 30000 samples from local CSV: /kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\n[CELL2] Reading CSV file...\n[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn→en task.\n[CELL2] Swap successful: src=Bengali, tgt=English\n[CELL2] Processing 30000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 30000/30000 [00:00<00:00, 114972.71it/s]\nUsing cls_token, but it is not set yet.\nUsing cls_token, but it is not set yet.\nUsing mask_token, but it is not set yet.\nUsing mask_token, but it is not set yet.\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 30000 pairs from CSV, skipped 0 rows\n[CELL2] Dataset initialized: 30000 valid pairs, 0 invalid\n[CELL2] DataLoader created: total_batch=100, per_gpu=50, workers=2\n[PHASE 2] Dataset: 30000 samples, 300 batches\n[TIMING] Data loading: 0.83s\n[PHASE 3] Initializing model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3754e9527dc94c2ba2f5cdbb3d13f133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09eb85f5aed149918d70d890e7c81505"}},"metadata":{}},{"name":"stdout","text":"[PHASE 3] Using DataParallel on [0, 1]\n[PHASE 3] Resized embeddings: 128112 -> 128104\n[PHASE 3] Model initialized\n[TIMING] Model init: 15.49s\n[PHASE 3.5] Initial model warmup...\n================================================================================\n[WARMUP] Starting fast model warmup...\n================================================================================\n[WARMUP] Running fast inference pass (batch=16, seqlen=48)...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767797887.416796      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767797887.476643      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767797887.986171      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767797887.986199      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767797887.986207      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767797887.986209      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"[TOKENIZER] Suppressing further language warnings\n[WARMUP] Completed in 15.51s\n[WARMUP] Model warmup successful\n================================================================================\n[PHASE 3.5] Initial warmup successful\n[TIMING] Initial warmup: 15.51s\n[PHASE 4] Training for 1 epoch(s)...\n[PHASE 4] Creating optimizers...\n[PHASE 4] Created optimizer with 509 parameters (lr=2e-05)\n[PHASE 4] Created phi_optimizer with 44 parameters (lr=1e-05)\n[TRAIN] Starting training: epochs=1, batch=100, accum_steps=16\n[TRAIN] Validation: enabled\n[TRAIN] DP enabled: True, GPUs: 2, Device: cuda:0\n[TRAIN] Discovery frequency: 200 steps\n[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt after all epochs\n\n================================================================================\nEPOCH 1/1 STARTED\n================================================================================\n[TRAIN] TRG statistics reset for epoch 1\n[TRAIN] ASBN statistics reset for epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  66%|██████▋   | 199/300 [45:41<23:32, 13.98s/it, fwd_loss=4.3094 bwd_loss=0.2693 rate=100.0% clusters=6357 next_disc_in=1]   ","output_type":"stream"},{"name":"stdout","text":"\n[TRAIN] Triggering periodic discovery at step 200...\n[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.25 resv=12.20\n  GPU 1: alloc=1.30 resv=8.30\n[TRAIN-DEBUG] step=200 loss=4.1804 clusters=6375\n\n[CLUSTER] Top 5 clusters:\n------------------------------------------------------------------------------------------\nRank  Token          Count       Protos    Mu             Tau         \n------------------------------------------------------------------------------------------\n1     সিদ্ধান্ত      53          6         24.379122      3.906033    \n2     লাগে           51          4         23.324217      4.017983    \n3     আলোয়          51          4         23.527962      3.982990    \n4     পরিবর্তন       51          4         22.262837      4.183382    \n5     মান            51          4         24.489047      3.032672    \n------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  69%|██████▉   | 207/300 [47:32<21:51, 14.10s/it, fwd_loss=4.2356 bwd_loss=0.2647 rate=100.0% clusters=6482 next_disc_in=193]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEPOCH 1 COMPREHENSIVE VALIDATION (Step 208)\n================================================================================\n\n[VALIDATION] Testing 10 samples:\n--------------------------------------------------------------------------------\n   1. no expl         কল=tap/call                    -> I closed the call.\n   2. no expl         কাল=tomorrow/yesterday         -> I will buy the book tomorrow..\n   3. no expl         পাতা=leaf/page                 -> the page is shred.\n   4. no expl         ব্যাংক=bank/embankment         -> he went to the bank.\n   5. no expl         No ambiguity                   -> I is good.\n   6. no expl         No ambiguity                   -> she speak very sweetly.\n   7. no expl         No ambiguity                   -> this is my book.\n   8. no expl         No ambiguity                   -> the weather is good today.\n   9. no expl         ফল=fruit/result                -> the fruit is very good.\n  10. no expl         মাথা=head/top                  -> he he he is pain....\n\n--------------------------------------------------------------------------------\n[VALIDATION] DSCD Prototype Quality Check:\n  - Quality Score: 92.6%\n  - Multi-sense tokens: 1194\n  - Total prototypes: 3370\n\n--------------------------------------------------------------------------------\n[VALIDATION] ASBN Training Statistics:\n  - Domain Loss: 0.0000\n  - Domain Accuracy: 0.00%\n  - Source Accuracy: 0.00%\n  - Target Accuracy: 0.00%\n\n--------------------------------------------------------------------------------\n[VALIDATION] TRG Explanation Statistics:\n  - Total explanations: 4\n  - High confidence rate: 25.0%\n  - DSCD homograph rate: 100.0%\n--------------------------------------------------------------------------------\n\n[VALIDATION] Summary:\n  - Translations: 10/10 successful\n  - Explanations generated: 0\n  - Avg explanation confidence: 0.000\n  - DSCD homographs explained: 0\n  - Reference homographs explained: 0\n  - DSCD Quality Score: 92.6%\n  - Multi-sense tokens: 1194\n  - ASBN Domain Accuracy: 0.00%\n\n[VALIDATION] Health Warnings:\n  - No explanations generated\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  69%|██████▉   | 208/300 [47:58<26:39, 17.39s/it, fwd_loss=4.3017 bwd_loss=0.2689 rate=100.0% clusters=6498 next_disc_in=192]","output_type":"stream"},{"name":"stdout","text":"================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 300/300 [1:09:11<00:00, 13.84s/it, fwd_loss=3.7065 bwd_loss=0.2317 rate=100.0% clusters=7675 next_disc_in=100]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEPOCH 1/1 SUMMARY\n================================================================================\n  Duration (min): 69.19\n  Optimizer updates: 19\n  Batches: processed=300, skipped=0\n  Success rate: 105.6%\n  Clustered tokens: 7675\n  Avg epoch loss: 5.337965\n================================================================================\n\n[TRAIN] Running synchronous DSCD clustering after epoch 1...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# debug cell","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12: DEEP DIAGNOSTIC DEBUGGER (ROOT CAUSE ANALYSIS) - FIXED\n# ==============================================================================\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\ndef deep_debug_tatn(model, tokenizer, test_sentences):\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN DEEP DIAGNOSTIC REPORT (FIXED)\")\n    print(\"=\" * 80)\n\n    # 1. Configuration Check\n    print(\"[1] CONFIGURATION CHECK\")\n    try:\n        if \"REAL_AMB_SPAN_THRESHOLD\" in globals():\n            span_thresh = float(REAL_AMB_SPAN_THRESHOLD)\n        elif \"_SPAN_THRESHOLD\" in globals():\n            span_thresh = float(_SPAN_THRESHOLD)\n        else:\n            span_thresh = 0.05\n\n        if \"REAL_AMB_UNCERTAINTY_THRESHOLD\" in globals():\n            tau_low = float(REAL_AMB_UNCERTAINTY_THRESHOLD)\n        elif \"_TAU_LOW\" in globals():\n            tau_low = float(_TAU_LOW)\n        else:\n            tau_low = 0.15\n\n        print(f\"   SPAN_THRESHOLD: {span_thresh}\")\n        print(f\"   TAU_LOW (Uncertainty): {tau_low}\")\n    except Exception:\n        print(\"   Warning: Could not read global config variables. Using defaults.\")\n        span_thresh = 0.05\n        tau_low = 0.15\n\n    # 2. Component Health Check\n    print(\"\\n[2] COMPONENT HEALTH CHECK\")\n    core = model.module if hasattr(model, \"module\") else model\n\n    # Check DSCD\n    dscd = getattr(core, \"dscd\", None)\n    if dscd:\n        try:\n            stores = {}\n            lock = None\n            if hasattr(dscd, \"bufferlock\"):\n                lock = dscd.bufferlock\n            elif hasattr(dscd, \"clusteringlock\"):\n                lock = dscd.clusteringlock\n\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototypestores\", {}))\n            else:\n                stores = dict(getattr(dscd, \"prototypestores\", {}))\n\n            homographs = []\n            for k, v in stores.items():\n                try:\n                    sz = v.size() if hasattr(v, \"size\") and callable(getattr(v, \"size\", None)) else 0\n                except Exception:\n                    sz = 0\n                if sz >= 2:\n                    homographs.append(k)\n\n            print(f\"   DSCD: Alive. Found {len(homographs)} homographs.\")\n            if len(homographs) > 0:\n                print(f\"   Example homographs: {homographs[:5]}\")\n            else:\n                print(\"   CRITICAL WARNING: DSCD found 0 homographs. TRG will never trigger.\")\n        except Exception as e:\n            print(f\"   DSCD Error: {e}\")\n    else:\n        print(\"   CRITICAL ERROR: DSCD module missing.\")\n\n    # Check ASBN\n    asbn = getattr(core, \"asbn\", None)\n    if asbn:\n        print(\"   ASBN: Module exists.\")\n        try:\n            try:\n                device = next(asbn.parameters()).device\n            except Exception:\n                device = next(core.parameters()).device\n\n            embed_dim = getattr(getattr(core, \"mbart\", None), \"config\", None)\n            if embed_dim is not None and hasattr(embed_dim, \"d_model\"):\n                embed_dim = int(embed_dim.d_model)\n            else:\n                embed_dim = 1024\n\n            dummy_h = torch.randn(2, 10, embed_dim, device=device)\n\n            if hasattr(asbn, \"d_domain\"):\n                d_dom_out = asbn.d_domain(dummy_h)\n                mean_val = float(d_dom_out.mean().item())\n                std_val = float(d_dom_out.std().item())\n                print(f\"   ASBN Discriminator Output Mean: {mean_val:.4f} (Should not be exactly 0 or +/- inf)\")\n                if std_val < 1e-6:\n                    print(\"   CRITICAL WARNING: ASBN Discriminator collapsed (zero variance).\")\n        except Exception as e:\n            print(f\"   ASBN Probe Failed: {e}\")\n    else:\n        print(\"   ASBN: Module missing.\")\n\n    # 3. Sentence-Level Trace\n    print(\"\\n[3] SENTENCE TRACE ANALYSIS\")\n    core.eval()\n\n    # --------------------------------------------------------------------------\n    # Alignment logic for manual trace\n    # --------------------------------------------------------------------------\n    def _build_alignment(text, tok):\n        words = text.split()\n        try:\n            enc = tok(text, return_offsets_mapping=True, add_special_tokens=False)\n            offsets = enc[\"offset_mapping\"]\n        except Exception:\n            return {}\n\n        m = {}\n        current_pos = 0\n        w_spans = []\n        for w in words:\n            start = text.find(w, current_pos)\n            if start != -1:\n                w_spans.append((start, start + len(w), w))\n                current_pos = start + len(w)\n\n        for idx, (s, e) in enumerate(offsets):\n            for ws, we, wtext in w_spans:\n                if s >= ws and e <= we:\n                    m[idx] = wtext\n                    break\n        return m\n\n    for sent_txt, target_word in test_sentences:\n        print(f\"\\nAnalyzing: '{sent_txt}' (Target: {target_word})\")\n        print(\"-\" * 60)\n\n        try:\n            device = next(core.parameters()).device\n        except Exception:\n            device = torch.device(\"cpu\")\n\n        inputs = tokenizer(sent_txt, return_tensors=\"pt\", padding=True).to(device)\n\n        token_word_map = _build_alignment(sent_txt, tokenizer)\n\n        with torch.no_grad():\n            if hasattr(core, \"mbart\"):\n                try:\n                    encoder_out = core.mbart.model.encoder(\n                        input_ids=inputs[\"input_ids\"],\n                        attention_mask=inputs.get(\"attention_mask\", None),\n                    )\n                except Exception:\n                    encoder_out = core.mbart.get_encoder()(\n                        input_ids=inputs[\"input_ids\"],\n                        attention_mask=inputs.get(\"attention_mask\", None),\n                    )\n            else:\n                print(\"   Error: core.mbart not found\")\n                continue\n\n            hidden_states = encoder_out.last_hidden_state\n\n            # DSCD Trace: call with same signature as main pipeline\n            dscd_out = None\n            try:\n                dscd_out = core.dscd.forward(\n                    tokenembeddings=hidden_states,\n                    tokentypes=None,\n                    trainmode=False,\n                    tokenwordmap=[token_word_map],\n                    inputids=inputs[\"input_ids\"],\n                    attentionmask=inputs.get(\"attention_mask\", None),\n                )\n            except TypeError:\n                try:\n                    dscd_out = core.dscd(\n                        hidden_states,\n                        tokentypes=None,\n                        trainmode=False,\n                        tokenwordmap=[token_word_map],\n                        inputids=inputs[\"input_ids\"],\n                        attentionmask=inputs.get(\"attention_mask\", None),\n                    )\n                except Exception as e:\n                    print(f\"   DSCD call failed: {e}\")\n                    continue\n            except Exception as e:\n                print(f\"   DSCD call failed: {e}\")\n                continue\n\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n            except Exception:\n                tokens = [str(i) for i in range(int(inputs[\"input_ids\"].size(1)))]\n\n            def _get_metric(key_main, key_alt=None, default_val=0.0):\n                if not isinstance(dscd_out, dict):\n                    return default_val\n                if key_main in dscd_out:\n                    val = dscd_out[key_main]\n                elif key_alt is not None and key_alt in dscd_out:\n                    val = dscd_out[key_alt]\n                else:\n                    return default_val\n                if isinstance(val, (list, tuple)):\n                    return val[0] if len(val) > 0 else default_val\n                return val\n\n            proto_assignments = _get_metric(\"protoassignments\", \"proto_assignments\", [])\n            span_preds = _get_metric(\"spanpreds\", \"span_preds\", [])\n            uncertainties = _get_metric(\"uncertainties\", None, [])\n            gates = _get_metric(\"gates\", None, [])\n\n            if isinstance(span_preds, torch.Tensor):\n                span_preds = span_preds[0]\n            if isinstance(uncertainties, torch.Tensor):\n                uncertainties = uncertainties[0]\n            if isinstance(gates, torch.Tensor):\n                gates = gates[0]\n            if isinstance(proto_assignments, torch.Tensor):\n                proto_assignments = proto_assignments[0]\n\n            found_target = False\n\n            print(f\"   {'Token':<15} {'Word':<15} {'Span':<10} {'Uncert':<10} {'Gate':<10} {'ProtoID':<8} {'Status'}\")\n            print(f\"   {'-'*15} {'-'*15} {'-'*10} {'-'*10} {'-'*10} {'-'*8} {'-'*20}\")\n\n            for i, tok in enumerate(tokens):\n                mapped_word = token_word_map.get(i, \"\")\n\n                clean_tok = tok.replace(\" \", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip().lower()\n                clean_map = mapped_word.strip().lower()\n\n                is_target = (\n                    (target_word in clean_tok)\n                    or (clean_tok in target_word)\n                    or (target_word in clean_map)\n                    or (clean_map in target_word)\n                )\n\n                try:\n                    s_val = float(span_preds[i]) if i < len(span_preds) else 0.0\n                except Exception:\n                    s_val = 0.0\n\n                try:\n                    u_val = float(uncertainties[i]) if i < len(uncertainties) else 0.0\n                except Exception:\n                    u_val = 0.0\n\n                try:\n                    g_val = float(gates[i]) if i < len(gates) else 0.0\n                except Exception:\n                    g_val = 0.0\n\n                try:\n                    p_val = int(proto_assignments[i]) if i < len(proto_assignments) else -1\n                except Exception:\n                    p_val = -1\n\n                status = []\n                if s_val < span_thresh:\n                    status.append(f\"Low Span (<{span_thresh})\")\n                if u_val < tau_low:\n                    status.append(f\"Low Uncert (<{tau_low})\")\n                if p_val == -1:\n                    status.append(\"No Proto\")\n\n                is_subword = (\n                    bool(mapped_word)\n                    and not tok.startswith(\" \")\n                    and not tok.startswith(\"Ġ\")\n                    and not tok.startswith(\"▁\")\n                    and i > 0\n                )\n                if is_subword:\n                    status.append(\"Subword Fragment\")\n\n                status_str = \" | \".join(status) if status else \"READY\"\n\n                should_explain = (s_val > span_thresh) or (u_val > tau_low)\n                if should_explain:\n                    status_str += \" [TRIGGER]\"\n\n                if is_target or should_explain:\n                    print(\n                        f\"   {tok:<15} {mapped_word:<15} {s_val:<10.4f} {u_val:<10.4f} \"\n                        f\"{g_val:<10.4f} {p_val:<8} {status_str}\"\n                    )\n                    found_target = True\n\n            if not found_target:\n                print(f\"   Note: Target word '{target_word}' not matched or completely filtered.\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ROOT CAUSE CONCLUSION\")\n    print(\"=\" * 80)\n    print(\"1. If 'Span' is 0.0000 -> DSCD clustering is too tight or Threshold too high.\")\n    print(\"2. If 'Uncert' is low -> Model is confident (ASBN might be too strong).\")\n    print(\"3. If 'No Proto' -> Word wasn't seen enough in training to form a cluster.\")\n    print(\"4. **Subword Fragment**: If you see this, Cell 5 aggregation fix is REQUIRED to combine these metrics.\")\n\n# ==============================================================================\n# EXECUTE DEBUGGER\n# ==============================================================================\nif \"trained_model\" in globals() and \"tokenizer\" in globals():\n    debug_sentences = [\n        (\"আমি কল বন্ধ করেছি।\", \"কল\"),\n        (\"কাল আমি বই কিনব।\", \"কাল\"),\n        (\"পাতা ঝরে পড়েছে।\", \"পাতা\"),\n    ]\n    deep_debug_tatn(trained_model, tokenizer, debug_sentences)\nelse:\n    print(\"Error: trained_model and tokenizer not found. Run Cell 11 first.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13: EXTENDED INFERENCE TESTING (FINAL)\n# ==============================================================================\nimport os\nimport time\nimport traceback\nimport json\nfrom typing import Tuple, Any, Dict, List, Optional\nfrom collections import defaultdict\nimport torch\nimport gc\n\ntry:\n    _DEVICE = (\n        DEVICE\n        if isinstance(DEVICE, torch.device)\n        else torch.device(str(DEVICE))\n        if isinstance(DEVICE, str)\n        else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    )\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\n    _TAU_LOW = float(TAU_LOW)\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in HOMOGRAPH_REFERENCE_LIST_BN)\n    cell0_loaded = True\nexcept (NameError, TypeError, ValueError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _SPAN_THRESHOLD = 0.20\n    _TAU_LOW = 0.15\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"কল\",\n        \"কাল\",\n        \"পাতা\",\n        \"ব্যাংক\",\n        \"ফল\",\n        \"মাথা\",\n        \"বার\",\n        \"হার\",\n        \"তারা\",\n    }\n    cell0_loaded = False\n    print(\"[TEST] Using fallback config (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\n\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n\n\ndef _compute_similarity(translation: str, expected: str) -> float:\n    try:\n        trans_words = set(translation.lower().split())\n        exp_words = set(expected.lower().split())\n        if not exp_words:\n            return 0.0\n        overlap = len(trans_words & exp_words)\n        return overlap / len(exp_words)\n    except Exception:\n        return 0.0\n\n\ndef _get_dscd_homographs(model) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, \"getdiscoveredhomographs\"):\n            try:\n                return set(dscd.getdiscoveredhomographs())\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, \"bufferlock\"):\n            lock = dscd.bufferlock\n        elif hasattr(dscd, \"clusteringlock\"):\n            lock = dscd.clusteringlock\n\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototypestores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototypestores\", {}) or {})\n\n        for token, store in stores.items():\n            try:\n                size_ok = False\n                size_attr = getattr(store, \"size\", None)\n                try:\n                    if callable(size_attr):\n                        size_ok = size_attr() >= 2\n                    elif isinstance(size_attr, int):\n                        size_ok = size_attr >= 2\n                except Exception:\n                    size_ok = False\n\n                if not size_ok:\n                    counts = getattr(store, \"counts\", None)\n                    if isinstance(counts, (list, tuple)) and len(counts) >= 2:\n                        size_ok = True\n\n                if size_ok:\n                    clean = (\n                        str(token)\n                        .replace(\"▁\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .replace(\"##\", \"\")\n                        .strip()\n                        .lower()\n                    )\n                    if clean:\n                        homographs.add(clean)\n            except Exception:\n                continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\ntrained_model_available = \"trained_model\" in globals() and globals().get(\"trained_model\") is not None\ntokenizer_available = \"tokenizer\" in globals() and globals().get(\"tokenizer\") is not None\ntranslate_available = \"translate_with_explanations\" in globals()\n\nif not trained_model_available:\n    _safe_print(\"trained_model not found - will try checkpoint\")\nif not tokenizer_available:\n    _safe_print(\"tokenizer not found - run pipeline first\")\nif not translate_available:\n    _safe_print(\"translate_with_explanations not found - run Cell 8\")\n\n\ndef try_load_checkpoint(checkpoint_path: str, tokenizer) -> Tuple[bool, Any]:\n    if not os.path.exists(checkpoint_path):\n        return False, f\"Not found: {checkpoint_path}\"\n\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        return False, \"Model class not available\"\n\n    _safe_print(f\"[TEST] Loading: {checkpoint_path}\")\n\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n    except Exception as e:\n        _safe_print(f\"[TEST] Load failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    state = None\n    if isinstance(ckpt, dict):\n        for k in (\"model_state_dict\", \"state_dict\", \"model\"):\n            if k in ckpt and isinstance(ckpt[k], dict):\n                state = ckpt[k]\n                break\n        if state is None:\n            values_sample = list(ckpt.values())[:10]\n            if any(torch.is_tensor(v) for v in values_sample):\n                state = ckpt\n    else:\n        state = ckpt\n\n    if state is None:\n        return False, \"No model state found\"\n\n    try:\n        _safe_print(f\"[TEST] Model state: {len(state)} keys\")\n    except Exception:\n        _safe_print(\"[TEST] Model state: (unknown size)\")\n\n    dscd_state = None\n    if isinstance(ckpt, dict) and \"dscd_state\" in ckpt:\n        dscd_state = ckpt[\"dscd_state\"]\n        if isinstance(dscd_state, dict):\n            # handle plain state_dict or wrapped structure\n            proto_obj = dscd_state\n            if \"prototypestores\" in dscd_state and isinstance(dscd_state[\"prototypestores\"], dict):\n                proto_obj = dscd_state[\"prototypestores\"]\n            num_tokens = len(proto_obj)\n            _safe_print(f\"[TEST] DSCD state: {num_tokens} tokens\")\n            if num_tokens == 0:\n                _safe_print(\"[TEST] DSCD empty - warmup needed\")\n        else:\n            _safe_print(\"[TEST] DSCD state invalid\")\n    else:\n        _safe_print(\"[TEST] No DSCD state\")\n\n    try:\n        model_inst = MemoryOptimizedTATNWithExplanations(tokenizer)\n    except Exception as e:\n        _safe_print(f\"[TEST] Instantiation failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    try:\n        mbart = getattr(model_inst, \"mbart\", None)\n        if mbart and hasattr(mbart, \"get_input_embeddings\"):\n            cur = mbart.get_input_embeddings().num_embeddings\n            tok_len = getattr(\n                tokenizer,\n                \"vocab_size\",\n                len(tokenizer) if hasattr(tokenizer, \"__len__\") else None,\n            )\n            if tok_len and cur != tok_len:\n                try:\n                    mbart.resize_token_embeddings(tok_len)\n                    _safe_print(f\"[TEST] Resized: {cur} -> {tok_len}\")\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    try:\n        res = model_inst.load_state_dict(state, strict=False)\n        missing = []\n        if isinstance(res, dict):\n            missing = res.get(\"missing_keys\", []) or res.get(\"missing\", [])\n        _safe_print(f\"[TEST] State loaded (missing: {len(missing)})\")\n    except Exception:\n        try:\n            new_state = {k.replace(\"module.\", \"\", 1): v for k, v in state.items()}\n            model_inst.load_state_dict(new_state, strict=False)\n            _safe_print(\"[TEST] Loaded (stripped prefixes)\")\n        except Exception as e2:\n            _safe_print(f\"[TEST] Load failed: {type(e2).__name__}\")\n            _maybe_traceback(e2)\n            return False, e2\n\n    if dscd_state:\n        try:\n            dscd = getattr(model_inst, \"dscd\", None)\n            if dscd and hasattr(dscd, \"load_state_dict\"):\n                dscd.load_state_dict(dscd_state)\n                num_tokens = len(getattr(dscd, \"prototypestores\", {}) or {}) if hasattr(\n                    dscd, \"prototypestores\"\n                ) else 0\n                _safe_print(f\"[TEST] DSCD loaded: {num_tokens} tokens\")\n                if num_tokens == 0:\n                    _safe_print(\"[TEST] DSCD has 0 tokens - warmup needed\")\n            else:\n                _safe_print(\"[TEST] No DSCD load_state_dict\")\n        except Exception as e:\n            _safe_print(f\"[TEST] DSCD load failed: {type(e).__name__}\")\n            _maybe_traceback(e)\n\n    try:\n        model_inst.to(_DEVICE)\n        model_inst.eval()\n    except Exception as e:\n        _safe_print(f\"[TEST] Device move failed: {type(e).__name__}\")\n        return False, e\n\n    _safe_print(f\"[TEST] Ready on: {_DEVICE}\")\n    return True, model_inst\n\n\nif os.path.exists(_CHECKPOINT_PATH) and tokenizer_available:\n    succ, model_or_err = try_load_checkpoint(_CHECKPOINT_PATH, globals().get(\"tokenizer\"))\n    if succ:\n        globals()[\"trained_model\"] = model_or_err\n        trained_model_available = True\n        _safe_print(\"[TEST] Checkpoint loaded\")\n    else:\n        _safe_print(\"[TEST] Checkpoint load failed\")\n\n\ndef maybe_run_warmup_if_needed(model, tokenizer, warmup_sents: int = 4000) -> bool:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n\n        if dscd is None:\n            _safe_print(\"[TEST] No DSCD - skip warmup\")\n            return False\n\n        lock = None\n        if hasattr(dscd, \"bufferlock\"):\n            lock = dscd.bufferlock\n        elif hasattr(dscd, \"clusteringlock\"):\n            lock = dscd.clusteringlock\n\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototypestores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototypestores\", {}) or {})\n\n        initial_count = len(stores)\n\n        if initial_count > 0:\n            multi_sense = sum(\n                1\n                for store in stores.values()\n                if hasattr(store, \"size\")\n                and callable(getattr(store, \"size\", None))\n                and store.size() >= 2\n            )\n            _safe_print(f\"[TEST] DSCD has {initial_count} tokens ({multi_sense} multi-sense)\")\n            return True\n\n        _safe_print(\"[TEST] DSCD empty - running warmup...\")\n\n        warmup_fn = globals().get(\"dscd_discovery_warmup\", None)\n        if warmup_fn is None or not callable(warmup_fn):\n            _safe_print(\"[TEST] Warmup function not available\")\n            return False\n\n        try:\n            warmup_start = time.time()\n            warmup_fn(\n                model,\n                tokenizer,\n                num_sents=warmup_sents,\n                batch_size=64,\n                max_len=globals().get(\"MAX_LENGTH\", 48),\n            )\n            warmup_time = time.time() - warmup_start\n\n            if lock:\n                with lock:\n                    stores_after = dict(getattr(dscd, \"prototypestores\", {}) or {})\n            else:\n                stores_after = dict(getattr(dscd, \"prototypestores\", {}) or {})\n\n            final_count = len(stores_after)\n            multi_sense = sum(\n                1\n                for store in stores_after.values()\n                if hasattr(store, \"size\")\n                and callable(getattr(store, \"size\", None))\n                and store.size() >= 2\n            )\n\n            if final_count > 0:\n                ratio = multi_sense / final_count if final_count > 0 else 0\n                _safe_print(f\"[TEST] Warmup success ({warmup_time:.1f}s)\")\n                _safe_print(\n                    f\"[TEST]    Tokens: {final_count}, \"\n                    f\"Multi-sense: {multi_sense} ({ratio:.1%})\"\n                )\n                if ratio < 0.1:\n                    _safe_print(\"[TEST] Low multi-sense ratio (<10%)\")\n                return True\n            else:\n                _safe_print(\"[TEST] Warmup complete but NO prototypes\")\n                return False\n\n        except Exception as e:\n            _safe_print(f\"[TEST] Warmup failed: {type(e).__name__}\")\n            _maybe_traceback(e)\n            return False\n\n    except Exception as e:\n        _safe_print(f\"[TEST] Warmup check failed: {type(e).__name__}\")\n        return False\n\n\ntest_sentences: List[Tuple[str, str, str]] = [\n    (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\"),\n    (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\"),\n    (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\"),\n    (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\"),\n    (\"আমি ভালো আছি।\", \"I am fine\", \"Simple\"),\n    (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Adjective\"),\n    (\"এটা আমার বই।\", \"This is my book\", \"Demonstrative\"),\n    (\"তুমি কি আমাকে সাহায্য করতে পারো? \", \"Can you help me?\", \"Question\"),\n    (\"আজ আবহাওয়া ভালো।\", \"Weather is good\", \"Simple\"),\n    (\"আমরা বাংলাদেশে বাস করি।\", \"We live in Bangladesh\", \"Country\"),\n    (\"সূর্য পূর্ব দিকে ওঠে।\", \"Sun rises in east\", \"Directional\"),\n    (\"পাখি আকাশে উড়ে।\", \"Birds fly in sky\", \"Simple present\"),\n    (\"সে স্কুলে যাচ্ছে।\", \"She is going to school\", \"Continuous\"),\n]\n\navg_conf = 0.0\navg_span = 0.0\navg_u = 0.0\navg_time = 0.0\n\nif not (trained_model_available and tokenizer_available and translate_available):\n    _safe_print(\"\\nCannot run tests - missing prerequisites\")\n    _safe_print(\"   Run Cells 0-11 or load checkpoint\")\nelse:\n    warmup_success = False\n    try:\n        warmup_success = maybe_run_warmup_if_needed(\n            globals().get(\"trained_model\"),\n            globals().get(\"tokenizer\"),\n            warmup_sents=4000,\n        )\n    except Exception as e:\n        _safe_print(f\"[TEST] Warmup failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n\n    dscd_homographs = _get_dscd_homographs(globals().get(\"trained_model\"))\n    _safe_print(f\"\\n[TEST] DSCD discovered: {len(dscd_homographs)} homographs\")\n    if dscd_homographs and _DEBUG_DISCOVERY:\n        _safe_print(f\"[TEST] Sample: {list(dscd_homographs)[:10]}\")\n\n    _safe_print(f\"\\n[COMPONENT HEALTH]\")\n    try:\n        core = globals().get(\"trained_model\")\n        core = core.module if hasattr(core, \"module\") else core\n\n        dscd = getattr(core, \"dscd\", None)\n        if dscd and hasattr(dscd, \"get_prototype_summary\"):\n            try:\n                dscd_stats = dscd.get_prototype_summary()\n                _safe_print(\n                    f\"  DSCD: {dscd_stats.get('total_tokens', 0)} tokens, \"\n                    f\"{dscd_stats.get('num_homographs', 0)} homographs\"\n                )\n            except Exception:\n                pass\n\n        asbn = getattr(core, \"asbn\", None)\n        if asbn and hasattr(asbn, \"get_detailed_stats\"):\n            try:\n                asbn_stats = asbn.get_detailed_stats()\n                _safe_print(\n                    f\"  ASBN: {asbn_stats.get('domain_accuracy', 0):.2%} domain accuracy\"\n                )\n            except Exception:\n                pass\n\n        trg = getattr(core, \"trg_system\", None)\n        if trg and hasattr(trg, \"get_statistics\"):\n            try:\n                trg_stats = trg.get_statistics()\n                _safe_print(\n                    f\"  TRG: {trg_stats.get('explanations_generated', 0)} total explanations\"\n                )\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    total = len(test_sentences)\n    successes = 0\n    tests_with_explanations = 0\n    total_ambiguous = 0\n\n    quality_metrics = {\n        \"confidences\": [],\n        \"spans\": [],\n        \"uncertainties\": [],\n        \"similarities\": [],\n    }\n\n    dscd_homographs_explained = set()\n    reference_homographs_explained = set()\n    homograph_explanations = defaultdict(list)\n\n    inference_times = []\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"EXTENDED INFERENCE TESTING\")\n    _safe_print(\"=\" * 80)\n    _safe_print(\"Configuration:\")\n    _safe_print(f\"  Cell 0: {'Loaded' if cell0_loaded else 'Fallback'}\")\n    _safe_print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    _safe_print(f\"  Uncertainty threshold: {_TAU_LOW}\")\n    _safe_print(f\"  Tests: {total}\")\n    _safe_print(\"=\" * 80)\n\n    if not warmup_success:\n        _safe_print(\"\\nWARNING: Warmup failed\")\n        _safe_print(\"   Homograph detection may not work\\n\")\n\n    for idx, (sent, expected, note) in enumerate(test_sentences, 1):\n        _safe_print(\"\\n\" + \"-\" * 70)\n        _safe_print(f\"Test {idx}/{total}: {note}\")\n        _safe_print(f\"Input: {sent}\")\n\n        try:\n            model_for_infer = globals().get(\"trained_model\")\n            tokenizer = globals().get(\"tokenizer\")\n\n            if model_for_infer is None or tokenizer is None:\n                raise RuntimeError(\"Model/tokenizer missing\")\n\n            inf_start = time.time()\n            res = translate_with_explanations(model_for_infer, tokenizer, sent)\n            inf_time = time.time() - inf_start\n            inference_times.append(inf_time)\n\n            if res is None or not isinstance(res, dict):\n                _safe_print(\"[TEST] Invalid result - skip\")\n                continue\n\n            translation = str(res.get(\"translation\", \"\"))\n            amb_count = int(res.get(\"ambiguous_words_detected\", 0))\n            explanations = res.get(\"explanations\", []) or []\n\n            _safe_print(f\"Translation: {translation}\")\n            _safe_print(f\"Time: {inf_time:.3f}s\")\n\n            similarity = _compute_similarity(translation, expected)\n            quality_metrics[\"similarities\"].append(similarity)\n            _safe_print(f\"Similarity: {similarity:.1%}\")\n\n            _safe_print(f\"Ambiguous: {amb_count}\")\n\n            if amb_count > 0:\n                tests_with_explanations += 1\n                total_ambiguous += amb_count\n                _safe_print(\"Explanations:\")\n\n                for j, e in enumerate(explanations, 1):\n                    try:\n                        word = e.get(\"ambiguous_word\", e.get(\"token\", \"N/A\"))\n                        conf = float(e.get(\"confidence\", 0.5))\n                        u = float(e.get(\"uncertainty\", 0.0))\n                        s = float(e.get(\"span\", 0.0))\n\n                        quality_metrics[\"confidences\"].append(conf)\n                        quality_metrics[\"spans\"].append(s)\n                        quality_metrics[\"uncertainties\"].append(u)\n\n                        clean = (\n                            str(word)\n                            .replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .strip()\n                            .lower()\n                        )\n\n                        if clean in dscd_homographs:\n                            dscd_homographs_explained.add(clean)\n                            homograph_explanations[clean].append(\n                                {\n                                    \"sentence\": sent,\n                                    \"confidence\": conf,\n                                    \"span\": s,\n                                    \"uncertainty\": u,\n                                }\n                            )\n\n                        if clean in _HOMOGRAPH_REFERENCE_LIST_BN:\n                            reference_homographs_explained.add(clean)\n\n                        marker = \"[HIGH]\" if s > _SPAN_THRESHOLD else \"      \"\n                        _safe_print(\n                            f\"  {j}. {marker} '{word}' conf={conf:.3f} u={u:.3f} s={s:.3f}\"\n                        )\n\n                    except Exception:\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n            else:\n                _safe_print(\"No ambiguity\")\n\n            if translation and translation.strip():\n                successes += 1\n                _safe_print(\"Success\")\n            else:\n                _safe_print(\"Failed\")\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        except Exception as e:\n            _safe_print(f\"Test {idx} failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                _maybe_traceback(e)\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"TEST SUMMARY\")\n    _safe_print(\"=\" * 80)\n\n    _safe_print(\"\\n[TRANSLATION]\")\n    _safe_print(f\"  Total: {total}\")\n    if total > 0:\n        _safe_print(f\"  Success: {successes} ({successes/total*100:.1f}%)\")\n        _safe_print(f\"  Failed: {total - successes} ({(total - successes)/total*100:.1f}%)\")\n\n        if quality_metrics[\"similarities\"]:\n            avg_sim = sum(quality_metrics[\"similarities\"]) / len(\n                quality_metrics[\"similarities\"]\n            )\n            _safe_print(f\"  Avg similarity: {avg_sim:.1%}\")\n\n    if inference_times:\n        avg_time = sum(inference_times) / len(inference_times)\n        _safe_print(\"\\n[PERFORMANCE]\")\n        _safe_print(f\"  Avg time: {avg_time:.3f}s per sentence\")\n        _safe_print(f\"  Throughput: {1 / avg_time:.1f} sentences/sec\")\n\n    _safe_print(\"\\n[AMBIGUITY]\")\n    _safe_print(\n        f\"  Tests with explanations: {tests_with_explanations}/{total} \"\n        f\"({tests_with_explanations/total*100:.1f}%)\"\n    )\n    _safe_print(f\"  Total ambiguous: {total_ambiguous}\")\n    if total > 0:\n        _safe_print(f\"  Avg per sentence: {total_ambiguous/total:.2f}\")\n\n    if quality_metrics[\"confidences\"]:\n        avg_conf = sum(quality_metrics[\"confidences\"]) / len(\n            quality_metrics[\"confidences\"]\n        )\n        avg_span = (\n            sum(quality_metrics[\"spans\"]) / len(quality_metrics[\"spans\"])\n            if quality_metrics[\"spans\"]\n            else 0.0\n        )\n        avg_u = (\n            sum(quality_metrics[\"uncertainties\"])\n            / len(quality_metrics[\"uncertainties\"])\n            if quality_metrics[\"uncertainties\"]\n            else 0.0\n        )\n\n        high_conf = sum(1 for c in quality_metrics[\"confidences\"] if c >= 0.65)\n\n        _safe_print(\"\\n[QUALITY]\")\n        _safe_print(f\"  Avg confidence: {avg_conf:.3f}\")\n        _safe_print(f\"  Avg span: {avg_span:.3f}\")\n        _safe_print(f\"  Avg uncertainty: {avg_u:.3f}\")\n        _safe_print(\n            f\"  High confidence: {high_conf}/{len(quality_metrics['confidences'])} \"\n            f\"({high_conf/len(quality_metrics['confidences']):.1%})\"\n        )\n    else:\n        _safe_print(\"\\n[QUALITY]\")\n        _safe_print(\"  NO EXPLANATIONS\")\n        _safe_print(\"     Possible causes:\")\n        _safe_print(\"     1.  DSCD empty (warmup failed)\")\n        _safe_print(\"     2. TRG thresholds too strict\")\n\n    _safe_print(\"\\n[HOMOGRAPHS (DATA-DRIVEN)]\")\n    _safe_print(f\"  DSCD discovered: {len(dscd_homographs)}\")\n    _safe_print(f\"  Explained: {len(dscd_homographs_explained)}\")\n    if dscd_homographs:\n        try:\n            _safe_print(\n                f\"  Rate: \"\n                f\"{len(dscd_homographs_explained)/len(dscd_homographs):.1%}\"\n            )\n        except ZeroDivisionError:\n            _safe_print(\"  Rate: 0.0%\")\n\n    if dscd_homographs_explained:\n        _safe_print(\"\\n  Explained:\")\n        for homo in sorted(dscd_homographs_explained):\n            exps = homograph_explanations[homo]\n            avg_conf_local = (\n                sum(e[\"confidence\"] for e in exps) / len(exps) if exps else 0.0\n            )\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST_BN else \"   \"\n            _safe_print(f\"    {in_ref} '{homo}': {len(exps)}x conf={avg_conf_local:.3f}\")\n\n    _safe_print(\"\\n[REFERENCE COMPARISON]\")\n    _safe_print(f\"  Size: {len(_HOMOGRAPH_REFERENCE_LIST_BN)}\")\n    _safe_print(f\"  Explained: {len(reference_homographs_explained)}\")\n    try:\n        coverage = (\n            len(reference_homographs_explained) / len(_HOMOGRAPH_REFERENCE_LIST_BN)\n            if len(_HOMOGRAPH_REFERENCE_LIST_BN) > 0\n            else 0.0\n        )\n        _safe_print(f\"  Coverage: {coverage:.1%}\")\n    except Exception:\n        _safe_print(\"  Coverage: N/A\")\n\n    _safe_print(\"\\n[HEALTH]\")\n    warnings = []\n\n    if successes < total * 0.7:\n        warnings.append(\"Low success (<70%)\")\n    if tests_with_explanations == 0:\n        warnings.append(\"NO explanations\")\n    if quality_metrics[\"confidences\"] and avg_conf < 0.5:\n        warnings.append(\"Low confidence (<0.5)\")\n    if dscd_homographs and len(dscd_homographs_explained) < len(dscd_homographs) * 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n\n    if warnings:\n        for w in warnings:\n            _safe_print(f\"  - {w}\")\n    else:\n        _safe_print(\"  All systems OK\")\n\n    try:\n        results = {\n            \"total_tests\": total,\n            \"successes\": successes,\n            \"tests_with_explanations\": tests_with_explanations,\n            \"quality_metrics\": {\n                \"avg_confidence\": avg_conf if quality_metrics[\"confidences\"] else 0,\n                \"avg_span\": avg_span if quality_metrics[\"spans\"] else 0,\n                \"avg_uncertainty\": avg_u\n                if quality_metrics[\"uncertainties\"]\n                else 0,\n            },\n            \"dscd_discovered\": len(dscd_homographs),\n            \"dscd_explained\": len(dscd_homographs_explained),\n            \"reference_explained\": len(reference_homographs_explained),\n            \"avg_inference_time\": (sum(inference_times) / len(inference_times))\n            if inference_times\n            else 0,\n        }\n\n        results_path = \"/kaggle/working/test_results.json\"\n        with open(results_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(results, f, indent=2, ensure_ascii=False)\n        _safe_print(f\"\\nResults saved: {results_path}\")\n    except Exception:\n        pass\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(f\"Thresholds: span>{_SPAN_THRESHOLD}, uncertainty>{_TAU_LOW}\")\n    _safe_print(\"Testing complete (DATA-DRIVEN)\")\n    _safe_print(\"=\" * 80)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 13: Extended testing ready (FINAL)\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\" F1:  Correct checkpoint path (removed space)\")\nprint(\" F2:  DEBUG flag integration\")\nprint(\" F3:  Correct DSCD dict key (dscd_state)\")\nprint(\" F4:  Performance metrics (time/throughput)\")\nprint(\" F5:  Memory cleanup between tests\")\nprint(\" F6:  Warmup validation (multi-sense ratio)\")\nprint(\" F7:  Component health (DSCD/ASBN/TRG)\")\nprint(\" F8:  Removed stray spaces in attribute access (e.g., . replace -> .replace)\")\nprint(\" F9:  Safe fallbacks before calling .size()/.prototype_stores\")\nprint(\" F10: Results export (JSON)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"zWd0uRn7H4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}