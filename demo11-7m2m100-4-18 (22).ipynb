{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14014111,"sourceType":"datasetVersion","datasetId":8927824}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers sentence-transformers\n!pip install transformers==4.30.2 --no-deps\n!pip install \"tokenizers<0.14\" sacremoses\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"id":"W8IIWAEHH4Jy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: TATN CONFIGURATION (BENGALI → ENGLISH)\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n    print(\"[WARN] pandas not available; CSV loading will fail\")\n\ntry:\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\n    _HAS_M2M_TOKENIZER = True\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer\n        _HAS_M2M_TOKENIZER = True\n    except Exception:\n        M2M100Tokenizer = None\n        _HAS_M2M_TOKENIZER = False\n        print(\"[WARN] M2M100Tokenizer not available\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nNUM_GPUS = torch.cuda.device_count()\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    mode = \"Single GPU Mode\" if torch.cuda.is_available() else \"CPU Mode\"\n    print(f\"[Cell 0] {mode}\")\n\nprint(f\"[Cell 0] Device: {DEVICE} (visible GPUs: {NUM_GPUS})\")\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    \"/kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\"\n)\n\nif not os.path.exists(DATASET_CSV_PATH):\n    print(f\"[WARN] Dataset CSV not found at: {DATASET_CSV_PATH}\")\n    print(\"[WARN] Training will use fallback dataset if file is not accessible\")\nelif not os.path.isfile(DATASET_CSV_PATH):\n    print(f\"[ERROR] Dataset path exists but is not a file: {DATASET_CSV_PATH}\")\nelif not os.access(DATASET_CSV_PATH, os.R_OK):\n    print(f\"[ERROR] Dataset CSV is not readable: {DATASET_CSV_PATH}\")\nelse:\n    print(f\"[INFO] Dataset CSV found: {DATASET_CSV_PATH}\")\n    if _HAS_PANDAS:\n        try:\n            _test_df = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n            if \"src\" not in _test_df.columns or \"tgt\" not in _test_df.columns:\n                print(\"[ERROR] CSV missing required columns 'src' and/or 'tgt'\")\n                print(f\"[ERROR] Found columns: {list(_test_df.columns)}\")\n            else:\n                print(f\"[INFO] CSV validation passed (columns: {list(_test_df.columns)})\")\n            del _test_df\n        except Exception as e:\n            print(f\"[WARN] Could not validate CSV structure: {e}\")\n\nBATCH_SIZE = 100\nNUM_SAMPLES = 30000\nMAX_LENGTH = 52\n\nLR_NMT = 2e-5\nLR_TRG = 1e-5\nLR_PHI = 1e-5\n\nEPOCHS = 1\nGRAD_CLIP_NORM = 1.0\nUSE_AMP = True\nPRINT_INTERVAL = 100\nSEED = 42\n\nACCUMULATION_STEPS = 16\n\nif BATCH_SIZE <= 0:\n    print(f\"[ERROR] BATCH_SIZE must be positive, got {BATCH_SIZE}. Setting to 1.\")\n    BATCH_SIZE = 1\n\nif NUM_SAMPLES <= 0:\n    print(f\"[ERROR] NUM_SAMPLES must be positive, got {NUM_SAMPLES}. Setting to 1000.\")\n    NUM_SAMPLES = 1000\n\nif MAX_LENGTH <= 0:\n    print(f\"[ERROR] MAX_LENGTH must be positive, got {MAX_LENGTH}. Setting to 48.\")\n    MAX_LENGTH = 48\n\nif ACCUMULATION_STEPS <= 0:\n    print(f\"[ERROR] ACCUMULATION_STEPS must be positive, got {ACCUMULATION_STEPS}. Setting to 1.\")\n    ACCUMULATION_STEPS = 1\n\nif EPOCHS <= 0:\n    print(f\"[ERROR] EPOCHS must be positive, got {EPOCHS}. Setting to 1.\")\n    EPOCHS = 1\n\nMC_DROPOUT_PASSES = 5\nTRG_EVIDENCE_K = 3\nMAX_SILVER_BUFFER = 100\n\nNUM_WORKERS = 2\nPIN_MEMORY = True\nPREFETCH_FACTOR = 2\nGRADIENT_CHECKPOINTING = True\n\nDEBUG_DISCOVERY = False\nDEBUG_TIMING = True\nDEBUG_VERBOSE = False\n\nDSCD_BUFFER_SIZE = 80\nDSCD_MAX_PROTOS = 8\nDSCD_N_MIN = 2\nDSCD_DISPERSION_THRESHOLD = 0.70\nDSCD_EMBED_DIM = 1024\nDSCD_TEMPERATURE = 0.7\nDSCD_DROPOUT = 0.1\nDSCD_AUGMENT_SCALE = 0.1\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_ENABLE_ONLINE_CLUSTERING = True\nDSCD_ONLINE_CLUSTERING_FREQUENCY = 10\nDSCD_WARMUP_SAMPLES = 8000\nDSCD_NEWSENSE_LAMBDA = 1.5\nDSCD_USE_COSINE_DISTANCE = True\n\nPERIODIC_DISCOVERY_FREQUENCY = 1\n_MAX_TOKENS_PER_DISCOVERY = 100\nDSCD_MIN_LETTERS = 2\nDSCD_MIN_LETTER_FRACTION = 0.5\nDSCD_MAX_CLUSTERING_POINTS = 500\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = False\n\nENABLE_TRG_TRAINING = True\nENABLE_TRG_INFERENCE = True\n\nAPPLY_DSCD_AUGMENTATION = False\n\nCLUSTERING_TIMEOUT = 60\nMEMORY_CLEANUP_FREQUENCY = 200\nVALIDATION_CHECK_INTERVAL = 200\nVERBOSE_LOGGING = False\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_SAVE_AFTER_TRAINING = True\nCHECKPOINT_FILENAME = \"tatn_final.pt\"\nCHECKPOINT_INTERVAL = 99999999\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = 25000\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\nSAVE_DSCD_STATE = True\n\nif not os.path.exists(CHECKPOINT_DIR):\n    try:\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n        print(f\"[INFO] Created checkpoint directory: {CHECKPOINT_DIR}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to create checkpoint directory: {e}\")\n        CHECKPOINT_DIR = \"./\"\n        print(f\"[INFO] Falling back to current directory: {CHECKPOINT_DIR}\")\n\nTAU_LOW = 0.15\nTAU_HIGH = 0.85\nTAU_ACCEPT = 0.8\n\nTRG_MAX_GEN_LEN = 16\nTRG_GEN_EMBED = 64\nTRG_GEN_HID = 64\n\nTRG_SPAN_THRESHOLD = 0.15\nTRG_UNCERTAINTY_THRESHOLD = 0.70\nTRG_TEMPERATURE = 1.0\n\nASBN_HIDDEN_DIM = 64\nASBN_LAMBDA = 0.1\nASBN_DROPOUT = 0.1\n\nLAMBDA_ASBN = 0.05\nLAMBDA_DSCD = 0.15\n\nTRAIN_DOMAIN = 0\nTEST_DOMAIN = 1\nUSE_DOMAIN_LABELS = True\n\nGRL_ALPHA_START = 0.0\nGRL_ALPHA_END = 1.0\nGRL_ALPHA_SCHEDULE = \"linear\"\n\n_total_steps_estimate = NUM_SAMPLES // (BATCH_SIZE * ACCUMULATION_STEPS)\nif _total_steps_estimate <= 0:\n    _total_steps_estimate = 1\n    print(f\"[WARN] Computed GRL steps <= 0, setting to minimum of 1\")\nGRL_ALPHA_STEPS = max(1, _total_steps_estimate * EPOCHS)\n\nSOURCE_LANGUAGE = \"bn\"\nTARGET_LANGUAGE = \"en\"\n\nM2M100_BN_TOKEN_ID = 128025\nM2M100_EN_TOKEN_ID = 128022\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = {\n    \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n    \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    \"দিন\", \"রাত\", \"জল\", \"বাড়ি\", \"পার্ক\", \"নদী\", \"বন\", \"ফুল\", \"গাছ\",\n    \"চোখ\", \"মুখ\", \"পা\", \"কান\", \"গলা\", \"নাক\", \"দাঁত\", \"কোমর\",\n    \"পড়া\", \"দেখা\", \"যাওয়া\", \"আসা\", \"খেলা\", \"লেখা\", \"বলা\", \"শোনা\",\n    \"চলা\", \"ধরা\", \"দেওয়া\", \"নেওয়া\",\n    \"সময়\", \"বছর\", \"মাস\", \"সাল\", \"ঘন্টা\", \"মুহূর্ত\",\n    \"গরম\", \"শীত\", \"বাতাস\", \"আগুন\", \"পাথর\", \"মাটি\",\n    \"ভাব\", \"রং\", \"আলো\", \"ছায়া\", \"শব্দ\", \"অর্থ\",\n}\n\nHOMOGRAPH_WATCHLIST_BN: Set[str] = HOMOGRAPH_REFERENCE_LIST_BN.copy()\nHOMOGRAPH_WATCHLIST: Set[str] = HOMOGRAPH_WATCHLIST_BN.copy()\nUSE_WATCHLIST_PRIORITIZATION = False\nWATCHLIST_ONLY_FOR_TRG = False\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t)\n    t = t.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t).lower().strip()\n    return t\n\ndef normalize_token_key(token: str) -> str:\n    if not token:\n        return \"\"\n    token = str(token)\n    token = token.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n    for punct in \".,!?;:\\\"'()-\":\n        token = token.replace(punct, \"\")\n    return token.strip()\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(\n                    f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\"\n                )\n            except Exception:\n                print(f\"  GPU {i}: memory stats unavailable\")\n    else:\n        print(\"[GPU MONITOR] No CUDA devices available\")\n\ndef get_checkpoint_path() -> str:\n    if not os.path.exists(CHECKPOINT_DIR):\n        try:\n            os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n        except Exception:\n            pass\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    if (\n        CHECKPOINT_INTERVAL < 99999999\n        and global_step >= CHECKPOINT_INTERVAL\n        and global_step % CHECKPOINT_INTERVAL == 0\n    ):\n        return True\n    return False\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({SOURCE_LANGUAGE, TARGET_LANGUAGE})\n    return s\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    return get_tokenizer_special_tokens(tokenizer)\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        min_len = 2\n        if len(clean) < min_len:\n            result = False\n        else:\n            has_bengali_chars = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if '\\u0980' <= c <= '\\u09FF')\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    bengali_ratio = bengali_count / alphanum_count\n                    result = bengali_ratio >= 0.5\n\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n\n    return result\n\ndef fallback_is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    language: str = \"bn\",\n) -> bool:\n    return is_valid_token(token, special_tokens, None, language)\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    try:\n        encoded = tokenizer(\n            text,\n            return_offsets_mapping=True,\n            max_length=max_length,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        toks = tokenizer.convert_ids_to_tokens(encoded.get(\"input_ids\", []))\n        offsets = encoded.get(\"offset_mapping\", [(0, 0)] * len(toks))\n        return toks, offsets\n    except Exception:\n        return None, None\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        count = len(self.discovery_times)\n        return {\n            \"count\": count,\n            \"total\": total,\n            \"avg\": total / count,\n            \"max\": max(self.discovery_times),\n        }\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nif USE_MULTI_GPU:\n    effective_batch *= NUM_GPUS\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TATN CONFIGURATION (Bengali to English)\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs)\")\nprint(f\"Dataset: {DATASET_CSV_PATH}\")\nprint(f\"Samples: {NUM_SAMPLES:,} | Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS} | AMP: {USE_AMP}\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer: {DSCD_BUFFER_SIZE} | n_min: {DSCD_N_MIN} | Max protos: {DSCD_MAX_PROTOS}\")\nprint(f\"  Dispersion threshold: {DSCD_DISPERSION_THRESHOLD} (normalized space)\")\nprint(f\"  Use cosine distance: {DSCD_USE_COSINE_DISTANCE}\")\nprint(f\"  Online clustering: {DSCD_ENABLE_ONLINE_CLUSTERING} (every {DSCD_ONLINE_CLUSTERING_FREQUENCY} additions)\")\nprint(f\"  New-sense lambda: {DSCD_NEWSENSE_LAMBDA}\")\nprint(f\"  Periodic discovery: Every {PERIODIC_DISCOVERY_FREQUENCY} optimizer updates\")\nprint(f\"  Max tokens per discovery: {_MAX_TOKENS_PER_DISCOVERY}\")\nprint(f\"  Clustering timeout: {CLUSTERING_TIMEOUT}s\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  MC Dropout passes: {MC_DROPOUT_PASSES} | TAU_LOW: {TAU_LOW}\")\nprint(f\"  TRG_SPAN_THRESHOLD: {TRG_SPAN_THRESHOLD} | TRG_UNCERTAINTY_THRESHOLD: {TRG_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  TAU_HIGH: {TAU_HIGH} | Temperature: {TRG_TEMPERATURE}\")\nprint()\nprint(\"ASBN / Loss:\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN} | LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint(f\"  Domain labels: {USE_DOMAIN_LABELS} | GRL: {GRL_ALPHA_SCHEDULE}\")\nprint(f\"  GRL steps: {GRL_ALPHA_STEPS}\")\nprint(f\"  ASBN inference: {ENABLE_ASBN_INFERENCE}\")\nprint()\nprint(\"Augmentation:\")\nprint(f\"  Apply DSCD augmentation: {APPLY_DSCD_AUGMENTATION}\")\nprint()\nprint(\"Debug Flags:\")\nprint(f\"  Discovery logging: {DEBUG_DISCOVERY}\")\nprint(f\"  Timing monitoring: {DEBUG_TIMING}\")\nprint(f\"  Verbose mode: {DEBUG_VERBOSE}\")\nprint()\nprint(\"Validation:\")\nprint(f\"  Check interval: {VALIDATION_CHECK_INTERVAL} steps\")\nprint()\nprint(\"Language Tokens:\")\nprint(f\"  Bengali (bn): {M2M100_BN_TOKEN_ID}\")\nprint(f\"  English (en): {M2M100_EN_TOKEN_ID}\")\nprint()\nprint(\"Checkpoint:\")\nprint(f\"  Path: {get_checkpoint_path()}\")\nprint(f\"  Save strategy: Final only\")\nprint(f\"  Save DSCD state: {SAVE_DSCD_STATE}\")\nprint()\nprint(\"Discovery Mode:\")\nprint(f\"  Watchlist initialized with {len(HOMOGRAPH_WATCHLIST_BN)} reference words\")\nprint(f\"  Reference list: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words (for prioritization)\")\nprint(\"  Watchlist prioritization: DISABLED (pure unsupervised)\")\nprint(\"=\" * 80)\n\nif not _HAS_PANDAS:\n    print(\"[ERROR] pandas not available - CSV loading will fail!\")\nif not _HAS_M2M_TOKENIZER:\n    print(\"[ERROR] M2M100Tokenizer not available - tokenization will fail!\")\n\ntry:\n    test_file = os.path.join(CHECKPOINT_DIR, \".test_write\")\n    with open(test_file, \"w\") as f:\n        f.write(\"test\")\n    os.remove(test_file)\n    print(f\"[INFO] Checkpoint directory writable: {CHECKPOINT_DIR}\")\nexcept Exception as e:\n    print(f\"[ERROR] Checkpoint directory not writable: {e}\")\n\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: Configuration loaded successfully\")\nprint(\"=\" * 80)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: TOKENIZER UTILITIES (BENGALI-FOCUSED) - FIXED VERSION\n# ===========================================================================================\n\nimport threading\nimport unicodedata\nfrom typing import Tuple, List, Dict, Optional, Set, Union, Any\nimport numpy as np\nimport torch\n\n# Configuration Loading with Fallbacks\ntry:\n    if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0:\n        SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\n    else:\n        SAFE_OFFSET_MAX_LEN = 48\nexcept (NameError, ValueError, TypeError):\n    SAFE_OFFSET_MAX_LEN = 48\n\nif SAFE_OFFSET_MAX_LEN <= 0:\n    print(f\"[WARN] SAFE_OFFSET_MAX_LEN invalid ({SAFE_OFFSET_MAX_LEN}), setting to 48\")\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _TARGET_LANG = \"en\"\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept (NameError, TypeError):\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n\n# M2M100/SentencePiece special character\n_SPIECE_UNDERLINE = \"\\u2581\" \n\ndef _special_token_cache_key(tokenizer) -> str:\n    if tokenizer is None:\n        return \"none_tokenizer__vocab=None\"\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None)\n    if not name:\n        name = \"unknown_tokenizer\"\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    if tokenizer is None:\n        return {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\", \"__bn__\", \"__en__\"}\n    \n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n    special_tokens: Set[str] = set()\n    try:\n        if hasattr(tokenizer, \"all_special_tokens\"):\n            try:\n                result = getattr(tokenizer, \"all_special_tokens\")\n                if isinstance(result, (list, tuple, set)):\n                    special_tokens.update(str(x) for x in result if x)\n            except Exception:\n                pass\n        if hasattr(tokenizer, \"additional_special_tokens\"):\n            try:\n                result = getattr(tokenizer, \"additional_special_tokens\")\n                if isinstance(result, (list, tuple, set)):\n                    special_tokens.update(str(x) for x in result if x)\n            except Exception:\n                pass\n        for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\",\n                     \"cls_token\", \"sep_token\", \"mask_token\"):\n            if hasattr(tokenizer, attr):\n                try:\n                    tok = getattr(tokenizer, attr)\n                    if tok:\n                        special_tokens.add(str(tok))\n                except Exception:\n                    pass\n        try:\n            stm = (\n                getattr(tokenizer, \"special_tokens_map\", None)\n                or getattr(tokenizer, \"special_tokens_map_extended\", None)\n            )\n            if isinstance(stm, dict):\n                for v in stm.values():\n                    if isinstance(v, str) and v:\n                        special_tokens.add(v)\n                    elif isinstance(v, list):\n                        special_tokens.update(str(x) for x in v if x)\n        except Exception:\n            pass\n    except Exception:\n        special_tokens = set()\n\n    # Ensure standard M2M100/BERT specials are included\n    special_tokens.update({\n        \"__bn__\", \"__en__\",\n        \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n        \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n    })\n\n    try:\n        vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n        preserved = {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\", \"__bn__\", \"__en__\"}\n        try:\n            preserved.add(_SOURCE_LANG)\n            preserved.add(_TARGET_LANG)\n        except Exception:\n            preserved.update({\"bn\", \"en\"})\n        \n        if isinstance(vocab, dict):\n            special_tokens = {\n                tok\n                for tok in special_tokens\n                if tok in vocab or tok in preserved\n            }\n        else:\n            special_tokens.update(preserved)\n    except Exception:\n        pass\n\n    with _SPECIAL_TOKENS_LOCK:\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n    \n    return special_tokens\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    \"\"\"Normalizes the offset_mapping structure from various HF tokenizer outputs.\"\"\"\n    if not isinstance(enc, dict):\n        return enc\n    \n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                # Handle Tensor or Numpy array\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0:\n                        # Case: Batch size > 0, check nesting\n                        if isinstance(arr[0], list):\n                             # Flatten or take first if logic dictates processing single sample\n                             # Assuming this function is called for single-sample contexts primarily\n                             enc[\"offset_mapping\"] = [\n                                (x[0], x[1]) if (isinstance(x, (list, tuple)) and len(x) >= 2) else (None, None)\n                                for x in arr[0]\n                            ]\n                        else:\n                            # Direct list of offsets (rare for batch output but possible)\n                            pass\n                        return enc\n                \n                # Handle raw list\n                if isinstance(off, (list, tuple)) and len(off) > 0:\n                    if isinstance(off[0], (list, tuple)):\n                        # It's a list of lists (batch)\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1]) if (isinstance(x, (list, tuple)) and len(x) >= 2) else (None, None)\n                            for x in off[0]\n                        ]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # Try accessing .data attribute (common in BatchEncoding)\n    try:\n        data = getattr(enc, \"data\", None)\n        if (\n            data\n            and isinstance(data, dict)\n            and \"offset_mapping\" in data\n            and data[\"offset_mapping\"] is not None\n        ):\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [\n                    (x[0], x[1])\n                    if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                    else (None, None)\n                    for x in om[0]\n                ]\n                return enc\n    except Exception:\n        pass\n\n    # Fallback: Create dummy offsets if missing\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif (\n                isinstance(input_ids, (list, tuple))\n                and len(input_ids) > 0\n                and isinstance(input_ids[0], (list, tuple))\n            ):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\ndef safe_offsets_tokenize(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n    include_special_tokens: bool = False,\n) -> dict:\n    if tokenizer is None:\n        return {\n            \"input_ids\": torch.tensor([[0]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n            \"offset_mapping\": []\n        }\n    \n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = max(1, int(max_length))\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        if _DEBUG_VERBOSE:\n            print(\"[WARN] Failed to convert input to string, using empty string\")\n        text = \"\"\n\n    # Truncate text to avoid excessively long tokenization calls\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit] if len(text) > char_limit else text\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    # Attempt 1: Fast Tokenizer\n    if is_fast:\n        try:\n            enc = tokenizer(\n                sample_text,\n                return_offsets_mapping=True,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=False,\n                max_length=eff_max,\n                add_special_tokens=include_special_tokens,\n            )\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            pass\n\n    # Attempt 2: Slow Tokenizer / Fallback\n    try:\n        enc = tokenizer(\n            sample_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=eff_max,\n            add_special_tokens=include_special_tokens,\n        )\n    except Exception as e:\n        if _DEBUG_VERBOSE:\n            print(f\"[WARN] Tokenization failed: {e}, returning empty encoding\")\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\n            \"input_ids\": torch.tensor([[pad_id]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n        }\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    # Manual Offset Calculation (Fallback for Slow Tokenizers)\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                try:\n                    input_ids = enc.data[\"input_ids\"][0]\n                except Exception:\n                    input_ids = None\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n        \n        # FIX: Robust character stripping including M2M100 underscore\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n            \n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n                \n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            \n            if idx == -1:\n                offsets_list.append((None, None))\n                # Do not advance cur_pos if not found to allow re-sync\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\ndef reconstruct_word_spans(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n) -> Tuple[Dict[int, Optional[str]], List[str]]:\n    global _LANGUAGE_WARNING_COUNT\n\n    if tokenizer is None:\n        return {}, []\n    \n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = max(1, int(max_length))\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any('\\u0980' <= c <= '\\u09FF' for c in text)\n    has_english = any('a' <= c.lower() <= 'z' for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (\n            sum(1 for c in text if '\\u0980' <= c <= '\\u09FF')\n            / max(1, len(text))\n            * 100.0\n        )\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(\n            f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), \"\n            f\"English: {has_english}\"\n        )\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        encoded = safe_offsets_tokenize(\n            tokenizer, text, max_length=eff_max, include_special_tokens=False\n        )\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if not tokens:\n        return {}, []\n\n    # Normalize offsets structure\n    if isinstance(offsets, list) and len(offsets) > 0:\n        if all(isinstance(x, tuple) for x in offsets):\n            offsets_list = offsets\n        elif isinstance(offsets[0], (list, tuple)):\n            offsets_list = [\n                (x[0], x[1])\n                if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                else (None, None)\n                for x in offsets[0]\n            ]\n        else:\n            offsets_list = [(None, None)] * len(tokens)\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, Optional[str]] = {}\n    words: List[str] = []\n\n    used_any_offset = any(\n        isinstance(o, tuple) and o[0] is not None and o[1] is not None\n        for o in offsets_list\n    )\n    \n    # Path A: Use offsets if available (More accurate)\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n        word_token_indices: List[int] = []\n\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            # Validate offsets\n            if off_start is not None and off_end is not None:\n                if off_start < 0 or off_end < 0:\n                    off_start, off_end = None, None\n                else:\n                    off_start = max(0, min(off_start, text_len))\n                    off_end = max(off_start, min(off_end, text_len))\n\n            # Case: Special token or missing offset -> Close current word\n            if off_start is None or off_end is None or tok in special_tokens:\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                            for tidx in word_token_indices:\n                                token_word_map[tidx] = wtext\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                word_token_indices = []\n                token_word_map[idx] = None\n                continue\n\n            # Case: Start of new word\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n                word_token_indices = [idx]\n            else:\n                # Case: Gap detected (likely space) -> New word\n                if word_end is not None and off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                            for tidx in word_token_indices:\n                                token_word_map[tidx] = wtext\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                    word_token_indices = [idx]\n                else:\n                    # Case: Continuation of current word\n                    if word_end is not None:\n                        word_end = max(word_end, off_end)\n                    else:\n                        word_end = off_end\n                    word_token_indices.append(idx)\n\n        # Flush last word\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n                    for tidx in word_token_indices:\n                        token_word_map[tidx] = wtext\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    # Path B: Fallback (Heuristic reconstruction based on SentencePiece underscore)\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    current_indices: List[int] = []\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n                    for tidx in current_indices:\n                        token_word_map[tidx] = word\n                current_parts = []\n                current_indices = []\n            token_word_map[i] = None\n            continue\n        \n        # FIX: Include \\u2581 in clean list\n        clean = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            token_word_map[i] = None\n            continue\n\n        # Check for start of word markers: ▁ (old), Ġ (RoBERTa), \\u2581 (M2M100/SP)\n        is_start = tok.startswith(\"▁\") or tok.startswith(\"Ġ\") or tok.startswith(_SPIECE_UNDERLINE)\n\n        if is_start:\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n                    for tidx in current_indices:\n                        token_word_map[tidx] = word\n            current_parts = [clean]\n            current_indices = [i]\n        else:\n            current_parts.append(clean)\n            current_indices.append(i)\n            # Safety check for runaway words\n            if len(\"\".join(current_parts)) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                    for tidx in current_indices[:-1]:\n                        token_word_map[tidx] = word\n                current_parts = [clean]\n                current_indices = [i]\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n            for tidx in current_indices:\n                token_word_map[tidx] = word\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    # Path C: Simple split fallback (Least accurate but safe)\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n\n        if tokens and word_list:\n            word_idx = 0\n            current_word = word_list[0] if word_list else None\n\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    token_word_map[i] = None\n                    continue\n\n                if tok.startswith(\"▁\") or tok.startswith(\"Ġ\") or tok.startswith(_SPIECE_UNDERLINE):\n                    if word_idx < len(word_list) - 1:\n                        word_idx += 1\n                    current_word = word_list[word_idx] if word_idx < len(word_list) else None\n\n                token_word_map[i] = current_word\n\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\ndef is_word_token(\n    clean_token: str,\n    min_letters: int = 2,\n    min_letter_fraction: float = 0.5,\n) -> bool:\n    if not clean_token or not isinstance(clean_token, str):\n        return False\n    if len(clean_token) < min_letters:\n        return False\n    letter_count = sum(1 for c in clean_token if c.isalpha())\n    if letter_count == 0:\n        return False\n    alphanum_count = sum(1 for c in clean_token if c.isalnum())\n    if alphanum_count == 0:\n        return False\n    letter_ratio = letter_count / alphanum_count\n    return letter_ratio >= min_letter_fraction\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    sample_bn = \"কাল আমি বাজারে যাব।\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        enc_bn = safe_offsets_tokenize(\n            tokenizer, sample_bn, max_length=32, include_special_tokens=False\n        )\n        enc_len = (\n            int(enc_bn[\"input_ids\"].shape[-1])\n            if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn\n            else \"N/A\"\n        )\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(\n            any('\\u0980' <= c <= '\\u09FF' for c in w) for w in words_bn\n        )\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(\n            any('a' <= c.lower() <= 'z' for c in w) for w in words_en\n        )\n        print(f\"  Contains English words: {has_english_words}\")\n\n        if has_bengali_words and not any(\n            'a' <= c.lower() <= 'z' for c in \"\".join(words_bn)\n        ):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\niswordtoken = is_word_token\n\nprint(\"Cell 1: Tokenizer utilities loaded\")\n","metadata":{"id":"WZE9PkHyH4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (BENGALI → ENGLISH TASK) - FIXED\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\n    if _NUM_SAMPLES <= 0:\n        print(f\"[CELL2] WARNING: NUM_SAMPLES={_NUM_SAMPLES} invalid, using 50000\")\n        _NUM_SAMPLES = 50000\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\n    if _MAX_LENGTH <= 0:\n        print(f\"[CELL2] WARNING: MAX_LENGTH={_MAX_LENGTH} invalid, using 48\")\n        _MAX_LENGTH = 48\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n    print(\"[CELL2] WARNING: SOURCE_LANGUAGE/TARGET_LANGUAGE not defined, using defaults bn/en\")\n\ntry:\n    _M2M_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\n    _M2M_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept NameError:\n    _M2M_BN_TOKEN_ID = 128025\n    _M2M_EN_TOKEN_ID = 128022\n    print(\"[CELL2] WARNING: M2M100 token IDs not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\n    if _NUM_WORKERS < 0:\n        _NUM_WORKERS = 0\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\n    if _PREFETCH_FACTOR <= 0:\n        _PREFETCH_FACTOR = 2\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\n    _USE_DOMAIN_LABELS = bool(USE_DOMAIN_LABELS)\nexcept NameError:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = False\n    print(\"[CELL2] WARNING: Domain label config not found, disabling domain labels\")\n\n_has_normalize = (\"normalize_bengali\" in globals()) and (\"normalize_english\" in globals())\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n_BENGALI_CHAR_RE = re.compile(r'[\\u0980-\\u09FF]')\n\ndef is_bengali_text(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    try:\n        if dataset is not None and hasattr(dataset, \"_tokenizer_name_or_path\") and dataset._tokenizer_name_or_path:\n            try:\n                from transformers import M2M100Tokenizer\n                dataset.tokenizer = M2M100Tokenizer.from_pretrained(dataset._tokenizer_name_or_path)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n                if DEBUG_CELL2:\n                    print(f\"[CELL2-WORKER-{worker_id}] Tokenizer reloaded successfully\")\n            except Exception as e:\n                cell2_dbg(\"worker_tokenizer_reload\", f\"Worker {worker_id} tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] Tokenizer rebind failed in worker {worker_id}\")\n\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\ndef load_and_preprocess_optimized(\n    num_samples: Optional[int] = None,\n    split: str = \"train\",\n) -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(f\"num_samples must be positive, got {num_samples}\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    try:\n        print(\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH, dtype=str)\n        if df.empty:\n            print(\"[CELL2] ERROR: CSV file is empty\")\n            return _get_fallback_dataset()\n\n        if \"src\" not in df.columns or \"tgt\" not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing required columns. Found columns: {list(df.columns)}\")\n            print(\"[CELL2] Expected format: src (Bengali), tgt (English) OR src (English), tgt (Bengali)\")\n            return _get_fallback_dataset()\n\n        df[\"src\"] = df[\"src\"].fillna(\"\").astype(str)\n        df[\"tgt\"] = df[\"tgt\"].fillna(\"\").astype(str)\n\n        if len(df) == 0:\n            print(\"[CELL2] ERROR: No rows in dataframe after loading\")\n            return _get_fallback_dataset()\n\n        sample_src = str(df[\"src\"].iloc[0])\n        sample_tgt = str(df[\"tgt\"].iloc[0])\n\n        src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n        tgt_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_tgt))\n        src_is_english = bool(re.search(r'[a-zA-Z]', sample_src)) and not src_is_bengali\n        tgt_is_english = bool(re.search(r'[a-zA-Z]', sample_tgt)) and not tgt_is_bengali\n\n        if src_is_english and tgt_is_bengali:\n            print(\"[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn→en task.\")\n            df_temp = df.copy()\n            df = pd.DataFrame({\n                'src': df_temp['tgt'].values,\n                'tgt': df_temp['src'].values\n            })\n            \n            if len(df) > 0:\n                sample_src = str(df[\"src\"].iloc[0])\n                sample_tgt = str(df[\"tgt\"].iloc[0])\n                src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n                tgt_is_english = bool(re.search(r'[a-zA-Z]', sample_tgt)) and not bool(_BENGALI_CHAR_RE.search(sample_tgt))\n                if not src_is_bengali or not tgt_is_english:\n                    print(\"[CELL2] ERROR: Swap failed, after swap src is not Bengali or tgt is not English.\")\n                    return _get_fallback_dataset()\n                else:\n                    print(\"[CELL2] Swap successful: src=Bengali, tgt=English\")\n        elif not src_is_bengali or not tgt_is_english:\n            print(\"[CELL2] WARNING: After column check, src not Bengali or tgt not English. Proceeding but output may be incorrect.\")\n\n        df = df.head(num_samples)\n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n\n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n\n        for row_tuple in tqdm(df.itertuples(index=False), total=len(df), desc=\"Loading dataset\"):\n            try:\n                src_val = row_tuple.src\n                tgt_val = row_tuple.tgt\n                if pd.isna(src_val) or pd.isna(tgt_val):\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", \"NaN value detected\")\n                    continue\n                bn = str(src_val).strip()\n                en = str(tgt_val).strip()\n                if not bn or not en:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", \"Empty src/tgt field\")\n                    continue\n                if not is_bengali_text(bn):\n                    skipped += 1\n                    cell2_dbg(\"not_bengali_src\", \"src field not Bengali\")\n                    continue\n                if not re.search(r'[a-zA-Z]', en):\n                    skipped += 1\n                    cell2_dbg(\"not_english_tgt\", \"tgt field not English\")\n                    continue\n                max_words = max(20, _MAX_LENGTH // 2)\n                if len(bn.split()) > max_words or len(en.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", \"Text too long\")\n                    continue\n                if _has_normalize:\n                    bn_norm = normalize_bengali(bn)\n                    en_norm = normalize_english(en)\n                else:\n                    bn_norm = bn.strip()\n                    en_norm = en.lower().strip()\n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", \"Empty after normalization\")\n                    continue\n                pairs.append((bn_norm, en_norm))\n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception: {type(e).__name__}\")\n                continue\n\n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            print(\"[CELL2] Check that src column contains Bengali and tgt column contains English.\")\n            return _get_fallback_dataset()\n\n        return pairs\n\n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    print(\"[CELL2] Using fallback dataset (50 unique samples)\")\n    fallback_pairs = [\n        (\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\"),\n        (\"সে আমাকে পরে কল করবে।\", \"he will call me later.\"),\n        (\"আমরা প্রতিদিন তাজা ফল খাই।\", \"we eat fresh fruits every day.\"),\n        (\"তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\", \"his hard work has brought good results.\"),\n        (\"গাছে নতুন পাতাগুলো গজিয়েছে।\", \"new leaves have sprouted on the tree.\"),\n        (\"আমি বইয়ের পাতা উল্টাচ্ছি।\", \"i am turning the pages of the book.\"),\n        (\"কাল আমি বাজারে গিয়েছিলাম।\", \"yesterday i went to the market.\"),\n        (\"কাল আমি তোমার সাথে দেখা করব।\", \"tomorrow i will meet you.\"),\n        (\"তারা আকাশে উজ্জ্বল।\", \"the stars are bright in the sky.\"),\n        (\"তারা বাড়িতে নেই।\", \"they are not at home.\"),\n        (\"ব্যাংক নদীর ধারে ভেঙে গেছে।\", \"the bank by the river has collapsed.\"),\n        (\"আমি ব্যাংকে টাকা জমা দিয়েছি।\", \"i deposited money in the bank.\"),\n        (\"বার বার চেষ্টা করতে হবে।\", \"you have to try again and again.\"),\n        (\"আমি বার খুলে ভিতরে ঢুকলাম।\", \"i opened the bar and entered.\"),\n        (\"তার মাথা ব্যথা করছে।\", \"his head is hurting.\"),\n        (\"আমি মাথা নেড়ে সম্মতি দিলাম।\", \"i nodded my head in agreement.\"),\n        (\"সে হার মেনে নিয়েছে।\", \"he accepted defeat.\"),\n        (\"আমি গলায় সোনার হার পরেছি।\", \"i am wearing a gold necklace.\"),\n        (\"পানি খুব ঠান্ডা।\", \"the water is very cold.\"),\n        (\"আমি পানি খাচ্ছি।\", \"i am drinking water.\"),\n        (\"দল খেলায় জিতেছে।\", \"the team won the game.\"),\n        (\"আমি মাটি দল দিয়ে ফেললাম।\", \"i trampled the soil.\"),\n        (\"বাজার থেকে সবজি কিনলাম।\", \"i bought vegetables from the market.\"),\n        (\"বাজার অনেক ভিড় ছিল।\", \"the market was very crowded.\"),\n        (\"তার নাম আহমেদ।\", \"his name is ahmed.\"),\n        (\"নাম না করে কাজ করো।\", \"work without making a name.\"),\n        (\"কথা বলা বন্ধ করো।\", \"stop talking.\"),\n        (\"তার কথা শুনে ভালো লাগল।\", \"i felt good hearing his words.\"),\n        (\"বই পড়তে ভালো লাগে।\", \"i like reading books.\"),\n        (\"আমি একটি নতুন বই কিনেছি।\", \"i bought a new book.\"),\n        (\"ঘর পরিষ্কার করা হয়েছে।\", \"the house has been cleaned.\"),\n        (\"আমি ঘরে বসে আছি।\", \"i am sitting at home.\"),\n        (\"মন ভালো নেই।\", \"my mind is not good.\"),\n        (\"আমার মন চায় বেড়াতে যেতে।\", \"my mind wants to go for a walk.\"),\n        (\"হাত ধুয়ে নাও।\", \"wash your hands.\"),\n        (\"আমি তার হাত ধরলাম।\", \"i held his hand.\"),\n        (\"দিন কেটে যাচ্ছে।\", \"the day is passing by.\"),\n        (\"আজ কি দিন?\", \"what day is today?\"),\n        (\"রাত হয়ে এসেছে।\", \"night has come.\"),\n        (\"আমি রাত জেগে পড়েছি।\", \"i studied staying up at night.\"),\n        (\"জল খুব গরম।\", \"the water is very hot.\"),\n        (\"আমি জল দিয়ে গাছ সিঞ্চন করেছি।\", \"i watered the plants.\"),\n        (\"বাড়ি যাচ্ছি।\", \"i am going home.\"),\n        (\"আমার বাড়ি ঢাকায়।\", \"my house is in dhaka.\"),\n        (\"পার্কে অনেক মানুষ।\", \"there are many people in the park.\"),\n        (\"আমি প্রতিদিন পার্কে হাঁটি।\", \"i walk in the park every day.\"),\n        (\"নদী বইছে।\", \"the river is flowing.\"),\n        (\"আমি নদীর ধারে দাঁড়িয়ে আছি।\", \"i am standing by the river.\"),\n        (\"বন খুব সুন্দর।\", \"the forest is very beautiful.\"),\n        (\"আমি বন দেখতে গিয়েছিলাম।\", \"i went to see the forest.\"),\n    ]\n    if _has_normalize:\n        return [\n            (normalize_bengali(bn), normalize_english(en))\n            for bn, en in fallback_pairs\n        ]\n    else:\n        return [(bn.strip(), en.lower().strip()) for bn, en in fallback_pairs]\n\nclass MemoryEfficientDataset(Dataset):\n    def __init__(\n        self,\n        pairs: List[Tuple[str, str]],\n        tokenizer: Any = None,\n        max_length: Optional[int] = None,\n        split: str = \"train\",\n    ):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = max(1, int(max_length))\n        self.tokenizer = tokenizer\n        self.split = split\n\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n\n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid\")\n\n        try:\n            if self.tokenizer is not None and \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            elif self.tokenizer is not None:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n        \n        if not self.special_tokens:\n            self.special_tokens = {\n                f\"__{_SOURCE_LANG}__\",\n                f\"__{_TARGET_LANG}__\",\n                \"</s>\",\n                \"<pad>\",\n                \"<s>\",\n                \"<unk>\",\n            }\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                if self.tokenizer is not None:\n                    self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                try:\n                    if isinstance(enc[\"input_ids\"], torch.Tensor):\n                        input_ids = enc[\"input_ids\"].squeeze(0)\n                    else:\n                        input_ids = torch.tensor(enc[\"input_ids\"][0] if isinstance(enc[\"input_ids\"], list) else enc[\"input_ids\"])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0])\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids))\n                if isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                elif isinstance(attention_mask, torch.Tensor) and attention_mask.dim() > 1:\n                    attention_mask = attention_mask.squeeze(0)\n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=False,\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception as e:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {e}\")\n\n            if not token_word_map and tokens:\n                try:\n                    current_word_parts: List[str] = []\n                    for idx, tok in enumerate(tokens):\n                        if not isinstance(tok, str) or tok in self.special_tokens:\n                            continue\n                        clean = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                        if not clean:\n                            continue\n                        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n                            current_word_parts = [clean]\n                            token_word_map[idx] = clean\n                        else:\n                            current_word_parts.append(clean)\n                            word = \"\".join(current_word_parts)\n                            token_word_map[idx] = word\n                            for prev_idx in range(max(0, idx - len(current_word_parts) + 1), idx):\n                                if prev_idx in token_word_map:\n                                    token_word_map[prev_idx] = word\n                except Exception as e:\n                    cell2_dbg(\"fallback_wm\", f\"Fallback word map failed: {e}\")\n\n            return input_ids, attention_mask, tokens, token_word_map\n\n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                truncation=True,\n                padding=False,\n                return_tensors=\"pt\",\n                add_special_tokens=False,\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            \n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            \n            if labels.size(0) < self.max_length:\n                pad_length = self.max_length - labels.size(0)\n                pad_tensor = torch.full((pad_length,), -100, dtype=torch.long)\n                labels = torch.cat([labels, pad_tensor], dim=0)\n            elif labels.size(0) > self.max_length:\n                labels = labels[:self.max_length]\n            \n            return labels\n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"আমি\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception:\n            pad_id = 1\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label,\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx}\")\n                return self._make_safe_sample(\"oob\")\n\n            src, tgt = self.pairs[idx]\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            if DEBUG_CELL2 and idx < 3:\n                has_bengali = is_bengali_text(src)\n                has_english = any('a' <= c.lower() <= 'z' for c in src)\n                print(f\"[CELL2-GETITEM-{idx}] src sample: {src[:50]}\")\n                print(f\"[CELL2-GETITEM-{idx}] Bengali: {has_bengali}, English: {has_english}\")\n                if not has_bengali:\n                    print(f\"[CELL2] WARNING: src_text is NOT Bengali at idx={idx}!\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    try:\n        t = tensor.view(-1).long()\n    except Exception:\n        t = tensor.flatten().long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    if not batch:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n    \n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n    inputs, masks, labs, twmaps, srcs, toks, domains = [], [], [], [], [], [], []\n\n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", _TRAIN_DOMAIN)\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = att.flatten().long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(int(domain))\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.full((len(inputs),), _TRAIN_DOMAIN, dtype=torch.long)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels,\n    }\n\ndef create_optimized_dataloader(\n    dataset: Dataset,\n    batch_size: Optional[int] = None,\n    shuffle: bool = True,\n    split: str = \"train\",\n) -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n\n    batch_size = max(1, int(batch_size))\n    original_batch_size = batch_size\n    adjusted = False\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        remainder = batch_size % _NUM_GPUS\n        if remainder != 0:\n            new_batch_size = batch_size - remainder\n            if new_batch_size == 0:\n                new_batch_size = _NUM_GPUS\n                if DEBUG_CELL2:\n                    print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Increasing to {new_batch_size}.\")\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n\n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size {original_batch_size} to {batch_size} (DP-divisible, GPUs={_NUM_GPUS})\")\n\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = max(2, _PREFETCH_FACTOR)\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        per_gpu = batch_size // _NUM_GPUS\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\nprint(\"Cell 2: Memory-efficient data loading ready\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE - FIXED WITH WORD-LEVEL AGGREGATION\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\n\nPRINT_INTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HAS_CLUSTERING = True\nexcept Exception:\n    HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available\")\n\ntry:\n    from sklearn.cluster import KMeans\n    HAS_KMEANS = True\nexcept Exception:\n    HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available\")\n\ntry:\n    _DSCD_MAX_PROTOS = int(DSCD_MAX_PROTOS)\n    _DSCD_BUFFER_SIZE = int(DSCD_BUFFER_SIZE)\n    _DSCD_N_MIN = int(DSCD_N_MIN)\n    _DSCD_DISPERSION_THRESHOLD = float(DSCD_DISPERSION_THRESHOLD)\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    _DSCD_ENABLE_TRAINING_CLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\n    _DSCD_USE_COSINE_DISTANCE = bool(DSCD_USE_COSINE_DISTANCE)\n    _DSCD_ENABLE_ONLINE_CLUSTERING = bool(DSCD_ENABLE_ONLINE_CLUSTERING)\n    _DSCD_ONLINE_CLUSTERING_FREQUENCY = int(DSCD_ONLINE_CLUSTERING_FREQUENCY)\n    _APPLY_DSCD_AUGMENTATION = bool(APPLY_DSCD_AUGMENTATION)\nexcept (NameError, ValueError, TypeError):\n    _DSCD_MAX_PROTOS = 8\n    _DSCD_BUFFER_SIZE = 50\n    _DSCD_N_MIN = 2\n    _DSCD_DISPERSION_THRESHOLD = 0.70\n    _VERBOSE_LOGGING = True\n    _DSCD_ENABLE_TRAINING_CLUSTERING = True\n    _DSCD_USE_COSINE_DISTANCE = True\n    _DSCD_ENABLE_ONLINE_CLUSTERING = True\n    _DSCD_ONLINE_CLUSTERING_FREQUENCY = 10\n    _APPLY_DSCD_AUGMENTATION = False\n    print(\"[CELL3] WARNING: Using default DSCD config\")\n\nif _DSCD_MAX_PROTOS <= 0:\n    _DSCD_MAX_PROTOS = 8\nif _DSCD_BUFFER_SIZE <= 0:\n    _DSCD_BUFFER_SIZE = 50\nif _DSCD_N_MIN <= 0:\n    _DSCD_N_MIN = 2\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _MAX_TOKENS_PER_DISCOVERY = int(globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150))\n    if _MAX_TOKENS_PER_DISCOVERY <= 0:\n        _MAX_TOKENS_PER_DISCOVERY = 150\nexcept Exception:\n    _MAX_TOKENS_PER_DISCOVERY = 150\n\ntry:\n    _DSCD_NEWSENSE_LAMBDA = float(DSCD_NEWSENSE_LAMBDA)\n    if _DSCD_NEWSENSE_LAMBDA <= 0:\n        _DSCD_NEWSENSE_LAMBDA = 1.5\nexcept (NameError, TypeError):\n    _DSCD_NEWSENSE_LAMBDA = 1.5\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(HOMOGRAPH_REFERENCE_LIST_BN)\n    print(f\"[CELL3] Loaded reference list for evaluation: {len(_HOMOGRAPH_REFERENCE_LIST_BN)} words\")\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"\n    }\n    print(\"[CELL3] Using default reference list\")\n\ntry:\n    _DSCD_MIN_LETTERS = int(DSCD_MIN_LETTERS)\n    _DSCD_MIN_LETTER_FRACTION = float(DSCD_MIN_LETTER_FRACTION)\n    _DSCD_MAX_CLUSTERING_POINTS = int(DSCD_MAX_CLUSTERING_POINTS)\nexcept (NameError, TypeError):\n    _DSCD_MIN_LETTERS = 2\n    _DSCD_MIN_LETTER_FRACTION = 0.5\n    _DSCD_MAX_CLUSTERING_POINTS = 500\n\nif _DSCD_MIN_LETTERS <= 0:\n    _DSCD_MIN_LETTERS = 2\nif _DSCD_MIN_LETTER_FRACTION <= 0 or _DSCD_MIN_LETTER_FRACTION > 1:\n    _DSCD_MIN_LETTER_FRACTION = 0.5\nif _DSCD_MAX_CLUSTERING_POINTS <= 0:\n    _DSCD_MAX_CLUSTERING_POINTS = 500\n\n_TRG_PUNCT_SET = set(\".,;:!?\\\\\\\"'-()[]{}/\")\n_PUNCT_SET = _TRG_PUNCT_SET\n\n\ndef normalize_token_key(token: str) -> Optional[str]:\n    if token is None or not isinstance(token, str):\n        return None\n    \n    token = unicodedata.normalize(\"NFKC\", str(token))\n    token = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\")\n    token = token.strip().lower()\n    \n    if not token or len(token) < 2:\n        return None\n    \n    letter_count = 0\n    total_chars = 0\n    \n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith('L'):\n            letter_count += 1\n        if not ch.isspace():\n            total_chars += 1\n    \n    if total_chars == 0 or letter_count == 0:\n        return None\n    \n    if letter_count < 2:\n        return None\n    \n    if (letter_count / total_chars) < 0.5:\n        return None\n    \n    if all(c in _PUNCT_SET for c in token):\n        return None\n    \n    return token\n\n\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if token == \"\":\n        return False\n    \n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith(\"L\"):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n    \n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if (letters / total) < min_letter_fraction:\n        return False\n    \n    return True\n\n\ndef reconstruct_word_embeddings(\n    token_embeddings: torch.Tensor,\n    input_ids: Optional[torch.Tensor],\n    tokenizer,\n    device: torch.device\n) -> Tuple[torch.Tensor, List[Dict[int, str]]]:\n    batch_size = token_embeddings.size(0)\n    embed_dim = token_embeddings.size(-1)\n    word_embeddings_list = []\n    word_maps_batch = []\n    \n    for b in range(batch_size):\n        try:\n            if input_ids is not None:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n            else:\n                tokens = [f\"tok_{i}\" for i in range(token_embeddings.size(1))]\n        except Exception:\n            tokens = [f\"tok_{i}\" for i in range(token_embeddings.size(1))]\n        \n        words = []\n        word_spans = []\n        current_word = \"\"\n        word_start = 0\n        \n        for j, token in enumerate(tokens):\n            if token in [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"[CLS]\", \"[SEP]\", \"<BOS>\", \"<EOS>\"]:\n                continue\n            \n            if token.startswith(\"▁\") or token.startswith(\"_\"):\n                if current_word and word_start < j:\n                    words.append(current_word)\n                    word_spans.append((word_start, j))\n                \n                current_word = token[1:] if len(token) > 1 else \"\"\n                word_start = j\n            else:\n                current_word += token\n        \n        if current_word and word_start < len(tokens):\n            words.append(current_word)\n            word_spans.append((word_start, len(tokens)))\n        \n        word_embs = []\n        word_map = {}\n        \n        for word_idx, (start, end) in enumerate(word_spans):\n            if end > start and start >= 0 and end <= token_embeddings.size(1):\n                subword_embs = token_embeddings[b, start:end, :]\n                word_emb = torch.mean(subword_embs, dim=0)\n                word_embs.append(word_emb)\n                \n                if word_idx < len(words):\n                    word_map[word_idx] = words[word_idx]\n        \n        if word_embs:\n            word_emb_tensor = torch.stack(word_embs, dim=0)\n        else:\n            word_emb_tensor = torch.zeros(1, embed_dim, device=device)\n        \n        word_embeddings_list.append(word_emb_tensor)\n        word_maps_batch.append(word_map)\n    \n    max_words = max(w.size(0) for w in word_embeddings_list) if word_embeddings_list else 1\n    padded_word_embs = []\n    \n    for w_emb in word_embeddings_list:\n        if w_emb.size(0) < max_words:\n            pad = torch.zeros(max_words - w_emb.size(0), embed_dim, device=device)\n            w_emb = torch.cat([w_emb, pad], dim=0)\n        padded_word_embs.append(w_emb)\n    \n    word_embeddings = torch.stack(padded_word_embs, dim=0)\n    \n    return word_embeddings, word_maps_batch\n\n\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embed_dim, max_protos: Optional[int] = None):\n        if max_protos is None:\n            max_protos = _DSCD_MAX_PROTOS\n        self.embed_dim = max(1, int(embed_dim))\n        self.max_protos = max(1, int(max_protos))\n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n\n    def add_prototype(self, vector: torch.Tensor, current_time: Optional[float] = None, count: int = 1) -> None:\n        if current_time is None:\n            current_time = time.time()\n        try:\n            v = vector.detach().cpu().clone().to(dtype=torch.float32)\n        except Exception:\n            try:\n                v = torch.tensor(np.asarray(vector), dtype=torch.float32)\n            except Exception:\n                v = torch.zeros(self.embed_dim, dtype=torch.float32)\n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(max(1, int(count)))\n            self.creation_time.append(float(current_time))\n        else:\n            try:\n                min_idx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            except Exception:\n                min_idx = 0\n            min_idx = max(0, min(min_idx, len(self.centroids) - 1))\n            self.centroids[min_idx] = v\n            if min_idx < len(self.counts):\n                self.counts[min_idx] = max(1, int(count))\n            else:\n                while len(self.counts) <= min_idx:\n                    self.counts.append(1)\n                self.counts[min_idx] = max(1, int(count))\n            if min_idx < len(self.creation_time):\n                self.creation_time[min_idx] = float(current_time)\n            else:\n                while len(self.creation_time) <= min_idx:\n                    self.creation_time.append(float(current_time))\n                self.creation_time[min_idx] = float(current_time)\n\n    def update_prototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignment_distance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.add_prototype(vector, time.time(), count=1)\n            return\n        eta = max(0.0, min(1.0, float(eta)))\n        try:\n            old_centroid = self.centroids[idx]\n            new_vector = vector.detach().cpu().to(dtype=torch.float32)\n            if not isinstance(old_centroid, torch.Tensor):\n                old_centroid = torch.tensor(np.asarray(old_centroid), dtype=torch.float32)\n            self.centroids[idx] = (1.0 - eta) * old_centroid + eta * new_vector\n            try:\n                self.counts[idx] = int(self.counts[idx]) + 1\n            except Exception:\n                self.counts[idx] = 1\n        except Exception:\n            try:\n                self.centroids[idx] = vector.detach().cpu().to(dtype=torch.float32)\n                self.counts[idx] = int(self.counts[idx]) + 1 if idx < len(self.counts) else 1\n            except Exception:\n                pass\n        if assignment_distance is not None:\n            self.update_rolling_stats(float(assignment_distance))\n\n    def update_rolling_stats(self, d: float) -> None:\n        d = max(0.0, float(d))\n        if not self.distances:\n            self.mu = float(d)\n            self.tau = max(1e-6, abs(float(d) * 0.1))\n            self.distances = [float(d)]\n            return\n        prev_mu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n        self.tau = max(1e-6, self.tau)\n        self.distances.append(float(d))\n        if len(self.distances) > 50:\n            self.distances.pop(0)\n\n    def get_adaptive_threshold(self, lam: float = 1.0) -> float:\n        lam = max(0.0, float(lam))\n        return float(self.mu + lam * self.tau)\n\n    def size(self) -> int:\n        return len(self.centroids)\n\n    def ensure_consistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            if len(self.counts) > n:\n                self.counts = self.counts[:n]\n            else:\n                self.counts.extend([1] * (n - len(self.counts)))\n        if len(self.creation_time) != n:\n            if len(self.creation_time) > n:\n                self.creation_time = self.creation_time[:n]\n            else:\n                self.creation_time.extend([time.time()] * (n - len(self.creation_time)))\n\n\nclass SigmaNet(nn.Module):\n    def __init__(self, embed_dim: int = 1024):\n        super().__init__()\n        embed_dim = max(1, int(embed_dim))\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.mlp(x)\n\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        buffer_size: Optional[int] = None,\n        max_protos: Optional[int] = None,\n        n_min: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        language: str = \"bn\",\n        enable_training_clustering: Optional[bool] = None,\n        max_clustering_points: Optional[int] = None,\n        max_candidates_per_step: int = 2,\n        dscd_min_letters: int = 2,\n        dscd_min_letter_fraction: float = 0.6,\n    ):\n        super().__init__()\n        if buffer_size is None:\n            buffer_size = _DSCD_BUFFER_SIZE\n        if max_protos is None:\n            max_protos = _DSCD_MAX_PROTOS\n        if n_min is None:\n            n_min = _DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = _DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = _DSCD_MAX_CLUSTERING_POINTS\n        if enable_training_clustering is None:\n            enable_training_clustering = _DSCD_ENABLE_TRAINING_CLUSTERING\n\n        self.embed_dim = max(1, int(embed_dim))\n        self.buffer_size = max(1, int(buffer_size))\n        self.max_protos = max(1, int(max_protos))\n        self.n_min = max(1, int(n_min))\n        self.dispersion_threshold = max(0.0, float(dispersion_threshold))\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscd_min_letters = max(1, int(dscd_min_letters))\n        self.dscd_min_letter_fraction = max(0.0, min(1.0, float(dscd_min_letter_fraction)))\n        self.use_cosine_distance = _DSCD_USE_COSINE_DISTANCE\n        self.enable_online_clustering = _DSCD_ENABLE_ONLINE_CLUSTERING\n        self.online_clustering_frequency = _DSCD_ONLINE_CLUSTERING_FREQUENCY\n        self.apply_augmentation = _APPLY_DSCD_AUGMENTATION\n\n        self.sigma_net = SigmaNet(embed_dim=self.embed_dim)\n\n        try:\n            if tokenizer is not None and \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []) if tokenizer is not None else [])\n        except Exception:\n            self.special_tokens = set()\n        self._dscd_allowed_tokens: Set[str] = set()\n        self._dscd_ignored_tokens: Set[str] = set()\n        self._dscd_cache_max_size = 10000\n        self.prototype_stores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.prototypestores = self.prototype_stores\n        self.buffers: Dict[str, deque] = {}\n        self.buffers_raw: Dict[str, deque] = {}\n        self.discovered_log: List[Dict[str, Any]] = []\n        self.discovered_homographs: Set[str] = set()\n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n        self.dispersion_cache: Dict[str, float] = {}\n        self.dispersion_last_updated: Dict[str, float] = {}\n        self.dispersion_lock = threading.Lock()\n        self.clustering_lock = threading.Lock()\n        self.buffer_lock = threading.Lock()\n        from collections import deque as thread_deque\n        self.active_threads = thread_deque(maxlen=100)\n        self.thread_lock = threading.Lock()\n        self.last_cluster_time: Dict[str, float] = {}\n        self.cluster_cooldown_seconds = 2.0\n        self.enable_training_clustering = bool(enable_training_clustering)\n        self.discovery_count = 0\n        self.discovery_times: List[float] = []\n        self.clustered_tokens: Set[str] = set()\n        self.cluster_stats: Dict[str, Dict[str, Any]] = {}\n        self.max_clustering_points = max(1, int(max_clustering_points))\n        self.max_candidates_per_step = max(1, int(max_candidates_per_step))\n        self.token_addition_counts: Dict[str, int] = {}\n\n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        state = super().state_dict(destination, prefix, keep_vars)\n        plain_stores = {}\n\n        for token, store in self.prototype_stores.items():\n            try:\n                if hasattr(store, 'centroids') and len(store.centroids) > 0:\n                    cent_list = []\n                    for c in store.centroids:\n                        if isinstance(c, torch.Tensor):\n                            cent_list.append(c.detach().cpu().to(dtype=torch.float32))\n                        else:\n                            cent_list.append(torch.tensor(np.asarray(c), dtype=torch.float32))\n                    centroids_tensor = torch.stack(cent_list, dim=0)\n                else:\n                    centroids_tensor = torch.empty((0, self.embed_dim), dtype=torch.float32)\n            except Exception:\n                centroids_tensor = torch.empty((0, self.embed_dim), dtype=torch.float32)\n\n            plain_stores[token] = {\n                'centroids': centroids_tensor,\n                'counts': list(store.counts) if hasattr(store, 'counts') else [],\n                'creation_time': list(store.creation_time) if hasattr(store, 'creation_time') else [],\n                'mu': float(store.mu) if hasattr(store, 'mu') else 0.0,\n                'tau': float(store.tau) if hasattr(store, 'tau') else 1e-6,\n                'size': int(store.size()) if hasattr(store, 'size') else 0,\n            }\n\n        state[prefix + 'prototype_stores_data'] = plain_stores\n        state[prefix + 'discovered_homographs'] = list(self.discovered_homographs)\n        return state\n\n    def load_state_dict(self, state_dict, strict=True):\n        prefix = ''\n        plain_stores = state_dict.pop(prefix + 'prototype_stores_data', {})\n        discovered = state_dict.pop(prefix + 'discovered_homographs', [])\n        super().load_state_dict(state_dict, strict=strict)\n\n        if not plain_stores:\n            print(\"[DSCD] WARNING: Empty prototype_stores in checkpoint\")\n            return\n\n        self.prototype_stores = {}\n        self.discovered_homographs = set(discovered) if discovered else set()\n\n        for token, store_dict in plain_stores.items():\n            store = MemoryEfficientPrototypeStore(embed_dim=self.embed_dim, max_protos=self.max_protos)\n            centroids_data = store_dict.get('centroids', torch.empty((0, self.embed_dim), dtype=torch.float32))\n\n            store.centroids = []\n            try:\n                if isinstance(centroids_data, torch.Tensor):\n                    if centroids_data.numel() > 0 and centroids_data.size(0) > 0:\n                        for i in range(centroids_data.size(0)):\n                            t = centroids_data[i].detach().cpu().to(dtype=torch.float32)\n                            store.centroids.append(t)\n                else:\n                    for c in centroids_data:\n                        if isinstance(c, torch.Tensor):\n                            store.centroids.append(c.detach().cpu().to(dtype=torch.float32))\n                        else:\n                            store.centroids.append(torch.tensor(np.asarray(c), dtype=torch.float32))\n            except Exception:\n                store.centroids = []\n\n            store.counts = store_dict.get('counts', [])\n            store.creation_time = store_dict.get('creation_time', [])\n            store.mu = float(store_dict.get('mu', 0.0))\n            store.tau = max(1e-6, float(store_dict.get('tau', 1e-6)))\n            store.ensure_consistency()\n            self.prototype_stores[token] = store\n\n        print(f\"[DSCD] Loaded {len(self.prototype_stores)} tokens, {sum(s.size() for s in self.prototype_stores.values())} prototypes\")\n\n    @staticmethod\n    def clean_token(token):\n        if token is None:\n            return \"\"\n        token = unicodedata.normalize(\"NFKC\", str(token))\n        token = token.replace('▁', '').replace('Ġ', '').replace('##', '')\n        for punct in ['।', '.', ',', '!', '?', ':', ';', '-']:\n            token = token.replace(punct, '')\n        return token.strip()\n\n    def is_valid_multisense(self, token):\n        if token not in self.prototype_stores:\n            return False\n        store = self.prototype_stores[token]\n        total_occurrences = sum(store.counts) if hasattr(store, 'counts') and store.counts else 0\n        min_per_proto = min(store.counts) if hasattr(store, 'counts') and store.counts else 0\n        return store.size() >= 2 and total_occurrences >= 10 and min_per_proto >= 2\n\n    def get_prototype_summary(self) -> Dict[str, Any]:\n        total_tokens = len(self.prototype_stores)\n        total_prototypes = sum(store.size() for store in self.prototype_stores.values())\n\n        num_homographs = 0\n        for store in self.prototype_stores.values():\n            if self._is_multisense_store(store):\n                num_homographs += 1\n\n        return {\n            'total_tokens': total_tokens,\n            'total_prototypes': total_prototypes,\n            'num_homographs': num_homographs,\n        }\n\n    def _is_multisense_store(self, store: MemoryEfficientPrototypeStore) -> bool:\n        k = store.size()\n        if k < 2:\n            return False\n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(2, self.n_min // 2))\n        if strong < 2:\n            return False\n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.detach().cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n            if len(cents) < 2:\n                return False\n            cents = np.stack(cents, axis=0)\n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            if dists.size == 0:\n                return False\n            tri_idx = np.triu_indices(len(cents), k=1)\n            if tri_idx[0].size == 0:\n                return False\n            tri = dists[tri_idx]\n            if tri.size == 0:\n                return False\n            min_dist = float(tri.min())\n            base = max(store.tau, 1e-3)\n            return (min_dist / base) >= _DSCD_NEWSENSE_LAMBDA\n        except Exception:\n            return True\n\n    def _discover_homographs_for_tokens(\n        self,\n        token_names: List[str],\n        min_cluster_samples: int,\n        dispersion_threshold: float,\n        global_step: int,\n    ) -> int:\n        discovered_in_run: List[str] = []\n        for idx, token in enumerate(token_names):\n            try:\n                success = self._cluster_buffer_to_prototypes_kmeans(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        if clean_token:\n                            self.discovered_homographs.add(clean_token)\n                            discovered_in_run.append(clean_token)\n            except Exception:\n                continue\n        try:\n            self.discovered_log.append(\n                {\n                    \"timestamp\": time.time(),\n                    \"global_step\": global_step,\n                    \"candidates_processed\": len(token_names),\n                    \"discovered_count\": len(discovered_in_run),\n                    \"homographs\": discovered_in_run,\n                    \"total_discovered\": len(self.discovered_homographs),\n                }\n            )\n        except Exception:\n            pass\n        return len(discovered_in_run)\n\n    def discover_homographs(\n        self,\n        nmin: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        min_cluster_size: int = 5,\n        progress: bool = False,\n        max_candidates: int = 500,\n    ) -> int:\n        if nmin is None:\n            nmin = self.n_min\n        if dispersion_threshold is None:\n            dispersion_threshold = self.dispersion_threshold\n        \n        nmin = max(1, int(nmin))\n        min_cluster_size = max(1, int(min_cluster_size))\n        max_candidates = max(1, int(max_candidates))\n        \n        min_cluster_samples = nmin\n        \n        if progress:\n            print(f\"[DISCOVER] Starting: nmin={nmin}, dispersion={dispersion_threshold}\")\n\n        buffer_snapshot = {}\n        with self.buffer_lock:\n            for token, buffer in list(self.buffers_raw.items()):\n                buffer_snapshot[token] = len(buffer)\n\n        candidates: List[Tuple[str, float, int, float]] = []\n        for token, buffer_size in buffer_snapshot.items():\n            if buffer_size >= min_cluster_samples:\n                dispersion = self.get_dispersion(token)\n                if dispersion >= dispersion_threshold:\n                    rank_score = dispersion * buffer_size\n                    candidates.append((token, rank_score, buffer_size, dispersion))\n\n        if not candidates:\n            return 0\n\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_candidates]\n        \n        if progress:\n            print(f\"[DISCOVER] Checking {len(candidates)} tokens...\")\n\n        discovered: List[str] = []\n        for idx, (token, score, buf_size, disp) in enumerate(candidates):\n            try:\n                if progress and idx % 100 == 0 and idx > 0:\n                    print(f\"[DISCOVER] Progress: {idx}/{len(candidates)} ({100*idx/len(candidates):.1f}%) - discovered={len(discovered)}\")\n                \n                with self.clustering_lock:\n                    success = self._cluster_buffer_to_prototypes_kmeans(token)\n                \n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        if clean_token:\n                            self.discovered_homographs.add(clean_token)\n                            discovered.append(clean_token)\n            except Exception:\n                continue\n        \n        if progress:\n            print(f\"[DISCOVER] Complete: {len(discovered)}/{len(candidates)} discovered\")\n\n        try:\n            self.discovered_log.append(\n                {\n                    \"timestamp\": time.time(),\n                    \"candidates\": len(candidates),\n                    \"discovered\": len(discovered),\n                    \"homographs\": discovered[:20],\n                }\n            )\n        except Exception:\n            pass\n\n        return len(discovered)\n\n    def periodic_discovery_check(\n        self,\n        global_step: int,\n        discovery_frequency: int = 200,\n        max_tokens_per_discovery: int = 150,\n    ) -> int:\n        self.last_periodic_check = global_step\n        max_tokens_per_discovery = max(1, int(max_tokens_per_discovery))\n\n        buffer_snapshot = {}\n        with self.buffer_lock:\n            for token, buffer in list(self.buffers_raw.items()):\n                buffer_snapshot[token] = len(buffer)\n\n        candidates: List[Tuple[str, float, int, float]] = []\n        for token, buffersize in buffer_snapshot.items():\n            if buffersize >= self.n_min:\n                dispersion = self.get_dispersion(token)\n                if dispersion > self.dispersion_threshold:\n                    rankscore = dispersion * buffersize\n                    candidates.append((token, rankscore, buffersize, dispersion))\n\n        if not candidates:\n            return 0\n\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_tokens_per_discovery]\n        token_names = [c[0] for c in candidates]\n\n        discovered = self._discover_homographs_for_tokens(\n            token_names, self.n_min, self.dispersion_threshold, global_step\n        )\n\n        if discovered > 0 and _DEBUG_DISCOVERY:\n            print(f\"[DSCD-DISCOVERY] Step {global_step}: Found {discovered} homographs from {len(token_names)} candidates\")\n\n        return discovered\n\n    def get_dispersion(self, token_type: str) -> float:\n        with self.dispersion_lock:\n            if token_type in self.dispersion_cache:\n                try:\n                    last_update = self.dispersion_last_updated.get(token_type, 0.0)\n                    if time.time() - last_update < 3600:\n                        return self.dispersion_cache[token_type]\n                except Exception:\n                    pass\n\n        with self.buffer_lock:\n            if token_type not in self.buffers_raw or len(self.buffers_raw[token_type]) < 2:\n                return 0.0\n\n            try:\n                embeddings: List[np.ndarray] = []\n                for emb in self.buffers_raw[token_type]:\n                    try:\n                        if isinstance(emb, torch.Tensor):\n                            embeddings.append(emb.detach().cpu().numpy())\n                        else:\n                            embeddings.append(np.asarray(emb, dtype=np.float32))\n                    except Exception:\n                        continue\n\n                if len(embeddings) < 2:\n                    return 0.0\n\n                embeddings_np = np.stack(embeddings, axis=0)\n                centroid = embeddings_np.mean(axis=0)\n                distances = np.linalg.norm(embeddings_np - centroid[None, :], axis=1)\n                dispersion = float(distances.std())\n\n                with self.dispersion_lock:\n                    self.dispersion_cache[token_type] = dispersion\n                    self.dispersion_last_updated[token_type] = time.time()\n\n                return dispersion\n            except Exception:\n                return 0.0\n\n    def validate_prototypes(\n        self,\n        homograph_list: Optional[List[str]] = None,\n        cluster_missing: bool = False,\n    ) -> Dict[str, Any]:\n        if homograph_list is None:\n            try:\n                homograph_list = list(_HOMOGRAPH_REFERENCE_LIST_BN)\n            except Exception:\n                homograph_list = [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"]\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[DSCD-VALIDATION] Prototype Quality Check\")\n        print(\"=\" * 80)\n\n        validation_results: Dict[str, Any] = {\n            \"total_tokens\": len(self.prototype_stores),\n            \"total_prototypes\": 0,\n            \"multi_sense_tokens\": 0,\n            \"homographs_found\": 0,\n            \"homographs_missing\": [],\n            \"avg_prototypes_per_token\": 0.0,\n            \"avg_samples_per_prototype\": 0.0,\n            \"quality_score\": 0.0,\n        }\n\n        total_samples = 0\n        for token, store in self.prototype_stores.items():\n            num_protos = len(store.centroids)\n            validation_results[\"total_prototypes\"] += num_protos\n            if self._is_multisense_store(store):\n                validation_results[\"multi_sense_tokens\"] += 1\n            try:\n                total_samples += sum(store.counts)\n            except Exception:\n                pass\n\n        if validation_results[\"total_tokens\"] > 0:\n            validation_results[\"avg_prototypes_per_token\"] = (\n                validation_results[\"total_prototypes\"] / validation_results[\"total_tokens\"]\n            )\n\n        if validation_results[\"total_prototypes\"] > 0:\n            validation_results[\"avg_samples_per_prototype\"] = (\n                total_samples / validation_results[\"total_prototypes\"]\n            )\n\n        print(\"\\n[VALIDATION] Reference Homograph Coverage:\")\n        print(\"-\" * 80)\n\n        missing_tokens_to_cluster: List[str] = []\n\n        for homograph in homograph_list:\n            clean_h = normalize_token_key(homograph)\n            if not clean_h:\n                continue\n\n            found = False\n            found_key = None\n            found_protos = 0\n\n            if homograph in self.prototype_stores:\n                found = True\n                found_key = homograph\n                found_protos = len(self.prototype_stores[homograph].centroids)\n            elif clean_h in self.prototype_stores:\n                found = True\n                found_key = clean_h\n                found_protos = len(self.prototype_stores[clean_h].centroids)\n            else:\n                for key in self.prototype_stores.keys():\n                    clean_key = normalize_token_key(key)\n                    if clean_key and (clean_key == clean_h or clean_h in clean_key or clean_key in clean_h):\n                        found = True\n                        found_key = key\n                        found_protos = len(self.prototype_stores[key].centroids)\n                        break\n\n            if found and self._is_multisense_store(self.prototype_stores[found_key]):\n                validation_results[\"homographs_found\"] += 1\n                try:\n                    counts = self.prototype_stores[found_key].counts\n                    print(f\"  '{homograph}' -> {found_protos} prototypes (counts={counts})\")\n                except Exception:\n                    print(f\"  '{homograph}' -> {found_protos} prototypes\")\n            elif found and found_protos == 1:\n                validation_results[\"homographs_missing\"].append(homograph)\n                print(f\"  '{homograph}' -> Only 1 prototype\")\n                if cluster_missing:\n                    missing_tokens_to_cluster.append(found_key)\n            else:\n                validation_results[\"homographs_missing\"].append(homograph)\n                print(f\"  '{homograph}' -> NOT FOUND\")\n                if cluster_missing:\n                    if homograph in self.buffers_raw or clean_h in self.buffers_raw:\n                        key_to_cluster = homograph if homograph in self.buffers_raw else clean_h\n                        if len(self.buffers_raw[key_to_cluster]) >= max(5, self.n_min // 2):\n                            print(\"     -> Found in buffer, will cluster\")\n                            missing_tokens_to_cluster.append(key_to_cluster)\n\n        if cluster_missing and missing_tokens_to_cluster:\n            print(f\"\\n[VALIDATION] Clustering {len(missing_tokens_to_cluster)} missing tokens...\")\n            for token in missing_tokens_to_cluster:\n                try:\n                    with self.clustering_lock:\n                        self._cluster_buffer_to_prototypes_kmeans(token)\n                    if (\n                        token in self.prototype_stores\n                        and self._is_multisense_store(self.prototype_stores[token])\n                    ):\n                        print(f\"  Successfully clustered '{token}'\")\n                except Exception as e:\n                    print(f\"  Failed to cluster '{token}': {e}\")\n\n        homograph_coverage = (\n            validation_results[\"homographs_found\"] / len(homograph_list)\n            if homograph_list\n            else 0.0\n        )\n        multi_sense_ratio = (\n            validation_results[\"multi_sense_tokens\"] / validation_results[\"total_tokens\"]\n            if validation_results[\"total_tokens\"] > 0\n            else 0.0\n        )\n        validation_results[\"quality_score\"] = (\n            homograph_coverage * 0.6 + multi_sense_ratio * 0.4\n        )\n\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(f\"  - Total tokens:       {validation_results['total_tokens']}\")\n        print(f\"  - Total prototypes:   {validation_results['total_prototypes']}\")\n        print(f\"  - Multi-sense tokens: {validation_results['multi_sense_tokens']}\")\n        print(\n            f\"  - Reference found:    {validation_results['homographs_found']}/{len(homograph_list)}\"\n        )\n        print(f\"  - Quality Score:      {validation_results['quality_score']:.2%}\")\n        print(\"=\" * 80 + \"\\n\")\n\n        return validation_results\n\n    def should_track_token(self, token_text: str) -> bool:\n        if not token_text or not isinstance(token_text, str):\n            return False\n\n        if len(self._dscd_allowed_tokens) > self._dscd_cache_max_size:\n            self._dscd_allowed_tokens.clear()\n        if len(self._dscd_ignored_tokens) > self._dscd_cache_max_size:\n            self._dscd_ignored_tokens.clear()\n\n        if token_text in self._dscd_allowed_tokens:\n            return True\n        if token_text in self._dscd_ignored_tokens:\n            return False\n\n        if token_text in self.special_tokens:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        clean = normalize_token_key(token_text)\n        if not clean:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        self._dscd_allowed_tokens.add(token_text)\n        return True\n\n    def _canonical_token_key(\n        self,\n        raw_token: str,\n        token_word_map: Optional[Dict[int, Optional[str]]],\n        idx: int,\n    ) -> Optional[str]:\n        canonical: Optional[str] = None\n        try:\n            if (\n                token_word_map\n                and isinstance(token_word_map, dict)\n                and idx in token_word_map\n                and token_word_map[idx]\n            ):\n                canonical = str(token_word_map[idx]).strip()\n        except Exception:\n            canonical = None\n\n        if not canonical:\n            canonical = str(raw_token).strip()\n        return canonical\n\n    def cleanup_threads(self) -> None:\n        try:\n            with self.thread_lock:\n                alive = [th for th in list(self.active_threads) if th.is_alive()]\n                self.active_threads.clear()\n                self.active_threads.extend(alive)\n        except Exception:\n            pass\n\n    def cleanup_memory(self) -> None:\n        try:\n            for token_type, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            for token_type, buffer in list(self.buffers_raw.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            try:\n                now = time.time()\n                expired = [k for k, v in self.dispersion_last_updated.items() if now - v > 3600]\n                for k in expired:\n                    self.dispersion_cache.pop(k, None)\n                    self.dispersion_last_updated.pop(k, None)\n            except Exception:\n                pass\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    def forward(\n        self,\n        token_embeddings: Optional[torch.Tensor],\n        token_types: Optional[List[List[str]]] = None,\n        train_mode: bool = True,\n        token_word_map: Optional[List[Dict[int, Optional[str]]]] = None,\n        h_all: Optional[torch.Tensor] = None,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Dict[str, Any]:\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n\n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n\n        try:\n            word_embeddings, word_maps_from_reconstruction = reconstruct_word_embeddings(\n                token_embeddings, input_ids, self.tokenizer, device\n            )\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[DSCD-FIX2] Word reconstruction failed: {str(e)[:200]}\")\n            word_embeddings = token_embeddings\n            word_maps_from_reconstruction = [{} for _ in range(batch_size)]\n\n        if input_ids is not None and token_types is None:\n            batch_size_ids, seq_len_subwords = input_ids.shape\n            token_types = []\n            for b in range(batch_size_ids):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(\n                            self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n                        )\n                    except Exception:\n                        token_types.append([f\"tok_{i}\" for i in range(seq_len_subwords)])\n                else:\n                    token_types.append([f\"tok_{i}\" for i in range(seq_len_subwords)])\n        if token_types is None:\n            word_seq_len = int(word_embeddings.size(1))\n            token_types = [[f\"tok_{i}\" for i in range(word_seq_len)] for _ in range(batch_size)]\n\n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n            self.cleanup_threads()\n\n        all_outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n\n        word_seq_len = int(word_embeddings.size(1))\n        for b in range(batch_size):\n            word_map_b = (\n                word_maps_from_reconstruction[b]\n                if b < len(word_maps_from_reconstruction)\n                else {}\n            )\n            token_types_b = (\n                token_types[b]\n                if token_types and len(token_types) > b\n                else [f\"tok_{i}\" for i in range(word_seq_len)]\n            )\n            batch_outputs = self.process_sequence(\n                word_embeddings[b],\n                token_types_b,\n                device,\n                word_map=word_map_b,\n                train_mode=train_mode,\n            )\n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n\n        try:\n            h_aug_list: List[torch.Tensor] = []\n            max_seq_len = word_seq_len\n            for b in range(batch_size):\n                h_batch_list = all_outputs[\"h_augmented\"][b]\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = torch.zeros((max_seq_len, self.embed_dim), device=device)\n                h_aug_list.append(h_batch)\n            all_outputs[\"h_augmented\"] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs[\"h_augmented\"] = word_embeddings\n\n        try:\n            proto_assign_tensor: List[torch.Tensor] = []\n            for row in all_outputs[\"proto_assignments\"]:\n                try:\n                    stacked = torch.stack(\n                        [\n                            x if isinstance(x, torch.Tensor) else torch.tensor(int(x), dtype=torch.long)\n                            for x in row\n                        ],\n                        dim=0,\n                    )\n                    proto_assign_tensor.append(stacked)\n                except Exception:\n                    proto_assign_tensor.append(\n                        torch.tensor(\n                            [\n                                int(x) if not isinstance(x, torch.Tensor) else int(x.item())\n                                for x in row\n                            ],\n                            dtype=torch.long,\n                        )\n                    )\n            all_outputs[\"proto_assignments\"] = proto_assign_tensor\n        except Exception:\n            pass\n\n        return all_outputs\n\n    def process_sequence(\n        self,\n        token_embeddings: torch.Tensor,\n        token_types: List[Any],\n        device: torch.device,\n        word_map: Optional[Dict[int, Optional[str]]] = None,\n        train_mode: bool = True,\n    ) -> Dict[str, List[Any]]:\n        seq_len = int(token_embeddings.size(0))\n        outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n\n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f\"tok_{j}\"\n            if not isinstance(raw_tok, str):\n                raw_tok = str(raw_tok)\n            token_key = raw_tok.replace(\"▁\", \"\").strip()\n            if not token_key:\n                token_key = raw_tok\n\n            h_j = token_embeddings[j]\n\n            if not self.should_track_token(token_key):\n                outputs[\"proto_assignments\"].append(torch.tensor(-1, dtype=torch.long))\n                outputs[\"proto_probs\"].append(0.0)\n                outputs[\"uncertainties\"].append(0.5)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n\n            h_norm = F.normalize(h_j.unsqueeze(0), p=2, dim=-1).squeeze(0)\n            h_raw = h_j.detach().cpu().clone()\n\n            with self.buffer_lock:\n                if token_key not in self.buffers:\n                    self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                    self.buffers_raw[token_key] = deque(maxlen=self.buffer_size)\n                    self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(\n                        self.embed_dim, max_protos=self.max_protos\n                    )\n                    self.token_addition_counts[token_key] = 0\n\n                self.buffers[token_key].append(h_norm.detach().cpu().clone())\n                self.buffers_raw[token_key].append(h_raw)\n                self.token_addition_counts[token_key] += 1\n\n                if (\n                    self.enable_online_clustering\n                    and train_mode\n                    and self.token_addition_counts[token_key]\n                    % self.online_clustering_frequency\n                    == 0\n                    and len(self.buffers_raw[token_key]) >= self.n_min\n                ):\n                    try:\n                        thread = threading.Thread(\n                            target=self._cluster_buffer_to_prototypes_kmeans,\n                            args=(token_key,),\n                            daemon=True,\n                        )\n                        thread.start()\n                        with self.thread_lock:\n                            self.active_threads.append(thread)\n                    except Exception:\n                        pass\n\n            store = self.prototype_stores[token_key]\n            with self.buffer_lock:\n                centroids_snapshot: List[torch.Tensor] = []\n                for c in store.centroids:\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            centroids_snapshot.append(c.detach().clone().to(device))\n                        else:\n                            centroids_snapshot.append(\n                                torch.tensor(np.asarray(c), dtype=torch.float32, device=device)\n                            )\n                    except Exception:\n                        continue\n\n                if not centroids_snapshot or len(centroids_snapshot) < 1:\n                    outputs[\"proto_assignments\"].append(torch.tensor(-1, dtype=torch.long))\n                    outputs[\"proto_probs\"].append(0.0)\n                    outputs[\"uncertainties\"].append(0.5)\n                    outputs[\"span_preds\"].append(0.0)\n                    outputs[\"gates\"].append(0.0)\n                    outputs[\"h_augmented\"].append(h_j)\n                    continue\n\n            try:\n                centroids_stacked = torch.stack(centroids_snapshot, dim=0)\n                h_norm_expanded = h_norm.unsqueeze(0)\n\n                if self.use_cosine_distance:\n                    centroids_normed = F.normalize(centroids_stacked, p=2, dim=-1)\n                    h_query_normed = F.normalize(h_norm_expanded, p=2, dim=-1)\n                    cosine_sims = torch.mm(h_query_normed, centroids_normed.t()).squeeze(0)\n                    cosine_dists = 1.0 - cosine_sims\n                    dists_torch = cosine_dists\n                else:\n                    dists_torch = torch.norm(centroids_stacked - h_norm_expanded, dim=1)\n\n                dist_np = dists_torch.detach().cpu().numpy()\n                if dist_np.size == 0:\n                    raise ValueError(\"Empty distances\")\n\n                min_dist = float(dist_np.min())\n                min_idx = int(np.argmin(dist_np))\n                max_dist = float(dist_np.max())\n                mean_dist = float(dist_np.mean())\n                std_dist = float(dist_np.std()) if dist_np.size > 1 else 0.0\n                K_prototypes = len(dist_np)\n\n                if store.size() < self.max_protos:\n                    adaptive_threshold = store.get_adaptive_threshold(_DSCD_NEWSENSE_LAMBDA)\n                    if min_dist > adaptive_threshold:\n                        store.add_prototype(h_norm, time.time(), count=1)\n                        assignment = store.size() - 1\n                        new_centroid = h_norm.detach().clone().to(device)\n                        centroids_snapshot.append(new_centroid)\n                        centroids_stacked = torch.stack(centroids_snapshot, dim=0)\n                        if self.use_cosine_distance:\n                            centroids_normed = F.normalize(centroids_stacked, p=2, dim=-1)\n                            h_query_normed = F.normalize(h_norm_expanded, p=2, dim=-1)\n                            cosine_sims = torch.mm(h_query_normed, centroids_normed.t()).squeeze(0)\n                            cosine_dists = 1.0 - cosine_sims\n                            dists_torch = cosine_dists\n                        else:\n                            dists_torch = torch.norm(centroids_stacked - h_norm_expanded, dim=1)\n                        dist_np = dists_torch.detach().cpu().numpy()\n                        K_prototypes = len(dist_np)\n                    else:\n                        assignment = min_idx\n                else:\n                    assignment = min_idx\n\n                if self.use_cosine_distance:\n                    sims_np = 1.0 - dist_np\n                else:\n                    max_possible_dist = 3.0\n                    sims_np = 1.0 - np.clip(dist_np / max_possible_dist, 0, 1)\n\n                T_softmax = 0.7\n                exp_sims = np.exp(sims_np / T_softmax)\n                probs = exp_sims / (exp_sims.sum() + 1e-12)\n                prob_list = probs.tolist()\n                p_max = float(probs.max())\n\n                entropy_raw = -np.sum(probs * np.log(probs + 1e-10))\n                max_entropy = np.log(K_prototypes) if K_prototypes > 1 else 1.0\n                H_norm = float(entropy_raw / max_entropy) if max_entropy > 0 else 0.0\n\n                try:\n                    h_j_for_sigma = h_j.unsqueeze(0)\n                    with torch.no_grad():\n                        log_sigma = self.sigma_net(h_j_for_sigma).squeeze()\n                        sigma_j = torch.exp(log_sigma / 2.0).item()\n                        sigma_norm = min(max(sigma_j, 0.0), 1.0)\n                except Exception:\n                    sigma_norm = 0.5\n\n                if K_prototypes >= 2:\n                    sorted_dists = np.sort(dist_np)\n                    d1 = float(sorted_dists[0])\n                    d2 = float(sorted_dists[1]) if len(sorted_dists) > 1 else float(sorted_dists[0])\n                    span = max(0.0, d2 - d1)\n                    dist_uncertainty = d1 / (d1 + d2) if (d1 + d2) > 0 else 0.5\n                    span_normalized = 1.0 - min(1.0, span / 0.5)\n\n                    w_entropy = 0.5\n                    w_distance = 0.3\n                    w_span = 0.2\n                    uncertainty = (\n                        w_entropy * H_norm\n                        + w_distance * dist_uncertainty\n                        + w_span * span_normalized\n                    )\n                    uncertainty = max(0.0, min(1.0, uncertainty))\n                else:\n                    span = 0.0\n                    uncertainty = 0.5\n                    d1 = float(dist_np[0]) if len(dist_np) > 0 else 1.0\n\n                is_potential_homograph = (\n                    (H_norm >= 0.6)\n                    or (span < 0.15 and K_prototypes >= 2)\n                    or (uncertainty >= 0.5)\n                )\n\n                gate_base = 1.0 / (1.0 + d1)\n                entropy_penalty = 1.0 - H_norm\n                gate = gate_base * entropy_penalty\n                gate = max(0.0, min(1.0, gate))\n\n                h_aug = h_j\n                if self.apply_augmentation:\n                    best_sim = 1.0 - min_dist\n                    if span < 0.5 and best_sim > 0.3:\n                        try:\n                            centroid_t = centroids_snapshot[assignment]\n                            h_aug = h_j + centroid_t\n                        except Exception:\n                            h_aug = h_j\n\n                try:\n                    store.update_rolling_stats(min_dist)\n                except Exception:\n                    pass\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[DSCD] Assignment error for {token_key}: {str(e)[:200]}\")\n                assignment = -1\n                prob_list = 0.0\n                uncertainty = 0.5\n                span = 0.0\n                gate = 0.0\n                h_aug = h_j\n\n            outputs[\"proto_assignments\"].append(torch.tensor(assignment, dtype=torch.long))\n            outputs[\"proto_probs\"].append(prob_list)\n            outputs[\"uncertainties\"].append(uncertainty)\n            outputs[\"span_preds\"].append(span)\n            outputs[\"gates\"].append(gate)\n            outputs[\"h_augmented\"].append(h_aug)\n\n        return outputs\n\n    def print_clusters_summary(self) -> None:\n        try:\n            items: List[Tuple[str, int, int, float, float, int]] = []\n            for token, store in self.prototype_stores.items():\n                try:\n                    proto_sample_count = sum(getattr(store, \"counts\", []))\n                except Exception:\n                    proto_sample_count = 0\n                buffer_len = len(self.buffers_raw.get(token, [])) if token in self.buffers_raw else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, \"mu\", 0.0)\n                tau = getattr(store, \"tau\", 0.0)\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n            items.sort(key=lambda x: x[1], reverse=True)\n            top5 = items[:5]\n            if _VERBOSE_LOGGING:\n                print(\"[CLUSTER] Top 5 clusters:\")\n                print(\"-\" * 100)\n                print(\n                    f\"{'Rank':<6} {'Token':<18} {'Count':<12} \"\n                    f\"{'Protos':<8} {'BufLen':<8} {'mu':<15} {'tau':<12}\"\n                )\n                print(\"-\" * 100)\n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(top5, 1):\n                    tok_str = str(tok)[:18]\n                    print(\n                        f\"{rank:<6} {tok_str:<18} {cnt:<12} {prot:<8} \"\n                        f\"{buflen:<8} {mu:<15.6f} {tau:<12.6f}\"\n                    )\n                print(\"-\" * 100)\n            total_samples = sum(item[1] for item in items)\n            total_protos = sum(item[2] for item in items)\n            total_buffers = sum(item[5] for item in items)\n            print(\n                f\"Total: {len(items)} clusters, {total_samples} samples, \"\n                f\"{total_protos} protos, {total_buffers} buffers\"\n            )\n        except Exception as e:\n            try:\n                if _VERBOSE_LOGGING:\n                    print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n            except Exception:\n                pass\n\n    def _cluster_buffer_to_prototypes_kmeans(self, token_type: str) -> bool:\n        try:\n            if not self.should_track_token(token_type):\n                if _DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping non-word token: {token_type}\")\n                return False\n\n            with self.buffer_lock:\n                if token_type not in self.buffers_raw:\n                    return False\n                buf_snapshot = [\n                    e.clone() if isinstance(e, torch.Tensor) else e\n                    for e in self.buffers_raw[token_type]\n                ]\n                if len(buf_snapshot) < self.n_min:\n                    if _DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] {token_type} buffer {len(buf_snapshot)} \"\n                            f\"< n_min {self.n_min}\"\n                        )\n                    return False\n\n            emb_list: List[np.ndarray] = []\n            for e in buf_snapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        try:\n                            emb_list.append(e.detach().cpu().numpy())\n                        except Exception:\n                            emb_list.append(np.asarray(e.cpu(), dtype=np.float32))\n                    else:\n                        emb_list.append(np.asarray(e, dtype=np.float32))\n                except Exception:\n                    continue\n\n            if len(emb_list) == 0:\n                return False\n\n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                new_embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                new_embeddings = np.stack(emb_list, axis=0)\n\n            if new_embeddings.shape[0] < 2:\n                return False\n\n            norms = np.linalg.norm(new_embeddings, axis=1)\n            zero_vectors = norms < 1e-6\n            if np.all(zero_vectors):\n                if _DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {token_type} all zero vectors, skipping\")\n                return False\n            if np.any(zero_vectors):\n                new_embeddings = new_embeddings[~zero_vectors]\n            if new_embeddings.shape[0] < 2:\n                return False\n\n            norms = np.linalg.norm(new_embeddings, axis=1)\n            new_embeddings_normalized = new_embeddings / (norms[:, None] + 1e-10)\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {token_type} buf_len={len(buf_snapshot)}, \"\n                    f\"sampled={new_embeddings_normalized.shape[0]}, \"\n                    f\"mean_norm={norms.mean():.4f}\"\n                )\n\n            store = self.prototype_stores[token_type]\n            existing_centroids: List[np.ndarray] = []\n            if hasattr(store, \"centroids\") and len(store.centroids) > 0:\n                for c in store.centroids:\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            try:\n                                c_np = c.detach().cpu().numpy()\n                            except Exception:\n                                c_np = np.asarray(c, dtype=np.float32)\n                        else:\n                            c_np = np.asarray(c, dtype=np.float32)\n                        c_norm = np.linalg.norm(c_np)\n                        if c_norm > 1e-6:\n                            c_normalized = c_np / c_norm\n                            existing_centroids.append(c_normalized)\n                    except Exception:\n                        continue\n\n            if len(existing_centroids) > 1:\n                existing_centroids_np = np.stack(existing_centroids, axis=0)\n                combined_embeddings = np.vstack([existing_centroids_np, new_embeddings_normalized])\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"[DSCD-CLUSTER] {token_type} Incremental - \"\n                        f\"{len(existing_centroids)} existing, \"\n                        f\"{new_embeddings_normalized.shape[0]} new, \"\n                        f\"{combined_embeddings.shape[0]} total embeddings\"\n                    )\n                embeddings = combined_embeddings\n            else:\n                embeddings = new_embeddings_normalized\n\n            protos_added = 0\n            new_centroids: List[torch.Tensor] = []\n            new_counts: List[int] = []\n            new_times: List[float] = []\n\n            if HAS_KMEANS:\n                try:\n                    min_k = 1\n                    max_k = min(self.max_protos, len(embeddings), max(1, self.n_min))\n                    if max_k <= min_k:\n                        max_k = min_k\n                    if len(embeddings) >= 20:\n                        k_guess = min(max_k, max(2, int(np.sqrt(len(embeddings) / 2))))\n                    elif len(embeddings) >= 10:\n                        k_guess = min(max_k, 2)\n                    else:\n                        k_guess = 1\n                    k_guess = max(min_k, min(k_guess, len(embeddings)))\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n                        for c_idx in range(k_guess):\n                            mask = labels == c_idx\n                            cluster_size = int(mask.sum())\n                            if cluster_size >= self.n_min:\n                                centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                centroid_tensor = torch.from_numpy(centroid)\n                                new_centroids.append(centroid_tensor)\n                                new_counts.append(cluster_size)\n                                new_times.append(time.time())\n                                protos_added += 1\n                        store.centroids = new_centroids\n                        store.counts = new_counts\n                        store.creation_time = new_times\n                        try:\n                            store.labels = torch.tensor(labels)\n                        except Exception:\n                            store.labels = None\n                        if _DEBUG_DISCOVERY and protos_added > 0:\n                            print(\n                                f\"[DSCD-CLUSTER] KMeans created {protos_added} \"\n                                f\"prototypes for {token_type}\"\n                            )\n                except Exception as e:\n                    if _DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] KMeans failed for {token_type}: \"\n                            f\"{type(e).__name__} {str(e)[:200]}\"\n                        )\n\n            if protos_added == 0 and HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric=\"euclidean\")\n                    if condensed.size == 0:\n                        Z = None\n                    else:\n                        Z = linkage(condensed, method=\"average\")\n                    max_dist = condensed.max() if condensed.size > 0 else 1.0\n                    relative_threshold = self.dispersion_threshold\n                    absolute_threshold = relative_threshold * max_dist\n                    if Z is not None:\n                        clusters = fcluster(Z, t=absolute_threshold, criterion=\"distance\") - 1\n                        if clusters.size > 0:\n                            max_c = int(clusters.max())\n                            new_centroids = []\n                            new_counts = []\n                            new_times = []\n                            for cid in range(max_c + 1):\n                                mask = clusters == cid\n                                cluster_size = int(mask.sum())\n                                if cluster_size >= self.n_min:\n                                    centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                    centroid_tensor = torch.from_numpy(centroid)\n                                    new_centroids.append(centroid_tensor)\n                                    new_counts.append(cluster_size)\n                                    new_times.append(time.time())\n                                    protos_added += 1\n                            if len(new_centroids) > self.max_protos:\n                                sorted_indices = np.argsort(new_counts)[::-1][: self.max_protos]\n                                new_centroids = [new_centroids[i] for i in sorted_indices]\n                                new_counts = [new_counts[i] for i in sorted_indices]\n                                new_times = [new_times[i] for i in sorted_indices]\n                                protos_added = len(new_centroids)\n                            store.centroids = new_centroids\n                            store.counts = new_counts\n                            store.creation_time = new_times\n                            try:\n                                store.labels = torch.tensor(clusters)\n                            except Exception:\n                                store.labels = None\n                            if _DEBUG_DISCOVERY and protos_added > 0:\n                                print(\n                                    f\"[DSCD-CLUSTER] Hierarchical created {protos_added} \"\n                                    f\"prototypes for {token_type}\"\n                                )\n                except Exception as e:\n                    if _DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] Hierarchical failed for {token_type}: \"\n                            f\"{type(e).__name__} {str(e)[:200]}\"\n                        )\n\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {token_type} final store.size={store.size()} \"\n                    f\"protos, counts={getattr(store, 'counts', [])}\"\n                )\n            try:\n                if store.centroids:\n                    counts = store.counts if store.counts else [1] * len(store.centroids)\n                    total_count = sum(counts)\n                    mean_count = float(total_count) / max(1, len(counts))\n                    self.cluster_stats[str(token_type)] = {\n                        \"num_prototypes\": len(store.centroids),\n                        \"counts\": [int(c) for c in counts],\n                        \"total_samples\": int(total_count),\n                        \"mean_count\": float(mean_count),\n                        \"mu\": float(store.mu),\n                        \"tau\": float(store.tau),\n                    }\n            except Exception:\n                pass\n\n            return store.size() > 0\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-ERROR] Clustering error for {token_type}: \"\n                    f\"{type(e).__name__} {str(e)[:200]}\"\n                )\n            return False\n\n    def get_explanations(self, threshold_span: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for token_type, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({\"token\": str(token_type), \"protos\": store.size()})\n        return expl\n\n\nprint(\"=\" * 80)\nprint(\"[CELL3] DSCD MODULE - COMPLETE\")\nprint(\"=\" * 80)\n","metadata":{"id":"L25pcKUPH4J2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - COMPLETE FIXED VERSION (ALL 10 ERRORS RESOLVED)\n# ==============================================================================\n\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threading\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\n    if _MAX_LENGTH <= 0:\n        _MAX_LENGTH = 48\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n        if _GRL_ALPHA_STEPS <= 0:\n            _GRL_ALPHA_STEPS = 10000\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.0\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_should_track_token = \"should_track_token\" in globals()\n\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = float(alpha)\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.alpha * grad_output, None\n\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    alpha = max(0.0, float(alpha))\n    return GradientReversalFunction.apply(x, alpha)\n\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        input_dim = max(1, int(input_dim))\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        input_dim = max(1, int(input_dim))\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2),\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 1000,\n        encoder_grl_scale: float = 0.5,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = max(1, int(embed_dim))\n\n        self.bn_source = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n        self.bn_target = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n        self.freq_threshold = max(0.0, min(1.0, float(freq_threshold)))\n        self.uncertainty_threshold = max(0.0, min(1.0, float(uncertainty_threshold)))\n        self.gate_threshold = max(0.0, min(1.0, float(gate_threshold)))\n        self.warmup_steps = max(0, int(warmup_steps))\n        self.current_step = 0\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 0.5, \"xl\": 0.8, \"domain\": 1.0}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = max(0.0, float(encoder_grl_scale))\n        self.stats_reset_interval = 1000\n        \n        self.correct_domain = 0\n        self.correct_source = 0\n        self.correct_target = 0\n        self.total_samples = 0\n        self.total_source = 0\n        self.total_target = 0\n        self.domain_loss_accumulator = 0.0\n        self.asbn_loss_accumulator = 0.0\n        self._stats_lock = threading.Lock()\n        \n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[ASBN-INIT] Initialized MemoryEfficientASBNModule:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - warmup_steps: {self.warmup_steps}\")\n            print(f\"  - encoder_grl_scale: {self.encoder_grl_scale}\")\n            print(f\"  - GRL_ALPHA_STEPS: {_GRL_ALPHA_STEPS}\")\n            print(f\"  - thresholds: freq={self.freq_threshold}, uncert={self.uncertainty_threshold}, gate={self.gate_threshold}\")\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(max(1, _GRL_ALPHA_STEPS)))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(max(1, _GRL_ALPHA_STEPS)))\n            denom = _GRL_ALPHA_START if abs(_GRL_ALPHA_START) > 1e-3 else 1e-3\n            ratio = _GRL_ALPHA_END / denom\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n        return max(0.0, float(alpha))\n\n    def get_asbn_stats(self) -> Dict[str, float]:\n        return self.get_detailed_stats()\n\n    def get_detailed_stats(self) -> Dict[str, float]:\n        with self._stats_lock:\n            if self.total_samples > 0:\n                domain_acc = self.correct_domain / self.total_samples\n            else:\n                domain_acc = 0.0\n            \n            if self.total_source > 0:\n                source_acc = self.correct_source / self.total_source\n            else:\n                source_acc = 0.0\n            \n            if self.total_target > 0:\n                target_acc = self.correct_target / self.total_target\n            else:\n                target_acc = 0.0\n            \n            if self.total_samples > 0:\n                avg_domain_loss = self.domain_loss_accumulator / self.total_samples\n                avg_asbn_loss = self.asbn_loss_accumulator / self.total_samples\n            else:\n                avg_domain_loss = 0.0\n                avg_asbn_loss = 0.0\n            \n            return {\n                \"domain_loss\": avg_domain_loss,\n                \"domain_accuracy\": domain_acc,\n                \"source_accuracy\": source_acc,\n                \"target_accuracy\": target_acc,\n                \"asbn_loss\": avg_asbn_loss,\n                \"num_updates\": self.total_samples,\n            }\n\n    def reset_stats(self) -> None:\n        with self._stats_lock:\n            self.correct_domain = 0\n            self.correct_source = 0\n            self.correct_target = 0\n            self.total_samples = 0\n            self.total_source = 0\n            self.total_target = 0\n            self.domain_loss_accumulator = 0.0\n            self.asbn_loss_accumulator = 0.0\n            \n            self.stats = {\n                \"domain_loss\": 0.0,\n                \"domain_accuracy\": 0.0,\n                \"source_accuracy\": 0.0,\n                \"target_accuracy\": 0.0,\n                \"asbn_loss\": 0.0,\n                \"num_updates\": 0,\n            }\n\n    def critic_parameters(self):\n        return (\n            list(self.d_domain.parameters())\n            + list(self.d_freq.parameters())\n            + list(self.d_ctx.parameters())\n            + list(self.d_xl.parameters())\n        )\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (\n                self.d_domain,\n                self.d_freq,\n                self.d_ctx,\n                self.d_xl,\n                self.bn_source,\n                self.bn_target,\n            ):\n                try:\n                    try:\n                        param_list = list(mod.parameters())\n                        if len(param_list) > 0:\n                            p = param_list[0]\n                            if p.device != device:\n                                mod.to(device)\n                        else:\n                            mod.to(device)\n                    except Exception:\n                        mod.to(device)\n                except Exception:\n                    pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] Device migration failed:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    print(\"[ASBN] Device migration failed\")\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        batch_size = max(1, int(batch_size))\n        seq_len = max(1, int(seq_len))\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n            if isinstance(proto_probs, torch.Tensor):\n                p = proto_probs.detach().to(device)\n                if p.dim() == 3:\n                    B, T, K = p.shape\n                    if B > 0 and T > 0 and K > 0:\n                        b_max = min(batch_size, B)\n                        t_max = min(seq_len, T)\n                        pmax_vals = p[:b_max, :t_max].max(dim=2)[0]\n                        pmax_vals = torch.where(torch.isfinite(pmax_vals), pmax_vals, torch.full_like(pmax_vals, 0.5))\n                        pmax[:b_max, :t_max] = pmax_vals\n                    return pmax\n                if p.dim() == 2:\n                    if batch_size >= 1 and p.size(0) > 0 and p.size(1) > 0:\n                        t_max = min(seq_len, p.size(0))\n                        pmax_vals = p[:t_max].max(dim=1)[0]\n                        pmax_vals = torch.where(torch.isfinite(pmax_vals), pmax_vals, torch.full_like(pmax_vals, 0.5))\n                        pmax[0, :t_max] = pmax_vals\n                        return pmax\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            t_max = min(seq_len, row.size(0))\n                            if t_max > 0:\n                                pmax_vals = row[:t_max].max(dim=1)[0].to(device)\n                                pmax_vals = torch.where(torch.isfinite(pmax_vals), pmax_vals, torch.full_like(pmax_vals, 0.5))\n                                pmax[b, :t_max] = pmax_vals\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        val_item = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        if arr.size > 0:\n                                            val_item = float(np.max(arr))\n                                        else:\n                                            val_item = 0.5\n                                    if np.isfinite(val_item):\n                                        pmax[b, t] = val_item\n                                    else:\n                                        pmax[b, t] = 0.5\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1 and len(proto_probs) > 0:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    val_item = float(val.max().item())\n                                else:\n                                    arr = np.asarray(val, dtype=np.float32)\n                                    if arr.size > 0:\n                                        val_item = float(np.max(arr))\n                                    else:\n                                        val_item = 0.5\n                                if np.isfinite(val_item):\n                                    pmax[0, t] = val_item\n                                else:\n                                    pmax[0, t] = 0.5\n                            except Exception:\n                                pmax[0, t] = 0.5\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] parse_proto_probs exception: {e}\")\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device,\n                            default: float = 0.0) -> torch.Tensor:\n        batch_size = max(1, int(batch_size))\n        seq_len = max(1, int(seq_len))\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n            if isinstance(mat, torch.Tensor):\n                m = mat.detach().to(device)\n                if m.dim() == 3:\n                    B, T, C = m.shape\n                    if B > 0 and T > 0 and C > 0:\n                        b_max = min(batch_size, B)\n                        t_max = min(seq_len, T)\n                        out[:b_max, :t_max] = m[:b_max, :t_max, 0]\n                elif m.dim() == 2:\n                    if m.size(0) == batch_size and m.size(1) > 0:\n                        t_max = min(seq_len, m.size(1))\n                        out[:, :t_max] = m[:, :t_max]\n                    elif batch_size == 1 and m.size(0) > 0:\n                        t_max = min(seq_len, m.size(0))\n                        out[0, :t_max] = m[:t_max, 0] if m.size(1) > 0 else m[:t_max]\n                elif m.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, m.size(0))\n                    out[0, :t_max] = m[:t_max]\n            elif isinstance(mat, (list, tuple, np.ndarray)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                            t_max = min(seq_len, row.size(0))\n                            for t in range(t_max):\n                                out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                                except Exception:\n                                    out[b, t] = float(default)\n                elif batch_size == 1 and len(mat) > 0:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            out[0, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                        except Exception:\n                            out[0, t] = float(default)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    pass\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor,\n                                    gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        try:\n            base = float(self.lambda_base.get(lambda_type, 0.2))\n            lam = base * (1.0 - pmax + 0.1) * (uncertainty + 0.1) * (gate + 0.1)\n            lam = torch.clamp(lam, min=0.01, max=float(max(0.01, self.lambda_max)))\n            lam = torch.where(torch.isfinite(lam), lam, torch.zeros_like(lam))\n            return lam\n        except Exception:\n            return torch.ones_like(pmax) * 0.1\n\n    def forward(self, h: torch.Tensor, domain_labels: Optional[torch.Tensor] = None, \n                global_step: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            return h, torch.tensor(0.0, device=dev)\n        \n        if global_step is not None:\n            self.current_step = max(0, int(global_step))\n        \n        B, T, H = h.size()\n        device = h.device\n\n        if domain_labels is not None:\n            try:\n                domain_labels = domain_labels.to(device).long()\n            except Exception:\n                try:\n                    domain_labels = domain_labels.long()\n                    domain_labels = domain_labels.to(device)\n                except Exception:\n                    domain_labels = torch.ones((B,), dtype=torch.long, device=device)\n            if domain_labels.dim() == 0:\n                domain_labels = domain_labels.unsqueeze(0).expand(B)\n            elif domain_labels.numel() == 1 and B > 1:\n                domain_labels = domain_labels.view(1).expand(B).contiguous()\n            elif domain_labels.size(0) != B:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[ASBN] Domain label size mismatch: {domain_labels.size(0)} vs batch {B}, using first label\")\n                domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n\n        try:\n            self._ensure_discriminators_on_device(device)\n            h_flat = h.view(B * T, H)\n\n            if domain_labels is not None:\n                domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n            else:\n                domain_expanded = torch.ones(B * T, dtype=torch.long, device=device)\n\n            source_mask = domain_expanded == 0\n            target_mask = domain_expanded == 1\n            h_normalized = h_flat.clone()\n\n            if source_mask.sum() >= 2:\n                try:\n                    h_normalized[source_mask] = self.bn_source(h_flat[source_mask])\n                except Exception:\n                    h_normalized[source_mask] = h_flat[source_mask]\n            elif source_mask.sum() == 1:\n                h_normalized[source_mask] = h_flat[source_mask]\n\n            if target_mask.sum() >= 2:\n                try:\n                    h_normalized[target_mask] = self.bn_target(h_flat[target_mask])\n                except Exception:\n                    h_normalized[target_mask] = h_flat[target_mask]\n            elif target_mask.sum() == 1:\n                h_normalized[target_mask] = h_flat[target_mask]\n\n            h_out = h_normalized.view(B, T, H)\n\n            domain_loss = torch.tensor(0.0, device=device)\n            \n            if self.training and _ENABLE_ASBN_TRAINING and self.current_step >= self.warmup_steps:\n                if domain_labels is not None:\n                    try:\n                        grl_alpha = self.get_grl_alpha(self.current_step)\n                        \n                        valid_indices = torch.arange(B * T, device=device)\n                        if valid_indices.numel() > 0:\n                            sel_emb = h_normalized[valid_indices]\n                            sel_labels = domain_expanded[valid_indices]\n                            \n                            if sel_emb.size(0) > 0:\n                                domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                                domain_logits = self.d_domain(domain_input).to(device)\n                                domain_loss = F.cross_entropy(domain_logits, sel_labels)\n                                \n                                with torch.no_grad():\n                                    domain_preds = torch.argmax(domain_logits, dim=1)\n                                    correct = (domain_preds == sel_labels).sum().item()\n                                    \n                                    source_mask_sel = sel_labels == 0\n                                    target_mask_sel = sel_labels == 1\n                                    \n                                    with self._stats_lock:\n                                        self.correct_domain += correct\n                                        self.total_samples += sel_labels.size(0)\n                                        self.domain_loss_accumulator += float(domain_loss.item()) * sel_labels.size(0)\n                                        self.asbn_loss_accumulator += float(domain_loss.item()) * sel_labels.size(0)\n                                        \n                                        if source_mask_sel.any():\n                                            source_correct = (domain_preds[source_mask_sel] == sel_labels[source_mask_sel]).sum().item()\n                                            self.correct_source += source_correct\n                                            self.total_source += source_mask_sel.sum().item()\n                                        \n                                        if target_mask_sel.any():\n                                            target_correct = (domain_preds[target_mask_sel] == sel_labels[target_mask_sel]).sum().item()\n                                            self.correct_target += target_correct\n                                            self.total_target += target_mask_sel.sum().item()\n                                    \n                                    if self.total_samples >= self.stats_reset_interval:\n                                        if _DEBUG_DISCOVERY:\n                                            stats = self.get_detailed_stats()\n                                            print(f\"\\n[ASBN-STATS] After {stats['num_updates']} samples:\")\n                                            print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                                            print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                                            print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                                            print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                                        self.reset_stats()\n                    except Exception as e:\n                        if _VERBOSE_LOGGING:\n                            print(f\"[ASBN] Domain loss computation failed: {e}\")\n\n            if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n                print(f\"[ASBN] Applied BN: {int(source_mask.sum())} source, {int(target_mask.sum())} target tokens\")\n\n            return h_out, domain_loss\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] Forward failed: {e}\")\n            return h, torch.tensor(0.0, device=device)\n\n    def forward_with_grl_simplified(self, h: torch.Tensor, proto_probs: Any, uncertainties: Any, gates: Any,\n                                   token_word_map: Optional[List[Dict[int, str]]] = None,\n                                   domain_labels: Optional[torch.Tensor] = None,\n                                   global_step: Optional[int] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        if global_step is not None:\n            self.current_step = max(0, int(global_step))\n        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n        if self.current_step < self.warmup_steps:\n            if _DEBUG_DISCOVERY and self.current_step % 100 == 0:\n                print(f\"[ASBN] Warmup: {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        device = h.device\n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n        B, T, H = h.size()\n\n        if domain_labels is not None:\n            try:\n                domain_labels = domain_labels.to(device).long()\n            except Exception:\n                try:\n                    domain_labels = domain_labels.long()\n                    domain_labels = domain_labels.to(device)\n                except Exception:\n                    domain_labels = torch.ones((B,), dtype=torch.long, device=device)\n            if domain_labels.dim() == 0:\n                domain_labels = domain_labels.unsqueeze(0).expand(B)\n            elif domain_labels.numel() == 1 and B > 1:\n                domain_labels = domain_labels.view(1).expand(B).contiguous()\n            elif domain_labels.size(0) != B:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[ASBN-GRL] Domain label size mismatch: {domain_labels.size(0)} vs batch {B}\")\n                domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n        batch_indices = torch.arange(B, device=device).unsqueeze(1).expand(B, T)\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                if token_str in self.special_tokens:\n                                    sel_mask[b, t] = False\n                                elif len(token_str.strip()) == 0:\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    try:\n                        print(\"[ASBN] Token filtering failed:\", traceback.format_exc().splitlines()[-1])\n                    except Exception:\n                        pass\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        batch_idx = batch_indices.view(-1)[sel_idx] if sel_idx.numel() > 0 else torch.tensor([], dtype=torch.long, device=device)\n        if sel_idx.numel() == 0:\n            if _DEBUG_DISCOVERY:\n                print(\"[ASBN-GRL] No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n        seq_len_feature = float(T) / float(max(int(_MAX_LENGTH), 1))\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n        grl_alpha = self.get_grl_alpha(global_step)\n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n        freq_logits = self.d_freq(freq_input_grl).to(device)\n        ctx_logits = self.d_ctx(ctx_input_grl).to(device)\n        xl_logits = self.d_xl(xl_input_grl).to(device)\n        freq_label = (pmax_flat > self.freq_threshold).long().to(device)\n        ctx_label = (U_flat < self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat > self.gate_threshold).long().to(device)\n        \n        if freq_logits.size(0) == 0 or freq_label.size(0) == 0:\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n        \n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        mean_weighted = torch.mean(weighted) if weighted.numel() > 0 else torch.tensor(0.0, device=device)\n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n        if domain_labels is not None:\n            try:\n                if domain_labels.dim() == 0:\n                    domain_labels = domain_labels.unsqueeze(0)\n                if domain_labels.size(0) == 1 and B > 1:\n                    domain_labels = domain_labels.expand(B)\n                elif domain_labels.size(0) != B:\n                    domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n                if batch_idx.numel() > 0 and batch_idx.max() < domain_labels.size(0):\n                    domain_flat = domain_labels[batch_idx].to(device).long()\n                else:\n                    domain_flat = torch.tensor([], dtype=torch.long, device=device)\n                \n                if domain_flat.numel() > 0:\n                    domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                    domain_logits = self.d_domain(domain_input).to(device)\n                    domain_loss = F.cross_entropy(domain_logits, domain_flat)\n                    with torch.no_grad():\n                        domain_preds = torch.argmax(domain_logits, dim=1)\n                        correct = (domain_preds == domain_flat).sum().item()\n                        domain_accuracy = (domain_preds == domain_flat).float().mean() if domain_flat.numel() > 0 else torch.tensor(0.0, device=device)\n                        \n                        source_mask = domain_flat == 0\n                        target_mask = domain_flat == 1\n                        \n                        with self._stats_lock:\n                            self.correct_domain += correct\n                            self.total_samples += domain_flat.size(0)\n                            self.domain_loss_accumulator += float(domain_loss.item()) * domain_flat.size(0)\n                            \n                            if source_mask.any():\n                                source_correct = (domain_preds[source_mask] == domain_flat[source_mask]).sum().item()\n                                self.correct_source += source_correct\n                                self.total_source += source_mask.sum().item()\n                            \n                            if target_mask.any():\n                                target_correct = (domain_preds[target_mask] == domain_flat[target_mask]).sum().item()\n                                self.correct_target += target_correct\n                                self.total_target += target_mask.sum().item()\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN-GRL] Domain loss failed: {e}\")\n        encoder_loss = self.encoder_grl_scale * (mean_weighted + domain_loss)\n        try:\n            with torch.no_grad():\n                with self._stats_lock:\n                    self.asbn_loss_accumulator += float(encoder_loss.item()) * sel_emb.size(0) if isinstance(encoder_loss, torch.Tensor) else 0.0\n                    \n                    if self.total_samples >= self.stats_reset_interval:\n                        if _DEBUG_DISCOVERY:\n                            stats = self.get_detailed_stats()\n                            print(f\"\\n[ASBN-STATS] After {stats['num_updates']} samples:\")\n                            print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                            print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                            print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                            print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                            print(f\"  ASBN loss: {stats['asbn_loss']:.4f}\")\n                        self.reset_stats()\n        except Exception:\n            pass\n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            try:\n                print(f\"\\n[ASBN-STEP-{self.current_step}]\")\n                print(f\"  GRL alpha: {grl_alpha:.3f}\")\n                print(f\"  Encoder loss: {float(encoder_loss.item() if isinstance(encoder_loss, torch.Tensor) else encoder_loss):.4f}\")\n                print(f\"  Domain loss: {float(domain_loss.item() if isinstance(domain_loss, torch.Tensor) else domain_loss):.4f}\")\n                print(f\"  Domain acc: {float(domain_accuracy.item() if isinstance(domain_accuracy, torch.Tensor) else domain_accuracy):.2%}\")\n            except Exception:\n                pass\n        return encoder_loss, mean_weighted, domain_loss, domain_accuracy\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[ASBN-TEST] Testing ASBN module\")\n        print(\"=\" * 60)\n        try:\n            try:\n                device = next(self.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cpu\")\n            h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n            domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n            h_out, domain_loss = self.forward(h, domain_labels, global_step=self.warmup_steps + 1)\n            assert h_out.shape == h.shape, \"Forward output shape mismatch\"\n            print(\"  forward() passed\")\n            proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n            uncertainties = torch.rand(batch_size, seq_len, device=device)\n            gates = torch.rand(batch_size, seq_len, device=device)\n            self.train()\n            self.current_step = self.warmup_steps + 1\n            enc_loss, adv_loss, dom_loss, dom_acc = self.forward_with_grl_simplified(\n                h,\n                proto_probs,\n                uncertainties,\n                gates,\n                domain_labels=domain_labels,\n                global_step=self.current_step,\n            )\n            assert (isinstance(enc_loss, torch.Tensor) and enc_loss.item() >= 0.0) or (not isinstance(enc_loss, torch.Tensor)), \"Encoder loss negative\"\n            assert 0.0 <= (float(dom_acc.item()) if isinstance(dom_acc, torch.Tensor) else float(dom_acc)) <= 1.0, \"Domain accuracy out of range\"\n            print(\"  forward_with_grl_simplified() passed\")\n            stats = self.get_detailed_stats()\n            assert \"domain_loss\" in stats, \"Missing domain_loss in stats\"\n            print(\"  Statistics tracking passed\")\n            print(\"\\nAll ASBN tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n        except Exception as e:\n            print(f\"\\nASBN test failed: {e}\")\n            traceback.print_exc()\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN Ready (COMPLETE FIXED - All 10 Errors Resolved)\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Embed dimension: Dynamically set\")\nprint(f\"  - GRL alpha: {_GRL_ALPHA_START} -> {_GRL_ALPHA_END} over {_GRL_ALPHA_STEPS} steps\")\nprint(f\"  - GRL schedule: {_GRL_ALPHA_SCHEDULE}\")\nprint(f\"  - ASBN training: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\nprint(f\"  - Source language: {_SOURCE_LANGUAGE}\")\nprint(f\"  - Max length: {_MAX_LENGTH}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"XrNq18UsH4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG (TRANSLATION RATIONALE GENERATION) - FIXED\n# ==============================================================================\n\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threading\nimport time\n\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\n    if _TRG_EVIDENCE_K <= 0:\n        _TRG_EVIDENCE_K = 3\nexcept (NameError, ValueError, TypeError):\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\n    if _TRG_GEN_EMBED <= 0:\n        _TRG_GEN_EMBED = 64\nexcept (NameError, ValueError, TypeError):\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\n    if _MAX_SILVER_BUFFER <= 0:\n        _MAX_SILVER_BUFFER = 50\nexcept (NameError, ValueError, TypeError):\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\n_CELL5_PROTOID_MINUS1_COUNT = 0\n_CELL5_PROTOID_NONE_COUNT = 0\n_CELL5_MISSING_PROTO_LOOKUP_COUNT = 0\n_CELL5_EXPLANATION_SUCCESS_COUNT = 0\n_CELL5_EXPLANATION_SKIP_COUNT = 0\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept NameError:\n    _DEBUG_TIMING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TAU_HIGH = float(TAU_HIGH)\n    if _TAU_HIGH < 0 or _TAU_HIGH > 1:\n        _TAU_HIGH = 0.85\nexcept (NameError, ValueError, TypeError):\n    _TAU_HIGH = 0.85\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\n    if _TAU_LOW < 0 or _TAU_LOW > 1:\n        _TAU_LOW = 0.25\nexcept (NameError, ValueError, TypeError):\n    _TAU_LOW = 0.25\n\ntry:\n    _TAU_ACCEPT = float(TAU_ACCEPT)\n    if _TAU_ACCEPT < 0 or _TAU_ACCEPT > 1:\n        _TAU_ACCEPT = 0.80\nexcept (NameError, ValueError, TypeError):\n    _TAU_ACCEPT = 0.80\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(\n        globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", _TAU_LOW)\n    )\n    if _TRG_UNCERTAINTY_THRESHOLD < 0 or _TRG_UNCERTAINTY_THRESHOLD > 1:\n        _TRG_UNCERTAINTY_THRESHOLD = _TAU_LOW\nexcept Exception:\n    _TRG_UNCERTAINTY_THRESHOLD = 0.25\n\ntry:\n    _TRG_SPAN_THRESHOLD = float(globals().get(\"SPAN_THRESHOLD\", 0.05))\n    if _TRG_SPAN_THRESHOLD < 0 or _TRG_SPAN_THRESHOLD > 1:\n        _TRG_SPAN_THRESHOLD = 0.05\nexcept Exception:\n    _TRG_SPAN_THRESHOLD = 0.05\n\ntry:\n    _TRG_TEMPERATURE = float(TRG_TEMPERATURE)\n    if _TRG_TEMPERATURE <= 0:\n        _TRG_TEMPERATURE = 1.0\nexcept (NameError, ValueError, TypeError):\n    _TRG_TEMPERATURE = 1.0\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = (\n        int(MAX_EXPLANATIONS_PER_SENTENCE)\n        if \"MAX_EXPLANATIONS_PER_SENTENCE\" in globals()\n        else 10\n    )\n    if _MAX_EXPLANATIONS_PER_SENTENCE <= 0:\n        _MAX_EXPLANATIONS_PER_SENTENCE = 10\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\n_TRG_PUNCT_SET = set(\".,;:!?\\\\\\\"\\'-()[]{}/\")\n\n\ndef _fallback_is_valid_token(\n    token: str, special_tokens: set, tokenizer=None, language: str = \"bn\"\n) -> bool:\n    if token is None:\n        return False\n\n    if not isinstance(token, str):\n        try:\n            token = str(token)\n        except Exception:\n            return False\n\n    token = token.strip()\n    if not token:\n        return False\n\n    if token in special_tokens:\n        return False\n\n    clean = (\n        token.replace(\"▁\", \"\")\n        .replace(\"Ġ\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"@@\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if len(clean) < 2:\n        return False\n\n    if not any(c.isalpha() for c in clean):\n        return False\n\n    if all(c in _TRG_PUNCT_SET for c in clean):\n        return False\n\n    if clean.isdigit():\n        return False\n\n    return True\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    if not isinstance(raw_token, str):\n        return False\n\n    try:\n        if token_word_map is not None and isinstance(token_word_map, dict):\n            if idx in token_word_map:\n                w = token_word_map[idx]\n                if isinstance(w, str) and w.strip():\n                    return True\n\n        if raw_token.startswith(\"▁\") or raw_token.startswith(\"Ġ\"):\n            return True\n\n        clean = (\n            raw_token.replace(\"▁\", \"\")\n            .replace(\"Ġ\", \"\")\n            .replace(\"##\", \"\")\n            .replace(\"@@\", \"\")\n            .replace(\"</w>\", \"\")\n            .strip()\n        )\n\n        if len(clean) < 2:\n            return False\n\n        if all(ch in \".,;:!?\\\"'()[]{}-/\" for ch in clean):\n            return False\n\n        if any(c.isalpha() for c in clean):\n            return True\n\n        return False\n\n    except Exception:\n        return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on: '{evidence}'.   \"\n                \"Pattern matches learned data.   {alternatives_text}\"\n            ),\n            \"medium_confidence\": (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty.   {alternatives_text}\"\n            ),\n            \"low_confidence\": (\n                \"Uncertain; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. {alternatives_text} Review recommended.\"\n            ),\n            \"fallback\": (\"Token '{token}' analyzed.   Context: '{evidence}'.\"),\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n\n        token = (\n            str(evidence.get(\"token\", \"unknown\"))\n            .replace(\"▁\", \"\")\n            .replace(\"Ġ\", \"\")\n            .replace(\"##\", \"\")\n            .replace(\"@@\", \"\")\n            .replace(\"</w>\", \"\")\n        )\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = (\n            \", \".join(\n                [\n                    str(tok).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\")\n                    for tok in evidence_tokens[:_TRG_EVIDENCE_K]\n                ]\n            )\n            or \"limited context\"\n        )\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)}.\"\n\n        if confidence >= _TAU_ACCEPT:\n            template_key = \"high_confidence\"\n        elif confidence >= _TRG_UNCERTAINTY_THRESHOLD:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(\n            template_key, self.explanation_templates[\"fallback\"]\n        )\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token '{token}' -> '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    try:\n                        self.special_tokens = get_cached_special_tokens(tokenizer)\n                    except Exception:\n                        self.special_tokens = set()\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor,\n    ) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n\n        seq_len = (\n            len(tgt_preds)\n            if isinstance(tgt_preds, list)\n            else int(tgt_preds.size(0))\n        )\n        if span_end > seq_len:\n            return None\n\n        if span_start >= span_end:\n            return None\n\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n\n        if token_idx >= seq_len:\n            return None\n\n        try:\n            evidence_tokens: List[str] = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        evidence_tokens.append(f\"token_{i}\")\n\n            return evidence_tokens if evidence_tokens else None\n\n        except Exception:\n            return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n    ) -> Dict:\n        if not isinstance(tokens, list):\n            return self._create_fallback_evidence(token_idx, [])\n\n        if not isinstance(token_idx, int):\n            return self._create_fallback_evidence(0, tokens)\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(\n                max(0, min(token_idx, len(tokens) - 1)) if len(tokens) > 0 else 0, tokens\n            )\n\n        raw_token = tokens[token_idx]\n\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n\n            evidence_tokens: Optional[List[str]] = None\n            if decoder_attention is not None and isinstance(\n                decoder_attention, torch.Tensor\n            ):\n                try:\n                    if decoder_attention.dim() == 4:\n                        if (\n                            decoder_attention.size(0) > 1\n                            and decoder_attention.size(1) > 1\n                        ):\n                            attn_avg = decoder_attention.mean(dim=(0, 1))\n                        elif decoder_attention.size(0) > 1:\n                            attn_avg = decoder_attention.mean(dim=1)\n                        else:\n                            attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 3:\n                        attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 2:\n                        if token_idx < decoder_attention.size(0):\n                            vec = decoder_attention[token_idx]\n                        else:\n                            vec = decoder_attention.reshape(-1)\n                    elif decoder_attention.dim() == 1:\n                        vec = decoder_attention\n                    else:\n                        vec = None\n\n                    if vec is not None and vec.numel() > 0:\n                        k = min(5, int(vec.size(0)))\n                        if k > 0:\n                            top_k_indices = torch.topk(vec, k=k).indices.cpu().numpy()\n                            evidence_tokens = []\n                            for i in top_k_indices:\n                                if i < len(tokens) and i != token_idx:\n                                    evidence_tokens.append(tokens[int(i)])\n\n                except Exception:\n                    evidence_tokens = None\n\n            if evidence_tokens is None:\n                evidence_tokens = self._extract_context_window(\n                    token_idx, tokens, token_word_map\n                )\n\n            seen: Dict[str, bool] = {}\n            dedup_evidence: List[str] = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen[t] = True\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:_TRG_EVIDENCE_K]\n\n            top_senses = self._compute_sense_alternatives_fast(\n                proto_probs, temperature=_TRG_TEMPERATURE\n            )\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            if (\n                token_word_map\n                and token_idx in token_word_map\n                and isinstance(token_word_map[token_idx], str)\n                and token_word_map[token_idx].strip()\n            ):\n                token_value = token_word_map[token_idx]\n            else:\n                token_value = raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"[TRG] Evidence error @ {token_idx}: {e}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _extract_context_window(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        token_word_map: Optional[dict],\n    ) -> List[str]:\n        context_window = 2\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens: List[str] = []\n\n        for i in range(start_idx, end_idx):\n            if i == token_idx or i >= len(tokens):\n                continue\n            rtok = tokens[i]\n            clean_token = (\n                str(rtok)\n                .replace(\"▁\", \"\")\n                .replace(\"Ġ\", \"\")\n                .replace(\"##\", \"\")\n                .replace(\"@@\", \"\")\n                .replace(\"</w>\", \"\")\n                .strip()\n            )\n\n            if not _is_word_start(rtok, token_word_map, i):\n                continue\n\n            if _has_is_valid_token:\n                try:\n                    ok = is_valid_token(\n                        rtok,\n                        self.special_tokens,\n                        self.tokenizer,\n                        language=self.language,\n                    )\n                except Exception:\n                    ok = _fallback_is_valid_token(\n                        rtok, self.special_tokens, self.tokenizer, self.language\n                    )\n            else:\n                ok = _fallback_is_valid_token(\n                    rtok, self.special_tokens, self.tokenizer, self.language\n                )\n\n            if ok and len(clean_token) > 0:\n                if (\n                    token_word_map\n                    and isinstance(token_word_map.get(i, \"\"), str)\n                    and token_word_map[i].strip()\n                ):\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    evidence_tokens.append(clean_token)\n\n        return evidence_tokens\n\n    def _safe_extract_proto_probs(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all and len(pp_all) > 0:\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return row[token_idx].detach().cpu().flatten()\n                    return row.detach().cpu().flatten()\n                if isinstance(row, (list, tuple)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().flatten()\n                        if isinstance(val, (list, tuple, np.ndarray)):\n                            return torch.as_tensor(\n                                val, dtype=torch.float32\n                            ).flatten()\n                        return torch.tensor([float(val)], dtype=torch.float32)\n                    if len(row) > 0:\n                        maybe = row[0]\n                        if isinstance(maybe, torch.Tensor):\n                            return maybe.detach().cpu().flatten()\n        except Exception:\n            pass\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    elif row.ndim == 1 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    else:\n                        return 0.0\n                elif isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    span_val = (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n                else:\n                    return 0.0\n\n                if span_val < 0.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Negative span {span_val:.3f} -> 0.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 0.0\n                if span_val > 1.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Span {span_val:.3f} > 1.0 -> 1.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 1.0\n\n                return span_val\n\n        except Exception:\n            pass\n        return 0.0\n\n    def compute_span(self, sense_probs) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n\n            if len(probs) < 2:\n                return 0.0\n\n            sorted_probs = sorted([float(p) for p in probs], reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n\n            return max(0.0, min(1.0, span))\n\n        except Exception:\n            return 0.0\n\n    def _compute_sense_alternatives_fast(\n        self, proto_probs: torch.Tensor, temperature: float = 1.0\n    ) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n\n            probs = proto_probs.flatten().float()\n            probs = torch.clamp(probs, min=1e-10, max=1.0)\n\n            temperature = max(0.01, float(temperature))\n\n            if temperature != 1.0 and probs.numel() > 1:\n                probs = probs / (probs.sum() + 1e-10)\n                log_probs = torch.log(probs + 1e-10)\n                scaled_log_probs = log_probs / temperature\n                probs = F.softmax(scaled_log_probs, dim=0)\n\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [\n                    (f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item()))\n                    for i in range(top_k)\n                ]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(\n        self, token_idx: int, tokens: List[str]\n    ) -> Dict:\n        if isinstance(tokens, list) and 0 <= token_idx < len(tokens):\n            token = tokens[token_idx]\n        else:\n            token = \"UNK\"\n\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"discovered_homographs\"):\n                    homograph_tokens = set(self.dscd_module.discovered_homographs)\n                elif hasattr(self.dscd_module, \"prototype_stores\"):\n                    for token, store in self.dscd_module.prototype_stores.items():\n                        if hasattr(store, \"size\") and callable(store.size):\n                            try:\n                                if store.size() >= 2:\n                                    clean = (\n                                        str(token)\n                                        .replace(\"▁\", \"\")\n                                        .replace(\"Ġ\", \"\")\n                                        .replace(\"##\", \"\")\n                                        .replace(\"@@\", \"\")\n                                        .replace(\"</w>\", \"\")\n                                        .strip()\n                                    )\n                                    homograph_tokens.add(clean)\n                            except Exception:\n                                pass\n                        elif hasattr(store, \"centroids\"):\n                            try:\n                                if len(store.centroids) >= 2:\n                                    clean = (\n                                        str(token)\n                                        .replace(\"▁\", \"\")\n                                        .replace(\"Ġ\", \"\")\n                                        .replace(\"##\", \"\")\n                                        .replace(\"@@\", \"\")\n                                        .replace(\"</w>\", \"\")\n                                        .strip()\n                                    )\n                                    homograph_tokens.add(clean)\n                            except Exception:\n                                pass\n        except Exception:\n            pass\n        return homograph_tokens\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(\n        self,\n        embed_dim: Optional[int] = None,\n        tokenizer=None,\n        language: str = \"bn\",\n        dscd_module=None,\n    ):\n        super().__init__()\n        self.embed_dim = max(1, int(embed_dim) if embed_dim is not None else int(_TRG_GEN_EMBED))\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n\n        if dscd_module is None:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"[TRG] No DSCD module - homograph detection disabled\")\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    try:\n                        self.special_tokens = get_cached_special_tokens(tokenizer)\n                    except Exception:\n                        self.special_tokens = set()\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(\n            tokenizer, language=language, dscd_module=dscd_module\n        )\n\n        self.silver_buffer = deque(maxlen=max(1, int(_MAX_SILVER_BUFFER)))\n        self._silver_lock = threading.Lock()\n\n        self.stats_reset_interval = 1000\n        self.explanations_generated = 0\n        self.high_confidence_explanations = 0\n        self.low_confidence_explanations = 0\n        self.empty_evidence_count = 0\n        self.total_evidence_tokens = 0\n        self.tokens_filtered_word_start = 0\n        self.tokens_filtered_validity = 0\n        self.tokens_filtered_ambiguity = 0\n        self.dscd_homographs_explained = 0\n        self._stats_lock = threading.Lock()\n\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(\"[TRG] Initialized:\")\n            print(f\"  - Uncertainty: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\n            print(f\"  - Span: {_TRG_SPAN_THRESHOLD:.2f}\")\n            print(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\n            print(\"  - Mode: DATA-DRIVEN\")\n\n    def _update_stats(self, evidence: Dict, is_dscd_homograph: bool = False) -> None:\n        with self._stats_lock:\n            self.explanations_generated += 1\n\n            if is_dscd_homograph:\n                self.dscd_homographs_explained += 1\n\n            if not evidence.get(\"evidence_tokens\"):\n                self.empty_evidence_count += 1\n            else:\n                self.total_evidence_tokens += len(\n                    evidence[\"evidence_tokens\"]\n                )\n\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= _TAU_ACCEPT:\n                self.high_confidence_explanations += 1\n            elif confidence < _TRG_UNCERTAINTY_THRESHOLD:\n                self.low_confidence_explanations += 1\n\n            if self.explanations_generated >= self.stats_reset_interval:\n                if _DEBUG_DISCOVERY:\n                    current_stats = self.get_statistics()\n                    print(\n                        f\"\\n[TRG-STATS] After {self.explanations_generated}:\"\n                    )\n                    print(\n                        f\"  High conf: {current_stats['high_confidence_rate']:.2%}\"\n                    )\n                    print(\n                        f\"  DSCD: {current_stats['dscd_homograph_rate']:.2%}\"\n                    )\n                self.reset_statistics()\n\n    def _add_to_silver_buffer(\n        self, evidence: Dict, explanation: str, tokens: List[str]\n    ) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n\n            with self._silver_lock:\n                self.silver_buffer.append(entry)\n\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        is_dscd_homograph: bool = False,\n    ) -> Tuple[str, Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx,\n                tokens,\n                dscd_outputs,\n                token_word_map=token_word_map,\n                decoder_attention=decoder_attention,\n            )\n\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence, is_dscd_homograph=is_dscd_homograph)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception:\n            return \"\", {}\n\n    @staticmethod\n    def _to_list_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n\n        try:\n            if isinstance(x, torch.Tensor):\n                if x.ndim == 0:\n                    return [float(x.item())]\n                if x.ndim == 1:\n                    return [float(v.item()) for v in x]\n                if x.ndim == 2:\n                    return [float(v.item()) for v in x[0]]\n                return [float(v.item()) for v in x.flatten()]\n\n            if isinstance(x, (list, tuple)):\n                out: List[float] = []\n                for v in x:\n                    if isinstance(v, torch.Tensor):\n                        if v.ndim == 0:\n                            out.append(float(v.item()))\n                        elif v.numel() > 0:\n                            out.append(float(v.flatten()[0].item()))\n                        else:\n                            out.append(0.0)\n                    elif isinstance(v, (int, float, np.number)):\n                        out.append(float(v))\n                    else:\n                        try:\n                            out.append(float(v))\n                        except Exception:\n                            out.append(0.0)\n                return out\n\n            if isinstance(v, (int, float, np.number)):\n                return [float(x)]\n\n            return [float(x)]\n\n        except Exception:\n            return []\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        uncertainty_threshold: Optional[float] = None,\n        span_threshold: Optional[float] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        max_explanations: int = _MAX_EXPLANATIONS_PER_SENTENCE,\n    ) -> List[Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n        else:\n            uncertainty_threshold = float(uncertainty_threshold)\n\n        if span_threshold is None:\n            span_threshold = float(_TRG_SPAN_THRESHOLD)\n        else:\n            span_threshold = float(span_threshold)\n\n        max_explanations = max(1, int(max_explanations))\n\n        explanations: List[Dict] = []\n\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n\n            if not U_all or not U_all[0]:\n                return explanations\n\n            U = self._to_list_helper(U_all[0])\n            S = (\n                self._to_list_helper(S_all[0])\n                if S_all and S_all[0]\n                else [0.0] * len(U)\n            )\n\n            if len(S) < len(U):\n                S.extend([0.0] * (len(U) - len(S)))\n\n            if not U:\n                return explanations\n\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n\n            for idx in range(min(len(tokens), len(U))):\n                tok = tokens[idx]\n                clean_tok = (\n                    tok.replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                )\n\n                if not _is_word_start(tok, token_word_map, idx):\n                    with self._stats_lock:\n                        self.tokens_filtered_word_start += 1\n                    continue\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(\n                            tok,\n                            self.special_tokens,\n                            self.tokenizer,\n                            language=self.language,\n                        )\n                    except Exception:\n                        valid = _fallback_is_valid_token(\n                            tok, self.special_tokens, self.tokenizer, self.language\n                        )\n                else:\n                    valid = _fallback_is_valid_token(\n                        tok, self.special_tokens, self.tokenizer, self.language\n                    )\n\n                if not valid:\n                    with self._stats_lock:\n                        self.tokens_filtered_validity += 1\n                    continue\n\n                u = float(U[idx]) if idx < len(U) else 0.5\n                s = float(S[idx]) if idx < len(S) else 0.0\n\n                in_dscd = clean_tok in dscd_homographs\n\n                if in_dscd:\n                    priority = 1\n                elif u <= uncertainty_threshold and s >= span_threshold:\n                    priority = 2\n                elif u <= uncertainty_threshold:\n                    priority = 3\n                elif s >= span_threshold:\n                    priority = 4\n                else:\n                    with self._stats_lock:\n                        self.tokens_filtered_ambiguity += 1\n                    continue\n\n                candidates.append((idx, u, s, clean_tok, priority, idx))\n\n            if not candidates:\n                return explanations\n\n            candidates.sort(key=lambda t: (t[4], t[1], -t[2], t[5]))\n\n            for (token_idx, u, s, clean_tok, priority, _) in candidates[\n                : max_explanations\n            ]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=token_word_map,\n                        decoder_attention=decoder_attention,\n                        is_dscd_homograph=(priority == 1),\n                    )\n                    if explanation_text and evidence:\n                        explanations.append(\n                            {\n                                \"token_idx\": token_idx,\n                                \"token\": (\n                                    token_word_map[token_idx]\n                                    if token_word_map\n                                    and token_idx in token_word_map\n                                    else tokens[token_idx]\n                                    .replace(\"▁\", \"\")\n                                    .replace(\"Ġ\", \"\")\n                                    .replace(\"##\", \"\")\n                                    .replace(\"@@\", \"\")\n                                    .replace(\"</w>\", \"\")\n                                ),\n                                \"explanation\": explanation_text,\n                                \"uncertainty\": u,\n                                \"span\": s,\n                                \"dscd_discovered\": (priority == 1),\n                                \"priority\": priority,\n                            }\n                        )\n                except Exception:\n                    continue\n\n        except Exception:\n            pass\n\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self._stats_lock:\n            total = max(self.explanations_generated, 1)\n            if self.explanations_generated > 0:\n                avg_evidence_tokens = (\n                    self.total_evidence_tokens / total\n                )\n            else:\n                avg_evidence_tokens = 0.0\n\n            return {\n                \"explanations_generated\": self.explanations_generated,\n                \"high_confidence_explanations\": self.high_confidence_explanations,\n                \"low_confidence_explanations\": self.low_confidence_explanations,\n                \"empty_evidence_count\": self.empty_evidence_count,\n                \"total_evidence_tokens\": self.total_evidence_tokens,\n                \"tokens_filtered_word_start\": self.tokens_filtered_word_start,\n                \"tokens_filtered_validity\": self.tokens_filtered_validity,\n                \"tokens_filtered_ambiguity\": self.tokens_filtered_ambiguity,\n                \"dscd_homographs_explained\": self.dscd_homographs_explained,\n                \"high_confidence_rate\": self.high_confidence_explanations / total,\n                \"low_confidence_rate\": self.low_confidence_explanations / total,\n                \"empty_evidence_rate\": self.empty_evidence_count / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.dscd_homographs_explained / total,\n            }\n\n    def reset_statistics(self) -> None:\n        with self._stats_lock:\n            self.explanations_generated = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.empty_evidence_count = 0\n            self.total_evidence_tokens = 0\n            self.tokens_filtered_word_start = 0\n            self.tokens_filtered_validity = 0\n            self.tokens_filtered_ambiguity = 0\n            self.dscd_homographs_explained = 0\n\n    def clear_silver_buffer(self) -> None:\n        with self._silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[TRG-TEST] Testing\")\n        print(\"=\" * 60)\n\n        if not _ENABLE_TRG_INFERENCE:\n            print(\"TRG inference disabled, enabling for test...\")\n\n        try:\n            tokens = [\"▁আমি\", \"▁কল\", \"▁বন্ধ\", \"▁করেছি\", \"।\"]\n\n            dscd_outputs = {\n                \"proto_probs\": [[torch.tensor([0.6, 0.4]) for _ in tokens]],\n                \"uncertainties\": [[0.1, 0.5, 0.2, 0.1, 0.0]],\n                \"span_preds\": [[0.05, 0.3, 0.1, 0.05, 0.0]],\n                \"gates\": [[0.2, 0.8, 0.3, 0.2, 0.0]],\n            }\n\n            token_word_map = {\n                0: \"আমি\",\n                1: \"কল\",\n                2: \"বন্ধ\",\n                3: \"করেছি\",\n                4: \"।\",\n            }\n\n            self.eval()\n\n            explanations = self.process_sentence_for_explanations(\n                tokens=tokens,\n                dscd_outputs=dscd_outputs,\n                token_word_map=token_word_map,\n                max_explanations=3,\n            )\n\n            print(f\"  Generated {len(explanations)} explanations\")\n\n            if len(explanations) > 0:\n                for i, expl in enumerate(explanations, 1):\n                    print(\n                        f\"    {i}. '{expl['token']}' (u={expl['uncertainty']:.2f})\"\n                    )\n\n            stats = self.get_statistics()\n            print(f\"  Stats: {stats['explanations_generated']} total\")\n\n            self.reset_statistics()\n            stats_after = self.get_statistics()\n            assert stats_after[\"explanations_generated\"] == 0\n            print(\"  Reset OK\")\n\n            print(\"\\nAll tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n\n        except Exception as e:\n            print(f\"\\nTest failed: {e}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Ready (DATA-DRIVEN) - FIXED\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\nprint(f\"  - Span threshold: {_TRG_SPAN_THRESHOLD:.2f}\")\nprint(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\nprint(f\"  - TAU_HIGH: {_TAU_HIGH:.2f}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW:.2f}\")\nprint(f\"  - TAU_ACCEPT: {_TAU_ACCEPT:.2f}\")\nprint(f\"  - Max explanations: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(f\"  - Evidence K: {_TRG_EVIDENCE_K}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"svk-wKO7H4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TATN MODEL (COMPLETE INTEGRATION - ALL FIXES APPLIED)\n# ==============================================================================\n\nfrom typing import List, Dict, Optional, Any, Tuple\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\nimport signal\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            result = int(val)\n            if result <= 0 and default > 0:\n                return default\n            return result\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            result = float(val)\n            if 0 <= default <= 1:\n                if result < 0 or result > 1:\n                    return default\n            elif default >= 0:\n                if result < 0:\n                    return default\n            return result\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return bool(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n_DSCD_BUFFER_SIZE = max(1, _get_int_global(\"DSCD_BUFFER_SIZE\", 50))\n_DSCD_MAX_PROTOS = max(1, _get_int_global(\"DSCD_MAX_PROTOS\", 8))\n_DSCD_N_MIN = max(1, _get_int_global(\"DSCD_N_MIN\", 5))\n_DSCD_DISPERSION_THRESHOLD = max(0.0, min(1.0, _get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.50)))\n\n_ENABLE_ASBN_TRAINING = _get_bool_global(\"ENABLE_ASBN_TRAINING\", True)\n_ENABLE_TRG_INFERENCE = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\n_MEMORY_CLEANUP_FREQUENCY = max(0, _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 2000))\n\n_NUM_GPUS = max(\n    1,\n    _get_int_global(\n        \"NUM_GPUS\",\n        torch.cuda.device_count() if torch.cuda.is_available() else 1,\n    ),\n)\n_USE_GC = _get_bool_global(\"GRADIENT_CHECKPOINTING\", False)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\n    \"DSCD_ENABLE_TRAINING_CLUSTERING\", True\n)\n\n_LAMBDA_ASBN = max(0.0, _get_float_global(\"LAMBDA_ASBN\", 0.05))\n_LAMBDA_DSCD = max(0.0, _get_float_global(\"LAMBDA_DSCD\", 0.15))\n\n_VERBOSE_LOGGING = _get_bool_global(\"VERBOSE_LOGGING\", False)\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\n_PERIODIC_DISCOVERY_FREQUENCY = max(1, _get_int_global(\n    \"PERIODIC_DISCOVERY_FREQUENCY\", 150\n))\n_VALIDATION_CHECK_INTERVAL = max(1, _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 500))\n\n_SPAN_THRESHOLD = max(0.0, min(1.0, _get_float_global(\"SPAN_THRESHOLD\", 0.05)))\n_UNCERTAINTY_THRESHOLD = max(0.0, min(1.0, _get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.25)))\n\n_TRG_UNCERTAINTY_THRESHOLD = max(0.0, min(1.0, _get_float_global(\n    \"TRG_UNCERTAINTY_THRESHOLD\", _get_float_global(\"TAU_LOW\", 0.25)\n)))\n_TAU_LOW = max(0.0, min(1.0, _get_float_global(\"TAU_LOW\", 0.25)))\n\n_TRAIN_DOMAIN = _get_int_global(\"TRAIN_DOMAIN\", 0)\n_TEST_DOMAIN = _get_int_global(\"TEST_DOMAIN\", 1)\n_USE_DOMAIN_LABELS = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    _M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_BN_TOKEN_ID = 128025\n\n_LABEL_SMOOTHING = max(0.0, min(1.0, _get_float_global(\"LABEL_SMOOTHING\", 0.1)))\n_DECODER_DROPOUT = max(0.0, min(1.0, _get_float_global(\"DECODER_DROPOUT\", 0.1)))\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        if isinstance(enc_output[0], torch.Tensor):\n            return enc_output[0]\n    return None\n\ndef build_token_word_map_sentencepiece(input_ids: torch.Tensor, tokenizer) -> List[Dict[int, str]]:\n    batch_word_maps = []\n    \n    for batch_idx in range(input_ids.size(0)):\n        tokens = tokenizer.convert_ids_to_tokens(input_ids[batch_idx].tolist())\n        word_map = {}\n        current_word = \"\"\n        word_start_idx = 0\n        \n        for i, token in enumerate(tokens):\n            if token in ['<s>', '</s>', '<pad>', '<unk>', '']:\n                word_map[i] = None\n                continue\n            \n            if token.startswith('▁'):\n                if current_word:\n                    clean_word = current_word.replace('▁', '').strip()\n                    if clean_word:\n                        for j in range(word_start_idx, min(i, len(tokens))):\n                            word_map[j] = clean_word\n                \n                current_word = token\n                word_start_idx = i\n            else:\n                current_word += token\n        \n        if current_word:\n            clean_word = current_word.replace('▁', '').strip()\n            if clean_word:\n                for j in range(word_start_idx, len(tokens)):\n                    word_map[j] = clean_word\n        \n        batch_word_maps.append(word_map)\n    \n    return batch_word_maps\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int,\n) -> Dict[str, Any]:\n    if not isinstance(device, torch.device):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    defaults = {\n        \"h_augmented\": torch.zeros(\n            batch_size, seq_len, embed_dim, device=device, dtype=torch.float32\n        ),\n        \"proto_probs\": [\n            [\n                torch.tensor([1.0], device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"uncertainties\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"gates\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"span_preds\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"proto_assignments\": [\n            torch.zeros(seq_len, dtype=torch.long, device=device)\n            for _ in range(batch_size)\n        ],\n    }\n\n    if not isinstance(raw, dict):\n        if _DEBUG_DISCOVERY:\n            print(\"[NORMALIZE] DSCD returned non-dict, using defaults\")\n        return defaults\n\n    if \"h_augmented\" not in raw or raw[\"h_augmented\"] is None:\n        if _DEBUG_DISCOVERY:\n            print(\"[NORMALIZE] Missing h_augmented in DSCD output, using defaults\")\n        return defaults\n\n    out = defaults.copy()\n\n    try:\n        if \"h_augmented\" in raw and raw[\"h_augmented\"] is not None:\n            h = raw[\"h_augmented\"]\n            if isinstance(h, torch.Tensor) and h.shape == (\n                batch_size,\n                seq_len,\n                embed_dim,\n            ):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = (\n                        h.to(device).reshape(batch_size, seq_len, embed_dim)\n                    )\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    for list_key in (\"proto_probs\", \"uncertainties\", \"gates\", \"span_preds\"):\n        if list_key in raw and raw[list_key] is not None:\n            try:\n                val = raw[list_key]\n                if isinstance(val, list) and len(val) == batch_size:\n                    safe_batch = []\n                    for b_row in val:\n                        if isinstance(b_row, list):\n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < len(b_row):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            safe_row.append(v.to(device))\n                                        else:\n                                            safe_row.append(\n                                                torch.as_tensor(\n                                                    v,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    [1.0],\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                        else:\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    0.0,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                except Exception:\n                                    safe_row.append(\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                    )\n                            safe_batch.append(safe_row)\n                        else:\n                            if list_key == \"proto_probs\":\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            [1.0],\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                            else:\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                    out[list_key] = safe_batch\n            except Exception:\n                pass\n\n    try:\n        if \"proto_assignments\" in raw and raw[\"proto_assignments\"] is not None:\n            pa = raw[\"proto_assignments\"]\n            if isinstance(pa, list) and len(pa) == batch_size:\n                safe_pa = []\n                for b_row in pa:\n                    try:\n                        if isinstance(b_row, torch.Tensor):\n                            safe_pa.append(b_row.to(device).long())\n                        else:\n                            safe_pa.append(\n                                torch.tensor(\n                                    b_row, dtype=torch.long, device=device\n                                )\n                            )\n                    except Exception:\n                        safe_pa.append(\n                            torch.zeros(seq_len, dtype=torch.long, device=device)\n                        )\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n\n    return out\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n\n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self._config_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n\n        self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n            \"facebook/m2m100_418M\",\n            torch_dtype=torch.float32,\n            use_cache=False,\n        )\n        \n        try:\n            self.mbart.config.use_cache = False\n            self.mbart.config.label_smoothing_factor = _LABEL_SMOOTHING\n            self.mbart.config.dropout = _DECODER_DROPOUT\n            self.mbart.config.attention_dropout = _DECODER_DROPOUT\n            self.mbart.config.activation_dropout = _DECODER_DROPOUT\n            \n            if hasattr(self.mbart.config, 'decoder_dropout'):\n                self.mbart.config.decoder_dropout = _DECODER_DROPOUT\n            \n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Label smoothing: {_LABEL_SMOOTHING}\")\n                print(f\"[TATN-INIT] Decoder dropout: {_DECODER_DROPOUT}\")\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Failed to set dropout/label_smoothing: {e}\")\n\n        try:\n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                en_token_id = self.tokenizer.get_lang_id(_TARGET_LANGUAGE)\n                bn_token_id = self.tokenizer.get_lang_id(_SOURCE_LANGUAGE)\n            elif hasattr(self.tokenizer, \"lang_code_to_id\"):\n                en_token_id = self.tokenizer.lang_code_to_id.get(\n                    _TARGET_LANGUAGE, _M2M100_EN_TOKEN_ID\n                )\n                bn_token_id = self.tokenizer.lang_code_to_id.get(\n                    _SOURCE_LANGUAGE, _M2M100_BN_TOKEN_ID\n                )\n            else:\n                en_token_id = _M2M100_EN_TOKEN_ID\n                bn_token_id = _M2M100_BN_TOKEN_ID\n\n            if en_token_id is None:\n                en_token_id = _M2M100_EN_TOKEN_ID\n            if bn_token_id is None:\n                bn_token_id = _M2M100_BN_TOKEN_ID\n\n            with self._config_lock:\n                if hasattr(self.mbart.config, \"forced_bos_token_id\"):\n                    self.mbart.config.forced_bos_token_id = int(en_token_id)\n                if hasattr(self.mbart.config, \"decoder_start_token_id\"):\n                    self.mbart.config.decoder_start_token_id = int(en_token_id)\n            \n            self.en_token_id = int(en_token_id)\n            self.bn_token_id = int(bn_token_id)\n\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN-INIT] Language tokens: BN={bn_token_id}, EN={en_token_id}\"\n                )\n\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Failed to set language tokens: {e}\")\n            self.en_token_id = _M2M100_EN_TOKEN_ID\n            self.bn_token_id = _M2M100_BN_TOKEN_ID\n\n        try:\n            if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = max(1, int(getattr(self.mbart.config, \"d_model\", 1024)))\n\n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            try:\n                self.dscd = dscd_cls(\n                    embed_dim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffer_size=_DSCD_BUFFER_SIZE,\n                    max_protos=_DSCD_MAX_PROTOS,\n                    n_min=_DSCD_N_MIN,\n                    language=_SOURCE_LANGUAGE,\n                    dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                    enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                    max_clustering_points=500,\n                    max_candidates_per_step=1,\n                )\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to instantiate MemoryEfficientDSCDOnline: {e}\"\n                )\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found in globals()\")\n\n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(\n                    embed_dim, tokenizer, language=_SOURCE_LANGUAGE\n                )\n            except Exception:\n                class _StubASBN(nn.Module):\n                    def forward(self, h, domain_labels=None):\n                        dev = (\n                            h.device\n                            if isinstance(h, torch.Tensor)\n                            else torch.device(\"cpu\")\n                        )\n                        return h, torch.tensor(0.0, device=dev)\n\n                    def forward_with_grl_simplified(\n                        self, h, *args, **kwargs\n                    ):\n                        dev = (\n                            h.device\n                            if isinstance(h, torch.Tensor)\n                            else torch.device(\"cpu\")\n                        )\n                        zero = torch.tensor(0.0, device=dev)\n                        return zero, zero, zero, zero\n\n                self.asbn = _StubASBN()\n        else:\n            class _StubASBN(nn.Module):\n                def forward(self, h, domain_labels=None):\n                    dev = (\n                        h.device\n                        if isinstance(h, torch.Tensor)\n                        else torch.device(\"cpu\")\n                    )\n                    return h, torch.tensor(0.0, device=dev)\n\n                def forward_with_grl_simplified(self, h, *args, **kwargs):\n                    dev = (\n                        h.device\n                        if isinstance(h, torch.Tensor)\n                        else torch.device(\"cpu\")\n                    )\n                    zero = torch.tensor(0.0, device=dev)\n                    return zero, zero, zero, zero\n\n            self.asbn = _StubASBN()\n\n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg_system = trg_cls(\n                    embed_dim,\n                    tokenizer,\n                    language=_SOURCE_LANGUAGE,\n                    dscd_module=self.dscd,\n                )\n            except Exception:\n                class _StubTRG:\n                    def process_sentence_for_explanations(\n                        self,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=None,\n                        uncertainty_threshold=0.1,\n                        span_threshold=0.05,\n                        decoder_attention=None,\n                        max_explanations=10,\n                    ):\n                        return []\n\n                self.trg_system = _StubTRG()\n        else:\n            class _StubTRG:\n                def process_sentence_for_explanations(\n                    self,\n                    tokens,\n                    dscd_outputs,\n                    token_word_map=None,\n                    uncertainty_threshold=0.1,\n                    span_threshold=0.05,\n                    decoder_attention=None,\n                    max_explanations=10,\n                ):\n                    return []\n\n            self.trg_system = _StubTRG()\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"\\n\" + \"=\" * 80)\n            print(\"TATN Initialized - MemoryOptimizedTATNWithExplanations\")\n            print(\"=\" * 80)\n            print(f\"  - Embed dim: {embed_dim}\")\n            print(f\"  - Label smoothing: {_LABEL_SMOOTHING}\")\n            print(f\"  - Decoder dropout: {_DECODER_DROPOUT}\")\n            print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n            print(f\"  - Validation interval: {_VALIDATION_CHECK_INTERVAL}\")\n            print(f\"  - Lambda ASBN: {_LAMBDA_ASBN}, Lambda DSCD: {_LAMBDA_DSCD}\")\n            print(f\"  - TAU_LOW: {_TAU_LOW}\")\n            print(f\"  - SPAN_THRESHOLD: {_SPAN_THRESHOLD}\")\n            print(f\"  - UNCERTAINTY_THRESHOLD: {_UNCERTAINTY_THRESHOLD}\")\n            print(f\"  - TRG_UNCERTAINTY_THRESHOLD: {_TRG_UNCERTAINTY_THRESHOLD}\")\n            print(\"=\" * 80 + \"\\n\")\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(\n        proto_probs_list, gates_list=None, min_gate: float = 0.0\n    ) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n\n        if dev is None:\n            return torch.tensor(0.0)\n\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    try:\n                        if float(gl[j]) < min_gate:\n                            continue\n                    except Exception:\n                        pass\n\n                try:\n                    p = torch.clamp(probs.to(dev).float(), 1e-8, 1.0)\n                    H = -torch.sum(p * torch.log(p))\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / max(1, count)\n\n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> List[dict]:\n        if token_word_map is not None and len(token_word_map) == batch_size:\n            valid_count = sum(\n                1 for m in token_word_map if isinstance(m, dict) and len(m) > 0\n            )\n            if valid_count == batch_size:\n                if _DEBUG_DISCOVERY:\n                    total_words = sum(len(m) for m in token_word_map)\n                    print(\n                        f\"[TATN-WORDMAP] Using provided word maps: {total_words} words\"\n                    )\n                return token_word_map\n\n        word_maps_batch: List[dict] = []\n\n        if not _has_reconstruct_word_spans:\n            if _DEBUG_DISCOVERY:\n                print(\n                    \"[TATN-WORDMAP] Using SentencePiece subword merging for Bengali\"\n                )\n            word_maps_batch = build_token_word_map_sentencepiece(input_ids, self.tokenizer)\n            \n            total_words = sum(len(m) for m in word_maps_batch)\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-WORDMAP] Built {total_words} words from {batch_size} samples\")\n                if batch_size > 0 and len(word_maps_batch) > 0:\n                    sample_map = word_maps_batch[0]\n                    sample_items = list(sample_map.items())[:5]\n                    print(f\"[TATN-WORDMAP] Sample mapping (first 5): {sample_items}\")\n            \n            return word_maps_batch\n\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructing word maps for {batch_size} samples...\")\n\n        for b in range(batch_size):\n            try:\n                if (\n                    src_texts\n                    and b < len(src_texts)\n                    and isinstance(src_texts[b], str)\n                    and src_texts[b].strip()\n                ):\n                    orig_text = src_texts[b]\n                else:\n                    try:\n                        orig_text = self.tokenizer.decode(\n                            input_ids[b], skip_special_tokens=True\n                        )\n                    except Exception:\n                        orig_text = \"\"\n\n                if not orig_text.strip():\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n                    continue\n\n                wm, words = reconstruct_word_spans(\n                    self.tokenizer, orig_text, max_length=seq_len\n                )\n\n                if not isinstance(wm, dict):\n                    wm = {}\n\n                cleaned_wm: Dict[int, str] = {}\n                for idx, word in wm.items():\n                    if isinstance(idx, int) and 0 <= idx < seq_len:\n                        if isinstance(word, str) and word.strip():\n                            clean_word = (\n                                word.replace(\"▁\", \"\")\n                                .replace(\"Ġ\", \"\")\n                                .replace(\"##\", \"\")\n                                .replace(\"@@\", \"\")\n                                .replace(\"</w>\", \"\")\n                                .strip()\n                            )\n                            if clean_word:\n                                cleaned_wm[idx] = clean_word\n\n                if cleaned_wm:\n                    word_maps_batch.append(cleaned_wm)\n                else:\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n\n                if _DEBUG_DISCOVERY and b == 0:\n                    print(\n                        f\"[TATN-WORDMAP] Sample 0: {len(cleaned_wm)} word spans\"\n                    )\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"[TATN-WORDMAP] Reconstruction failed for sample {b}: {e}\"\n                    )\n                word_maps_batch.append(\n                    {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                )\n\n        total_words = sum(len(m) for m in word_maps_batch)\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructed {total_words} words\")\n\n        return word_maps_batch\n\n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None,\n    ) -> Optional[torch.Tensor]:\n        if not _USE_DOMAIN_LABELS:\n            if _DEBUG_DISCOVERY:\n                print(\"[TATN-DOMAIN] Domain labels disabled\")\n            return None\n\n        try:\n            import random\n            indices = list(range(batch_size))\n            random.shuffle(indices)\n            \n            num_train = max(1, batch_size // 2)\n            train_indices = set(indices[:num_train])\n            \n            domain_labels = torch.tensor(\n                [_TRAIN_DOMAIN if i in train_indices else _TEST_DOMAIN \n                 for i in range(batch_size)],\n                dtype=torch.long,\n                device=device\n            )\n            \n            if _DEBUG_DISCOVERY:\n                unique_domains = torch.unique(domain_labels).tolist()\n                domain_counts = {\n                    d: int((domain_labels == d).sum().item()) \n                    for d in unique_domains\n                }\n                print(f\"[TATN-DOMAIN] Mixed domain labels: {domain_counts}\")\n            \n            return domain_labels\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-DOMAIN] Failed to create domain labels: {e}\")\n            return None\n\n    @staticmethod\n    def _safe_take_key_static(\n        dscd_struct: Dict[str, Any],\n        key: str,\n        b_index: int,\n        seq_len: int,\n        device: torch.device,\n    ):\n        if key == \"proto_probs\":\n            out = [\n                torch.tensor([1.0], dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n        else:\n            out = [\n                torch.tensor(0.0, dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n\n        try:\n            val = dscd_struct.get(key, None)\n            if val is None:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[SAFE_TAKE] Key '{key}' not found in DSCD outputs for batch {b_index}\")\n                return out\n\n            if key == \"proto_probs\":\n                if isinstance(val, list) and len(val) > b_index:\n                    row = val[b_index]\n                    if isinstance(row, list):\n                        for t in range(min(seq_len, len(row))):\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.to(device)\n                            else:\n                                try:\n                                    out[t] = torch.as_tensor(\n                                        v,\n                                        dtype=torch.float32,\n                                        device=device,\n                                    ).flatten()\n                                except Exception:\n                                    pass\n                return out\n\n            if isinstance(val, list) and len(val) > b_index:\n                row = val[b_index]\n                if isinstance(row, list):\n                    for t in range(min(seq_len, len(row))):\n                        v = row[t]\n                        try:\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.to(device)\n                            else:\n                                out[t] = torch.tensor(\n                                    float(v), device=device\n                                )\n                        except Exception:\n                            pass\n                elif isinstance(row, torch.Tensor):\n                    if row.dim() == 1:\n                        for t in range(min(seq_len, int(row.size(0)))):\n                            try:\n                                out[t] = torch.tensor(\n                                    float(row[t].item()), device=device\n                                )\n                            except Exception:\n                                pass\n                return out\n\n            if isinstance(val, torch.Tensor):\n                if val.dim() >= 2 and int(val.size(0)) > b_index:\n                    for t in range(min(seq_len, int(val.size(1)))):\n                        try:\n                            if val.dim() == 3:\n                                v = val[b_index, t]\n                                if v.numel() == 1:\n                                    out[t] = torch.tensor(\n                                        float(v.item()), device=device\n                                    )\n                                else:\n                                    out[t] = v.to(device)\n                            else:\n                                v = val[b_index, t]\n                                out[t] = torch.tensor(\n                                    float(v.item()), device=device\n                                )\n                        except Exception:\n                            pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[SAFE_TAKE] Error extracting '{key}' for batch {b_index}: {e}\")\n            pass\n\n        return out\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = True,\n    ):\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(\n                f\"Expected 2D tensors, got {input_ids.shape}, {attention_mask.shape}\"\n            )\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        if (\n            torch.cuda.is_available()\n            and _MEMORY_CLEANUP_FREQUENCY > 0\n            and current_step % _MEMORY_CLEANUP_FREQUENCY == 0\n        ):\n            for i in range(min(_NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            if gc.isenabled():\n                gc.collect()\n\n        if self.training and _DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if (\n                current_step - self.last_discovery_step\n                >= _PERIODIC_DISCOVERY_FREQUENCY\n            ):\n                try:\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        print(\"\\n\" + \"=\" * 80)\n                        print(f\"[TATN] PERIODIC DISCOVERY @ step {current_step}\")\n                        print(\"=\" * 80)\n\n                    start_time = time.time()\n\n                    self.dscd.periodic_discovery_check(\n                        current_step, _PERIODIC_DISCOVERY_FREQUENCY\n                    )\n\n                    elapsed = time.time() - start_time\n                    self.last_discovery_step = current_step\n\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            num_homographs = len(self.dscd.discovered_homographs)\n                        except Exception:\n                            num_homographs = 0\n                        \n                        try:\n                            total_prototypes = sum(\n                                store.size() for store in self.dscd.prototype_stores.values()\n                            )\n                        except Exception:\n                            total_prototypes = 0\n                        \n                        try:\n                            total_tokens = len(self.dscd.prototype_stores)\n                        except Exception:\n                            total_tokens = 0\n                        \n                        print(f\"[TATN] Discovery completed in {elapsed:.2f}s\")\n                        print(f\"[TATN]   Homographs: {num_homographs}\")\n                        print(f\"[TATN]   Total prototypes: {total_prototypes}\")\n                        print(\"=\" * 80 + \"\\n\")\n\n                except Exception as e:\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        print(f\"[TATN] Discovery failed: {e}\")\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n        if not self.training and _VALIDATION_CHECK_INTERVAL > 0:\n            if (\n                current_step - self.last_validation_step\n                >= _VALIDATION_CHECK_INTERVAL\n            ):\n                try:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"\\n[TATN-VALIDATION] Step {current_step}\")\n                        \n                        try:\n                            total_tokens = len(self.dscd.prototype_stores)\n                        except Exception:\n                            total_tokens = 0\n                        \n                        try:\n                            total_prototypes = sum(\n                                store.size() for store in self.dscd.prototype_stores.values()\n                            )\n                        except Exception:\n                            total_prototypes = 0\n                        \n                        try:\n                            num_homographs = len(self.dscd.discovered_homographs)\n                        except Exception:\n                            num_homographs = 0\n                        \n                        print(f\"  - Tokens: {total_tokens}\")\n                        print(f\"  - Prototypes: {total_prototypes}\")\n                        print(f\"  - Homographs: {num_homographs}\")\n                    \n                    self.last_validation_step = current_step\n                except Exception:\n                    pass\n\n        enc_outputs = None\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids, attention_mask=attention_mask\n            )\n        except Exception:\n            try:\n                enc_outputs = self.mbart.get_encoder()(\n                    input_ids=input_ids, attention_mask=attention_mask\n                )\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Encoder failed: {e}\")\n                enc_outputs = None\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            try:\n                emb = self.mbart.get_input_embeddings()(input_ids).to(device)\n                h = emb\n            except Exception:\n                h = torch.zeros(\n                    batch_size,\n                    seq_len,\n                    int(getattr(self.mbart.config, \"d_model\", 1024)),\n                    device=device,\n                )\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN] Invalid encoder output shape: {h.shape if hasattr(h, 'shape') else 'None'}\")\n            h = torch.zeros(\n                batch_size,\n                seq_len,\n                int(getattr(self.mbart.config, \"d_model\", 1024)),\n                device=device,\n            )\n\n        embed_dim = int(h.size(-1))\n        training_mode = labels is not None and self.training\n\n        token_word_map = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n\n        domain_labels = self._extract_domain_labels(\n            batch_size, device, src_texts\n        )\n\n        if use_dscd:\n            try:\n                raw_dscd = self.dscd.forward(\n                    h,\n                    token_types=None,\n                    train_mode=self.training,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_word_map=token_word_map,\n                )\n                \n                if not isinstance(raw_dscd, dict) or \"h_augmented\" not in raw_dscd:\n                    if _DEBUG_DISCOVERY:\n                        print(\"[TATN] DSCD returned invalid output, using defaults\")\n                    raw_dscd = None\n                    \n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD forward failed: {e}\")\n                raw_dscd = None\n        else:\n            raw_dscd = None\n\n        if raw_dscd is None:\n            raw_dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [\n                    [\n                        torch.tensor(\n                            [1.0],\n                            dtype=torch.float32,\n                            device=device,\n                        )\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"span_preds\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"proto_assignments\": [\n                    torch.zeros(\n                        seq_len, dtype=torch.long, device=device\n                    )\n                    for _ in range(batch_size)\n                ],\n            }\n\n        dscd = _normalize_dscd_outputs(\n            raw_dscd, batch_size, seq_len, device, embed_dim\n        )\n        \n        if _DEBUG_DISCOVERY and current_step % 50 == 0:\n            print(f\"\\n[TATN-DSCD] Step {current_step} normalized outputs:\")\n            print(f\"  - h_augmented shape: {dscd['h_augmented'].shape}\")\n            if dscd['uncertainties'] and len(dscd['uncertainties']) > 0:\n                sample_unc = dscd['uncertainties'][0][:3]\n                unc_vals = [float(u.item()) if isinstance(u, torch.Tensor) else float(u) for u in sample_unc]\n                print(f\"  - uncertainties (first 3): {unc_vals}\")\n            if dscd['gates'] and len(dscd['gates']) > 0:\n                sample_gates = dscd['gates'][0][:3]\n                gate_vals = [float(g.item()) if isinstance(g, torch.Tensor) else float(g) for g in sample_gates]\n                print(f\"  - gates (first 3): {gate_vals}\")\n            if dscd['span_preds'] and len(dscd['span_preds']) > 0:\n                sample_spans = dscd['span_preds'][0][:3]\n                span_vals = [float(s.item()) if isinstance(s, torch.Tensor) else float(s) for s in sample_spans]\n                print(f\"  - span_preds (first 3): {span_vals}\")\n        \n        h_aug = dscd.get(\"h_augmented\", h)\n\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN] h_augmented shape mismatch \"\n                    f\"(expected {h.shape}, got {getattr(h_aug, 'shape', None)})\"\n                )\n            h_aug = h\n\n        asbn_bn_loss = torch.tensor(0.0, device=device)\n        if use_asbn and domain_labels is not None:\n            try:\n                if _DEBUG_DISCOVERY and current_step % 50 == 0:\n                    print(f\"[TATN-ASBN] Calling ASBN forward with domain_labels={domain_labels[:3].tolist() if domain_labels is not None else None}\")\n                h_aug, asbn_bn_loss = self.asbn.forward(\n                    h_aug, domain_labels=domain_labels\n                )\n                if _DEBUG_DISCOVERY and current_step % 50 == 0:\n                    print(f\"[TATN-ASBN] ASBN BN loss: {float(asbn_bn_loss.item()):.4f}\")\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] ASBN forward (BN) failed: {e}\")\n\n        try:\n            enc_for_decoder = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=(\n                    getattr(enc_outputs, \"hidden_states\", None)\n                    if enc_outputs\n                    else None\n                ),\n                attentions=(\n                    getattr(enc_outputs, \"attentions\", None)\n                    if enc_outputs\n                    else None\n                ),\n            )\n        except Exception:\n            enc_for_decoder = (h_aug,)\n\n        if training_mode:\n            try:\n                if labels is not None:\n                    decoder_input_ids = labels.clone()\n                    decoder_input_ids = torch.where(\n                        decoder_input_ids == -100,\n                        torch.tensor(\n                            self.tokenizer.pad_token_id,\n                            dtype=decoder_input_ids.dtype,\n                            device=decoder_input_ids.device,\n                        ),\n                        decoder_input_ids,\n                    )\n\n                    bos_column = torch.full(\n                        (batch_size, 1),\n                        int(self.mbart.config.decoder_start_token_id),\n                        dtype=torch.long,\n                        device=device,\n                    )\n                    decoder_input_ids = torch.cat(\n                        [bos_column, decoder_input_ids[:, :-1]], dim=1\n                    )\n                    decoder_attention_mask = (\n                        decoder_input_ids != self.tokenizer.pad_token_id\n                    ).long()\n                else:\n                    decoder_input_ids = None\n                    decoder_attention_mask = None\n\n                seq_outputs = self.mbart(\n                    input_ids=None,\n                    attention_mask=attention_mask,\n                    encoder_outputs=enc_for_decoder,\n                    decoder_input_ids=decoder_input_ids,\n                    decoder_attention_mask=decoder_attention_mask,\n                    labels=labels,\n                    use_cache=False,\n                    return_dict=True,\n                )\n                translation_loss = getattr(seq_outputs, \"loss\", None)\n                if translation_loss is None:\n                    translation_loss = torch.tensor(0.0, device=device)\n                else:\n                    if not torch.isfinite(translation_loss):\n                        if _DEBUG_DISCOVERY:\n                            print(\"[TATN] NaN/Inf in translation_loss\")\n                        translation_loss = torch.tensor(1.0, device=device)\n                    else:\n                        translation_loss = torch.clamp(translation_loss, 0.0, 100.0)\n                        \n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Decoder forward failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                translation_loss = torch.tensor(0.0, device=device)\n\n            if use_asbn and _ENABLE_ASBN_TRAINING:\n                try:\n                    if _DEBUG_DISCOVERY and current_step % 50 == 0:\n                        print(f\"[TATN-ASBN] Calling ASBN GRL forward\")\n                    asbn_ret = self.asbn.forward_with_grl_simplified(\n                        h_aug,\n                        dscd.get(\"proto_probs\", None),\n                        dscd.get(\"uncertainties\", None),\n                        dscd.get(\"gates\", None),\n                        token_word_map=token_word_map,\n                        domain_labels=domain_labels,\n                        global_step=current_step,\n                    )\n                    if isinstance(asbn_ret, (tuple, list)):\n                        asbn_loss = asbn_ret[0]\n                    else:\n                        asbn_loss = asbn_ret\n                    \n                    if not isinstance(asbn_loss, torch.Tensor):\n                        asbn_loss = torch.tensor(\n                            float(asbn_loss), device=device, dtype=torch.float32\n                        )\n                    else:\n                        asbn_loss = asbn_loss.to(device).float()\n                    \n                    if not torch.isfinite(asbn_loss):\n                        asbn_loss = torch.tensor(0.0, device=device)\n                    \n                    asbn_loss = torch.clamp(asbn_loss, 0.0, 10.0)\n                    \n                    if _DEBUG_DISCOVERY and current_step % 50 == 0:\n                        print(f\"[TATN-ASBN] ASBN GRL loss: {float(asbn_loss.item()):.4f}\")\n                    \n                except Exception as e:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"[TATN] ASBN forward failed: {e}\")\n                    asbn_loss = torch.tensor(0.0, device=device)\n            else:\n                asbn_loss = torch.tensor(0.0, device=device)\n\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(\n                    dscd.get(\"proto_probs\", []),\n                    gates_list=dscd.get(\"gates\", []),\n                    min_gate=0.0,\n                )\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(\n                        float(dscd_reg), device=device\n                    )\n                else:\n                    dscd_reg = dscd_reg.to(device)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device)\n                dscd_reg = torch.clamp(dscd_reg, 0.0, 10.0)\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD reg failed: {e}\")\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            total_loss = (\n                translation_loss\n                + _LAMBDA_ASBN * (asbn_loss + asbn_bn_loss)\n                + _LAMBDA_DSCD * dscd_reg\n            )\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n\n            if not torch.isfinite(total_loss):\n                if _DEBUG_DISCOVERY:\n                    print(\n                        \"[TATN] NaN/Inf detected in total_loss - \"\n                        \"using translation_loss only\"\n                    )\n                total_loss = (\n                    translation_loss\n                    if torch.isfinite(translation_loss)\n                    else torch.tensor(1.0, device=device)\n                )\n\n            try:\n                del enc_outputs, h, raw_dscd\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            return total_loss\n\n        explanations_list: List[List[Dict[str, Any]]] = []\n\n        if (not self.training) and _ENABLE_TRG_INFERENCE:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"\\n[TATN-INFERENCE] Starting TRG for {batch_size} samples\"\n                )\n\n            tokens_batch: List[List[str]] = []\n            special_tokens = {'<s>', '</s>', '<pad>', '<unk>', ''}\n\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    if hasattr(self.tokenizer, \"convert_ids_to_tokens\"):\n                        toks = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    else:\n                        toks = []\n                    if not toks:\n                        toks = [\"UNK\"] * seq_len\n                    elif len(toks) < seq_len:\n                        toks = toks + [\"\"] * (seq_len - len(toks))\n                    elif len(toks) > seq_len:\n                        toks = toks[:seq_len]\n                    \n                    toks = [t if t not in special_tokens else \"\" for t in toks]\n                except Exception:\n                    toks = [\"UNK\"] * seq_len\n\n                tokens_batch.append(toks)\n\n            decoder_attention = None\n\n            try:\n                total_explanations = 0\n                for b in range(batch_size):\n                    per_sent = {\n                        \"proto_probs\": self._safe_take_key_static(\n                            dscd, \"proto_probs\", b, seq_len, device\n                        ),\n                        \"uncertainties\": self._safe_take_key_static(\n                            dscd, \"uncertainties\", b, seq_len, device\n                        ),\n                        \"gates\": self._safe_take_key_static(\n                            dscd, \"gates\", b, seq_len, device\n                        ),\n                        \"span_preds\": self._safe_take_key_static(\n                            dscd, \"span_preds\", b, seq_len, device\n                        ),\n                    }\n\n                    try:\n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=(\n                                token_word_map[b]\n                                if token_word_map\n                                and b < len(token_word_map)\n                                else None\n                            ),\n                            uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                            span_threshold=_SPAN_THRESHOLD,\n                            decoder_attention=decoder_attention,\n                        )\n                        batch_exps = exps if isinstance(exps, list) else []\n                        explanations_list.append(batch_exps)\n                        total_explanations += len(batch_exps)\n\n                        if _DEBUG_DISCOVERY and b < 2:\n                            print(\n                                f\"[TATN-INFERENCE] Sample {b}: \"\n                                f\"{len(batch_exps)} explanations\"\n                            )\n\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(\n                                f\"[TATN-INFERENCE] TRG failed for sample {b}: {e}\"\n                            )\n                        explanations_list.append([])\n\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"\\n[TATN-INFERENCE] Total explanations: {total_explanations}\"\n                    )\n                    if total_explanations == 0:\n                        print(\"[TATN-INFERENCE] NO EXPLANATIONS GENERATED\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN-INFERENCE] TRG generation failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                explanations_list = [[] for _ in range(batch_size)]\n        else:\n            explanations_list = [[] for _ in range(batch_size)]\n\n        outputs = {\n            \"encoder_outputs\": enc_outputs,\n            \"dscd_outputs\": dscd,\n            \"sense_augmented_embeddings\": h_aug,\n            \"explanations\": explanations_list,\n            \"asbn_loss\": asbn_bn_loss,\n            \"ambiguity_signals\": {\n                \"span\": dscd.get(\"span_preds\", []),\n                \"uncertainty\": dscd.get(\"uncertainties\", []),\n                \"confidence\": [\n                    [\n                        1.0\n                        - (\n                            float(u)\n                            if isinstance(u, (float, int))\n                            else (\n                                float(u.item())\n                                if isinstance(u, torch.Tensor)\n                                else 1.0\n                            )\n                        )\n                        for u in row\n                    ]\n                    for row in dscd.get(\"uncertainties\", [])\n                ],\n                \"proto_probs\": dscd.get(\"proto_probs\", []),\n            },\n        }\n\n        try:\n            del h, raw_dscd\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return outputs\n\n    def generate(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        max_length: int = 128,\n        num_beams: int = 5,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> torch.Tensor:\n        device = input_ids.device\n        batch_size = input_ids.size(0)\n        \n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids, attention_mask=attention_mask\n            )\n\n            enc_wrapped = BaseModelOutput(\n                last_hidden_state=(\n                    enc_outputs.last_hidden_state\n                    if hasattr(enc_outputs, \"last_hidden_state\")\n                    else enc_outputs[0]\n                ),\n                hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                attentions=getattr(enc_outputs, \"attentions\", None),\n            )\n\n            forced_bos_id = self.en_token_id\n            if hasattr(self.mbart.config, \"forced_bos_token_id\"):\n                try:\n                    forced_bos_id = int(self.mbart.config.forced_bos_token_id)\n                except Exception:\n                    forced_bos_id = self.en_token_id\n\n            eos_token_id = self.tokenizer.eos_token_id\n            if eos_token_id is None:\n                eos_token_id = 2\n            \n            pad_token_id = self.tokenizer.pad_token_id\n            if pad_token_id is None:\n                pad_token_id = 1\n\n            gen_kwargs = {\n                \"input_ids\": None,\n                \"attention_mask\": attention_mask,\n                \"encoder_outputs\": enc_wrapped,\n                \"max_length\": min(max_length, 100),\n                \"min_length\": 1,\n                \"num_beams\": min(num_beams, 4),\n                \"early_stopping\": True,\n                \"no_repeat_ngram_size\": 3,\n                \"repetition_penalty\": 3.0,\n                \"length_penalty\": 1.2,\n                \"do_sample\": False,\n                \"forced_bos_token_id\": forced_bos_id,\n                \"eos_token_id\": eos_token_id,\n                \"pad_token_id\": pad_token_id,\n                \"num_return_sequences\": 1,\n                \"output_scores\": False,\n                \"return_dict_in_generate\": False,\n            }\n            \n            gen_kwargs.update(kwargs)\n\n            try:\n                class TimeoutException(Exception):\n                    pass\n                \n                def timeout_handler(signum, frame):\n                    raise TimeoutException(\"Generation timeout\")\n                \n                old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n                signal.alarm(20)\n                \n                try:\n                    outputs = self.mbart.generate(**gen_kwargs)\n                finally:\n                    signal.alarm(0)\n                    signal.signal(signal.SIGALRM, old_handler)\n            except (TimeoutException, AttributeError):\n                if _DEBUG_DISCOVERY:\n                    print(\"[GENERATE] WARNING: Using fallback generation (timeout or signal not supported)\")\n                outputs = self.mbart.generate(**gen_kwargs)\n\n            if outputs.size(1) > max_length:\n                outputs = outputs[:, :max_length]\n\n            return outputs\n\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-GENERATE] Failed: {e}\")\n            \n            outputs = torch.full(\n                (batch_size, 10),\n                self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else 1,\n                dtype=torch.long,\n                device=device\n            )\n            outputs[:, 0] = self.en_token_id\n            outputs[:, -1] = self.tokenizer.eos_token_id if self.tokenizer.eos_token_id else 2\n            return outputs\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        **kwargs\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n            **kwargs\n        )\n\n    def get_component_stats(self) -> Dict[str, Any]:\n        stats: Dict[str, Any] = {\n            \"global_step\": self.global_step,\n            \"last_discovery_step\": self.last_discovery_step,\n            \"last_validation_step\": self.last_validation_step,\n        }\n\n        try:\n            total_tokens = len(self.dscd.prototype_stores)\n        except Exception:\n            total_tokens = 0\n        \n        try:\n            total_prototypes = sum(\n                store.size() for store in self.dscd.prototype_stores.values()\n            )\n        except Exception:\n            total_prototypes = 0\n        \n        try:\n            num_homographs = len(self.dscd.discovered_homographs)\n        except Exception:\n            num_homographs = 0\n        \n        stats[\"dscd\"] = {\n            \"total_tokens\": total_tokens,\n            \"total_prototypes\": total_prototypes,\n            \"num_homographs\": num_homographs,\n        }\n\n        try:\n            if hasattr(self.asbn, \"get_detailed_stats\"):\n                stats[\"asbn\"] = self.asbn.get_detailed_stats()\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.trg_system, \"get_statistics\"):\n                stats[\"trg\"] = self.trg_system.get_statistics()\n        except Exception:\n            pass\n\n        return stats\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 6: TATN Ready - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\nprint(\"Config:\")\nprint(f\"  - Source: {_SOURCE_LANGUAGE}, Target: {_TARGET_LANGUAGE}\")\nprint(f\"  - Label smoothing: {_LABEL_SMOOTHING}\")\nprint(f\"  - Decoder dropout: {_DECODER_DROPOUT}\")\nprint(f\"  - DSCD clustering: {_DSCD_ENABLE_TRAINING_CLUSTERING}\")\nprint(f\"  - ASBN training: {_ENABLE_ASBN_TRAINING}\")\nprint(f\"  - TRG inference: {_ENABLE_TRG_INFERENCE}\")\nprint(f\"  - Discovery freq: {_PERIODIC_DISCOVERY_FREQUENCY}\")\nprint(f\"  - λ_ASBN: {_LAMBDA_ASBN}, λ_DSCD: {_LAMBDA_DSCD}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW}\")\nprint(f\"  - SPAN_THRESHOLD: {_SPAN_THRESHOLD}\")\nprint(f\"  - UNCERTAINTY_THRESHOLD: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - TRG_UNCERTAINTY_THRESHOLD: {_TRG_UNCERTAINTY_THRESHOLD}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP (PURE UNSUPERVISED) - M2M100 PYTORCH 2.6 COMPATIBLE - FIXED\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\n_CELL7_WORDMAP_BUILT_COUNT = 0\n_CELL7_WORDMAP_PROVIDED_COUNT = 0\n_CELL7_FORWARD_CALL_COUNT = 0\n_CELL7_BACKWARD_SUCCESS_COUNT = 0\n\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept (NameError, ValueError, TypeError):\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept (NameError, ValueError, TypeError):\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept (NameError, ValueError, TypeError):\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept (NameError, ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept (NameError, TypeError):\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept (NameError, ValueError, TypeError):\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\",\n        \"তারা\", \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\n\ndef _build_token_to_word_map(tokenizer, input_ids):\n    batch_word_maps = []\n\n    for batch_idx in range(input_ids.size(0)):\n        tokens = tokenizer.convert_ids_to_tokens(input_ids[batch_idx].tolist())\n        word_map = {}\n        current_word = \"\"\n        word_start_idx = 0\n\n        for i, token in enumerate(tokens):\n            if token in [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"\"]:\n                word_map[i] = None\n                continue\n\n            if token.startswith(\"▁\"):\n                if current_word:\n                    clean_word = current_word.replace(\"▁\", \"\").strip()\n                    if clean_word:\n                        for j in range(word_start_idx, i):\n                            word_map[j] = clean_word\n\n                current_word = token\n                word_start_idx = i\n            else:\n                current_word += token\n\n        if current_word:\n            clean_word = current_word.replace(\"▁\", \"\").strip()\n            if clean_word:\n                for j in range(word_start_idx, len(tokens)):\n                    word_map[j] = clean_word\n\n        batch_word_maps.append(word_map)\n\n    return batch_word_maps\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        tokenizer = getattr(core, \"tokenizer\", None)\n        if tokenizer is None:\n            return set()\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, \"buffer_lock\"):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, \"clustering_lock\"):\n            lock = dscd.clustering_lock\n\n        word_prototype_counts = defaultdict(int)\n\n        if lock:\n            with lock:\n                stores = dict(dscd.prototype_stores)\n        else:\n            stores = dict(dscd.prototype_stores)\n\n        for token_key, store in stores.items():\n            try:\n                num_protos = 0\n                if hasattr(store, \"size\"):\n                    size_method = getattr(store, \"size\", None)\n                    if callable(size_method):\n                        try:\n                            num_protos = int(size_method())\n                        except Exception:\n                            num_protos = 0\n\n                clean_token = (\n                    str(token_key)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean_token:\n                    word_prototype_counts[clean_token] = max(\n                        word_prototype_counts[clean_token], num_protos\n                    )\n            except Exception:\n                continue\n\n        for word, count in word_prototype_counts.items():\n            if count >= 2:\n                homographs.add(word)\n\n        return homographs\n    except Exception:\n        return set()\n\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module,\n    tokenizer,\n    epoch: int,\n    global_step: int,\n    source_lang: str,\n    target_lang: str,\n    max_length: int,\n    device: torch.device,\n) -> Dict[str, Any]:\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n\n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n\n    if not isinstance(device, torch.device):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if _DEBUG_DISCOVERY:\n        print(f\"[VALIDATION] Model mode before eval(): training={was_training}\")\n\n    dscd_homographs = _get_dscd_homographs(model)\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[VALIDATION] DSCD discovered homographs: {len(dscd_homographs)}\")\n        if dscd_homographs:\n            print(f\"[VALIDATION] Sample: {list(dscd_homographs)[:10]}\")\n\n    validation_results = {\n        \"epoch\": epoch,\n        \"step\": global_step,\n        \"translations_success\": 0,\n        \"translations_failed\": 0,\n        \"explanations_generated\": 0,\n        \"dscd_homographs_explained\": 0,\n        \"reference_homographs_explained\": 0,\n        \"avg_explanation_confidence\": 0.0,\n        \"dscd_quality_score\": 0.0,\n        \"dscd_multi_sense_tokens\": 0,\n        \"dscd_total_prototypes\": 0,\n        \"asbn_domain_loss\": 0.0,\n        \"asbn_domain_accuracy\": 0.0,\n        \"asbn_source_accuracy\": 0.0,\n        \"asbn_target_accuracy\": 0.0,\n        \"trg_total_explanations\": 0,\n        \"validation_completed\": False,\n    }\n\n    try:\n        core_model.eval()\n        if _DEBUG_DISCOVERY:\n            print(f\"[VALIDATION] Model set to eval() mode\")\n\n        val_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল=tap/call\"),\n            (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল=tomorrow/yesterday\"),\n            (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা=leaf/page\"),\n            (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক=bank/embankment\"),\n            (\"আমি ভালো আছি।\", \"I am fine\", \"No ambiguity\"),\n            (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"No ambiguity\"),\n            (\"এটা আমার বই।\", \"This is my book\", \"No ambiguity\"),\n            (\"আজ আবহাওয়া ভালো।\", \"Weather is good today\", \"No ambiguity\"),\n            (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল=fruit/result\"),\n            (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা=head/top\"),\n        ]\n\n        print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n        print(\"-\" * 80)\n\n        confidences = []\n        dscd_homograph_words_detected = set()\n        reference_homograph_words_detected = set()\n\n        m2m100_obj = None\n        try:\n            m2m100_obj = getattr(core_model, \"mbart\", None)\n        except Exception:\n            m2m100_obj = None\n\n        try:\n            try:\n                tokenizer.src_lang = source_lang\n            except Exception:\n                pass\n\n            forced_id = None\n            try:\n                if hasattr(tokenizer, \"get_lang_id\"):\n                    for code in (target_lang, \"en_XX\", \"en\", \"eng\"):\n                        try:\n                            lid = tokenizer.get_lang_id(code)\n                            if lid is not None:\n                                forced_id = int(lid)\n                                break\n                        except Exception:\n                            continue\n                elif hasattr(tokenizer, \"lang_code_to_id\"):\n                    forced_id = tokenizer.lang_code_to_id.get(target_lang, None)\n                    if forced_id is not None:\n                        forced_id = int(forced_id)\n            except Exception:\n                forced_id = None\n\n            if forced_id is None:\n                try:\n                    forced_id = int(globals().get(\"M2M100_EN_TOKEN_ID\", 128022))\n                except Exception:\n                    forced_id = 128022\n\n            orig_use_cache = None\n            config_lock = getattr(core_model, \"_config_lock\", None)\n            try:\n                if m2m100_obj is not None and hasattr(m2m100_obj, \"config\") and hasattr(m2m100_obj.config, \"use_cache\"):\n                    orig_use_cache = m2m100_obj.config.use_cache\n                    if config_lock:\n                        with config_lock:\n                            m2m100_obj.config.use_cache = True\n                    else:\n                        m2m100_obj.config.use_cache = True\n            except Exception:\n                orig_use_cache = None\n\n            for idx, (src, expected, note) in enumerate(val_sentences, 1):\n                try:\n                    enc = tokenizer(\n                        src,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_length,\n                    )\n                    enc = {\n                        k: (\n                            v.to(device, non_blocking=True)\n                            if isinstance(v, torch.Tensor)\n                            else v\n                        )\n                        for k, v in enc.items()\n                    }\n\n                    if forced_id is not None:\n                        try:\n                            if m2m100_obj is not None and hasattr(m2m100_obj, \"config\"):\n                                if config_lock:\n                                    with config_lock:\n                                        m2m100_obj.config.forced_bos_token_id = int(forced_id)\n                                        m2m100_obj.config.decoder_start_token_id = int(forced_id)\n                                else:\n                                    m2m100_obj.config.forced_bos_token_id = int(forced_id)\n                                    m2m100_obj.config.decoder_start_token_id = int(forced_id)\n                        except Exception:\n                            pass\n\n                    out_ids = None\n                    try:\n                        gen_src = m2m100_obj if m2m100_obj is not None else core_model\n                        if hasattr(gen_src, \"generate\"):\n                            out_ids = gen_src.generate(\n                                enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                max_length=max_length,\n                                num_beams=2,\n                                do_sample=False,\n                                early_stopping=True,\n                                pad_token_id=int(getattr(tokenizer, \"pad_token_id\", 1)),\n                                forced_bos_token_id=int(forced_id) if forced_id is not None else None,\n                                repetition_penalty=2.0,\n                                no_repeat_ngram_size=3,\n                            )\n                    except AttributeError:\n                        if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                            print(\"[VALIDATION] Warning: generation raised AttributeError (protobuf incompatibility).\")\n                            _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                        out_ids = None\n                    except Exception as e:\n                        print(f\"[VALIDATION] Generation error: {type(e).__name__}: {str(e)[:200]}\")\n                        out_ids = None\n\n                    translation = \"\"\n                    if out_ids is not None and (\n                        (isinstance(out_ids, torch.Tensor) and out_ids.numel() > 0)\n                        or (isinstance(out_ids, (list, tuple)) and len(out_ids) > 0)\n                    ):\n                        try:\n                            if isinstance(out_ids, (list, tuple)):\n                                translation = (\n                                    tokenizer.batch_decode(out_ids, skip_special_tokens=True)[0]\n                                    if out_ids\n                                    else \"\"\n                                )\n                            else:\n                                translation = (\n                                    tokenizer.decode(out_ids[0], skip_special_tokens=True)\n                                    if out_ids.size(0) > 0\n                                    else \"\"\n                                )\n                        except AttributeError:\n                            if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                                print(\"[VALIDATION] Warning: decode raised AttributeError (protobuf).\")\n                                _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                            translation = \"\"\n                        except Exception as e:\n                            print(f\"[VALIDATION] Decode error: {type(e).__name__}: {str(e)[:200]}\")\n                            translation = \"\"\n                    else:\n                        translation = \"\"\n\n                    if translation:\n                        validation_results[\"translations_success\"] += 1\n                    else:\n                        validation_results[\"translations_failed\"] += 1\n                        print(f\"  {idx:2d}. Translation failed: {note[:30]:30s}\")\n                        continue\n\n                    explanation_status = \"\"\n                    try:\n                        translate_fn = globals().get(\"translate_with_explanations\", None)\n                        if translate_fn and callable(translate_fn):\n                            res = translate_fn(model, tokenizer, src, device=device)\n                            exps = res.get(\"explanations\", [])\n                            validation_results[\"explanations_generated\"] += len(exps)\n\n                            if exps:\n                                explanation_status = f\"{len(exps)} expl\"\n                                for exp in exps:\n                                    try:\n                                        conf = exp.get(\"confidence\", 0.5)\n                                        confidences.append(float(conf))\n                                        word = exp.get(\"ambiguous_word\", \"\").strip()\n                                        clean_word = (\n                                            word.replace(\"▁\", \"\")\n                                            .replace(\"Ġ\", \"\")\n                                            .replace(\"##\", \"\")\n                                            .replace(\"@@\", \"\")\n                                            .replace(\"</w>\", \"\")\n                                            .lower()\n                                        )\n\n                                        if clean_word in dscd_homographs:\n                                            validation_results[\"dscd_homographs_explained\"] += 1\n                                            dscd_homograph_words_detected.add(clean_word)\n\n                                        if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                                            validation_results[\"reference_homographs_explained\"] += 1\n                                            reference_homograph_words_detected.add(clean_word)\n                                    except Exception:\n                                        pass\n                            else:\n                                explanation_status = \"no expl\"\n                        else:\n                            explanation_status = \"unavailable\"\n                    except Exception as e:\n                        explanation_status = f\"error: {type(e).__name__}\"\n\n                    print(\n                        f\"  {idx:2d}. {explanation_status:15s} \"\n                        f\"{note[:30]:30s} -> {translation[:200]}\"\n                    )\n                    del enc\n                    if out_ids is not None:\n                        del out_ids\n\n                except Exception as e:\n                    validation_results[\"translations_failed\"] += 1\n                    print(f\"  {idx:2d}. ERROR: {note[:30]:30s} -> {type(e).__name__}\")\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n            try:\n                if m2m100_obj is not None and orig_use_cache is not None and hasattr(m2m100_obj, \"config\"):\n                    if config_lock:\n                        with config_lock:\n                            m2m100_obj.config.use_cache = orig_use_cache\n                    else:\n                        m2m100_obj.config.use_cache = orig_use_cache\n            except Exception:\n                pass\n\n            if torch.cuda.is_available():\n                try:\n                    torch.cuda.synchronize()\n                except Exception:\n                    pass\n\n            clear_all_gpu_caches()\n\n        except Exception as inner_e:\n            print(f\"[VALIDATION] Inner exception: {type(inner_e).__name__}: {str(inner_e)[:200]}\")\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n        try:\n            dscd = core_model.dscd if hasattr(core_model, \"dscd\") else None\n            if dscd and hasattr(dscd, \"prototype_stores\"):\n                lock = None\n                if hasattr(dscd, \"buffer_lock\"):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, \"clustering_lock\"):\n                    lock = dscd.clustering_lock\n\n                total_tokens = 0\n                total_prototypes = 0\n                multi_sense_tokens = 0\n\n                if lock:\n                    with lock:\n                        total_tokens = len(dscd.prototype_stores)\n                        for token, store in dscd.prototype_stores.items():\n                            try:\n                                num_protos = 0\n                                if hasattr(store, \"size\"):\n                                    size_method = getattr(store, \"size\", None)\n                                    if callable(size_method):\n                                        try:\n                                            num_protos = int(size_method())\n                                        except Exception:\n                                            num_protos = 0\n                                total_prototypes += num_protos\n                                if num_protos >= 2:\n                                    multi_sense_tokens += 1\n                            except Exception:\n                                continue\n                else:\n                    total_tokens = len(dscd.prototype_stores)\n                    for token, store in dscd.prototype_stores.items():\n                        try:\n                            num_protos = 0\n                            if hasattr(store, \"size\"):\n                                size_method = getattr(store, \"size\", None)\n                                if callable(size_method):\n                                    try:\n                                        num_protos = int(size_method())\n                                    except Exception:\n                                        num_protos = 0\n                            total_prototypes += num_protos\n                            if num_protos >= 2:\n                                multi_sense_tokens += 1\n                        except Exception:\n                            continue\n\n                quality_score = (\n                    multi_sense_tokens / total_tokens if total_tokens > 0 else 0.0\n                )\n\n                validation_results[\"dscd_quality_score\"] = quality_score\n                validation_results[\"dscd_multi_sense_tokens\"] = multi_sense_tokens\n                validation_results[\"dscd_total_prototypes\"] = total_prototypes\n\n                print(f\"  - Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n                print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n                print(f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\")\n            else:\n                print(\"  - Validation not available\")\n        except Exception as e:\n            print(f\"  - Validation failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] ASBN Training Statistics:\")\n        try:\n            asbn = core_model.asbn if hasattr(core_model, \"asbn\") else None\n            if asbn and hasattr(asbn, \"get_detailed_stats\"):\n                asbn_stats = asbn.get_detailed_stats()\n                validation_results[\"asbn_domain_loss\"] = asbn_stats.get(\"domain_loss\", 0.0)\n                validation_results[\"asbn_domain_accuracy\"] = asbn_stats.get(\"domain_accuracy\", 0.0)\n                validation_results[\"asbn_source_accuracy\"] = asbn_stats.get(\"source_accuracy\", 0.0)\n                validation_results[\"asbn_target_accuracy\"] = asbn_stats.get(\"target_accuracy\", 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n                print(f\"  - Source Accuracy: {validation_results['asbn_source_accuracy']:.2%}\")\n                print(f\"  - Target Accuracy: {validation_results['asbn_target_accuracy']:.2%}\")\n            elif asbn and hasattr(asbn, \"get_asbn_stats\"):\n                asbn_stats = asbn.get_asbn_stats()\n                validation_results[\"asbn_domain_loss\"] = asbn_stats.get(\"domain_loss\", 0.0)\n                validation_results[\"asbn_domain_accuracy\"] = asbn_stats.get(\"domain_accuracy\", 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n            else:\n                print(\"  - ASBN statistics not available\")\n        except Exception as e:\n            print(f\"  - ASBN stats retrieval failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] TRG Explanation Statistics:\")\n        try:\n            trg = core_model.trg_system if hasattr(core_model, \"trg_system\") else None\n            if trg and hasattr(trg, \"get_statistics\"):\n                trg_stats = trg.get_statistics()\n                validation_results[\"trg_total_explanations\"] = trg_stats.get(\"explanations_generated\", 0)\n                print(f\"  - Total explanations: {validation_results['trg_total_explanations']}\")\n                print(f\"  - High confidence rate: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                print(f\"  - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n            else:\n                print(\"  - TRG statistics not available\")\n        except Exception as e:\n            print(f\"  - TRG stats retrieval failed: {type(e).__name__}\")\n\n        if confidences:\n            validation_results[\"avg_explanation_confidence\"] = sum(confidences) / len(confidences)\n\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\")\n        print(f\"  - Explanations generated: {validation_results['explanations_generated']}\")\n        print(f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\")\n        print(f\"  - DSCD homographs explained: {validation_results['dscd_homographs_explained']}\")\n        print(f\"  - Reference homographs explained: {validation_results['reference_homographs_explained']}\")\n\n        if dscd_homograph_words_detected:\n            print(f\"  - DSCD homographs detected: {', '.join(sorted(dscd_homograph_words_detected))}\")\n\n        print(f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n        print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n        print(f\"  - ASBN Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n\n        warnings = []\n        if validation_results[\"translations_failed\"] > len(val_sentences) // 2:\n            warnings.append(\"High translation failure rate\")\n        if validation_results[\"explanations_generated\"] == 0:\n            warnings.append(\"No explanations generated\")\n        if validation_results[\"dscd_quality_score\"] < 0.3:\n            warnings.append(\"Low DSCD quality score\")\n        if validation_results[\"dscd_multi_sense_tokens\"] < 10:\n            warnings.append(\"Very few multi-sense tokens\")\n\n        if warnings:\n            print(\"\\n[VALIDATION] Health Warnings:\")\n            for w in warnings:\n                print(f\"  - {w}\")\n        else:\n            print(\"\\n[VALIDATION] All systems healthy\")\n\n        validation_results[\"validation_completed\"] = True\n\n    except Exception as e:\n        print(f\"\\n[VALIDATION] Critical error: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n        validation_results[\"validation_completed\"] = False\n\n    finally:\n        try:\n            if was_training:\n                core_model.train()\n                if _DEBUG_DISCOVERY:\n                    print(f\"[VALIDATION] Model restored to train() mode\")\n            else:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[VALIDATION] Model kept in eval() mode (was not training)\")\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[VALIDATION] Failed to restore model mode: {e}\")\n        clear_all_gpu_caches()\n\n    print(\"=\" * 80 + \"\\n\")\n    return validation_results\n\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model\n        while hasattr(core, \"module\"):\n            core = core.module\n\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n\n        stores = getattr(dscd, \"prototype_stores\", None)\n        if stores is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, \"buffer_lock\"):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, \"clustering_lock\"):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                return len(stores)\n        else:\n            return len(stores)\n\n    except Exception:\n        return 0\n\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model\n        while hasattr(core, \"module\"):\n            core = core.module\n        return getattr(core, \"dscd\", None)\n    except Exception:\n        return None\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n\n    try:\n        dscd_homographs = _get_dscd_homographs(model)\n        items = []\n        homograph_items = []\n\n        lock = None\n        if hasattr(dscd, \"buffer_lock\"):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, \"clustering_lock\"):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores_snapshot = list(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = list(dscd.prototype_stores.items())\n\n        for token, store in stores_snapshot:\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                protos = 0\n                if hasattr(store, \"size\"):\n                    size_method = getattr(store, \"size\", None)\n                    if callable(size_method):\n                        try:\n                            protos = int(size_method())\n                        except Exception:\n                            protos = 0\n                \n                clean_token = (\n                    str(token)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                is_homograph = clean_token in dscd_homographs\n                item = (\n                    token,\n                    total_count,\n                    protos,\n                    len(dscd.buffers.get(token, [])) if hasattr(dscd, \"buffers\") else 0,\n                    is_homograph,\n                )\n                items.append(item)\n                if is_homograph:\n                    homograph_items.append(item)\n            except Exception:\n                continue\n\n        items.sort(key=lambda x: x[1], reverse=True)\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen, is_homo) in enumerate(items[:top_n], 1):\n                marker = \"HOMO\" if is_homo else \"    \"\n                print(\n                    f\"{marker} {i:2d}. {str(tok)[:20]:20s} \"\n                    f\"samples={cnt:4d} protos={prot} buf={buflen}\"\n                )\n            if homograph_items:\n                print(f\"[CLUSTER-DBG] DSCD-discovered homographs: {len(homograph_items)}\")\n                for tok, cnt, prot, buflen, _ in homograph_items[:5]:\n                    print(f\"  HOMO {str(tok)[:20]:20s} samples={cnt:4d} protos={prot}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}\")\n\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model\n        while hasattr(core, \"module\"):\n            core = core.module\n\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        if hasattr(dscd, \"discovered_log\") and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Step {global_step}: {total_discovered} discovery events\")\n\n                recent = (\n                    dscd.discovered_log[-3:]\n                    if len(dscd.discovered_log) >= 3\n                    else dscd.discovered_log\n                )\n                for entry in recent:\n                    discovered = entry.get(\"discovered\", 0)\n                    candidates = entry.get(\"candidates\", 0)\n                    print(f\"  - {discovered}/{candidates} homographs discovered\")\n        else:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] No discoveries yet at step {global_step}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[DISCOVERY-STATUS] Error: {e}\")\n\n\ndef _check_gradients(model: torch.nn.Module, global_step: int):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        asbn = getattr(core, \"asbn\", None)\n        dscd_grad_count = 0\n        dscd_total_params = 0\n        if dscd is not None:\n            for p in dscd.parameters():\n                dscd_total_params += 1\n                if p.grad is not None:\n                    dscd_grad_count += 1\n        asbn_grad_count = 0\n        asbn_total_params = 0\n        if asbn is not None:\n            for p in asbn.parameters():\n                asbn_total_params += 1\n                if p.grad is not None:\n                    asbn_grad_count += 1\n        dscd_status = \"✓\" if dscd_grad_count > 0 else \"✗ NO GRADS\"\n        asbn_status = \"✓\" if asbn_grad_count > 0 else \"✗ NO GRADS\"\n        print(\n            f\"[GRAD-CHECK] Step {global_step}: DSCD {dscd_status} ({dscd_grad_count}/{dscd_total_params}) | ASBN {asbn_status} ({asbn_grad_count}/{asbn_total_params})\"\n        )\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[GRAD-CHECK] Error: {e}\")\n\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True,\n    enable_asbn_training: bool = True,\n) -> torch.nn.Module:\n    global _CELL7_WORDMAP_BUILT_COUNT, _CELL7_WORDMAP_PROVIDED_COUNT\n    global _CELL7_FORWARD_CALL_COUNT, _CELL7_BACKWARD_SUCCESS_COUNT\n\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n\n    print(\n        f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, \"\n        f\"accum_steps={accumulation_steps}\"\n    )\n    print(\n        f\"[TRAIN] Validation: \"\n        f\"{'enabled' if enable_validation and validate_every > 0 else 'disabled'}\"\n    )\n    print(\n        f\"[TRAIN] ASBN Training: \"\n        f\"{'ENABLED' if enable_asbn_training and phi_optimizer is not None else 'DISABLED'}\"\n    )\n    print(\n        f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\"\n    )\n    print(f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(\n        \"[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt \"\n        \"after all epochs\"\n    )\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"THRESHOLD CONFIGURATION VERIFICATION\")\n    print(\"=\" * 80)\n    try:\n        span_th = globals().get(\"SPAN_THRESHOLD\", 0.15)\n        tau_low = globals().get(\"TAU_LOW\", 0.25)\n        unc_th = globals().get(\"UNCERTAINTY_THRESHOLD\", 0.25)\n        trg_unc = globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", 0.25)\n        print(f\"  SPAN_THRESHOLD: {span_th}\")\n        print(f\"  TAU_LOW: {tau_low}\")\n        print(f\"  UNCERTAINTY_THRESHOLD: {unc_th}\")\n        print(f\"  TRG_UNCERTAINTY_THRESHOLD: {trg_unc}\")\n    except Exception as e:\n        print(f\"  Warning: Could not verify thresholds: {e}\")\n    print(\"=\" * 80 + \"\\n\")\n\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=_USE_AMP and torch.cuda.is_available())\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"asbn_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"asbn_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"trg_explanation_history\": [],\n    }\n\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n    last_asbn_loss = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(\"=\" * 80)\n\n        model.train()\n        if _DEBUG_DISCOVERY:\n            print(f\"[TRAIN] Model set to train() at epoch start\")\n\n        try:\n            core = model.module if hasattr(model, \"module\") else model\n            trg = getattr(core, \"trg_system\", None)\n            if trg and hasattr(trg, \"reset_statistics\"):\n                try:\n                    trg.reset_statistics()\n                    print(f\"[TRAIN] TRG statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] TRG stats reset failed: {e}\")\n\n        try:\n            core = model.module if hasattr(model, \"module\") else model\n            asbn = getattr(core, \"asbn\", None)\n            if asbn and hasattr(asbn, \"reset_stats\"):\n                try:\n                    asbn.reset_stats()\n                    print(f\"[TRAIN] ASBN statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] ASBN stats reset failed: {e}\")\n\n        try:\n            optimizer.zero_grad(set_to_none=True)\n            if phi_optimizer is not None:\n                phi_optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n\n        progress = None\n        try:\n            progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", dynamic_ncols=True)\n\n            for batch_idx, batch in enumerate(progress):\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n\n                if _DEBUG_DISCOVERY and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    print(f\"[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} GlobalStep={global_step}\")\n                    _check_discovery_status(model, global_step)\n\n                if global_step % 50 == 0:\n                    try:\n                        core_model = model.module if hasattr(model, \"module\") else model\n                        dscd = getattr(core_model, \"dscd\", None)\n                        if dscd and hasattr(dscd, \"buffers\"):\n                            lock = dscd.buffer_lock if hasattr(dscd, \"buffer_lock\") else None\n                            if lock:\n                                with lock:\n                                    total_buffers = len(dscd.buffers)\n                                    buffer_sizes = [len(dscd.buffers[t]) for t in dscd.buffers]\n                                    avg_size = sum(buffer_sizes) / len(buffer_sizes) if buffer_sizes else 0\n                                    ready_for_discovery = sum(1 for s in buffer_sizes if s >= 10)\n                            else:\n                                total_buffers = len(dscd.buffers)\n                                buffer_sizes = [len(dscd.buffers[t]) for t in dscd.buffers]\n                                avg_size = sum(buffer_sizes) / len(buffer_sizes) if buffer_sizes else 0\n                                ready_for_discovery = sum(1 for s in buffer_sizes if s >= 10)\n                            \n                            if _DEBUG_DISCOVERY:\n                                print(f\"[BUFFER] Step {global_step}: {total_buffers} tokens, avg_size={avg_size:.1f}, ready={ready_for_discovery}\")\n                    except Exception:\n                        pass\n\n                if enable_validation and validate_every and validate_every > 0 and global_step % validate_every == 0:\n                    if accumulated_steps > 0:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n\n                    if _DEBUG_DISCOVERY:\n                        print(f\"[TRAIN] Running validation at step {global_step}\")\n\n                    validation_results = None\n                    try:\n                        validation_results = comprehensive_epoch_validation(\n                            model,\n                            tokenizer,\n                            epoch,\n                            global_step,\n                            _SOURCE_LANGUAGE,\n                            _TARGET_LANGUAGE,\n                            _MAX_LENGTH,\n                            _DEVICE,\n                        )\n                        if validation_results is not None and validation_results.get(\"validation_completed\", False):\n                            training_stats[\"epoch_validations\"].append(validation_results)\n                    finally:\n                        model.train()\n                        if _DEBUG_DISCOVERY:\n                            print(f\"[TRAIN] Model restored to train() after validation\")\n                else:\n                    pending_validation = True\n\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    continue\n\n                try:\n                    input_ids = batch[\"input_ids\"]\n                    attention_mask = batch[\"attention_mask\"]\n                    labels = batch[\"labels\"]\n\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        bsz = int(input_ids.size(0))\n                        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            continue\n                        if keep != bsz:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n\n                    input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                    attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                    labels = labels.to(_DEVICE, non_blocking=True)\n\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        continue\n\n                    token_word_map_for_batch = batch.get(\"token_word_map\", None)\n                    if token_word_map_for_batch is None or not isinstance(token_word_map_for_batch, list):\n                        try:\n                            token_word_map_for_batch = _build_token_to_word_map(tokenizer, input_ids)\n                            _CELL7_WORDMAP_BUILT_COUNT += 1\n                        except Exception:\n                            token_word_map_for_batch = [{} for _ in range(input_ids.size(0))]\n                    else:\n                        _CELL7_WORDMAP_PROVIDED_COUNT += 1\n                    \n                    if not token_word_map_for_batch or len(token_word_map_for_batch) != input_ids.size(0):\n                        token_word_map_for_batch = [{} for _ in range(input_ids.size(0))]\n\n                    _CELL7_FORWARD_CALL_COUNT += 1\n                    forward_kwargs = {\n                        \"input_ids\": input_ids,\n                        \"attention_mask\": attention_mask,\n                        \"labels\": labels,\n                        \"src_texts\": batch.get(\"src_text\", None),\n                        \"token_word_map\": token_word_map_for_batch,\n                    }\n\n                    loss_tensor = None\n                    forward_out = None\n\n                    with get_amp_ctx():\n                        forward_out = model(**forward_kwargs)\n\n                    if isinstance(forward_out, torch.Tensor):\n                        loss_tensor = forward_out\n                    elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                        loss_tensor = forward_out[\"loss\"]\n                    elif isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                        loss_tensor = forward_out[0]\n                    else:\n                        loss_tensor = torch.tensor(0.0, device=_DEVICE, dtype=torch.float32)\n\n                    if loss_tensor is None:\n                        loss_tensor = torch.tensor(0.0, device=_DEVICE, dtype=torch.float32)\n\n                    if not isinstance(loss_tensor, torch.Tensor):\n                        loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE, dtype=torch.float32)\n                    else:\n                        loss_tensor = loss_tensor.to(_DEVICE)\n\n                    if loss_tensor.numel() > 1:\n                        loss_val = float(loss_tensor.mean().item())\n                        loss_tensor = loss_tensor.mean()\n                    else:\n                        loss_val = float(loss_tensor.item())\n\n                    if not torch.isfinite(loss_tensor):\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"nan_loss\"] += 1\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        continue\n\n                    last_forward_loss = loss_val\n                    epoch_losses.append(loss_val)\n                    training_stats[\"total_loss\"].append(loss_val)\n\n                    loss_scaled = loss_tensor / max(1, accumulation_steps)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n\n                    if scaler.is_enabled():\n                        scaler.scale(loss_scaled).backward()\n                    else:\n                        loss_scaled.backward()\n\n                    _CELL7_BACKWARD_SUCCESS_COUNT += 1\n\n                    accumulated_steps += 1\n\n                    if accumulated_steps >= accumulation_steps:\n                        try:\n                            model_core = model.module if hasattr(model, \"module\") else model\n                            \n                            if scaler.is_enabled():\n                                scaler.unscale_(optimizer)\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                                scaler.step(optimizer)\n                                \n                                if phi_optimizer is not None and enable_asbn_training:\n                                    scaler.unscale_(phi_optimizer)\n                                    torch.nn.utils.clip_grad_norm_(\n                                        model_core.asbn.critic_parameters(),\n                                        _GRAD_CLIP_NORM,\n                                    )\n                                    scaler.step(phi_optimizer)\n                                \n                                scaler.update()\n                            else:\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                                optimizer.step()\n                                \n                                if phi_optimizer is not None and enable_asbn_training:\n                                    torch.nn.utils.clip_grad_norm_(\n                                        model_core.asbn.critic_parameters(),\n                                        _GRAD_CLIP_NORM,\n                                    )\n                                    phi_optimizer.step()\n\n                            optimizer.zero_grad(set_to_none=True)\n                            training_stats[\"optimizer_updates\"] += 1\n\n                            if phi_optimizer is not None and enable_asbn_training:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                                training_stats[\"asbn_updates\"] += 1\n\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"[OOM] OOM at step {global_step}\")\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                    if phi_optimizer is not None:\n                                        phi_optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    p.grad = None\n                                clear_all_gpu_caches()\n                                accumulated_steps = 0\n                                continue\n                            else:\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n\n                        except Exception:\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n\n                        finally:\n                            accumulated_steps = 0\n\n                    if pending_validation:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n\n                        validation_results = None\n                        try:\n                            validation_results = comprehensive_epoch_validation(\n                                model,\n                                tokenizer,\n                                epoch,\n                                global_step,\n                                _SOURCE_LANGUAGE,\n                                _TARGET_LANGUAGE,\n                                _MAX_LENGTH,\n                                _DEVICE,\n                            )\n                            if validation_results is not None and validation_results.get(\"validation_completed\", False):\n                                training_stats[\"epoch_validations\"].append(validation_results)\n                        finally:\n                            model.train()\n\n                        pending_validation = False\n\n                    if global_step % DEBUG_PRINT_INTERVAL == 0:\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cluster_count = _get_cluster_count(model)\n                        print(\n                            f\"[TRAIN-DEBUG] step={global_step} \"\n                            f\"loss={last_forward_loss:.4f} asbn_loss={last_asbn_loss:.4f} clusters={cluster_count}\"\n                        )\n                        _print_top_clusters(model, top_n=5)\n\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"[OOM] Caught OOM at step {global_step}\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            p.grad = None\n                        clear_all_gpu_caches()\n                        accumulated_steps = 0\n                        continue\n                    else:\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None:\n                                phi_optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        continue\n\n                except Exception as e:\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    print(f\"[EXCEPTION] Exception at step {global_step}: {type(e).__name__}: {e}\")\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                        if phi_optimizer is not None:\n                            phi_optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    continue\n\n                processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n                expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n                success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n\n                cluster_count = _get_cluster_count(model)\n                next_disc_str = \"NA\"\n                try:\n                    if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                        steps_to_next = _PERIODIC_DISCOVERY_FREQUENCY - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                        if steps_to_next >= _PERIODIC_DISCOVERY_FREQUENCY:\n                            steps_to_next = 0\n                        next_disc_str = f\"next_disc_in={steps_to_next}\"\n                except Exception:\n                    next_disc_str = \"next_disc=err\"\n\n                progress.set_postfix_str(\n                    f\"fwd_loss={last_forward_loss:.4f} asbn_loss={last_asbn_loss:.4f} \"\n                    f\"rate={success_rate:.1f}% clusters={cluster_count} {next_disc_str}\"\n                )\n\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n\n        if accumulated_steps > 0:\n            try:\n                model_core = model.module if hasattr(model, \"module\") else model\n                \n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    scaler.step(optimizer)\n                    if phi_optimizer is not None and enable_asbn_training:\n                        scaler.unscale_(phi_optimizer)\n                        torch.nn.utils.clip_grad_norm_(\n                            model_core.asbn.critic_parameters(),\n                            _GRAD_CLIP_NORM,\n                        )\n                        scaler.step(phi_optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    optimizer.step()\n                    if phi_optimizer is not None and enable_asbn_training:\n                        torch.nn.utils.clip_grad_norm_(\n                            model_core.asbn.critic_parameters(),\n                            _GRAD_CLIP_NORM,\n                        )\n                        phi_optimizer.step()\n\n                optimizer.zero_grad(set_to_none=True)\n                if phi_optimizer is not None:\n                    phi_optimizer.zero_grad(set_to_none=True)\n\n                training_stats[\"optimizer_updates\"] += 1\n                if enable_asbn_training and phi_optimizer is not None:\n                    training_stats[\"asbn_updates\"] += 1\n\n            except Exception:\n                pass\n\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n        expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n        success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n        cluster_count = _get_cluster_count(model)\n\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} SUMMARY\")\n        print(\"=\" * 80)\n        print(f\" Duration (min): {epoch_duration_min:.2f}\")\n        print(f\" Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\" ASBN updates: {training_stats['asbn_updates']}\")\n        print(f\" Batches: processed={processed_batches}, skipped={training_stats['skipped_batches']}\")\n        print(f\" Success rate: {success_rate:.1f}%\")\n        print(f\" Clustered Token Types: {cluster_count}\")\n        print(f\" Avg Epoch Loss: {avg_epoch_loss:.4f}\")\n\n        if skip_reasons:\n            print(f\" Skip reasons: {dict(skip_reasons)}\")\n\n        print(\"=\" * 80)\n\n        try:\n            print(f\"[TRAIN] Running comprehensive validation after epoch {epoch}...\")\n            try:\n                optimizer.zero_grad(set_to_none=True)\n                if phi_optimizer is not None:\n                    phi_optimizer.zero_grad(set_to_none=True)\n            except Exception:\n                pass\n\n            validation_results = None\n            try:\n                validation_results = comprehensive_epoch_validation(\n                    model=model,\n                    tokenizer=tokenizer,\n                    epoch=epoch,\n                    global_step=global_step,\n                    source_lang=_SOURCE_LANGUAGE,\n                    target_lang=_TARGET_LANGUAGE,\n                    max_length=_MAX_LENGTH,\n                    device=_DEVICE,\n                )\n            finally:\n                model.train()\n\n            if validation_results is not None and validation_results.get(\"validation_completed\", False):\n                training_stats[\"epoch_validations\"].append(validation_results)\n                training_stats[\"dscd_quality_history\"].append(validation_results.get(\"dscd_quality_score\", 0.0))\n                training_stats[\"asbn_domain_accuracy_history\"].append(validation_results.get(\"asbn_domain_accuracy\", 0.0))\n                training_stats[\"trg_explanation_history\"].append(validation_results.get(\"trg_total_explanations\", 0))\n\n                try:\n                    dscd = model.module.dscd if hasattr(model, \"module\") else getattr(model, \"dscd\", None)\n                    lock = None\n                    if dscd is not None:\n                        if hasattr(dscd, \"buffer_lock\"):\n                            lock = dscd.buffer_lock\n                        elif hasattr(dscd, \"clustering_lock\"):\n                            lock = dscd.clustering_lock\n\n                    if dscd is not None:\n                        if lock:\n                            with lock:\n                                total_tokens = len(dscd.prototype_stores)\n                        else:\n                            total_tokens = len(dscd.prototype_stores)\n\n                        multi_sense = validation_results.get(\"dscd_multi_sense_tokens\", 0)\n                        ratio = multi_sense / total_tokens if total_tokens > 0 else 0.0\n                        training_stats[\"multi_sense_ratio_history\"].append(ratio)\n                    else:\n                        training_stats[\"multi_sense_ratio_history\"].append(0.0)\n                except Exception:\n                    training_stats[\"multi_sense_ratio_history\"].append(0.0)\n            else:\n                print(\"[TRAIN] Validation incomplete\")\n\n        except Exception as e:\n            print(f\"[TRAIN] Epoch validation failed: {type(e).__name__}\")\n\n        print(\"-\" * 80)\n        if skip_reasons:\n            for k, v in skip_reasons.items():\n                print(f\"  {k}: {v}\")\n        print(\"=\" * 80)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRAINING COMPLETE - SAVING FINAL CHECKPOINT\")\n    print(\"=\" * 80)\n\n    try:\n        checkpoint_dir = Path(\"/kaggle/working\")\n        if not checkpoint_dir.exists():\n            checkpoint_dir = Path(\".\")\n            print(f\"[CHECKPOINT] /kaggle/working not found, using current directory: {checkpoint_dir.absolute()}\")\n        \n        checkpoint_path = checkpoint_dir / \"tatn_final.pt\"\n        core_model = model.module if hasattr(model, \"module\") else model\n\n        dscd_state = {}\n        try:\n            if hasattr(core_model, \"dscd\"):\n                dscd = core_model.dscd\n                lock = None\n                if hasattr(dscd, \"buffer_lock\"):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, \"clustering_lock\"):\n                    lock = dscd.clustering_lock\n                \n                prototype_stores_data = {}\n                if lock:\n                    with lock:\n                        stores = dict(dscd.prototype_stores)\n                else:\n                    stores = dict(dscd.prototype_stores)\n\n                for token, store in stores.items():\n                    try:\n                        centroids = getattr(store, \"centroids\", None)\n                        counts = getattr(store, \"counts\", None)\n                        \n                        if isinstance(centroids, torch.Tensor) and centroids.size(0) > 0:\n                            if isinstance(counts, list) and len(counts) > 0:\n                                if len(counts) == centroids.size(0) and sum(counts) > 0:\n                                    store_data = {\n                                        'centroids': centroids.tolist(),\n                                        'counts': [int(c) for c in counts]\n                                    }\n                                    prototype_stores_data[str(token)] = store_data\n                    except Exception:\n                        continue\n\n                dscd_state = {\n                    'prototype_stores_data': prototype_stores_data\n                }\n        except Exception:\n            dscd_state = {}\n\n        checkpoint_data = {\n            \"epochs_trained\": epochs,\n            \"global_steps\": global_step,\n            \"final_train_loss\": training_stats[\"epoch_losses\"][-1] if training_stats[\"epoch_losses\"] else 0.0,\n            \"model_state_dict\": core_model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"phi_optimizer_state_dict\": phi_optimizer.state_dict() if phi_optimizer is not None else None,\n            \"scaler_state_dict\": scaler.state_dict() if scaler is not None else None,\n            \"training_stats\": training_stats,\n            \"dscd_state\": dscd_state,\n            \"config\": {\n                \"SPAN_THRESHOLD\": globals().get(\"SPAN_THRESHOLD\", 0.15),\n                \"TAU_LOW\": globals().get(\"TAU_LOW\", 0.25),\n                \"UNCERTAINTY_THRESHOLD\": globals().get(\"UNCERTAINTY_THRESHOLD\", 0.25),\n                \"TRG_UNCERTAINTY_THRESHOLD\": globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", 0.25),\n                \"LAMBDA_ASBN\": globals().get(\"LAMBDA_ASBN\", 0.05),\n                \"LAMBDA_DSCD\": globals().get(\"LAMBDA_DSCD\", 0.15),\n                \"TRG_TEMPERATURE\": globals().get(\"TRG_TEMPERATURE\", 1.0),\n                \"PERIODIC_DISCOVERY_FREQUENCY\": _PERIODIC_DISCOVERY_FREQUENCY,\n                \"NUM_EPOCHS\": epochs,\n                \"BATCH_SIZE\": _BATCH_SIZE,\n                \"LEARNING_RATE\": optimizer.param_groups[0][\"lr\"] if optimizer.param_groups else 0.0,\n            },\n        }\n\n        torch.save(checkpoint_data, checkpoint_path)\n        print(f\"[CHECKPOINT] Saved to {checkpoint_path}\")\n        print(f\"[CHECKPOINT] Size: {checkpoint_path.stat().st_size / (1024**2):.2f} MB\")\n\n    except Exception as e:\n        print(f\"[CHECKPOINT] Failed to save: {type(e).__name__}: {e}\")\n\n    print(\"=\" * 80)\n    print(\"FINAL TRAINING STATISTICS\")\n    print(\"=\" * 80)\n\n    processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n    expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n    success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n\n    print(f\"[TRAIN] Success Rate: {success_rate:.1f}%\")\n    print(f\"[TRAIN] Total Steps: {global_step}\")\n    print(f\"[TRAIN] ASBN Updates: {training_stats['asbn_updates']}\")\n    print(f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\")\n\n    if training_stats[\"dscd_quality_history\"]:\n        print(\"[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats[\"dscd_quality_history\"], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n\n    if training_stats[\"asbn_domain_accuracy_history\"]:\n        print(\"[TRAIN] ASBN Domain Accuracy Trend:\")\n        for i, acc in enumerate(training_stats[\"asbn_domain_accuracy_history\"], 1):\n            print(f\"  Epoch {i}: {acc:.1%}\")\n\n    if training_stats[\"trg_explanation_history\"]:\n        print(\"[TRAIN] TRG Explanation Count Trend:\")\n        for i, count in enumerate(training_stats[\"trg_explanation_history\"], 1):\n            print(f\"  Epoch {i}: {count} explanations\")\n\n    print(\"=\" * 80)\n    return model\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training loop ready - M2M100 PYTORCH 2.6 COMPATIBLE - FIXED\")\nprint(\"=\" * 80)\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================================\n# CELL 8: INFERENCE PIPELINE WITH COMPLETE TRG INTEGRATION - FIXED VERSION\n# ================================================================================\n\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\nimport threading\nimport gc\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.05\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TRG_UNCERTAINTY_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _TRG_UNCERTAINTY_THRESHOLD = 0.25\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = int(MAX_EXPLANATIONS_PER_SENTENCE)\nexcept (NameError, ValueError, TypeError):\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\",\n        \"তারা\", \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_EN_TOKEN_ID = 128022\n\n_SUBWORD_PUNCT_SET = {\".\", \",\", \"!\", \"?\", \"-\"}\n\n\ndef get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, \"buffer_lock\"):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, \"clustering_lock\"):\n            lock = dscd.clustering_lock\n\n        word_prototype_counts = defaultdict(int)\n\n        if lock:\n            with lock:\n                stores = dict(dscd.prototype_stores)\n        else:\n            stores = dict(dscd.prototype_stores)\n\n        for token_key, store in stores.items():\n            try:\n                num_protos = 0\n                if hasattr(store, \"size\"):\n                    size_method = getattr(store, \"size\", None)\n                    if callable(size_method):\n                        try:\n                            num_protos = int(size_method())\n                        except Exception:\n                            num_protos = 0\n\n                clean_token = (\n                    str(token_key)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean_token:\n                    word_prototype_counts[clean_token] = max(\n                        word_prototype_counts[clean_token], num_protos\n                    )\n            except Exception:\n                continue\n\n        for word, count in word_prototype_counts.items():\n            if count >= 2:\n                homographs.add(word)\n\n        return homographs\n    except Exception:\n        return set()\n\n\ndef build_token_to_word_map(tokenizer, input_ids):\n    batch_word_maps = []\n\n    for batch_idx in range(input_ids.size(0)):\n        tokens = tokenizer.convert_ids_to_tokens(input_ids[batch_idx].tolist())\n        word_map = {}\n        current_word = \"\"\n        word_start_idx = 0\n\n        for i, token in enumerate(tokens):\n            if token in [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"\"]:\n                word_map[i] = None\n                continue\n\n            if token.startswith(\"▁\"):\n                if current_word:\n                    clean_word = current_word.replace(\"▁\", \"\").strip()\n                    if clean_word:\n                        for j in range(word_start_idx, i):\n                            word_map[j] = clean_word\n\n                current_word = token\n                word_start_idx = i\n            else:\n                current_word += token\n\n        if current_word:\n            clean_word = current_word.replace(\"▁\", \"\").strip()\n            if clean_word:\n                for j in range(word_start_idx, len(tokens)):\n                    word_map[j] = clean_word\n\n        batch_word_maps.append(word_map)\n\n    return batch_word_maps\n\n\nclass InferenceStatistics:\n    def __init__(self):\n        self.lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self.lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.avg_span = 0.0\n            self.avg_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        with self.lock:\n            self.total_inferences += 1\n\n            if result.get(\"translation\") and result[\"translation\"] != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get(\"explanations\", [])\n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                try:\n                    conf = exp.get(\"confidence\", 0.5)\n                    self.total_confidence += float(conf)\n\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf >= 0.4:\n                        pass\n                    else:\n                        self.low_confidence_explanations += 1\n\n                    word = str(exp.get(\"ambiguous_word\", \"\")).strip()\n                    clean_word = (\n                        word.replace(\"▁\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .replace(\"##\", \"\")\n                        .replace(\"@@\", \"\")\n                        .replace(\"</w>\", \"\")\n                        .lower()\n                    )\n\n                    if clean_word:\n                        self.token_counts[clean_word] += 1\n                        self.token_confidences[clean_word].append(float(conf))\n\n                        if dscd_homographs and clean_word in dscd_homographs:\n                            self.dscd_homographs_explained.add(clean_word)\n\n                        if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                            self.reference_homographs_explained.add(clean_word)\n\n                    self.avg_span = float(exp.get(\"span\", 0.0))\n                    self.avg_uncertainty = float(exp.get(\"uncertainty\", 0.0))\n                except Exception:\n                    pass\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self.lock:\n            total_exp = max(self.total_explanations, 1)\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n\n            return {\n                \"total_inferences\": self.total_inferences,\n                \"successful_translations\": self.successful_translations,\n                \"failed_translations\": self.failed_translations,\n                \"success_rate\": self.successful_translations / max(self.total_inferences, 1),\n                \"total_explanations\": self.total_explanations,\n                \"explanations_per_inference\": self.total_explanations / max(self.total_inferences, 1),\n                \"high_confidence_rate\": self.high_confidence_explanations / total_exp,\n                \"low_confidence_rate\": self.low_confidence_explanations / total_exp,\n                \"avg_confidence\": self.total_confidence / total_exp,\n                \"avg_span\": self.avg_span / total_exp,\n                \"avg_uncertainty\": self.avg_uncertainty / total_exp,\n                \"dscd_homographs_explained\": list(self.dscd_homographs_explained),\n                \"reference_homographs_explained\": list(self.reference_homographs_explained),\n                \"dscd_empty_warnings\": self.dscd_empty_warnings,\n                \"unique_tokens_explained\": unique_tokens,\n                \"diversity_ratio\": diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2f}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n\n        if summary[\"dscd_homographs_explained\"]:\n            print(f\"DSCD homographs explained: {len(summary['dscd_homographs_explained'])}\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'])}\")\n\n        if summary[\"reference_homographs_explained\"]:\n            print(f\"Reference homographs explained: {len(summary['reference_homographs_explained'])}\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'])}\")\n\n        if summary[\"dscd_empty_warnings\"] > 0:\n            print(f\"DSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80)\n\n\nINFERENCE_STATS = InferenceStatistics()\n\n\ndef to_device(batch_enc: Any, device: torch.device):\n    try:\n        if hasattr(batch_enc, \"to\"):\n            return batch_enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(batch_enc, dict):\n        out = {}\n        for k, v in batch_enc.items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                elif isinstance(v, dict):\n                    out[k] = to_device(v, device)\n                elif isinstance(v, (list, tuple)):\n                    out[k] = [t.to(device) if isinstance(t, torch.Tensor) else t for t in v]\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n\n    return batch_enc\n\n\ndef extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    if raw_out is None:\n        return {}\n\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"explanations\" in raw_out or \"proto_probs\" in raw_out:\n            return raw_out\n        for key in [\"dscd_outputs\", \"dscd\", \"dscd_out\"]:\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n        return raw_out\n\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return extract_dscd_outputs(item)\n\n    return {}\n\n\ndef get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    if not dscd:\n        return []\n\n    expl = dscd.get(\"explanations\", None)\n    if expl is None:\n        for alt in [\"explanations_per_sentence\", \"trg_explanations\", \"exps\"]:\n            if alt in dscd:\n                expl = dscd[alt]\n                break\n\n    if expl is None:\n        return []\n\n    if isinstance(expl, list):\n        if len(expl) > 0 and isinstance(expl[0], dict):\n            return [expl]\n        if len(expl) > 0 and isinstance(expl[0], list):\n            return expl\n\n    return []\n\n\ndef is_subword_token(token: str) -> bool:\n    if not token or len(token.strip()) == 0:\n        return True\n\n    token = token.strip()\n\n    if token.startswith(\"▁\") or token.startswith(\"Ġ\") or token.startswith(\"##\") or token.startswith(\"@@\"):\n        return True\n\n    if len(token) <= 2:\n        return True\n\n    if (len(token) == 1 and token in _SUBWORD_PUNCT_SET) or token.isdigit():\n        return True\n\n    return False\n\n\ndef should_filter_explanation(expl: Dict[str, Any], span_th: float, ut_h: float) -> bool:\n    try:\n        token = expl.get(\"ambiguous_word\", expl.get(\"token\", \"\"))\n        span = float(expl.get(\"span\", 0.0))\n        uncertainty = float(expl.get(\"uncertainty\", 0.0))\n\n        if is_subword_token(str(token)):\n            return True\n\n        if span < span_th and uncertainty > ut_h:\n            return True\n\n        return False\n    except Exception:\n        return True\n\n\ndef force_english_bos(tokenizer, mbart_model) -> Optional[int]:\n    forced_id = None\n\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in (_TARGET_LANGUAGE, \"en_XX\", \"en\", \"eng\"):\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = int(lid)\n                        break\n                except Exception:\n                    continue\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            forced_id = tokenizer.lang_code_to_id.get(_TARGET_LANGUAGE, None)\n            if forced_id is not None:\n                forced_id = int(forced_id)\n    except Exception:\n        forced_id = None\n\n    if forced_id is None:\n        forced_id = _M2M100_EN_TOKEN_ID\n\n    if forced_id is not None and hasattr(mbart_model, \"config\"):\n        try:\n            mbart_model.config.forced_bos_token_id = int(forced_id)\n            mbart_model.config.decoder_start_token_id = int(forced_id)\n        except Exception:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\"[INF] Could not set forced BOS on mbart config\")\n\n    return forced_id\n\n\ndef safe_generate(mbart, input_ids=None, encoder_outputs=None, attention_mask=None, max_length=64, num_beams=4, **kwargs):\n    try:\n        if encoder_outputs is not None:\n            return mbart.generate(\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n        else:\n            return mbart.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            if _DEBUG_DISCOVERY:\n                print(\"[INF] OOM during generation, reducing beam size...\")\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            if encoder_outputs is not None:\n                return mbart.generate(\n                    encoder_outputs=encoder_outputs,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n            else:\n                return mbart.generate(\n                    input_ids,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n        else:\n            raise\n\n\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n\n    device = _DEVICE if device is None else device\n    span_th = _SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    ut_h = _UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[INF] Starting inference\")\n        print(f\"[INF] Input: {input_sentence[:60]}\")\n        print(f\"[INF] Thresholds: span={span_th:.2f}, uncertainty={ut_h:.2f}\")\n\n    cleanup_vars = []\n    dscd_homographs = get_dscd_homographs(model)\n    encoder_hidden = None\n    encoder_hidden_adjusted = None\n\n    try:\n        try:\n            tokenizer.src_lang = _SOURCE_LANGUAGE\n        except Exception:\n            pass\n\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=_MAX_LENGTH,\n        )\n        enc = to_device(enc, device)\n        cleanup_vars.append(enc)\n\n        model.eval()\n        core = model.module if _USE_MULTI_GPU and hasattr(model, \"module\") else model\n        src_texts = [input_sentence]\n        dscd_validated = False\n\n        try:\n            dscd = core.dscd if hasattr(core, \"dscd\") else None\n            if dscd:\n                lock = None\n                if hasattr(dscd, \"buffer_lock\"):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, \"clustering_lock\"):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        num_stores = len(dscd.prototype_stores)\n                        multi_sense = sum(1 for store in dscd.prototype_stores.values() if store.size() >= 2)\n                else:\n                    num_stores = len(dscd.prototype_stores)\n                    multi_sense = sum(1 for store in dscd.prototype_stores.values() if store.size() >= 2)\n\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD state: {num_stores} tokens, {multi_sense} multi-sense, {len(dscd_homographs)} discovered\")\n\n                if num_stores == 0:\n                    print(\"[INF] WARNING: DSCD prototype stores are EMPTY\")\n                    if track_stats:\n                        INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] DSCD validation failed: {e}\")\n\n        with torch.inference_mode():\n            raw_dscd_out = {}\n            sense_probs = None\n\n            try:\n                if not hasattr(core, \"mbart\"):\n                    raise RuntimeError(\"Model backend missing .mbart\")\n\n                mbart = core.mbart\n\n                encoder_outputs_raw = mbart.model.encoder(\n                    input_ids=enc.get(\"input_ids\"),\n                    attention_mask=enc.get(\"attention_mask\"),\n                )\n                cleanup_vars.append(encoder_outputs_raw)\n\n                if hasattr(encoder_outputs_raw, \"last_hidden_state\"):\n                    encoder_hidden = encoder_outputs_raw.last_hidden_state\n                elif isinstance(encoder_outputs_raw, tuple):\n                    encoder_hidden = encoder_outputs_raw[0]\n                else:\n                    encoder_hidden = encoder_outputs_raw\n\n                cleanup_vars.append(encoder_hidden)\n\n                if not isinstance(encoder_hidden, torch.Tensor) or encoder_hidden.dim() != 3:\n                    raise RuntimeError(\n                        f\"Invalid encoder hidden: type={type(encoder_hidden)}, \"\n                        f\"shape={encoder_hidden.shape if isinstance(encoder_hidden, torch.Tensor) else 'N/A'}\"\n                    )\n\n                if _DEBUG_DISCOVERY:\n                    print(f\"[INF] Encoder hidden: {encoder_hidden.shape}\")\n\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                            use_dscd=True,\n                            use_asbn=False,\n                        )\n                    except TypeError:\n                        try:\n                            raw_dscd_out = core.forward_with_explanations(\n                                enc.get(\"input_ids\"),\n                                enc.get(\"attention_mask\"),\n                                src_texts,\n                            )\n                        except Exception:\n                            raw_dscd_out = core.forward_with_explanations(\n                                input_ids=enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                src_texts=src_texts,\n                            )\n                else:\n                    if _DEBUG_DISCOVERY:\n                        print(\"[INF] forward_with_explanations not found, using forward\")\n                    out = core.forward(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=src_texts,\n                        labels=None,\n                        use_dscd=True,\n                        use_asbn=False,\n                    )\n                    if isinstance(out, dict):\n                        raw_dscd_out = extract_dscd_outputs(out)\n\n                dscd_out = extract_dscd_outputs(raw_dscd_out)\n\n                if isinstance(raw_dscd_out, dict) and \"sense_augmented_embeddings\" in raw_dscd_out:\n                    encoder_hidden_adjusted = raw_dscd_out[\"sense_augmented_embeddings\"]\n                elif \"h_augmented\" in dscd_out:\n                    encoder_hidden_adjusted = dscd_out[\"h_augmented\"]\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n\n                cleanup_vars.append(encoder_hidden_adjusted)\n\n                if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    if encoder_hidden_adjusted.shape != encoder_hidden.shape:\n                        if _DEBUG_DISCOVERY:\n                            print(\"[INF] Shape mismatch, using original\")\n                        encoder_hidden_adjusted = encoder_hidden\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n\n                if \"proto_probs\" in dscd_out:\n                    sense_probs = dscd_out[\"proto_probs\"]\n                elif \"sense_probs\" in dscd_out:\n                    sense_probs = dscd_out[\"sense_probs\"]\n\n                if _DEBUG_DISCOVERY:\n                    print(\"[INF] DSCD forward completed\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD forward error: {e}\")\n                raw_dscd_out = {}\n                if \"encoder_hidden\" in locals() and encoder_hidden is not None:\n                    encoder_hidden_adjusted = encoder_hidden\n                else:\n                    encoder_hidden_adjusted = None\n\n            forced_id = force_english_bos(tokenizer, mbart)\n            orig_use_cache = getattr(mbart.config, \"use_cache\", None) if hasattr(mbart, \"config\") else None\n\n            if hasattr(mbart, \"config\"):\n                try:\n                    mbart.config.use_cache = True\n                except Exception:\n                    pass\n\n            try:\n                if _DEBUG_DISCOVERY:\n                    print(\"[INF] Generating translation...\")\n\n                if encoder_hidden_adjusted is not None and isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    encoder_hidden_adjusted = encoder_hidden_adjusted.to(device)\n                    from transformers.modeling_outputs import BaseModelOutput\n                    encoder_outputs_for_decoder = BaseModelOutput(last_hidden_state=encoder_hidden_adjusted)\n\n                    generated = safe_generate(\n                        mbart,\n                        encoder_outputs=encoder_outputs_for_decoder,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=4,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                        repetition_penalty=2.5,\n                        no_repeat_ngram_size=2,\n                        length_penalty=1.0,\n                        do_sample=False,\n                    )\n                else:\n                    generated = safe_generate(\n                        mbart,\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=4,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                        repetition_penalty=2.5,\n                        no_repeat_ngram_size=2,\n                        length_penalty=1.0,\n                        do_sample=False,\n                    )\n\n                cleanup_vars.append(generated)\n\n                translation = (\n                    tokenizer.decode(generated[0], skip_special_tokens=True)\n                    if generated is not None and len(generated) > 0\n                    else \"\"\n                )\n\n                if _DEBUG_DISCOVERY:\n                    print(f\"[INF] Translation: {translation[:60]}\")\n            finally:\n                if hasattr(mbart, \"config\") and orig_use_cache is not None:\n                    try:\n                        mbart.config.use_cache = orig_use_cache\n                    except Exception:\n                        pass\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] Calling TRG for explanations...\")\n\n            sentence_explanations = []\n\n            try:\n                trg = getattr(core, \"trg_system\", None)\n                if trg and hasattr(trg, \"process_sentence_for_explanations\"):\n                    token_word_map = build_token_to_word_map(tokenizer, enc.get(\"input_ids\"))\n\n                    if token_word_map and len(token_word_map) > 0:\n                        token_word_map_single = token_word_map[0]\n                    else:\n                        token_word_map_single = {}\n\n                    tokens_batch = tokenizer.convert_ids_to_tokens(enc.get(\"input_ids\")[0].tolist())\n\n                    dscd_for_trg = {}\n                    if \"uncertainties\" in dscd_out:\n                        dscd_for_trg[\"uncertainties\"] = dscd_out[\"uncertainties\"]\n                    if \"span_preds\" in dscd_out:\n                        dscd_for_trg[\"span_preds\"] = dscd_out[\"span_preds\"]\n                    if \"gates\" in dscd_out:\n                        dscd_for_trg[\"gates\"] = dscd_out[\"gates\"]\n                    if \"proto_probs\" in dscd_out:\n                        dscd_for_trg[\"proto_probs\"] = dscd_out[\"proto_probs\"]\n\n                    if _DEBUG_DISCOVERY:\n                        print(f\"[INF] TRG call with token_word_map: {len(token_word_map_single)} entries\")\n                        print(f\"[INF] TRG tokens: {len(tokens_batch)}\")\n\n                    try:\n                        trg_result = trg.process_sentence_for_explanations(\n                            tokens=tokens_batch,\n                            dscd_outputs=dscd_for_trg,\n                            token_word_map=token_word_map_single,\n                            uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                            span_threshold=_SPAN_THRESHOLD,\n                            decoder_attention=None,\n                            max_explanations=_MAX_EXPLANATIONS_PER_SENTENCE,\n                        )\n\n                        if isinstance(trg_result, list):\n                            sentence_explanations = trg_result\n                            if _DEBUG_DISCOVERY:\n                                print(f\"[INF] TRG generated {len(sentence_explanations)} explanations\")\n                        else:\n                            if _DEBUG_DISCOVERY:\n                                print(f\"[INF] TRG returned unexpected type: {type(trg_result)}\")\n                            sentence_explanations = []\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                            print(f\"[INF] TRG call error: {e}\")\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n                        sentence_explanations = []\n                else:\n                    if _DEBUG_DISCOVERY:\n                        print(\"[INF] TRG not available, falling back to DSCD outputs\")\n                    explanations_list = get_explanations_list(dscd_out)\n                    sentence_explanations = (\n                        explanations_list[0] if isinstance(explanations_list, list) and len(explanations_list) > 0 else []\n                    )\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(f\"[INF] TRG processing error: {e}\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n                dscd_out = extract_dscd_outputs(raw_dscd_out)\n                explanations_list = get_explanations_list(dscd_out)\n                sentence_explanations = (\n                    explanations_list[0] if isinstance(explanations_list, list) and len(explanations_list) > 0 else []\n                )\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] Raw explanations: {len(sentence_explanations)}\")\n\n            def is_real_ambiguity(e: Dict[str, Any]) -> bool:\n                try:\n                    s = float(e.get(\"span\", 0.0))\n                    u = float(e.get(\"uncertainty\", 0.0))\n                    return s >= span_th or u <= ut_h\n                except Exception:\n                    return False\n\n            real_amb_count = 0\n            out_explanations: List[Dict[str, Any]] = []\n            filtered_count = 0\n\n            quality_metrics = {\n                \"total_raw_explanations\": len(sentence_explanations) if isinstance(sentence_explanations, list) else 0,\n                \"filtered_explanations\": 0,\n                \"high_confidence_count\": 0,\n                \"low_confidence_count\": 0,\n                \"avg_confidence\": 0.0,\n                \"avg_span\": 0.0,\n                \"avg_uncertainty\": 0.0,\n            }\n\n            confidences: List[float] = []\n            spans: List[float] = []\n            uncertainties: List[float] = []\n\n            if isinstance(sentence_explanations, list):\n                for ex in sentence_explanations:\n                    try:\n                        word = ex.get(\"ambiguous_word\", ex.get(\"token\", \"\"))\n                        if not isinstance(word, str):\n                            word = \"\"\n\n                        clean_word = (\n                            word.replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .replace(\"##\", \"\")\n                            .replace(\"@@\", \"\")\n                            .replace(\"</w>\", \"\")\n                            .strip()\n                        )\n\n                        if clean_word:\n                            ex[\"ambiguous_word\"] = clean_word\n\n                        if should_filter_explanation(ex, span_th, ut_h):\n                            filtered_count += 1\n                            continue\n\n                        is_real = is_real_ambiguity(ex)\n                        if is_real:\n                            real_amb_count += 1\n\n                        confidence = ex.get(\"confidence\", None)\n                        if confidence is None:\n                            s = float(ex.get(\"span\", 0.0))\n                            u = float(ex.get(\"uncertainty\", 0.0))\n                            confidence = (s + (1.0 - u)) / 2.0\n\n                        confidence = float(confidence)\n                        confidences.append(confidence)\n                        spans.append(float(ex.get(\"span\", 0.0)))\n                        uncertainties.append(float(ex.get(\"uncertainty\", 0.0)))\n\n                        if confidence >= 0.65:\n                            quality_metrics[\"high_confidence_count\"] += 1\n                        elif confidence >= 0.4:\n                            pass\n                        else:\n                            quality_metrics[\"low_confidence_count\"] += 1\n\n                        out_explanations.append({\n                            \"ambiguous_word\": ex.get(\"ambiguous_word\", ex.get(\"token\", \"N/A\")),\n                            \"position\": ex.get(\"position\", ex.get(\"token_idx\", \"N/A\")),\n                            \"explanation\": ex.get(\"explanation\", ex.get(\"explain\", \"\")),\n                            \"uncertainty\": float(ex.get(\"uncertainty\", 0.0)),\n                            \"span\": float(ex.get(\"span\", 0.0)),\n                            \"confidence\": confidence,\n                            \"is_real_amb\": bool(is_real),\n                        })\n\n                    except Exception:\n                        continue\n\n            quality_metrics[\"filtered_explanations\"] = filtered_count\n\n            if confidences:\n                quality_metrics[\"avg_confidence\"] = sum(confidences) / len(confidences)\n            if spans:\n                quality_metrics[\"avg_span\"] = sum(spans) / len(spans)\n            if uncertainties:\n                quality_metrics[\"avg_uncertainty\"] = sum(uncertainties) / len(uncertainties)\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] Final: {len(out_explanations)} explanations (filtered {filtered_count})\")\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n\n            if track_stats:\n                INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n            return result\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": str(e)[:200],\n        }\n\n        if track_stats:\n            INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"আমি কল বন্ধ করেছি।\",\n            \"কাল আমি বই কিনব।\",\n            \"পাতা ঝরে পড়েছে।\",\n            \"তিনি ব্যাংক গেছেন।\",\n            \"আমি ভালো আছি।\",\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations\")\n    print(\"=\" * 80)\n\n    INFERENCE_STATS.reset()\n\n    for s in sentences:\n        print(f\"\\n{s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n\n        print(f\"Translation: {res.get('translation', 'N/A')}\")\n        print(f\"Ambiguous words detected: {res.get('ambiguous_words_detected', 0)}\")\n\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, high={quality.get('high_confidence_count', 0)}, low={quality.get('low_confidence_count', 0)}\")\n\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(f\"  {idx}. {ex['ambiguous_word']} (pos={ex.get('position', 'N/A')}, conf={ex['confidence']:.3f})\")\n                print(f\"     {ex.get('explanation', 'N/A')[:200]}\")\n        else:\n            print(\"  No explanations\")\n\n    print(\"=\" * 80)\n    INFERENCE_STATS.print_summary()\n\n\ndef dscd_discovery_warmup(model, tokenizer, num_sents: int = 8000, batch_size: int = 64, max_len: Optional[int] = None):\n    if max_len is None:\n        max_len = _MAX_LENGTH\n\n    core = model.module if _USE_MULTI_GPU and hasattr(model, \"module\") else model\n\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component\")\n            return\n\n        print(\"=\" * 80)\n        print(\"[WARMUP] Starting DSCD discovery warmup\")\n        print(\"=\" * 80)\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_nmin = getattr(dscd, \"nmin\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"nmin\"):\n                dscd.nmin = max(3, int(getattr(dscd, \"nmin\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for bn, _ in pairs[:num_sents]]\n            else:\n                base = [\"আমি কল বন্ধ করেছি।\", \"কাল আমি বই কিনব।\", \"পাতা ঝরে পড়েছে।\", \"তিনি ব্যাংক গেছেন।\", \"আমি ভালো আছি।\"]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n        except Exception:\n            texts = num_sents\n\n        processed = 0\n        core.eval()\n        print(f\"[WARMUP] Processing {len(texts)} sentences in batches of {batch_size}...\")\n\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n\n                try:\n                    enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n                    enc = to_device(enc, _DEVICE)\n\n                    if hasattr(core, \"forward_with_explanations\"):\n                        core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch,\n                            use_dscd=True,\n                            use_asbn=False,\n                        )\n                    else:\n                        core.mbart.model.encoder(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                        )\n\n                    processed += len(batch)\n\n                    current_time = time.time()\n                    if i % (batch_size * 10) == 0 or current_time - last_print > 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n\n                        print(\n                            f\"[WARMUP] {processed}/{len(texts)} \"\n                            f\"({processed/len(texts)*100:.1f}%) \"\n                            f\"rate={rate:.1f} sents/s \"\n                            f\"ETA {eta:.0f}s\"\n                        )\n                        last_print = current_time\n\n                    del enc\n\n                except Exception as e:\n                    print(f\"[WARMUP] Batch {i}/{batch_size} failed: {str(e)[:100]}\")\n                    continue\n\n        total_time = time.time() - start_time\n        print(f\"[WARMUP] Completed in {total_time:.1f}s ({processed/total_time:.1f} sents/s)\")\n        print(\"-\" * 80)\n\n        try:\n            lock = None\n            if hasattr(dscd, \"buffer_lock\"):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, \"clustering_lock\"):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            num_types = len(stores)\n            total_protos = sum(store.size() for store in stores.values()) if stores else 0\n            multi = sum(1 for store in stores.values() if store.size() >= 2) if stores else 0\n\n            print(\"[WARMUP] Summary:\")\n            print(f\"  - Token types: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n\n            dscd_homographs = get_dscd_homographs(model)\n            print(f\"[WARMUP] Discovered Homographs: {len(dscd_homographs)}\")\n\n            if dscd_homographs:\n                print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n            reference_found = dscd_homographs.intersection(_HOMOGRAPH_REFERENCE_LIST)\n            print(f\"[WARMUP] Reference List Comparison:\")\n            print(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n            print(f\"  - Found in DSCD: {len(reference_found)}\")\n            print(f\"  - Coverage: {len(reference_found)/len(_HOMOGRAPH_REFERENCE_LIST):.1%}\")\n\n            if num_types == 0:\n                print(\"[WARMUP] CRITICAL: NO PROTOTYPES CREATED\")\n            elif len(reference_found) < len(_HOMOGRAPH_REFERENCE_LIST) * 0.3:\n                print(\"[WARMUP] WARNING: Low reference coverage\")\n\n        except Exception as e:\n            print(f\"[WARMUP] Summary generation error: {e}\")\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\") and orig_enable is not None:\n                dscd.enable_training_clustering = orig_enable\n            if hasattr(dscd, \"nmin\") and orig_nmin is not None:\n                dscd.nmin = orig_nmin\n            if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n        print(\"=\" * 80)\n\n    except Exception as e:\n        print(f\"[WARMUP] ERROR: {type(e).__name__}: {str(e)}\")\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n\nprint(\"=\" * 80)\nprint(\"Cell 8: Inference pipeline ready with complete TRG integration\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Source language: {_SOURCE_LANGUAGE}\")\nprint(f\"  - Target language: {_TARGET_LANGUAGE}\")\nprint(f\"  - Max length: {_MAX_LENGTH}\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - TRG uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Max explanations per sentence: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(\"=\" * 80)\n","metadata":{"id":"7Dxg7ck0H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION (PURE DATA-DRIVEN) - M2M100 FIXED\n# ==============================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.05\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                return len(stores)\n        else:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            return len(stores)\n    except Exception:\n        return 0\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        word_prototype_counts = defaultdict(int)\n\n        if lock:\n            with lock:\n                prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                for token_key, store in prototype_stores.items():\n                    try:\n                        num_protos = store.size() if hasattr(store, 'size') else len(getattr(store, 'centroids', []))\n                        clean_token = (\n                            str(token_key)\n                            .replace('▁', '')\n                            .replace('Ġ', '')\n                            .replace('##', '')\n                            .replace('@@', '')\n                            .replace('</w>', '')\n                            .strip()\n                            .lower()\n                        )\n                        if clean_token:\n                            word_prototype_counts[clean_token] = max(word_prototype_counts[clean_token], num_protos)\n                    except Exception:\n                        continue\n        else:\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            for token_key, store in prototype_stores.items():\n                try:\n                    num_protos = store.size() if hasattr(store, 'size') else len(getattr(store, 'centroids', []))\n                    clean_token = (\n                        str(token_key)\n                        .replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace('@@', '')\n                        .replace('</w>', '')\n                        .strip()\n                        .lower()\n                    )\n                    if clean_token:\n                        word_prototype_counts[clean_token] = max(word_prototype_counts[clean_token], num_protos)\n                except Exception:\n                    continue\n\n        for word, count in word_prototype_counts.items():\n            if count >= 2:\n                homographs.add(word)\n\n        return homographs\n    except Exception:\n        return set()\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = store.size() if hasattr(store, 'size') else 0\n            except Exception:\n                n_protos = 0\n            \n            is_valid = True\n            try:\n                if not hasattr(store, 'centroids') or len(getattr(store, 'centroids', [])) == 0:\n                    is_valid = False\n                if not hasattr(store, 'counts') or sum(getattr(store, 'counts', [])) <= 0:\n                    is_valid = False\n            except Exception:\n                is_valid = False\n            \n            if is_valid:\n                cluster_info.append({\n                    'token': token,\n                    'count': total_count,\n                    'protos': n_protos,\n                    'mu': getattr(store, \"mu\", 0.0),\n                    'tau': getattr(store, \"tau\", 0.0)\n                })\n\n        cluster_info.sort(key=lambda x: x['count'], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info['token'])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n\n        print(\"-\" * 90)\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Pure Data-Driven)\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\", [\"কল\"]),\n        (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\", [\"কাল\"]),\n        (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\", [\"পাতা\"]),\n        (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\", [\"ব্যাংক\"]),\n        (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল = fruit/result\", [\"ফল\"]),\n        (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা = head/top\", [\"মাথা\"]),\n        (\"কল থেকে কল এসেছে।\", \"A call came from the tap\", \"Multiple কল\", [\"কল\"]),\n        (\"কালকে কাল মেঘ দেখা গেছে।\", \"Yesterday black clouds were seen\", \"Multiple কাল\", [\"কাল\"]),\n        (\"আজ ভাল আবহাওয়া।\", \"Weather is good today\", \"Simple\", []),\n        (\"আমি ভালো আছি।\", \"I am fine\", \"Simple\", []),\n        (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Simple\", []),\n        (\"এটা আমার বই।\", \"This is my book\", \"Simple\", []),\n        (\"তিনি ব্যাংকে কাজ করেন এবং ব্যাংকে বসে থাকেন।\",\n         \"He works at the bank and sits on the embankment\",\n         \"Long with multiple\", [\"ব্যাংক\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    quality_metrics = {\n        'total_confidence': 0.0,\n        'confidence_samples': 0,\n        'high_confidence_count': 0,\n        'medium_confidence_count': 0,\n        'low_confidence_count': 0,\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n    }\n\n    homograph_tracking = {\n        'test_expected_homographs': set(),\n        'dscd_discovered_homographs': set(),\n        'explained_homographs': set(),\n        'homograph_explanations': defaultdict(list),\n    }\n\n    error_tracking = {\n        'translation_failures': 0,\n        'dscd_failures': 0,\n        'trg_failures': 0,\n        'timeout_errors': 0,\n        'oom_errors': 0,\n        'other_errors': 0,\n        'error_details': [],\n        'per_test_status': [],\n    }\n\n    timing_metrics = {\n        'total_time': 0.0,\n        'per_test_times': [],\n        'avg_test_time': 0.0,\n    }\n\n    discovery_validated = False\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd and hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            discovery_validated = True\n            last_discovery = dscd.discovered_log[-1]\n            discovered = last_discovery.get('discovered', 0)\n            candidates = last_discovery.get('candidates', 0)\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] No discovery log found\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] Discovery validation failed: {e}\")\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn and hasattr(asbn, 'get_detailed_stats'):\n            result = asbn.get_detailed_stats()\n            if isinstance(result, dict):\n                asbn_stats = result\n        elif asbn and hasattr(asbn, 'get_asbn_stats'):\n            result = asbn.get_asbn_stats()\n            if isinstance(result, dict):\n                asbn_stats = result\n\n        if asbn_stats and _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN: domain_acc={asbn_stats.get('domain_accuracy', 0):.2%}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN stats failed: {e}\")\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg_system\", None)\n        if trg and hasattr(trg, 'get_statistics'):\n            result = trg.get_statistics()\n            if isinstance(result, dict):\n                trg_stats = result\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] TRG: {trg_stats.get('explanations_generated', 0)} total\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] TRG stats failed: {e}\")\n\n    homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking['dscd_discovered_homographs'] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", None)\n                        store_count = len(stores) if stores else 0\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", None)\n                    store_count = len(stores) if stores else 0\n\n                if store_count == 0 and 'dscd_discovery_warmup' in globals():\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(\n                            model, \n                            tokenizer, \n                            num_sents=4000, \n                            batch_size=64,\n                            max_len=_MAX_LENGTH\n                        )\n                        homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n                    except Exception as e:\n                        print(f\"[EVAL] Warmup failed: {e}\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    def _compute_similarity(pred: str, expected: str) -> float:\n        try:\n            pred_words = set(pred.lower().strip().split())\n            exp_words = set(expected.lower().strip().split())\n            if not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            return overlap / len(exp_words)\n        except Exception:\n            return 0.0\n\n    for _, _, _, expected_homos in test_sentences:\n        for h in expected_homos:\n            clean_h = h.strip().lower()\n            if clean_h:\n                homograph_tracking['test_expected_homographs'].add(clean_h)\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            'test_id': idx,\n            'success': False,\n            'translation_ok': False,\n            'explanations_count': 0,\n            'error': None,\n        }\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"[EVAL] translate_with_explanations not available\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'function_not_available'\n                error_tracking['per_test_status'].append(test_status)\n                continue\n\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                device=_DEVICE,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                track_stats=False\n            )\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n\n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0))\n                    u_val = float(expl.get(\"uncertainty\", 0.0))\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n\n                    marker = f\"[S>{_SPAN_THRESHOLD:.2f}]\" if span_val > _SPAN_THRESHOLD else \"          \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics['confidences'].append(conf_val)\n                    quality_metrics['spans'].append(span_val)\n                    quality_metrics['uncertainties'].append(u_val)\n                    quality_metrics['total_confidence'] = quality_metrics.get('total_confidence', 0.0) + conf_val\n                    quality_metrics['confidence_samples'] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics['high_confidence_count'] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics['medium_confidence_count'] += 1\n                    else:\n                        quality_metrics['low_confidence_count'] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = (\n                        str(word)\n                        .replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace('@@', '')\n                        .replace('</w>', '')\n                        .strip()\n                        .lower()\n                    )\n                    if clean_word:\n                        homograph_tracking['explained_homographs'].add(clean_word)\n                        homograph_tracking['homograph_explanations'][clean_word].append({\n                            'sentence': src_text,\n                            'confidence': conf_val,\n                            'span': span_val,\n                            'uncertainty': u_val,\n                        })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status['explanations_count'] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            if translation and translation.strip() and translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            ):\n                successful_translations += 1\n                test_status['translation_ok'] = True\n                test_status['success'] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'translation_failed'\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking['oom_errors'] += 1\n                test_status['error'] = 'oom'\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking['timeout_errors'] += 1\n                test_status['error'] = 'timeout'\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'runtime'\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking['other_errors'] += 1\n            test_status['error'] = type(e).__name__\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking['per_test_status'].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics['per_test_times'].append(test_time)\n\n        print(\"-\" * 60)\n\n    timing_metrics['total_time'] = time.time() - eval_start\n    if timing_metrics['per_test_times']:\n        timing_metrics['avg_test_time'] = (\n            sum(timing_metrics['per_test_times']) / len(timing_metrics['per_test_times'])\n        )\n    else:\n        timing_metrics['avg_test_time'] = 0.0\n\n    if quality_metrics['confidence_samples'] > 0:\n        quality_metrics['avg_confidence'] = (\n            quality_metrics['total_confidence'] / quality_metrics['confidence_samples']\n        )\n        quality_metrics['avg_span'] = (\n            sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n            if quality_metrics['spans']\n            else 0.0\n        )\n        quality_metrics['avg_uncertainty'] = (\n            sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n            if quality_metrics['uncertainties']\n            else 0.0\n        )\n\n        if quality_metrics['confidences']:\n            sorted_conf = sorted(quality_metrics['confidences'])\n            quality_metrics['confidence_p25'] = sorted_conf[len(sorted_conf) // 4]\n            quality_metrics['confidence_p50'] = sorted_conf[len(sorted_conf) // 2]\n            quality_metrics['confidence_p75'] = sorted_conf[3 * len(sorted_conf) // 4]\n    else:\n        quality_metrics['avg_confidence'] = 0.0\n        quality_metrics['avg_span'] = 0.0\n        quality_metrics['avg_uncertainty'] = 0.0\n\n    explained_from_dscd = set()\n    if (homograph_tracking.get('explained_homographs') and \n        homograph_tracking.get('dscd_discovered_homographs')):\n        explained_from_dscd = homograph_tracking['explained_homographs'].intersection(\n            homograph_tracking['dscd_discovered_homographs']\n        )\n\n    test_expected_discovered = set()\n    if (homograph_tracking.get('test_expected_homographs') and \n        homograph_tracking.get('dscd_discovered_homographs')):\n        test_expected_discovered = homograph_tracking['test_expected_homographs'].intersection(\n            homograph_tracking['dscd_discovered_homographs']\n        )\n\n    reference_discovered = set()\n    if (_HOMOGRAPH_REFERENCE_LIST and \n        homograph_tracking.get('dscd_discovered_homographs')):\n        reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n            homograph_tracking['dscd_discovered_homographs']\n        )\n\n    homograph_tracking['explained_from_dscd_rate'] = (\n        len(explained_from_dscd) / len(homograph_tracking['dscd_discovered_homographs'])\n        if homograph_tracking.get('dscd_discovered_homographs')\n        else 0.0\n    )\n    homograph_tracking['test_expected_discovery_rate'] = (\n        len(test_expected_discovered) / len(homograph_tracking['test_expected_homographs'])\n        if homograph_tracking.get('test_expected_homographs')\n        else 0.0\n    )\n    homograph_tracking['reference_discovery_rate'] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0, \"corrupted_stores\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototype_stores\") or {})\n            else:\n                stores = dict(getattr(dscd, \"prototype_stores\") or {})\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            corrupted = 0\n            \n            for key, store in stores.items():\n                try:\n                    sz = int(store.size()) if hasattr(store, \"size\") else 0\n                except Exception:\n                    sz = 0\n                \n                is_valid = True\n                try:\n                    if not hasattr(store, 'centroids') or len(getattr(store, 'centroids', [])) == 0:\n                        is_valid = False\n                    if not hasattr(store, 'counts') or sum(getattr(store, 'counts', [])) <= 0:\n                        is_valid = False\n                    if hasattr(store, 'mu') and (store.mu < 0 or store.mu > 10):\n                        is_valid = False\n                except Exception:\n                    is_valid = False\n                \n                if not is_valid:\n                    corrupted += 1\n                    continue\n                \n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n                    \n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n                \"corrupted_stores\": corrupted,\n            }\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] DSCD stats failed: {e}\")\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0, \"corrupted_stores\": 0}\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n\n    if 'confidence_p50' in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: \"\n            f\"{quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking['explained_homographs']:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking['explained_homographs'])[:10]:\n            exps = homograph_tracking['homograph_explanations'].get(homo, [])\n            count = len(exps)\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            in_dscd = \"[D]\" if homo in homograph_tracking['dscd_discovered_homographs'] else \"   \"\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\")\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats.get('corrupted_stores', 0) > 0:\n        print(f\"  Corrupted stores: {dscd_stats['corrupted_stores']}\")\n    if dscd_stats['total_words'] > 0:\n        print(\n            f\"  Multi-sense ratio: \"\n            f\"{dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        print(f\"  Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n        if 'source_accuracy' in asbn_stats:\n            print(f\"  Source accuracy: {asbn_stats['source_accuracy']:.2%}\")\n            print(f\"  Target accuracy: {asbn_stats['target_accuracy']:.2%}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        print(f\"  Total explanations: {trg_stats.get('explanations_generated', 0)}\")\n        print(f\"  High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum([\n        error_tracking['translation_failures'],\n        error_tracking['dscd_failures'],\n        error_tracking['trg_failures'],\n        error_tracking['timeout_errors'],\n        error_tracking['oom_errors'],\n        error_tracking['other_errors'],\n    ])\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            current_success = (\n                successful_translations / total_tests * 100.0\n            ) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            expl_delta = total_explanations - baseline_expl\n\n            baseline_qm = baseline_metrics.get('quality_metrics', {})\n            baseline_quality = baseline_qm.get('avg_confidence', 0) if isinstance(baseline_qm, dict) else 0\n            quality_delta = quality_metrics['avg_confidence'] - baseline_quality\n\n            print(f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics['avg_confidence']:.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n\n            baseline_ht = baseline_metrics.get('homograph_tracking', {})\n            if isinstance(baseline_ht, dict):\n                baseline_homo_rate = baseline_ht.get('explained_from_dscd_rate', 0)\n                homo_delta = (\n                    homograph_tracking['explained_from_dscd_rate'] - baseline_homo_rate\n                )\n                print(\n                    f\"  Explanation rate: \"\n                    f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                    f\"({homo_delta:+.1%})\"\n                )\n        except Exception as e:\n            print(f\"  Comparison failed: {e}\")\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"No explanations generated\")\n    if dscd_stats['total_words'] < 100:\n        warnings.append(\"Very few prototypes (<100)\")\n    if dscd_stats.get('corrupted_stores', 0) > dscd_stats['total_words'] * 0.1:\n        warnings.append(f\"High corruption rate ({dscd_stats.get('corrupted_stores', 0)} stores)\")\n    if quality_metrics['low_confidence_count'] > quality_metrics['high_confidence_count']:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking['explained_from_dscd_rate'] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    if asbn_stats and asbn_stats.get('domain_accuracy', 0) < 0.5:\n        warnings.append(\"ASBN domain accuracy <50%\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] All systems nominal\")\n\n    print(\"=\" * 80)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n    }\n\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\"*60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\"*60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model,\n            tokenizer,\n            run_warmup=False,\n            compare_baseline=False\n        )\n\n        assert 'total_tests' in result\n        assert 'quality_metrics' in result\n        assert 'homograph_tracking' in result\n\n        print(\"Evaluation pipeline test passed\")\n        print(\"=\"*60 + \"\\n\")\n        return True\n\n    except Exception as e:\n        print(f\"Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\"*60 + \"\\n\")\n        return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Testing & evaluation ready (PURE DATA-DRIVEN) - M2M100 FIXED\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\"  ✓ FIX #1:  Added </w> token marker removal in homograph cleaning\")\nprint(\"  ✓ FIX #2:  DSCD lock protection for prototype_stores access\")\nprint(\"  ✓ FIX #3:  Warmup call with max_len parameter\")\nprint(\"  ✓ FIX #4:  Test homograph tracking with proper cleaning\")\nprint(\"  ✓ FIX #5:  Added device parameter to translate_with_explanations\")\nprint(\"  ✓ FIX #6:  Quality metrics division by zero guards\")\nprint(\"  ✓ FIX #7:  Similarity computation with strip() normalization\")\nprint(\"  ✓ FIX #8:  Set intersection with None validation\")\nprint(\"  ✓ FIX #9:  DSCD stats with corruption detection\")\nprint(\"  ✓ FIX #10: ASBN/TRG stats with type validation\")\nprint(\"  ✓ FIX #11: Timing metrics with empty list guard\")\nprint(\"  ✓ FIX #12: Test status properly tracked per inference\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"8uL574F8H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE (FINAL INTEGRATION) - M2M100 FIXED\n# ==============================================================================\n\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Optional, Dict, Any\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef _g(name, default):\n    return globals().get(name, default)\n\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 48))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 1e-5))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", True))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 500))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 150))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 4000))\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(\n        _g(\"HOMOGRAPH_REFERENCE_LIST_BN\",\n           [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"])\n    )\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = bool(_g(\"FREEZE_ENCODER\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\n    _M2M100_EN_TOKEN_ID = int(_g(\"M2M100_EN_TOKEN_ID\", 128022))\n    _M2M100_BN_TOKEN_ID = int(_g(\"M2M100_BN_TOKEN_ID\", 128025))\nexcept (ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 48\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 1e-5\n    _ENABLE_ASBN_TRAINING = True\n    _VALIDATION_CHECK_INTERVAL = 500\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n    _DSCD_WARMUP_SAMPLES = 4000\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\"}\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = False\n    _DEBUG_TIMING = False\n    _M2M100_EN_TOKEN_ID = 128022\n    _M2M100_BN_TOKEN_ID = 128025\n\n_CHECKPOINT_DIR = \"/kaggle/working\"\n_CHECKPOINT_PATH = os.path.join(_CHECKPOINT_DIR, \"tatn_final.pt\")\n\n\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            globals()[\"clear_all_gpu_caches\"]()\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, None)\n        if result is None:\n            return default\n    return result\n\n\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import M2M100Tokenizer\n        tok = M2M100Tokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n        required = ['encode', 'decode', 'convert_ids_to_tokens', '__call__']\n        for method in required:\n            if not hasattr(tok, method):\n                raise RuntimeError(f\"Tokenizer missing: {method}\")\n        return tok\n    except Exception as e:\n        print(f\"[TOKENIZER] Load failed: {e}\")\n        raise\n\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        _safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    return True\n\n\ndef main_pipeline() -> Tuple[object, object]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN MAIN PIPELINE - COMPLETE INTEGRATION\")\n    print(\"=\" * 80)\n\n    span_thresh = _g('SPAN_THRESHOLD', None)\n    tau_low = _g('TAU_LOW', None)\n    unc_thresh = _g('UNCERTAINTY_THRESHOLD', None)\n    trg_unc_thresh = _g('TRG_UNCERTAINTY_THRESHOLD', None)\n\n    print(f\"Configuration:\")\n    print(f\"  - Span threshold: {span_thresh}\")\n    print(f\"  - TAU_LOW: {tau_low}\")\n    print(f\"  - Uncertainty threshold: {unc_thresh}\")\n    print(f\"  - TRG uncertainty threshold: {trg_unc_thresh}\")\n    print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - ASBN training: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\n    print(f\"  - Epochs: {_EPOCHS}\")\n    print(f\"  - Batch size: {_BATCH_SIZE}\")\n\n    if span_thresh is not None and tau_low is not None:\n        if abs(float(span_thresh) - 0.15) > 0.01:\n            print(f\"  ⚠️  WARNING: SPAN_THRESHOLD should be 0.15 (current: {span_thresh})\")\n        if abs(float(tau_low) - 0.25) > 0.01:\n            print(f\"  ⚠️  WARNING: TAU_LOW should be 0.25 (current: {tau_low})\")\n        if trg_unc_thresh is not None and abs(float(trg_unc_thresh) - 0.25) > 0.01:\n            print(f\"  ⚠️  WARNING: TRG_UNCERTAINTY_THRESHOLD should be 0.25 (current: {trg_unc_thresh})\")\n\n    print(\"=\" * 80)\n\n    pipeline_start = time.time()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    initialize_environment()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 1] Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    tokenizer_modified = False\n    try:\n        if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:\n            if hasattr(tokenizer, 'add_special_tokens'):\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n                tokenizer_modified = True\n    except Exception:\n        pass\n\n    vocab_size = getattr(tokenizer, 'vocab_size', None)\n    if vocab_size is None:\n        try:\n            vocab_size = len(tokenizer)\n        except Exception:\n            vocab_size = 'unknown'\n\n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size})\")\n    if tokenizer_modified:\n        print(f\"[PHASE 1] Tokenizer modified: pad token added\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(f\"\\n[PHASE 2] Loading data ({_NUM_SAMPLES} samples)...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals():\n        raise RuntimeError(\"MemoryEfficientDataset not found - run Cell 2\")\n    dataset = MemoryEfficientDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n    collate_fn = globals().get(\"safe_collate\", None)\n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = create_optimized_dataloader(dataset, batch_size=_BATCH_SIZE, shuffle=True)\n        except Exception:\n            dataloader_kwargs = {\n                'batch_size': _BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0,\n                'pin_memory': torch.cuda.is_available()\n            }\n            if collate_fn is not None:\n                dataloader_kwargs['collate_fn'] = collate_fn\n            train_loader = DataLoader(dataset, **dataloader_kwargs)\n    else:\n        dataloader_kwargs = {\n            'batch_size': _BATCH_SIZE,\n            'shuffle': True,\n            'num_workers': 0,\n            'pin_memory': torch.cuda.is_available()\n        }\n        if collate_fn is not None:\n            dataloader_kwargs['collate_fn'] = collate_fn\n        train_loader = DataLoader(dataset, **dataloader_kwargs)\n\n    try:\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {len(train_loader)} batches\")\n    except Exception:\n        print(\"[PHASE 2] Dataset loaded\")\n\n    del pairs\n    _safe_clear_gpu_caches()\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 3] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        raise RuntimeError(\"Model class not found - run Cell 6\")\n\n    model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        device_ids = list(range(_NUM_GPUS))\n        print(f\"[PHASE 3] Using DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        model = model_core\n\n    model = model.to(_DEVICE)\n    core_model = model.module if hasattr(model, \"module\") else model\n\n    try:\n        mbart = getattr(core_model, \"mbart\", None)\n        if mbart and hasattr(mbart, \"resize_token_embeddings\") and tokenizer_modified:\n            try:\n                current_size = mbart.get_input_embeddings().num_embeddings\n                if isinstance(vocab_size, int):\n                    target_size = vocab_size\n                else:\n                    target_size = current_size\n                if current_size != target_size:\n                    mbart.resize_token_embeddings(target_size)\n                    print(f\"[PHASE 3] Resized embeddings: {current_size} -> {target_size}\")\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    if _FREEZE_ENCODER:\n        try:\n            for p in core_model.mbart.model.encoder.parameters():\n                p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n\n    print(f\"[PHASE 3] Model initialized\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 4] Setting up optimizers...\")\n\n    try:\n        critic_params = list(core_model.asbn.critic_parameters()) if hasattr(core_model, \"asbn\") and hasattr(core_model.asbn, \"critic_parameters\") else []\n    except Exception:\n        critic_params = []\n\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n\n    phi_optimizer = None\n    if critic_params and _ENABLE_ASBN_TRAINING:\n        phi_optimizer = torch.optim.AdamW([p for p in critic_params if p.requires_grad], lr=_LR_PHI)\n        print(f\"[PHASE 4] ASBN optimizer created ({len([p for p in critic_params if p.requires_grad])} params)\")\n    else:\n        if _ENABLE_ASBN_TRAINING and not critic_params:\n            print(f\"[PHASE 4] WARNING: ASBN training enabled but no critic parameters found\")\n\n    print(f\"[PHASE 4] Optimizers ready\")\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 5] Baseline evaluation...\")\n    baseline_metrics = None\n\n    try:\n        dscd = getattr(core_model, 'dscd', None)\n        has_prototypes = False\n\n        if dscd:\n            prototype_stores = getattr(dscd, 'prototype_stores', None)\n            if prototype_stores is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        has_prototypes = len(prototype_stores) > 0\n                else:\n                    has_prototypes = len(prototype_stores) > 0\n\n        if has_prototypes:\n            print(\"[PHASE 5] Prototypes exist - skipping baseline\")\n        elif \"comprehensive_post_training_testing\" in globals():\n            baseline_metrics = comprehensive_post_training_testing(model, tokenizer, run_warmup=False)\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            print(f\"[PHASE 5] Baseline: {baseline_success:.1f}% success, {baseline_expl} explanations\")\n        else:\n            print(\"[PHASE 5] Skipping baseline (function not found)\")\n    except Exception as e:\n        print(f\"[PHASE 5] Baseline failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Baseline: {time.time() - phase_start:.2f}s\")\n\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 6] Training...\")\n    trained_model = model\n\n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            print(f\"[PHASE 6] Starting training with ASBN: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\n\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=(_VALIDATION_CHECK_INTERVAL > 0),\n                enable_asbn_training=_ENABLE_ASBN_TRAINING\n            )\n\n            print(\"[PHASE 6] Training complete\")\n        except Exception as e:\n            print(f\"[PHASE 6] Training failed: {e}\")\n            if _DEBUG_TIMING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n            trained_model = model\n    else:\n        print(\"[PHASE 6] Skipping training (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 7] Post-training validation...\")\n    try:\n        core_for_validation = trained_model.module if hasattr(trained_model, 'module') else trained_model\n        dscd = getattr(core_for_validation, 'dscd', None)\n        \n        if dscd is None:\n            print(\"[PHASE 7] No DSCD module\")\n        else:\n            prototype_stores = getattr(dscd, 'prototype_stores', None)\n            if prototype_stores is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n                if lock:\n                    with lock:\n                        stores = dict(prototype_stores)\n                else:\n                    stores = dict(prototype_stores)\n\n                def _store_size(s):\n                    try:\n                        if hasattr(s, 'size') and callable(s.size):\n                            return int(s.size())\n                        return 0\n                    except Exception:\n                        return 0\n\n                total_protos = sum(_store_size(store) for store in stores.values())\n                multi_sense = sum(1 for store in stores.values() if _store_size(store) >= 2)\n                print(\"[PHASE 7] DSCD status:\")\n                print(f\"  - Tokens: {len(stores)}\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multi_sense}\")\n\n                if len(stores) == 0 or total_protos == 0:\n                    print(\"[PHASE 7] WARNING: No prototypes created during training\")\n    except Exception as e:\n        print(f\"[PHASE 7] Validation failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Validation: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 8] Post-training evaluation...\")\n    eval_results: Dict[str, Any] = {}\n\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            try:\n                core_for_eval = trained_model.module if hasattr(trained_model, 'module') else trained_model\n                trg = getattr(core_for_eval, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n\n            eval_results = comprehensive_post_training_testing(\n                trained_model,\n                tokenizer,\n                run_warmup=False,\n                compare_baseline=(baseline_metrics is not None),\n                baseline_metrics=baseline_metrics\n            )\n            final_success = eval_results.get('success_rate_pct', 0)\n            final_expl = eval_results.get('total_explanations', 0)\n            print(f\"[PHASE 8] Evaluation: {final_success:.1f}% success, {final_expl} explanations\")\n        except Exception as e:\n            print(f\"[PHASE 8] Evaluation failed: {e}\")\n    else:\n        print(\"[PHASE 8] Skipping evaluation (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 9] Saving checkpoint...\")\n    try:\n        os.makedirs(_CHECKPOINT_DIR, exist_ok=True)\n        core_for_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        was_training = getattr(core_for_save, \"training\", False)\n        core_for_save.eval()\n\n        try:\n            model_state = {}\n            for k, v in core_for_save.state_dict().items():\n                if isinstance(v, torch.Tensor):\n                    model_state[k] = v.cpu().detach().clone()\n                else:\n                    model_state[k] = v\n\n            dscd_state = {}\n            if hasattr(core_for_save, 'dscd') and core_for_save.dscd is not None:\n                dscd_obj = core_for_save.dscd\n                if hasattr(dscd_obj, 'state_dict') and callable(getattr(dscd_obj, 'state_dict', None)):\n                    try:\n                        dscd_state = dscd_obj.state_dict()\n\n                        if 'prototype_stores_data' in dscd_state:\n                            stores = dscd_state['prototype_stores_data']\n                            if isinstance(stores, dict):\n                                valid_stores = {}\n                                corrupted_count = 0\n\n                                for token, store_dict in stores.items():\n                                    if not isinstance(store_dict, dict):\n                                        corrupted_count += 1\n                                        continue\n\n                                    centroids = store_dict.get('centroids', torch.empty(0))\n                                    counts = store_dict.get('counts', [])\n\n                                    is_valid = False\n                                    if isinstance(centroids, torch.Tensor) and centroids.size(0) > 0:\n                                        if isinstance(counts, list) and len(counts) > 0:\n                                            if len(counts) == centroids.size(0):\n                                                if sum(counts) > 0:\n                                                    is_valid = True\n\n                                    if is_valid:\n                                        valid_stores[token] = store_dict\n                                    else:\n                                        corrupted_count += 1\n\n                                dscd_state['prototype_stores_data'] = valid_stores\n\n                                if corrupted_count > 0:\n                                    print(f\"[PHASE 9] Filtered {corrupted_count} corrupted prototype stores\")\n                                print(f\"[PHASE 9] Validated {len(valid_stores)} DSCD stores\")\n                    except Exception as e:\n                        print(f\"[PHASE 9] DSCD state extraction failed: {e}\")\n                        dscd_state = {}\n\n            optimizer_state = None\n            if optimizer is not None:\n                try:\n                    optimizer_state = optimizer.state_dict()\n                except Exception:\n                    optimizer_state = None\n\n            phi_optimizer_state = None\n            if phi_optimizer is not None:\n                try:\n                    phi_optimizer_state = phi_optimizer.state_dict()\n                except Exception:\n                    phi_optimizer_state = None\n\n            checkpoint = {\n                'model_state_dict': model_state,\n                'dscd_state': dscd_state,\n                'optimizer_state_dict': optimizer_state,\n                'phi_optimizer_state_dict': phi_optimizer_state,\n                'baseline_metrics': baseline_metrics,\n                'eval_results': eval_results,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'config': {\n                    'epochs': _EPOCHS,\n                    'batch_size': _BATCH_SIZE,\n                    'span_threshold': _g('SPAN_THRESHOLD', 'N/A'),\n                    'tau_low': _g('TAU_LOW', 'N/A'),\n                    'uncertainty_threshold': _g('UNCERTAINTY_THRESHOLD', 'N/A'),\n                    'trg_uncertainty_threshold': _g('TRG_UNCERTAINTY_THRESHOLD', 'N/A'),\n                    'discovery_frequency': _PERIODIC_DISCOVERY_FREQUENCY,\n                    'enable_asbn_training': _ENABLE_ASBN_TRAINING,\n                }\n            }\n            torch.save(checkpoint, _CHECKPOINT_PATH)\n\n            verify = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n            has_model = 'model_state_dict' in verify and isinstance(verify.get('model_state_dict'), dict) and len(verify.get('model_state_dict', {})) > 0\n            has_dscd = 'dscd_state' in verify and isinstance(verify.get('dscd_state'), dict) and len(verify.get('dscd_state', {})) > 0\n            has_phi = 'phi_optimizer_state_dict' in verify and verify.get('phi_optimizer_state_dict') is not None\n\n            print(f\"[PHASE 9] Checkpoint saved: {_CHECKPOINT_PATH}\")\n            print(f\"  - Model: {'OK' if has_model else 'MISSING'}\")\n            print(f\"  - DSCD: {'OK' if has_dscd else 'MISSING'}\")\n            print(f\"  - Phi optimizer: {'OK' if has_phi else 'N/A'}\")\n\n            if has_dscd:\n                dscd_state_saved = verify.get('dscd_state', {})\n                if isinstance(dscd_state_saved, dict):\n                    stores_data = dscd_state_saved.get('prototype_stores_data', {})\n                    if isinstance(stores_data, dict):\n                        num_tokens = len(stores_data)\n                        multi_sense_count = sum(1 for store_dict in stores_data.values()\n                                               if isinstance(store_dict, dict) and\n                                               isinstance(store_dict.get('centroids'), torch.Tensor) and\n                                               store_dict.get('centroids').size(0) >= 2)\n\n                        print(f\"  - DSCD tokens: {num_tokens}\")\n                        print(f\"  - Multi-sense: {multi_sense_count}\")\n\n                        if num_tokens > 0:\n                            multi_sense_ratio = (multi_sense_count / num_tokens) * 100\n                            print(f\"  - Multi-sense ratio: {multi_sense_ratio:.1f}%\")\n\n                            if multi_sense_ratio > 50:\n                                print(f\"  - ⚠️  WARNING: Unusually high multi-sense ratio (expected 10-20%)\")\n                    else:\n                        print(f\"  - WARNING: prototype_stores_data is not a dict\")\n        finally:\n            if was_training:\n                try:\n                    core_for_save.train()\n                except Exception:\n                    pass\n    except Exception as e:\n        print(f\"[PHASE 9] Checkpoint failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Checkpoint: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 10] Final component validation...\")\n    try:\n        core_final = trained_model.module if hasattr(trained_model, 'module') else trained_model\n        dscd_ok = False\n        if hasattr(core_final, 'dscd'):\n            prototype_stores = getattr(core_final.dscd, 'prototype_stores', None)\n            if prototype_stores is not None:\n                lock = None\n                if hasattr(core_final.dscd, 'buffer_lock'):\n                    lock = core_final.dscd.buffer_lock\n                elif hasattr(core_final.dscd, 'clustering_lock'):\n                    lock = core_final.dscd.clustering_lock\n                if lock:\n                    with lock:\n                        dscd_ok = len(prototype_stores) > 0\n                else:\n                    dscd_ok = len(prototype_stores) > 0\n\n        asbn_ok = hasattr(core_final, 'asbn') and hasattr(core_final.asbn, 'forward')\n        trg_ok = hasattr(core_final, 'trg_system') and hasattr(core_final.trg_system, 'process_sentence_for_explanations')\n\n        print(f\"[PHASE 10] Component validation:\")\n        print(f\"  - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n        print(f\"  - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n        print(f\"  - TRG: {'OK' if trg_ok else 'MISSING'}\")\n\n        all_ok = dscd_ok and asbn_ok and trg_ok\n        if all_ok:\n            print(\"[PHASE 10] All components validated\")\n        else:\n            print(\"[PHASE 10] Some components missing\")\n    except Exception as e:\n        print(f\"[PHASE 10] Validation failed: {e}\")\n\n    pipeline_time = time.time() - pipeline_start\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"\\n[TIMING]\")\n    print(f\"  Total time: {pipeline_time:.2f}s ({pipeline_time/60:.2f} min)\")\n\n    print(f\"\\n[TRAINING]\")\n    print(f\"  Completed {_EPOCHS} epoch(s)\")\n\n    print(f\"\\n[EVALUATION]\")\n    if baseline_metrics and isinstance(baseline_metrics, dict) and eval_results and isinstance(eval_results, dict):\n        baseline_success = baseline_metrics.get('success_rate_pct', None)\n        final_success = eval_results.get('success_rate_pct', None)\n\n        if baseline_success is not None and final_success is not None:\n            try:\n                improvement = float(final_success) - float(baseline_success)\n                print(f\"  Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n                print(f\"  Improvement: {improvement:+.1f}%\")\n            except (ValueError, TypeError):\n                print(f\"  Baseline: {baseline_success}\")\n                print(f\"  Final: {final_success}\")\n\n        baseline_dscd_stats = baseline_metrics.get('dscd_stats', {})\n        final_dscd_stats = eval_results.get('dscd_stats', {})\n\n        baseline_dscd = None\n        if isinstance(baseline_dscd_stats, dict):\n            baseline_dscd = baseline_dscd_stats.get('multi_sense_words', None)\n\n        final_dscd = None\n        if isinstance(final_dscd_stats, dict):\n            final_dscd = final_dscd_stats.get('multi_sense_words', None)\n\n        if baseline_dscd is not None and final_dscd is not None:\n            print(f\"  DSCD multi-sense: {baseline_dscd} -> {final_dscd}\")\n\n        baseline_asbn_stats = baseline_metrics.get('asbn_stats', {})\n        final_asbn_stats = eval_results.get('asbn_stats', {})\n\n        baseline_asbn = None\n        if isinstance(baseline_asbn_stats, dict):\n            baseline_asbn = baseline_asbn_stats.get('domain_accuracy', None)\n\n        final_asbn = None\n        if isinstance(final_asbn_stats, dict):\n            final_asbn = final_asbn_stats.get('domain_accuracy', None)\n\n        if baseline_asbn is not None and final_asbn is not None:\n            try:\n                print(f\"  ASBN accuracy: {baseline_asbn:.2%} -> {final_asbn:.2%}\")\n            except (ValueError, TypeError):\n                print(f\"  ASBN accuracy: {baseline_asbn} -> {final_asbn}\")\n    elif eval_results and isinstance(eval_results, dict):\n        print(f\"  Success rate: {eval_results.get('success_rate_pct', 0):.1f}%\")\n    else:\n        print(\"  No results\")\n\n    print(f\"\\n[CHECKPOINT]\")\n    if os.path.exists(_CHECKPOINT_PATH):\n        try:\n            size_mb = os.path.getsize(_CHECKPOINT_PATH) / 1024**2\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n            print(f\"  - Size: {size_mb:.2f} MB\")\n        except Exception:\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n    else:\n        print(\"  Not saved\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Usage: trained_model, tokenizer = main_pipeline()\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    return trained_model, tokenizer\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 10: Main pipeline ready - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\n","metadata":{"id":"kEux2BVXH4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER (FINAL) - M2M100 PYTORCH 2.6 COMPATIBLE - FIXED\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\nfrom collections import defaultdict\n\ntry:\n    _NUM_SAMPLES = int(globals().get('NUM_SAMPLES', 30000))\n    _EPOCHS = int(globals().get('EPOCHS', 2))\n    _BATCH_SIZE = int(globals().get('BATCH_SIZE', 4))\n    _ACCUMULATION_STEPS = int(globals().get('ACCUMULATION_STEPS', 16))\n    \n    raw_device = globals().get('DEVICE', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        _DEVICE = torch.device(str(raw_device))\n\n    _ENABLE_ASBN_TRAINING = bool(globals().get('ENABLE_ASBN_TRAINING', True))\n    _ENABLE_TRG_INFERENCE = bool(globals().get('ENABLE_TRG_INFERENCE', True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(globals().get('PERIODIC_DISCOVERY_FREQUENCY', 150))\n    _VERBOSE_LOGGING = bool(globals().get('VERBOSE_LOGGING', False))\n    _DEBUG_DISCOVERY = bool(globals().get('DEBUG_DISCOVERY', False))\n    _DEBUG_TIMING = bool(globals().get('DEBUG_TIMING', False))\n    _NUM_GPUS = int(globals().get('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(globals().get('USE_MULTI_GPU', _NUM_GPUS > 1))\n    _SPAN_THRESHOLD = float(globals().get('SPAN_THRESHOLD', 0.15))\n    _TAU_LOW = float(globals().get('TAU_LOW', 0.25))\n    _UNCERTAINTY_THRESHOLD = float(globals().get('UNCERTAINTY_THRESHOLD', 0.25))\n    _TRG_UNCERTAINTY_THRESHOLD = float(globals().get('TRG_UNCERTAINTY_THRESHOLD', 0.25))\n    _M2M100_EN_TOKEN_ID = int(globals().get('M2M100_EN_TOKEN_ID', 128022))\n    _M2M100_BN_TOKEN_ID = int(globals().get('M2M100_BN_TOKEN_ID', 128025))\n    \n    raw_list = globals().get('HOMOGRAPH_REFERENCE_LIST_BN', [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_list)\n    cell0_loaded = 'NUM_SAMPLES' in globals()\n    \nexcept (NameError, TypeError, ValueError) as e:\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.15\n    _TAU_LOW = 0.25\n    _UNCERTAINTY_THRESHOLD = 0.25\n    _TRG_UNCERTAINTY_THRESHOLD = 0.25\n    _M2M100_EN_TOKEN_ID = 128022\n    _M2M100_BN_TOKEN_ID = 128025\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}\n    cell0_loaded = False\n    print(\"[EXEC] Using fallback configuration (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\n\ndef _format_duration(seconds: float) -> str:\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}min\"\n    else:\n        return f\"{seconds/3600:.2f}hr\"\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, default)\n        if result is default:\n            return default\n    return result\n\n\ndef _get_dscd_homographs(model):\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n\n        if dscd and hasattr(dscd, 'prototype_stores'):\n            homographs = set()\n            word_prototype_counts = defaultdict(int)\n\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            for token, store in stores.items():\n                try:\n                    num_protos = 0\n                    if hasattr(store, 'size'):\n                        size_method = getattr(store, 'size', None)\n                        if callable(size_method):\n                            try:\n                                num_protos = int(size_method())\n                            except Exception:\n                                num_protos = 0\n\n                    clean = (\n                        str(token)\n                        .replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace('@@', '')\n                        .replace('</w>', '')\n                        .strip()\n                        .lower()\n                    )\n                    if clean:\n                        word_prototype_counts[clean] = max(word_prototype_counts[clean], num_protos)\n                except Exception:\n                    continue\n\n            for word, count in word_prototype_counts.items():\n                if count >= 2:\n                    homographs.add(word)\n\n            return homographs\n    except Exception:\n        pass\n    return set()\n\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN - COMPLETE EXECUTION\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  TAU_LOW threshold: {_TAU_LOW}\")\n    print(f\"  Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  TRG uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"  Batch per GPU: {per_gpu}\")\n\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    if 'main_pipeline' not in globals():\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n\n            if _DEBUG_TIMING:\n                print(\"   Expected: ~15-45 min (config dependent)\")\n\n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n\n            if \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n\n                print(\"\\nFixes:\")\n                print(\"   1. Reduce BATCH_SIZE (try 2-4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10k-20k)\")\n                print(\"   3. Increase ACCUMULATION_STEPS (32-64)\")\n\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        print(\"\\n[THREAD CLEANUP]\")\n        print(\"Waiting for clustering threads to complete...\")\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n            dscd = getattr(core, 'dscd', None)\n            \n            if dscd and hasattr(dscd, 'active_threads'):\n                try:\n                    lock = None\n                    if hasattr(dscd, 'thread_lock'):\n                        lock = dscd.thread_lock\n                    elif hasattr(dscd, 'buffer_lock'):\n                        lock = dscd.buffer_lock\n                    \n                    if lock:\n                        with lock:\n                            threads = list(dscd.active_threads) if hasattr(dscd.active_threads, '__iter__') else []\n                    else:\n                        threads = list(dscd.active_threads) if hasattr(dscd.active_threads, '__iter__') else []\n                    \n                    if threads:\n                        print(f\"   Found {len(threads)} active threads\")\n                        \n                        completed = 0\n                        timed_out = 0\n                        \n                        for i, thread in enumerate(threads):\n                            if thread.is_alive():\n                                print(f\"   Waiting for thread {i+1}/{len(threads)}...\", end='', flush=True)\n                                thread.join(timeout=30)\n                                \n                                if thread.is_alive():\n                                    print(\" TIMEOUT (abandoned)\")\n                                    timed_out += 1\n                                else:\n                                    print(\" done\")\n                                    completed += 1\n                            else:\n                                completed += 1\n                        \n                        if lock:\n                            with lock:\n                                dscd.active_threads.clear()\n                        else:\n                            dscd.active_threads.clear()\n                        \n                        print(f\"   ✅ Cleanup complete: {completed} completed, {timed_out} timed out\")\n                        \n                        if timed_out > 0:\n                            print(f\"   ⚠️ Warning: {timed_out} threads abandoned\")\n                    else:\n                        print(\"   No active threads\")\n                        \n                except Exception as e:\n                    print(f\"   ⚠️ Thread cleanup error: {type(e).__name__}: {str(e)[:100]}\")\n            else:\n                print(\"   No thread tracking available\")\n                \n        except Exception as e:\n            print(f\"   ⚠️ Thread cleanup failed: {type(e).__name__}: {str(e)[:100]}\")\n        \n        time.sleep(1)\n        print(\"   Ready for evaluation\")\n\n        print(\"\\n[PROTOTYPE EXTRACTION & VERIFICATION]\")\n        print(\"Extracting and validating DSCD prototypes...\")\n        \n        prototype_extraction_success = False\n        total_prototypes_extracted = 0\n        multi_sense_tokens_extracted = 0\n        \n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n            dscd = getattr(core, 'dscd', None)\n            \n            if dscd and hasattr(dscd, 'prototype_stores'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n                \n                prototype_data = {}\n                \n                if lock:\n                    with lock:\n                        stores = dict(dscd.prototype_stores)\n                else:\n                    stores = dict(dscd.prototype_stores)\n                \n                for token, store in stores.items():\n                    try:\n                        centroids = getattr(store, 'centroids', None)\n                        counts = getattr(store, 'counts', None)\n                        \n                        is_valid = False\n                        if isinstance(centroids, torch.Tensor) and centroids.size(0) > 0:\n                            if isinstance(counts, list) and len(counts) > 0:\n                                if len(counts) == centroids.size(0) and sum(counts) > 0:\n                                    is_valid = True\n                        \n                        if is_valid:\n                            store_data = {\n                                'centroids': centroids.tolist() if isinstance(centroids, torch.Tensor) else centroids,\n                                'counts': [int(c) for c in counts]\n                            }\n                            prototype_data[str(token)] = store_data\n                            total_prototypes_extracted += len(counts)\n                            if len(counts) >= 2:\n                                multi_sense_tokens_extracted += 1\n                            \n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(f\"  ⚠️ Failed to extract prototypes for token '{token}': {type(e).__name__}\")\n                        continue\n                \n                print(f\"  Extracted prototypes:\")\n                print(f\"    - Tokens with prototypes: {len(prototype_data)}\")\n                print(f\"    - Total prototypes: {total_prototypes_extracted}\")\n                print(f\"    - Multi-sense tokens: {multi_sense_tokens_extracted}\")\n                \n                if len(prototype_data) > 0:\n                    multi_sense_ratio = (multi_sense_tokens_extracted / len(prototype_data)) * 100\n                    print(f\"    - Multi-sense ratio: {multi_sense_ratio:.1f}%\")\n                    \n                    if multi_sense_ratio > 50:\n                        print(f\"  ⚠️ WARNING: Unusually high multi-sense ratio (expected 10-20%)\")\n                \n                if len(prototype_data) == 0:\n                    print(\"  ⚠️ WARNING: No prototypes extracted!\")\n                    print(\"     -> Discovery may have failed\")\n                elif total_prototypes_extracted == 0:\n                    print(\"  ⚠️ WARNING: Prototypes extracted but all empty!\")\n                    print(\"     -> Clustering may have failed\")\n                else:\n                    print(\"  ✅ Prototype extraction successful\")\n                    prototype_extraction_success = True\n                    \n                    if os.path.exists(_CHECKPOINT_PATH):\n                        try:\n                            print(\"\\n  Verifying checkpoint prototypes...\")\n                            ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n                            \n                            dscd_state = ckpt.get('dscd_state', {})\n                            if isinstance(dscd_state, dict):\n                                stores_data = dscd_state.get('prototype_stores_data', {})\n                                \n                                if len(stores_data) != len(prototype_data):\n                                    print(f\"  ⚠️ Checkpoint mismatch: {len(stores_data)} saved vs {len(prototype_data)} extracted\")\n                                    print(\"     -> Updating checkpoint with fresh prototypes...\")\n                                    \n                                    dscd_state['prototype_stores_data'] = prototype_data\n                                    ckpt['dscd_state'] = dscd_state\n                                    ckpt['prototype_extraction_timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')\n                                    ckpt['prototype_stats'] = {\n                                        'total_tokens': len(prototype_data),\n                                        'total_prototypes': total_prototypes_extracted,\n                                        'multi_sense_tokens': multi_sense_tokens_extracted,\n                                        'multi_sense_ratio': multi_sense_ratio\n                                    }\n                                    \n                                    torch.save(ckpt, _CHECKPOINT_PATH)\n                                    print(\"  ✅ Checkpoint updated with verified prototypes\")\n                                else:\n                                    print(f\"  ✅ Checkpoint prototypes match extracted data ({len(stores_data)} tokens)\")\n                                    \n                                    corrupted = sum(1 for token, data in stores_data.items()\n                                                   if isinstance(data, dict) and\n                                                   (not data.get('centroids') or not data.get('counts') or\n                                                    len(data.get('centroids', [])) != len(data.get('counts', [])) or\n                                                    sum(data.get('counts', [])) <= 0))\n                                    \n                                    if corrupted > 0:\n                                        print(f\"  ⚠️ Found {corrupted} corrupted stores - cleaning...\")\n                                        dscd_state['prototype_stores_data'] = prototype_data\n                                        ckpt['dscd_state'] = dscd_state\n                                        torch.save(ckpt, _CHECKPOINT_PATH)\n                                        print(\"  ✅ Checkpoint cleaned and re-saved\")\n                            else:\n                                print(\"  ⚠️ Invalid DSCD state - saving fresh data\")\n                                ckpt['dscd_state'] = {'prototype_stores_data': prototype_data}\n                                ckpt['prototype_stats'] = {\n                                    'total_tokens': len(prototype_data),\n                                    'total_prototypes': total_prototypes_extracted,\n                                    'multi_sense_tokens': multi_sense_tokens_extracted,\n                                    'multi_sense_ratio': multi_sense_ratio\n                                }\n                                torch.save(ckpt, _CHECKPOINT_PATH)\n                                print(\"  ✅ Fresh prototypes saved\")\n                                \n                        except Exception as e:\n                            print(f\"  ⚠️ Failed to update checkpoint: {type(e).__name__}: {str(e)[:100]}\")\n            else:\n                print(\"  ⚠️ DSCD module not found or has no prototype_stores\")\n                \n        except Exception as e:\n            print(f\"  ⚠️ Prototype extraction failed: {type(e).__name__}: {str(e)[:150]}\")\n\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024**2)\n                print(f\"  File: {_CHECKPOINT_PATH}\")\n                print(f\"  Size: {size_mb:.1f} MB\")\n\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n\n                has_model = False\n                if 'model_state_dict' in ckpt:\n                    model_state = ckpt.get('model_state_dict')\n                    if isinstance(model_state, dict):\n                        has_model = len(model_state) > 0\n\n                has_dscd = False\n                if 'dscd_state' in ckpt:\n                    dscd_state = ckpt.get('dscd_state')\n                    if isinstance(dscd_state, dict):\n                        has_dscd = len(dscd_state) > 0\n\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n\n                if has_dscd:\n                    dscd_state_data = ckpt.get('dscd_state', {})\n                    num_tokens = 0\n                    corrupted_stores = 0\n                    \n                    if isinstance(dscd_state_data, dict):\n                        stores_data = dscd_state_data.get('prototype_stores_data', {})\n                        if isinstance(stores_data, dict):\n                            num_tokens = len(stores_data)\n                            \n                            for token, store_data in stores_data.items():\n                                if isinstance(store_data, dict):\n                                    centroids = store_data.get('centroids', [])\n                                    counts = store_data.get('counts', [])\n                                    if not centroids or not counts or len(centroids) != len(counts) or sum(counts) <= 0:\n                                        corrupted_stores += 1\n                    \n                    print(f\"  Tokens: {num_tokens}\")\n                    \n                    if corrupted_stores > 0:\n                        print(f\"  WARNING: {corrupted_stores} corrupted stores\")\n                    \n                    if num_tokens > 0 and corrupted_stores < num_tokens * 0.1:\n                        checkpoint_valid = True\n                        print(\"  Status: VALID\")\n                    elif num_tokens > 0:\n                        print(\"  Status: CORRUPTED\")\n                    else:\n                        print(\"  Status: EMPTY DSCD\")\n                        \n                    proto_stats = ckpt.get('prototype_stats')\n                    if proto_stats and isinstance(proto_stats, dict):\n                        print(f\"  Prototype stats:\")\n                        print(f\"    - Total prototypes: {proto_stats.get('total_prototypes', 'N/A')}\")\n                        print(f\"    - Multi-sense: {proto_stats.get('multi_sense_tokens', 'N/A')}\")\n                else:\n                    print(\"  Status: MISSING DSCD\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n\n        except Exception as e:\n            print(f\"  Validation failed: {type(e).__name__}: {str(e)[:100]}\")\n\n        print(\"\\n[COMPONENTS]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd = getattr(core, 'dscd', None)\n            if dscd and hasattr(dscd, 'get_prototype_summary'):\n                try:\n                    lock = None\n                    if hasattr(dscd, 'buffer_lock'):\n                        lock = dscd.buffer_lock\n                    elif hasattr(dscd, 'clustering_lock'):\n                        lock = dscd.clustering_lock\n                    \n                    if lock:\n                        with lock:\n                            dscd_stats = dscd.get_prototype_summary()\n                    else:\n                        dscd_stats = dscd.get_prototype_summary()\n                    \n                    if isinstance(dscd_stats, dict):\n                        print(\"  DSCD:\") \n                        print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                        print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                        print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                except Exception as e:\n                    print(f\"  DSCD stats failed: {type(e).__name__}\")\n\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                try:\n                    result = asbn.get_detailed_stats()\n                    if isinstance(result, dict):\n                        asbn_stats = result\n                        print(\"  ASBN:\")\n                        print(f\"    - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n                        if 'source_accuracy' in asbn_stats:\n                            print(f\"    - Source: {asbn_stats.get('source_accuracy', 0):.2%}\")\n                            print(f\"    - Target: {asbn_stats.get('target_accuracy', 0):.2%}\")\n                except Exception as e:\n                    print(f\"  ASBN stats failed: {type(e).__name__}\")\n\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'get_statistics'):\n                try:\n                    result = trg.get_statistics()\n                    if isinstance(result, dict):\n                        trg_stats = result\n                        print(\"  TRG:\")\n                        print(f\"    - Explanations: {trg_stats.get('explanations_generated', 0)}\")\n                        print(f\"    - High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                        print(f\"    - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n                except Exception as e:\n                    print(f\"  TRG stats failed: {type(e).__name__}\")\n\n        except Exception as e:\n            print(f\"  Stats failed: {type(e).__name__}: {str(e)[:100]}\")\n\n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing disambiguation on ambiguous sentences...\")\n        print(\"-\" * 80)\n\n        _safe_cleanup()\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n        test_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"কল (tap/call)\"),\n            (\"কাল আমি বই কিনব।\", \"কাল (tomorrow/yesterday)\"),\n            (\"পাতা ঝরে পড়েছে।\", \"পাতা (leaf/page)\"),\n        ]\n\n        inference_times = []\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"translate_with_explanations not available\")\n                print(\"   -> Run Cell 8 before Cell 11\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}. {desc}\")\n                        print(f\"   Input: {sentence}\")\n\n                        inf_start = time.time()\n                        res = translate_with_explanations(\n                            trained_model, \n                            tokenizer, \n                            sentence, \n                            device=_DEVICE,\n                            span_threshold=_SPAN_THRESHOLD,\n                            uncertainty_threshold=_TAU_LOW,\n                            track_stats=False\n                        )\n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n\n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations')\n\n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs and isinstance(exs, list):\n                                for exp in exs:\n                                    if isinstance(exp, dict):\n                                        word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                        clean = (\n                                            str(word)\n                                            .replace('▁', '')\n                                            .replace('Ġ', '')\n                                            .replace('##', '')\n                                            .replace('@@', '')\n                                            .replace('</w>', '')\n                                            .strip()\n                                            .lower()\n                                        )\n\n                                        if clean in dscd_homographs:\n                                            dscd_homographs_detected.add(clean)\n\n                                        try:\n                                            conf = float(exp.get('confidence', 0.5))\n                                            span = float(exp.get('span', 0.0))\n                                            u = float(exp.get('uncertainty', 0.0))\n                                            print(f\"   -> '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                        except Exception:\n                                            print(f\"   -> '{word}': (no metrics)\")\n\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n\n                        _safe_cleanup()\n\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__}: {str(e)[:100]}\")\n                        inference_failed += 1\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n\n                if inference_times and len(inference_times) > 0:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg per sentence\")\n\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                else:\n                    print(\"No DSCD homographs detected in test sentences\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries\")\n                    else:\n                        print(f\"   -> DSCD has {len(dscd_homographs)} homographs but none in test sentences\")\n\n        except Exception as e:\n            print(f\"Validation failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n[SYSTEM TEST]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd_ok = hasattr(core, 'dscd') and hasattr(core.dscd, 'forward')\n            asbn_ok = hasattr(core, 'asbn') and hasattr(core.asbn, 'forward')\n            trg_ok = hasattr(core, 'trg_system') and hasattr(core.trg_system, 'process_sentence_for_explanations')\n            \n            mbart_ok = False\n            if hasattr(core, 'mbart'):\n                mbart = core.mbart\n                if hasattr(mbart, 'generate'):\n                    try:\n                        has_encoder = hasattr(mbart, 'model') and hasattr(mbart.model, 'encoder')\n                        has_decoder = hasattr(mbart, 'model') and hasattr(mbart.model, 'decoder')\n                        mbart_ok = has_encoder and has_decoder\n                    except Exception:\n                        mbart_ok = hasattr(mbart, 'generate')\n\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n            print(f\"    - TRG: {'OK' if trg_ok else 'MISSING'}\")\n            print(f\"    - M2M100: {'OK' if mbart_ok else 'MISSING'}\")\n\n            all_ok = dscd_ok and asbn_ok and trg_ok and mbart_ok\n\n            if all_ok:\n                print(\"  All components operational\")\n            else:\n                print(\"  Some components missing\")\n\n        except Exception as e:\n            print(f\"  Test failed: {type(e).__name__}: {str(e)[:100]}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n\n        print(\"\\n1. Single translation:\")\n        print(\"   result = translate_with_explanations(trained_model, tokenizer, 'আমি কল বন্ধ করেছি।')\")\n\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(\"       res = translate_with_explanations(trained_model, tokenizer, sent)\")\n\n        print(\"\\n3. Load checkpoint:\")\n        print(\"   ckpt = torch.load('/kaggle/working/tatn_final.pt', weights_only=False)\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if not checkpoint_valid:\n            print(\"\\n⚠️ Checkpoint needs verification\")\n            if not prototype_extraction_success:\n                print(\"   -> Prototype extraction failed - re-run Cell 10\")\n            else:\n                print(\"   -> Prototypes extracted but checkpoint validation failed\")\n\n        if prototype_extraction_success:\n            print(f\"\\n✅ Prototypes saved: {total_prototypes_extracted} prototypes from {multi_sense_tokens_extracted} multi-sense tokens\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n\n        print(\"\\n[DIAGNOSTICS]\")\n\n        components = {\n            'Cell 0': 'NUM_SAMPLES' in globals(),\n            'Cell 1': 'reconstruct_word_spans' in globals(),\n            'Cell 2': 'MemoryEfficientDataset' in globals(),\n            'Cell 3': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8': 'translate_with_explanations' in globals(),\n            'Cell 9': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10': 'main_pipeline' in globals(),\n        }\n\n        all_present = True\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status} {comp}\")\n            if not present:\n                all_present = False\n\n        print(\"\\n[RECOVERY]\")\n\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"  ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"  Then RESTART kernel and re-run Cells 0-11\")\n\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory in Cell 0:\")\n            print(\"  BATCH_SIZE = 2\")\n            print(\"  NUM_SAMPLES = 15000\")\n            print(\"  ACCUMULATION_STEPS = 32\")\n            print(\"  Then re-run Cells 0-11\")\n\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0:\")\n            print(\"  VERBOSE_LOGGING = True\")\n            print(\"  DEBUG_DISCOVERY = True\")\n            print(\"  Then re-run Cell 11 for details\")\n\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check checkpoint exists:\")\n            print(f\"  os.path.exists('{_CHECKPOINT_PATH}')\")\n            print(\"  If yes, can load and skip training\")\n            print(\"  If no, re-run Cell 11\")\n\n        else:\n            print(\"\\n-> General steps:\")\n            print(\"  1. Enable DEBUG in Cell 0\")\n            print(\"  2. Re-run Cells 0-11\")\n            print(\"  3. Check GPU: torch.cuda.is_available()\")\n            print(\"  4. Verify data loaded\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS\")\n        if 'checkpoint_valid' in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID\")\n        else:\n            print(\"Checkpoint: CHECK NEEDED\")\n        \n        if 'prototype_extraction_success' in locals() and prototype_extraction_success:\n            print(f\"Prototypes: SAVED ({total_prototypes_extracted} total)\")\n        else:\n            print(\"Prototypes: EXTRACTION FAILED\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n\n    print(\"=\" * 80)\n\n    _safe_cleanup()\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution wrapper ready - ALL FIXES APPLIED\")\nprint(\"=\" * 80)\n","metadata":{"id":"9n4Hrn1wH4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}