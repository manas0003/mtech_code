{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14058448,"sourceType":"datasetVersion","datasetId":8948019}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Clean install (skip sentence-transformers)\n!pip uninstall -y transformers tokenizers sentence-transformers huggingface-hub\n\n# Step 2: Install only what you need\n!pip install transformers sacrebleu sacremoses\n\n# Step 3: Verify\nimport transformers\nimport tokenizers\nimport sacrebleu\n\nprint(f\"âœ… transformers: {transformers.__version__}\")\nprint(f\"âœ… tokenizers: {tokenizers.__version__}\")\nprint(f\"âœ… sacrebleu: {sacrebleu.__version__}\")\n\n# Step 4: Test M2M100\nfrom transformers import M2M100Tokenizer\ntokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\nprint(f\"âœ… M2M100 tokenizer loaded: {len(tokenizer)} tokens\")\n\n# Test Bengali\ntokenizer.src_lang = \"bn\"\ntext = \"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\"\nencoded = tokenizer(text, return_tensors=\"pt\")\nprint(f\"âœ… Bengali encoding: {encoded['input_ids'].shape}\")\n\nprint(\"\\nğŸ‰ Installation complete! You can now run your TATN notebook.\")\n","metadata":{"id":"W8IIWAEHH4Jy","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:05:54.705923Z","iopub.execute_input":"2026-01-16T16:05:54.706235Z","iopub.status.idle":"2026-01-16T16:06:44.157343Z","shell.execute_reply.started":"2026-01-16T16:05:54.706213Z","shell.execute_reply":"2026-01-16T16:06:44.156622Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.1\nUninstalling transformers-4.57.1:\n  Successfully uninstalled transformers-4.57.1\nFound existing installation: tokenizers 0.22.1\nUninstalling tokenizers-0.22.1:\n  Successfully uninstalled tokenizers-0.22.1\nFound existing installation: sentence-transformers 5.1.1\nUninstalling sentence-transformers-5.1.1:\n  Successfully uninstalled sentence-transformers-5.1.1\nFound existing installation: huggingface-hub 0.36.0\nUninstalling huggingface-hub-0.36.0:\n  Successfully uninstalled huggingface-hub-0.36.0\nCollecting transformers\n  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sacrebleu\n  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\nDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: sacremoses, portalocker, sacrebleu, huggingface-hub, tokenizers, transformers\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 portalocker-3.2.0 sacrebleu-2.6.0 sacremoses-0.1.1 tokenizers-0.22.2 transformers-4.57.6\nâœ… transformers: 4.57.6\nâœ… tokenizers: 0.22.2\nâœ… sacrebleu: 2.6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6579c81d4ce410da9e0457d7a293991"}},"metadata":{}},{"name":"stderr","text":"'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ec3ad013-e97f-4620-ab15-287033404bde)')' thrown while requesting HEAD https://huggingface.co/facebook/m2m100_418M/resolve/main/vocab.json\nRetrying in 1s [Retry 1/5].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2f0267bffa4bd6aa0d835c261f4c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77f5de892844ec0b3a9bcce7c2d613a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9262827e9f4b039ea1d6be2ab01e53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307fa8b422194ea4881b3b7d58028441"}},"metadata":{}},{"name":"stdout","text":"âœ… M2M100 tokenizer loaded: 128104 tokens\nâœ… Bengali encoding: torch.Size([1, 8])\n\nğŸ‰ Installation complete! You can now run your TATN notebook.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:44.158988Z","iopub.execute_input":"2026-01-16T16:06:44.159474Z","iopub.status.idle":"2026-01-16T16:06:44.163714Z","shell.execute_reply.started":"2026-01-16T16:06:44.159445Z","shell.execute_reply":"2026-01-16T16:06:44.163012Z"}},"outputs":[{"name":"stdout","text":"4.57.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: TATN CONFIGURATION - ULTRA MEMORY SAFE (EMERGENCY)\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n\ntry:\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\n    _HAS_M2M_TOKENIZER = True\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer\n        _HAS_M2M_TOKENIZER = True\n    except Exception:\n        M2M100Tokenizer = None\n        _HAS_M2M_TOKENIZER = False\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\nNUM_GPUS = torch.cuda.device_count()\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[Cell 0] Single GPU Mode\")\n\nprint(f\"[Cell 0] Device: {DEVICE}\")\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    \"/kaggle/input/samanantar/samanantar_bn_en.csv\"\n)\n\ndef _safe_int(value, default: int, name: str, min_val: int = 1) -> int:\n    try:\n        result = int(value)\n        if result < min_val:\n            return default\n        return result\n    except:\n        return default\n\ndef _safe_float(value, default: float, name: str, min_val: float = 0.0) -> float:\n    try:\n        result = float(value)\n        if result < min_val:\n            return default\n        return result\n    except:\n        return default\n\nBATCH_SIZE = _safe_int(50, 50, \"BATCH_SIZE\", min_val=1)\nNUM_SAMPLES = _safe_int(40000, 40000, \"NUM_SAMPLES\", min_val=1)\nMAX_LENGTH = _safe_int(64, 64, \"MAX_LENGTH\", min_val=8)\n\nLR_NMT = _safe_float(2e-5, 2e-5, \"LR_NMT\", min_val=1e-7)\nLR_TRG = _safe_float(1e-5, 1e-5, \"LR_TRG\", min_val=1e-7)\nLR_PHI = _safe_float(1e-4, 1e-4, \"LR_PHI\", min_val=1e-7)\n\nEPOCHS = _safe_int(1, 1, \"EPOCHS\", min_val=1)\nGRAD_CLIP_NORM = _safe_float(1.0, 1.0, \"GRAD_CLIP_NORM\", min_val=0.1)\nUSE_AMP = True\nPRINT_INTERVAL = _safe_int(200, 200, \"PRINT_INTERVAL\", min_val=1)\nSEED = _safe_int(42, 42, \"SEED\", min_val=0)\n\nACCUMULATION_STEPS = _safe_int(16, 16, \"ACCUMULATION_STEPS\", min_val=1)\n\nMC_DROPOUT_PASSES = _safe_int(3, 3, \"MC_DROPOUT_PASSES\", min_val=1)\nTRG_EVIDENCE_K = _safe_int(2, 2, \"TRG_EVIDENCE_K\", min_val=1)\nMAX_SILVER_BUFFER = _safe_int(30, 30, \"MAX_SILVER_BUFFER\", min_val=1)\n\nNUM_WORKERS = _safe_int(2, 2, \"NUM_WORKERS\", min_val=0)\nPIN_MEMORY = True\nPREFETCH_FACTOR = _safe_int(2, 2, \"PREFETCH_FACTOR\", min_val=1)\nGRADIENT_CHECKPOINTING = True\n\nDEBUG_DISCOVERY = False\nDEBUG_TIMING = True\nDEBUG_VERBOSE = False\n\nDSCD_BUFFER_SIZE = _safe_int(20, 20, \"DSCD_BUFFER_SIZE\", min_val=1)\nDSCD_MAX_PROTOS = _safe_int(3, 3, \"DSCD_MAX_PROTOS\", min_val=1)\nDSCD_N_MIN = _safe_int(3, 3, \"DSCD_N_MIN\", min_val=1)\nDSCD_DISPERSION_THRESHOLD = _safe_float(0.35, 0.35, \"DSCD_DISPERSION_THRESHOLD\", min_val=0.0)\nDSCD_EMBED_DIM = _safe_int(1024, 1024, \"DSCD_EMBED_DIM\", min_val=64)\nDSCD_TEMPERATURE = _safe_float(0.7, 0.7, \"DSCD_TEMPERATURE\", min_val=0.01)\nDSCD_DROPOUT = _safe_float(0.1, 0.1, \"DSCD_DROPOUT\", min_val=0.0)\nDSCD_AUGMENT_SCALE = _safe_float(0.05, 0.05, \"DSCD_AUGMENT_SCALE\", min_val=0.0)\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_WARMUP_SAMPLES = _safe_int(5000, 5000, \"DSCD_WARMUP_SAMPLES\", min_val=0)\n\nPERIODIC_DISCOVERY_FREQUENCY = _safe_int(200, 200, \"PERIODIC_DISCOVERY_FREQUENCY\", min_val=1)\nMAX_TOKENS_PER_DISCOVERY = _safe_int(100, 100, \"MAX_TOKENS_PER_DISCOVERY\", min_val=1)\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = True\nENABLE_TRG_TRAINING = True\nENABLE_TRG_INFERENCE = True\n\nCLUSTERING_TIMEOUT = _safe_int(3, 3, \"CLUSTERING_TIMEOUT\", min_val=1)\nMEMORY_CLEANUP_FREQUENCY = _safe_int(50, 50, \"MEMORY_CLEANUP_FREQUENCY\", min_val=1)\nVALIDATION_CHECK_INTERVAL = _safe_int(200, 200, \"VALIDATION_CHECK_INTERVAL\", min_val=1)\nVERBOSE_LOGGING = False\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_SAVE_AFTER_TRAINING = True\nCHECKPOINT_FILENAME = \"tatn_final.pt\"\nCHECKPOINT_INTERVAL = 99999999\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = _safe_int(10000, 10000, \"REPLAY_BUFFER_SIZE\", min_val=0)\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\n\nTAU_LOW = _safe_float(0.15, 0.15, \"TAU_LOW\", min_val=0.0)\nTAU_HIGH = _safe_float(0.85, 0.85, \"TAU_HIGH\", min_val=0.0)\nTAU_ACCEPT = _safe_float(0.8, 0.8, \"TAU_ACCEPT\", min_val=0.0)\n\nTRG_MAX_GEN_LEN = _safe_int(12, 12, \"TRG_MAX_GEN_LEN\", min_val=1)\nTRG_GEN_EMBED = _safe_int(64, 64, \"TRG_GEN_EMBED\", min_val=8)\nTRG_GEN_HID = _safe_int(64, 64, \"TRG_GEN_HID\", min_val=8)\n\nSPAN_THRESHOLD = _safe_float(0.20, 0.20, \"SPAN_THRESHOLD\", min_val=0.0)\nUNCERTAINTY_THRESHOLD = _safe_float(0.15, 0.15, \"UNCERTAINTY_THRESHOLD\", min_val=0.0)\nTRG_TEMPERATURE = _safe_float(1.0, 1.0, \"TRG_TEMPERATURE\", min_val=0.01)\n\nASBN_HIDDEN_DIM = _safe_int(64, 64, \"ASBN_HIDDEN_DIM\", min_val=8)\nASBN_LAMBDA = _safe_float(0.05, 0.05, \"ASBN_LAMBDA\", min_val=0.0)\nASBN_DROPOUT = _safe_float(0.1, 0.1, \"ASBN_DROPOUT\", min_val=0.0)\n\nLAMBDA_ASBN = _safe_float(0.3, 0.3, \"LAMBDA_ASBN\", min_val=0.0)\nLAMBDA_DSCD = _safe_float(0.15, 0.15, \"LAMBDA_DSCD\", min_val=0.0)\n\nTRAIN_DOMAIN = 0\nTEST_DOMAIN = 1\nUSE_DOMAIN_LABELS = True\n\nGRL_ALPHA_START = _safe_float(0.0, 0.0, \"GRL_ALPHA_START\", min_val=0.0)\nGRL_ALPHA_END = _safe_float(1.0, 1.0, \"GRL_ALPHA_END\", min_val=0.0)\nGRL_ALPHA_SCHEDULE = \"linear\"\nGRL_ALPHA_STEPS = 500\n\nSOURCE_LANGUAGE = \"bn\"\nTARGET_LANGUAGE = \"en\"\n\nM2M100_BN_TOKEN_ID = 128012\nM2M100_EN_TOKEN_ID = 128022\n\nFREEZE_ENCODER = False\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = {\n    \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦«à¦²\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n    \"à¦ªà¦¡à¦¼à¦¾\", \"à¦¦à§‡à¦–à¦¾\", \"à¦šà¦²à¦¾\", \"à¦§à¦°à¦¾\", \"à¦…à¦°à§à¦¥\", \"à¦¶à¦¬à§à¦¦\", \"à¦®à§à¦–\",\n    \"à¦¤à§‹à¦²à¦¾\", \"à¦¬à¦¾à¦à¦šà¦¾\", \"à¦®à¦¾à¦°à¦¾\", \"à¦‰à¦¤à§à¦¤à¦°\", \"à¦ªà¦¾à¦¤à§à¦°\", \"à¦¬à§‡à¦²à¦¾\", \"à¦—à¦¾à¦¨\",\n    \"à¦¨à¦¾à¦®\", \"à¦¬à¦²\", \"à¦šà¦¾à¦²\", \"à¦•à¦²à¦¾\", \"à¦§à¦¾à¦°à¦¾\", \"à¦ªà¦¤à§à¦°\", \"à¦°à¦¾à¦—\", \"à¦°à¦¸\",\n    \"à¦¤à§€à¦°\", \"à¦œà¦®à¦¾\", \"à¦®à¦¾à¦¨\", \"à¦¦à¦¾à¦¬à¦¿\", \"à¦†à¦¸à¦¨\", \"à¦¸à¦¾à¦¡à¦¼à¦¾\", \"à¦¬à¦¸à¦¾\", \"à¦ªà¦¦\",\n    \"à¦…à¦‚à¦¶\", \"à¦®à§‹à¦¡à¦¼\", \"à¦˜à¦°\", \"à¦®à¦¨\"\n}\n\nHOMOGRAPH_WATCHLIST_BN: Set[str] = set()\nHOMOGRAPH_WATCHLIST: Set[str] = set()\nUSE_WATCHLIST_PRIORITIZATION = False\nWATCHLIST_ONLY_FOR_TRG = False\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t)\n    t = t.replace(\"â–\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t).lower().strip()\n    return t\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\")\n            except Exception:\n                pass\n\ndef get_checkpoint_path() -> str:\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    return False\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({f\"__{SOURCE_LANGUAGE}__\", f\"__{TARGET_LANGUAGE}__\"})\n    return s\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 5000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n    clean = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        if len(clean) < 2:\n            result = False\n        else:\n            has_bengali_chars = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if \"\\u0980\" <= c <= \"\\u09FF\")\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    result = (bengali_count / alphanum_count) >= 0.5\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        return {\n            \"count\": len(self.discovery_times),\n            \"total\": total,\n            \"avg\": total / len(self.discovery_times),\n            \"max\": max(self.discovery_times),\n        }\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\ntry:\n    effective_batch = BATCH_SIZE * ACCUMULATION_STEPS\n    if USE_MULTI_GPU and NUM_GPUS > 0:\n        effective_batch *= NUM_GPUS\nexcept Exception:\n    effective_batch = BATCH_SIZE\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TATN CONFIGURATION - ULTRA MEMORY SAFE (EMERGENCY FIX)\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs)\")\nprint(f\"Dataset: {DATASET_CSV_PATH}\")\nprint(f\"Samples: {NUM_SAMPLES:,} | Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS}\")\nprint()\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: Ultra-conservative configuration loaded (EMERGENCY)\")\nprint(\"=\" * 80)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:44.164702Z","iopub.execute_input":"2026-01-16T16:06:44.164996Z","iopub.status.idle":"2026-01-16T16:06:44.564932Z","shell.execute_reply.started":"2026-01-16T16:06:44.164964Z","shell.execute_reply":"2026-01-16T16:06:44.564279Z"}},"outputs":[{"name":"stdout","text":"[Cell 0] Multi-GPU Mode: 2 GPUs available\n[Cell 0] Device: cuda:0\n\n================================================================================\nTATN CONFIGURATION - ULTRA MEMORY SAFE (EMERGENCY FIX)\n================================================================================\nUser: manas0003\nMulti-GPU: ENABLED (2 GPUs)\nDataset: /kaggle/input/samanantar/samanantar_bn_en.csv\nSamples: 40,000 | Batch: 50 | Accum: 16\nEffective batch: 1600\nMax length: 64 | Epochs: 1\n\n\n[GPU MONITOR] Checking 2 GPU(s):\n  GPU 0: 0.00GB allocated / 0.00GB reserved\n  GPU 1: 0.00GB allocated / 0.00GB reserved\n\n================================================================================\nCell 0: Ultra-conservative configuration loaded (EMERGENCY)\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: TOKENIZER UTILITIES - VOCABULARY-SAFE VERSION\n# ===========================================================================================\n\nimport threading\nfrom typing import Tuple, List, Dict, Optional, Set\nimport numpy as np\nimport torch\n\ntry:\n    if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0:\n        SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\n    else:\n        SAFE_OFFSET_MAX_LEN = 48\nexcept (NameError, ValueError, TypeError):\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _TARGET_LANG = TARGET_LANGUAGE\nexcept NameError:\n    _TARGET_LANG = \"en\"\n\ntry:\n    _DEBUG_VERBOSE = DEBUG_VERBOSE\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = DEBUG_DISCOVERY\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _M2M_BN_TOKEN_ID = M2M100_BN_TOKEN_ID\nexcept NameError:\n    _M2M_BN_TOKEN_ID = 128012\n\ntry:\n    _M2M_EN_TOKEN_ID = M2M100_EN_TOKEN_ID\nexcept NameError:\n    _M2M_EN_TOKEN_ID = 128022\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n_VOCAB_SIZE_CACHE: Dict[str, int] = {}\n\ndef _special_token_cache_key(tokenizer) -> str:\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None)\n    if not name:\n        name = \"unknown_tokenizer\"\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_vocab_size(tokenizer) -> int:\n    cache_key = _special_token_cache_key(tokenizer)\n    \n    if cache_key in _VOCAB_SIZE_CACHE:\n        return _VOCAB_SIZE_CACHE[cache_key]\n    \n    vocab_size = 128004\n    \n    try:\n        if hasattr(tokenizer, \"__len__\"):\n            vocab_size = len(tokenizer)\n        elif hasattr(tokenizer, \"vocab_size\"):\n            vocab_size = int(tokenizer.vocab_size)\n        elif hasattr(tokenizer, \"get_vocab\"):\n            vocab_size = len(tokenizer.get_vocab())\n    except Exception:\n        pass\n    \n    _VOCAB_SIZE_CACHE[cache_key] = vocab_size\n    return vocab_size\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens: Set[str] = set()\n        try:\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"all_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"additional_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\",\n                         \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            try:\n                stm = (\n                    getattr(tokenizer, \"special_tokens_map\", None)\n                    or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                )\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n        except Exception:\n            special_tokens = set()\n\n        special_tokens.update({\n            \"__bn__\", \"__en__\",\n            \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n        })\n\n        try:\n            vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n            special_tokens = {\n                tok\n                for tok in special_tokens\n                if tok in vocab or tok in {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n            }\n        except Exception:\n            pass\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in arr[0]\n                        ]\n                        return enc\n                if isinstance(off, (list, tuple)):\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in off[0]\n                        ]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    try:\n        data = getattr(enc, \"data\", None)\n        if (\n            data\n            and isinstance(data, dict)\n            and \"offset_mapping\" in data\n            and data[\"offset_mapping\"] is not None\n        ):\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [\n                    (x[0], x[1])\n                    if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                    else (None, None)\n                    for x in om[0]\n                ]\n                return enc\n    except Exception:\n        pass\n\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif (\n                isinstance(input_ids, (list, tuple))\n                and len(input_ids) > 0\n                and isinstance(input_ids[0], (list, tuple))\n            ):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\ndef safe_offsets_tokenize(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n    include_special_tokens: bool = False,\n) -> dict:\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        if _DEBUG_VERBOSE:\n            print(\"[WARN] Failed to convert input to string, using empty string\")\n        text = \"\"\n\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n    \n    vocab_size = get_tokenizer_vocab_size(tokenizer)\n\n    tokenize_kwargs = {\n        \"return_tensors\": \"pt\",\n        \"truncation\": True,\n        \"padding\": False,\n        \"max_length\": eff_max,\n        \"add_special_tokens\": include_special_tokens,\n    }\n    \n    try:\n        tokenizer.src_lang = _SOURCE_LANG\n    except Exception:\n        pass\n\n    if is_fast:\n        try:\n            tokenize_kwargs[\"return_offsets_mapping\"] = True\n            enc = tokenizer(sample_text, **tokenize_kwargs)\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            \n            if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n                enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], 0, vocab_size - 1)\n            \n            return enc\n        except Exception:\n            pass\n\n    try:\n        enc = tokenizer(sample_text, **tokenize_kwargs)\n        \n        if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n            enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], 0, vocab_size - 1)\n        \n    except Exception as e:\n        if _DEBUG_VERBOSE:\n            print(f\"[WARN] Tokenization failed: {e}, returning empty encoding\")\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\n            \"input_ids\": torch.tensor([[pad_id]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n        }\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"â–\", \"\").replace(\"##\", \"\").replace(\"Ä \", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\ndef reconstruct_word_spans(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n) -> Tuple[Dict[int, str], List[str]]:\n    global _LANGUAGE_WARNING_COUNT\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in text)\n    has_english = any(\"a\" <= c.lower() <= \"z\" for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (\n            sum(1 for c in text if \"\\u0980\" <= c <= \"\\u09FF\")\n            / max(1, len(text))\n            * 100.0\n        )\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(\n            f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), \"\n            f\"English: {has_english}\"\n        )\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n    vocab_size = get_tokenizer_vocab_size(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    try:\n        encoded = safe_offsets_tokenize(\n            tokenizer, text, max_length=eff_max, include_special_tokens=False\n        )\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n        input_ids = [min(max(0, tid), vocab_size - 1) for tid in input_ids]\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if isinstance(offsets, list) and len(offsets) > 0 and all(\n        isinstance(x, tuple) for x in offsets\n    ):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(\n        offsets[0], (list, tuple)\n    ):\n        offsets_list = [\n            (x[0], x[1])\n            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n            else (None, None)\n            for x in offsets[0]\n        ]\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, str] = {}\n    words: List[str] = []\n\n    used_any_offset = any(\n        isinstance(o, tuple) and o[0] is not None and o[1] is not None\n        for o in offsets_list\n    )\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n        current_accumulated_word = \"\"\n\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            if off_start is not None and off_end is not None:\n                if off_start < 0 or off_end < 0:\n                    if _DEBUG_VERBOSE:\n                        print(\n                            f\"[WARN] Negative offset detected: \"\n                            f\"({off_start}, {off_end}), skipping\"\n                        )\n                    off_start, off_end = None, None\n                else:\n                    off_start = max(0, min(off_start, text_len))\n                    off_end = max(off_start, min(off_end, text_len))\n\n            if off_start is None or off_end is None:\n                if current_accumulated_word:\n                    token_word_map[idx] = current_accumulated_word\n                \n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                continue\n\n            if tok in special_tokens:\n                continue\n\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                if off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            try:\n                current_word = text[word_start:word_end].strip()\n                if current_word:\n                    token_word_map[idx] = current_word\n                    current_accumulated_word = current_word\n            except Exception:\n                pass\n\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    running_word = \"\"\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            continue\n        \n        clean = (tok or \"\").replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            continue\n\n        if tok.startswith(\"â–\") or tok.startswith(\"Ä \"):\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n            current_parts = [clean]\n            running_word = clean\n        else:\n            current_parts.append(clean)\n            running_word = \"\".join(current_parts)\n            if len(running_word) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                current_parts = [clean]\n                running_word = clean\n\n        if running_word:\n            token_word_map[i] = running_word\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    try:\n        words_from_markers: List[str] = []\n        current_word_parts: List[str] = []\n        \n        for tok in tokens:\n            if tok in special_tokens:\n                continue\n            \n            clean = (tok or \"\").replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n            if not clean:\n                continue\n            \n            if tok.startswith(\"â–\") or tok.startswith(\"Ä \"):\n                if current_word_parts:\n                    words_from_markers.append(\"\".join(current_word_parts))\n                current_word_parts = [clean]\n            else:\n                current_word_parts.append(clean)\n        \n        if current_word_parts:\n            words_from_markers.append(\"\".join(current_word_parts))\n        \n        if words_from_markers:\n            word_list = words_from_markers\n        else:\n            word_list = [w for w in text.split() if w.strip()]\n        \n        token_word_map = {}\n\n        if tokens and word_list:\n            word_idx = 0\n\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    continue\n\n                if word_idx < len(word_list):\n                    current_word = word_list[word_idx]\n                    if clean in current_word or current_word.startswith(clean):\n                        token_word_map[i] = current_word\n                    else:\n                        word_idx = min(word_idx + 1, len(word_list) - 1)\n                        token_word_map[i] = word_list[word_idx]\n                else:\n                    if word_list:\n                        token_word_map[i] = word_list[-1]\n\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\ndef is_valid_token(\n    token: str,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\"\n) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    if special_tokens is None and tokenizer is not None:\n        special_tokens = get_tokenizer_special_tokens(tokenizer)\n    \n    if special_tokens and token in special_tokens:\n        return False\n    \n    clean = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    if not clean:\n        return False\n    \n    if len(clean) < 2:\n        return False\n    \n    if language == \"bn\":\n        has_bengali = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n        return has_bengali\n    \n    return True\n\ndef validate_tokenizer_vocab(tokenizer, expected_vocab_size: Optional[int] = None) -> bool:\n    actual_vocab_size = get_tokenizer_vocab_size(tokenizer)\n    \n    print(f\"[TOKENIZER-VALIDATION] Actual vocab size: {actual_vocab_size}\")\n    \n    if expected_vocab_size is not None:\n        if actual_vocab_size != expected_vocab_size:\n            print(f\"[TOKENIZER-VALIDATION] âŒ MISMATCH: Expected {expected_vocab_size}, got {actual_vocab_size}\")\n            return False\n        else:\n            print(f\"[TOKENIZER-VALIDATION] âœ… Vocab size matches: {actual_vocab_size}\")\n            return True\n    \n    bn_token_str = f\"__{_SOURCE_LANG}__\"\n    en_token_str = f\"__{_TARGET_LANG}__\"\n    \n    try:\n        bn_id = tokenizer.convert_tokens_to_ids(bn_token_str)\n        en_id = tokenizer.convert_tokens_to_ids(en_token_str)\n        \n        print(f\"[TOKENIZER-VALIDATION] Language tokens:\")\n        print(f\"  {bn_token_str} â†’ {bn_id}\")\n        print(f\"  {en_token_str} â†’ {en_id}\")\n        \n        if bn_id >= actual_vocab_size or en_id >= actual_vocab_size:\n            print(f\"[TOKENIZER-VALIDATION] âŒ Language token IDs exceed vocab size!\")\n            return False\n        \n        if expected_vocab_size is None:\n            try:\n                if bn_id != _M2M_BN_TOKEN_ID or en_id != _M2M_EN_TOKEN_ID:\n                    print(f\"[TOKENIZER-VALIDATION] âš ï¸  Language token IDs differ from Cell 0:\")\n                    print(f\"  Expected: bn={_M2M_BN_TOKEN_ID}, en={_M2M_EN_TOKEN_ID}\")\n                    print(f\"  Got: bn={bn_id}, en={en_id}\")\n                    print(f\"  â†’ Update Cell 0 with correct values\")\n            except NameError:\n                pass\n        \n        print(f\"[TOKENIZER-VALIDATION] âœ… Language tokens valid\")\n        return True\n        \n    except Exception as e:\n        print(f\"[TOKENIZER-VALIDATION] âŒ Language token validation failed: {e}\")\n        return False\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    sample_bn = \"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦¯à¦¾à¦¬à¥¤\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 0] Vocabulary validation:\")\n        validate_tokenizer_vocab(tokenizer)\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        enc_bn = safe_offsets_tokenize(\n            tokenizer, sample_bn, max_length=32, include_special_tokens=False\n        )\n        enc_len = (\n            int(enc_bn[\"input_ids\"].shape[-1])\n            if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn\n            else \"N/A\"\n        )\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(\n            any(\"\\u0980\" <= c <= \"\\u09FF\" for c in w) for w in words_bn\n        )\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(\n            any(\"a\" <= c.lower() <= \"z\" for c in w) for w in words_en\n        )\n        print(f\"  Contains English words: {has_english_words}\")\n\n        print(\"\\n[TEST 3] Token validation:\")\n        special_tokens = get_tokenizer_special_tokens(tokenizer)\n        test_tokens = [\"à¦•à¦¾à¦²\", \"â–à¦†à¦®à¦¿\", \"</s>\", \"##ing\", \"a\"]\n        for tok in test_tokens:\n            valid = is_valid_token(tok, special_tokens, tokenizer, \"bn\")\n            print(f\"  '{tok}': {'valid' if valid else 'invalid'}\")\n\n        if has_bengali_words and not any(\n            \"a\" <= c.lower() <= \"z\" for c in \"\".join(words_bn)\n        ):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\nisvalidtoken = is_valid_token\ngettokenizervocabsize = get_tokenizer_vocab_size\nvalidatetokenizervocab = validate_tokenizer_vocab\n\nprint(\"=\" * 80)\nprint(\"Cell 1: Tokenizer utilities loaded - VOCABULARY-SAFE VERSION\")\nprint(\"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Added get_tokenizer_vocab_size() function (line 66)\")\nprint(\"  âœ… FIX #2: Added vocab_size caching with _VOCAB_SIZE_CACHE (line 51)\")\nprint(\"  âœ… FIX #3: Added torch.clamp() for input_ids in safe_offsets_tokenize (line 288)\")\nprint(\"  âœ… FIX #4: Added vocab bounds check in reconstruct_word_spans (line 427)\")\nprint(\"  âœ… FIX #5: Added validate_tokenizer_vocab() function (line 675)\")\nprint(\"  âœ… FIX #6: Set tokenizer.src_lang before encoding (line 275)\")\nprint(\"  âœ… FIX #7: Added language token validation in test (line 720)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"WZE9PkHyH4J1","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:44.566022Z","iopub.execute_input":"2026-01-16T16:06:44.566268Z","iopub.status.idle":"2026-01-16T16:06:44.632038Z","shell.execute_reply.started":"2026-01-16T16:06:44.566248Z","shell.execute_reply":"2026-01-16T16:06:44.631303Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 1: Tokenizer utilities loaded - VOCABULARY-SAFE VERSION\n================================================================================\nCRITICAL FIXES APPLIED:\n  âœ… FIX #1: Added get_tokenizer_vocab_size() function (line 66)\n  âœ… FIX #2: Added vocab_size caching with _VOCAB_SIZE_CACHE (line 51)\n  âœ… FIX #3: Added torch.clamp() for input_ids in safe_offsets_tokenize (line 288)\n  âœ… FIX #4: Added vocab bounds check in reconstruct_word_spans (line 427)\n  âœ… FIX #5: Added validate_tokenizer_vocab() function (line 675)\n  âœ… FIX #6: Set tokenizer.src_lang before encoding (line 275)\n  âœ… FIX #7: Added language token validation in test (line 720)\n================================================================================\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING - VOCABULARY & PUNCTUATION SAFE\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n    print(\"[CELL2] WARNING: SOURCE_LANGUAGE/TARGET_LANGUAGE not defined, using defaults bn/en\")\n\ntry:\n    _M2M_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\n    _M2M_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept NameError:\n    _M2M_BN_TOKEN_ID = 128012\n    _M2M_EN_TOKEN_ID = 128022\n    print(\"[CELL2] WARNING: M2M100 token IDs not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/samanantar/samanantar_bn_en.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\n    _USE_DOMAIN_LABELS = bool(USE_DOMAIN_LABELS)\nexcept NameError:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = False\n    print(\"[CELL2] WARNING: Domain label config not found, disabling domain labels\")\n\n_has_normalize = (\"normalize_bengali\" in globals()) and (\"normalize_english\" in globals())\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n_BENGALI_CHAR_RE = re.compile(r\"[\\u0980-\\u09FF]\")\n_BENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}'])\n\ndef is_bengali_text(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\ndef separate_bengali_punctuation(text: str, language: str = \"bn\") -> str:\n    \"\"\"Separate Bengali/Indic punctuation marks from words.\"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n    \n    text = text.strip()\n    \n    if language in [\"bn\", \"hi\", \"te\", \"ta\", \"ml\", \"mr\", \"gu\", \"pa\"]:\n        text = re.sub(r'([à¥¤à¥¥])', r' \\1 ', text)\n    \n    text = re.sub(r'([,;:!?()\\[\\]{}])', r' \\1 ', text)\n    \n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef clean_and_normalize_text(text: str, language: str = \"bn\") -> str:\n    \"\"\"Clean, normalize and separate punctuation from text.\"\"\"\n    if not text or not isinstance(text, str):\n        return \"\"\n    \n    text = text.strip()\n    if not text:\n        return \"\"\n    \n    text = separate_bengali_punctuation(text, language)\n    \n    if _has_normalize:\n        if language == \"bn\":\n            text = normalize_bengali(text)\n        else:\n            text = normalize_english(text)\n    else:\n        text = text.strip()\n        if language == \"en\":\n            text = text.lower()\n    \n    return text\n\ndef is_punctuation_only(token: str) -> bool:\n    \"\"\"Check if token is pure punctuation.\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    if not clean:\n        return False\n    \n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in _COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in _BENGALI_PUNCT_SET or c in _COMMON_PUNCT_SET for c in clean)\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    try:\n        if dataset is not None and hasattr(dataset, \"_tokenizer_name_or_path\") and dataset._tokenizer_name_or_path:\n            try:\n                from transformers import M2M100Tokenizer\n                dataset.tokenizer = M2M100Tokenizer.from_pretrained(dataset._tokenizer_name_or_path)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n                dataset.vocab_size = len(dataset.tokenizer)\n                \n                try:\n                    dataset.tokenizer.src_lang = _SOURCE_LANG\n                except Exception:\n                    pass\n                \n                if DEBUG_CELL2:\n                    print(f\"[CELL2-WORKER-{worker_id}] Tokenizer reloaded (vocab={dataset.vocab_size})\")\n            except Exception as e:\n                cell2_dbg(\"worker_tokenizer_reload\", f\"Worker {worker_id} tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n                dataset.vocab_size = 128004\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] Tokenizer rebind failed in worker {worker_id}\")\n\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\ndef load_and_preprocess_optimized(\n    num_samples: Optional[int] = None,\n    split: str = \"train\",\n) -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    try:\n        print(\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        if df.empty:\n            print(\"[CELL2] ERROR: CSV file is empty\")\n            return _get_fallback_dataset()\n\n        if \"src\" not in df.columns or \"tgt\" not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing required columns. Found columns: {list(df.columns)}\")\n            print(\"[CELL2] Expected format: src (Bengali), tgt (English) OR src (English), tgt (Bengali)\")\n            return _get_fallback_dataset()\n\n        sample_size = min(10, len(df))\n        sample_rows = df.head(sample_size)\n        \n        src_bengali_count = sum(1 for s in sample_rows[\"src\"] if is_bengali_text(str(s)))\n        tgt_bengali_count = sum(1 for s in sample_rows[\"tgt\"] if is_bengali_text(str(s)))\n        \n        src_is_bengali = src_bengali_count > sample_size * 0.5\n        tgt_is_bengali = tgt_bengali_count > sample_size * 0.5\n\n        if not src_is_bengali and tgt_is_bengali:\n            print(\"[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bnâ†’en task.\")\n            df = df.rename(columns={\"src\": \"_temp_tgt\", \"tgt\": \"_temp_src\"})\n            df = df.rename(columns={\"_temp_src\": \"src\", \"_temp_tgt\": \"tgt\"})\n            \n            sample_rows = df.head(sample_size)\n            src_bengali_count = sum(1 for s in sample_rows[\"src\"] if is_bengali_text(str(s)))\n            src_is_bengali = src_bengali_count > sample_size * 0.5\n            \n            if not src_is_bengali:\n                print(\"[CELL2] ERROR: Swap failed, src is still not Bengali.\")\n                return _get_fallback_dataset()\n            else:\n                print(\"[CELL2] Swap successful: src=Bengali, tgt=English\")\n        elif not src_is_bengali:\n            print(\"[CELL2] WARNING: src column does not appear to be Bengali. Proceeding but output may be incorrect.\")\n\n        df = df.head(num_samples)\n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n\n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n\n        for row_tuple in tqdm(df.itertuples(index=False), total=len(df), desc=\"Loading dataset\"):\n            try:\n                src_val = row_tuple.src\n                tgt_val = row_tuple.tgt\n                if pd.isna(src_val) or pd.isna(tgt_val):\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", \"NaN value detected\")\n                    continue\n                bn = str(src_val).strip()\n                en = str(tgt_val).strip()\n                if not bn or not en:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", \"Empty src/tgt field\")\n                    continue\n                if not is_bengali_text(bn):\n                    skipped += 1\n                    cell2_dbg(\"not_bengali_src\", \"src field not Bengali\")\n                    continue\n                if not re.search(r\"[a-zA-Z]\", en):\n                    skipped += 1\n                    cell2_dbg(\"not_english_tgt\", \"tgt field not English\")\n                    continue\n                \n                max_words = max(20, _MAX_LENGTH // 2)\n                if len(bn.split()) > max_words or len(en.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", \"Text too long\")\n                    continue\n                \n                bn_norm = clean_and_normalize_text(bn, language=\"bn\")\n                en_norm = clean_and_normalize_text(en, language=\"en\")\n                \n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", \"Empty after normalization\")\n                    continue\n                \n                pairs.append((bn_norm, en_norm))\n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception: {type(e).__name__}\")\n                continue\n\n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            print(\"[CELL2] Check that src column contains Bengali and tgt column contains English.\")\n            return _get_fallback_dataset()\n\n        return pairs\n\n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    print(\"[CELL2] Using fallback dataset (50 unique samples)\")\n    fallback_pairs = [\n        (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿\", \"i turned off the tap\"),\n        (\"à¦¸à§‡ à¦†à¦®à¦¾à¦•à§‡ à¦ªà¦°à§‡ à¦•à¦² à¦•à¦°à¦¬à§‡\", \"he will call me later\"),\n        (\"à¦†à¦®à¦°à¦¾ à¦ªà§à¦°à¦¤à¦¿à¦¦à¦¿à¦¨ à¦¤à¦¾à¦œà¦¾ à¦«à¦² à¦–à¦¾à¦‡\", \"we eat fresh fruits every day\"),\n        (\"à¦¤à¦¾à¦° à¦•à¦ à§‹à¦° à¦ªà¦°à¦¿à¦¶à§à¦°à¦®à§‡à¦° à¦­à¦¾à¦²à§‹ à¦«à¦² à¦¹à¦¯à¦¼à§‡à¦›à§‡\", \"his hard work has brought good results\"),\n        (\"à¦—à¦¾à¦›à§‡ à¦¨à¦¤à§à¦¨ à¦ªà¦¾à¦¤à¦¾à¦—à§à¦²à§‹ à¦—à¦œà¦¿à¦¯à¦¼à§‡à¦›à§‡\", \"new leaves have sprouted on the tree\"),\n        (\"à¦†à¦®à¦¿ à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦ªà¦¾à¦¤à¦¾ à¦‰à¦²à§à¦Ÿà¦¾à¦šà§à¦›à¦¿\", \"i am turning the pages of the book\"),\n        (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦¾à¦œà¦¾à¦°à§‡ à¦—à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦²à¦¾à¦®\", \"yesterday i went to the market\"),\n        (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¤à§‹à¦®à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦¦à§‡à¦–à¦¾ à¦•à¦°à¦¬\", \"tomorrow i will meet you\"),\n        (\"à¦¤à¦¾à¦°à¦¾ à¦†à¦•à¦¾à¦¶à§‡ à¦‰à¦œà§à¦œà§à¦¬à¦²\", \"the stars are bright in the sky\"),\n        (\"à¦¤à¦¾à¦°à¦¾ à¦¬à¦¾à¦¡à¦¼à¦¿à¦¤à§‡ à¦¨à§‡à¦‡\", \"they are not at home\"),\n        (\"à¦¬à§à¦¯à¦¾à¦‚à¦• à¦¨à¦¦à§€à¦° à¦§à¦¾à¦°à§‡ à¦­à§‡à¦™à§‡ à¦—à§‡à¦›à§‡\", \"the bank by the river has collapsed\"),\n        (\"à¦†à¦®à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦•à§‡ à¦Ÿà¦¾à¦•à¦¾ à¦œà¦®à¦¾ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à¦¿\", \"i deposited money in the bank\"),\n        (\"à¦¬à¦¾à¦° à¦¬à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡\", \"you have to try again and again\"),\n        (\"à¦†à¦®à¦¿ à¦¬à¦¾à¦° à¦–à§à¦²à§‡ à¦­à¦¿à¦¤à¦°à§‡ à¦¢à§à¦•à¦²à¦¾à¦®\", \"i opened the bar and entered\"),\n        (\"à¦¤à¦¾à¦° à¦®à¦¾à¦¥à¦¾ à¦¬à§à¦¯à¦¥à¦¾ à¦•à¦°à¦›à§‡\", \"his head is hurting\"),\n        (\"à¦†à¦®à¦¿ à¦®à¦¾à¦¥à¦¾ à¦¨à§‡à¦¡à¦¼à§‡ à¦¸à¦®à§à¦®à¦¤à¦¿ à¦¦à¦¿à¦²à¦¾à¦®\", \"i nodded my head in agreement\"),\n        (\"à¦¸à§‡ à¦¹à¦¾à¦° à¦®à§‡à¦¨à§‡ à¦¨à¦¿à¦¯à¦¼à§‡à¦›à§‡\", \"he accepted defeat\"),\n        (\"à¦†à¦®à¦¿ à¦—à¦²à¦¾à¦¯à¦¼ à¦¸à§‹à¦¨à¦¾à¦° à¦¹à¦¾à¦° à¦ªà¦°à§‡à¦›à¦¿\", \"i am wearing a gold necklace\"),\n        (\"à¦ªà¦¾à¦¨à¦¿ à¦–à§à¦¬ à¦ à¦¾à¦¨à§à¦¡à¦¾\", \"the water is very cold\"),\n        (\"à¦†à¦®à¦¿ à¦ªà¦¾à¦¨à¦¿ à¦–à¦¾à¦šà§à¦›à¦¿\", \"i am drinking water\"),\n        (\"à¦¦à¦² à¦–à§‡à¦²à¦¾à¦¯à¦¼ à¦œà¦¿à¦¤à§‡à¦›à§‡\", \"the team won the game\"),\n        (\"à¦†à¦®à¦¿ à¦®à¦¾à¦Ÿà¦¿ à¦¦à¦² à¦¦à¦¿à¦¯à¦¼à§‡ à¦«à§‡à¦²à¦²à¦¾à¦®\", \"i trampled the soil\"),\n        (\"à¦¬à¦¾à¦œà¦¾à¦° à¦¥à§‡à¦•à§‡ à¦¸à¦¬à¦œà¦¿ à¦•à¦¿à¦¨à¦²à¦¾à¦®\", \"i bought vegetables from the market\"),\n        (\"à¦¬à¦¾à¦œà¦¾à¦° à¦…à¦¨à§‡à¦• à¦­à¦¿à¦¡à¦¼ à¦›à¦¿à¦²\", \"the market was very crowded\"),\n        (\"à¦¤à¦¾à¦° à¦¨à¦¾à¦® à¦†à¦¹à¦®à§‡à¦¦\", \"his name is ahmed\"),\n        (\"à¦¨à¦¾à¦® à¦¨à¦¾ à¦•à¦°à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‹\", \"work without making a name\"),\n        (\"à¦•à¦¥à¦¾ à¦¬à¦²à¦¾ à¦¬à¦¨à§à¦§ à¦•à¦°à§‹\", \"stop talking\"),\n        (\"à¦¤à¦¾à¦° à¦•à¦¥à¦¾ à¦¶à§à¦¨à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦²\", \"i felt good hearing his words\"),\n        (\"à¦¬à¦‡ à¦ªà¦¡à¦¼à¦¤à§‡ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à§‡\", \"i like reading books\"),\n        (\"à¦†à¦®à¦¿ à¦à¦•à¦Ÿà¦¿ à¦¨à¦¤à§à¦¨ à¦¬à¦‡ à¦•à¦¿à¦¨à§‡à¦›à¦¿\", \"i bought a new book\"),\n        (\"à¦˜à¦° à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡\", \"the house has been cleaned\"),\n        (\"à¦†à¦®à¦¿ à¦˜à¦°à§‡ à¦¬à¦¸à§‡ à¦†à¦›à¦¿\", \"i am sitting at home\"),\n        (\"à¦®à¦¨ à¦­à¦¾à¦²à§‹ à¦¨à§‡à¦‡\", \"my mind is not good\"),\n        (\"à¦†à¦®à¦¾à¦° à¦®à¦¨ à¦šà¦¾à¦¯à¦¼ à¦¬à§‡à¦¡à¦¼à¦¾à¦¤à§‡ à¦¯à§‡à¦¤à§‡\", \"my mind wants to go for a walk\"),\n        (\"à¦¹à¦¾à¦¤ à¦§à§à¦¯à¦¼à§‡ à¦¨à¦¾à¦“\", \"wash your hands\"),\n        (\"à¦†à¦®à¦¿ à¦¤à¦¾à¦° à¦¹à¦¾à¦¤ à¦§à¦°à¦²à¦¾à¦®\", \"i held his hand\"),\n        (\"à¦¦à¦¿à¦¨ à¦•à§‡à¦Ÿà§‡ à¦¯à¦¾à¦šà§à¦›à§‡\", \"the day is passing by\"),\n        (\"à¦†à¦œ à¦•à¦¿ à¦¦à¦¿à¦¨\", \"what day is today\"),\n        (\"à¦°à¦¾à¦¤ à¦¹à¦¯à¦¼à§‡ à¦à¦¸à§‡à¦›à§‡\", \"night has come\"),\n        (\"à¦†à¦®à¦¿ à¦°à¦¾à¦¤ à¦œà§‡à¦—à§‡ à¦ªà¦¡à¦¼à§‡à¦›à¦¿\", \"i studied staying up at night\"),\n        (\"à¦œà¦² à¦–à§à¦¬ à¦—à¦°à¦®\", \"the water is very hot\"),\n        (\"à¦†à¦®à¦¿ à¦œà¦² à¦¦à¦¿à¦¯à¦¼à§‡ à¦—à¦¾à¦› à¦¸à¦¿à¦à§à¦šà¦¨ à¦•à¦°à§‡à¦›à¦¿\", \"i watered the plants\"),\n        (\"à¦¬à¦¾à¦¡à¦¼à¦¿ à¦¯à¦¾à¦šà§à¦›à¦¿\", \"i am going home\"),\n        (\"à¦†à¦®à¦¾à¦° à¦¬à¦¾à¦¡à¦¼à¦¿ à¦¢à¦¾à¦•à¦¾à¦¯à¦¼\", \"my house is in dhaka\"),\n        (\"à¦ªà¦¾à¦°à§à¦•à§‡ à¦…à¦¨à§‡à¦• à¦®à¦¾à¦¨à§à¦·\", \"there are many people in the park\"),\n        (\"à¦†à¦®à¦¿ à¦ªà§à¦°à¦¤à¦¿à¦¦à¦¿à¦¨ à¦ªà¦¾à¦°à§à¦•à§‡ à¦¹à¦¾à¦à¦Ÿà¦¿\", \"i walk in the park every day\"),\n        (\"à¦¨à¦¦à§€ à¦¬à¦‡à¦›à§‡\", \"the river is flowing\"),\n        (\"à¦†à¦®à¦¿ à¦¨à¦¦à§€à¦° à¦§à¦¾à¦°à§‡ à¦¦à¦¾à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦†à¦›à¦¿\", \"i am standing by the river\"),\n        (\"à¦¬à¦¨ à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°\", \"the forest is very beautiful\"),\n        (\"à¦†à¦®à¦¿ à¦¬à¦¨ à¦¦à§‡à¦–à¦¤à§‡ à¦—à¦¿à¦¯à¦¼à§‡à¦›à¦¿à¦²à¦¾à¦®\", \"i went to see the forest\"),\n    ]\n    \n    processed_pairs = []\n    for bn, en in fallback_pairs:\n        bn_clean = clean_and_normalize_text(bn, \"bn\")\n        en_clean = clean_and_normalize_text(en, \"en\")\n        if bn_clean and en_clean:\n            processed_pairs.append((bn_clean, en_clean))\n    \n    return processed_pairs\n\nclass MemoryEfficientDataset(Dataset):\n    def __init__(\n        self,\n        pairs: List[Tuple[str, str]],\n        tokenizer: Any = None,\n        max_length: Optional[int] = None,\n        split: str = \"train\",\n    ):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        self.split = split\n        \n        self.vocab_size = len(tokenizer) if tokenizer is not None else 128004\n        print(f\"[CELL2] Dataset vocab size: {self.vocab_size}\")\n\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n\n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid, split={self.split}\")\n\n        try:\n            if \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {\n                f\"__{_SOURCE_LANG}__\",\n                f\"__{_TARGET_LANG}__\",\n                \"</s>\",\n                \"<pad>\",\n                \"<s>\",\n                \"<unk>\",\n            }\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n                self.vocab_size = len(self.tokenizer) if self.tokenizer is not None else 128004\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            try:\n                self.tokenizer.src_lang = _SOURCE_LANG\n            except Exception:\n                pass\n\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(\n                    self.tokenizer, \n                    src_text, \n                    max_length=self.max_length, \n                    include_special_tokens=True\n                )\n                try:\n                    if isinstance(enc[\"input_ids\"], torch.Tensor):\n                        input_ids = enc[\"input_ids\"].squeeze(0) if enc[\"input_ids\"].dim() > 1 else enc[\"input_ids\"]\n                    else:\n                        input_ids = torch.tensor(enc[\"input_ids\"][0]) if isinstance(enc[\"input_ids\"], list) and len(enc[\"input_ids\"]) > 0 else torch.tensor(enc[\"input_ids\"])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0] if enc.get(\"input_ids\") else [1])\n                \n                attention_mask = enc.get(\"attention_mask\", None)\n                if attention_mask is None:\n                    attention_mask = torch.ones_like(input_ids)\n                elif isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                elif isinstance(attention_mask, torch.Tensor):\n                    attention_mask = attention_mask.squeeze(0) if attention_mask.dim() > 1 else attention_mask\n                \n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=True,\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception as e:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {e}\")\n\n            if not token_word_map and tokens:\n                try:\n                    current_word: List[str] = []\n                    for idx, tok in enumerate(tokens):\n                        if isinstance(tok, str) and tok not in self.special_tokens:\n                            if is_punctuation_only(tok):\n                                continue\n                            \n                            clean = tok.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n                            if clean:\n                                if tok.startswith(\"â–\") or tok.startswith(\"Ä \"):\n                                    current_word = [clean]\n                                else:\n                                    current_word.append(clean)\n                                word_str = \"\".join(current_word)\n                                if not is_punctuation_only(word_str):\n                                    token_word_map[idx] = word_str\n                except Exception as e:\n                    cell2_dbg(\"fallback_wm\", f\"Fallback word map failed: {e}\")\n\n            return input_ids, attention_mask, tokens, token_word_map\n\n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                self.vocab_size = len(self.tokenizer) if self.tokenizer is not None else 128004\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            try:\n                self.tokenizer.src_lang = _TARGET_LANG\n            except Exception:\n                pass\n\n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=True,\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            \n            labels = torch.clamp(labels, 0, self.vocab_size - 1)\n            \n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            labels[labels == int(pad_id)] = -100\n            return labels\n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"à¦†à¦®à¦¿\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            \n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n            \n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception:\n            pad_id = 1\n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label,\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx}\")\n                return self._make_safe_sample(\"oob\")\n\n            src, tgt = self.pairs[idx]\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            if DEBUG_CELL2 and idx < 3:\n                has_bengali = is_bengali_text(src)\n                has_english = any(\"a\" <= c.lower() <= \"z\" for c in src)\n                print(f\"[CELL2-GETITEM-{idx}] src sample: {src[:50]}\")\n                print(f\"[CELL2-GETITEM-{idx}] Bengali: {has_bengali}, English: {has_english}\")\n                if not has_bengali:\n                    print(f\"[CELL2] WARNING: src_text is NOT Bengali at idx={idx}!\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            \n            domain_label = _TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    t = tensor.view(-1).long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    \n    default_domain = _TRAIN_DOMAIN\n    \n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([default_domain], dtype=torch.long),\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n    inputs, masks, labs, twmaps, srcs, toks, domains = [], [], [], [], [], [], []\n\n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", default_domain)\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(domain)\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([default_domain], dtype=torch.long),\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.full((len(inputs),), default_domain, dtype=torch.long)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels,\n    }\n\ndef create_optimized_dataloader(\n    dataset: Dataset,\n    batch_size: Optional[int] = None,\n    shuffle: bool = True,\n    split: str = \"train\",\n) -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n\n    batch_size = int(batch_size)\n    original_batch_size = batch_size\n    adjusted = False\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        new_batch_size = (batch_size // _NUM_GPUS) * _NUM_GPUS\n        if new_batch_size == 0:\n            if DEBUG_CELL2:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original.\")\n        else:\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n\n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size {original_batch_size} to {batch_size} (DP-divisible, GPUs={_NUM_GPUS})\")\n\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\nprint(\"=\" * 80)\nprint(\"Cell 2: Memory-efficient data loading ready - PUNCTUATION & VOCAB SAFE\")\nprint(\"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Added separate_bengali_punctuation() function (line 114)\")\nprint(\"  âœ… FIX #2: Added clean_and_normalize_text() with punctuation separation (line 127)\")\nprint(\"  âœ… FIX #3: Added is_punctuation_only() validation (line 151)\")\nprint(\"  âœ… FIX #4: Integrated punctuation separation in load_and_preprocess_optimized (line 300)\")\nprint(\"  âœ… FIX #5: Filter punctuation in word map construction (line 587)\")\nprint(\"  âœ… FIX #6: Fallback dataset now uses clean_and_normalize_text (line 387)\")\nprint(\"  âœ… FIX #7: Added _BENGALI_PUNCT_SET and _COMMON_PUNCT_SET (lines 110-111)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:44.633286Z","iopub.execute_input":"2026-01-16T16:06:44.633554Z","iopub.status.idle":"2026-01-16T16:06:45.706729Z","shell.execute_reply.started":"2026-01-16T16:06:44.633533Z","shell.execute_reply":"2026-01-16T16:06:45.706112Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCell 2: Memory-efficient data loading ready - PUNCTUATION & VOCAB SAFE\n================================================================================\nCRITICAL FIXES APPLIED:\n  âœ… FIX #1: Added separate_bengali_punctuation() function (line 114)\n  âœ… FIX #2: Added clean_and_normalize_text() with punctuation separation (line 127)\n  âœ… FIX #3: Added is_punctuation_only() validation (line 151)\n  âœ… FIX #4: Integrated punctuation separation in load_and_preprocess_optimized (line 300)\n  âœ… FIX #5: Filter punctuation in word map construction (line 587)\n  âœ… FIX #6: Fallback dataset now uses clean_and_normalize_text (line 387)\n  âœ… FIX #7: Added _BENGALI_PUNCT_SET and _COMMON_PUNCT_SET (lines 110-111)\n================================================================================\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE - FIXED VERSION\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\n\nPRINT_INTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    _HAS_CLUSTERING = True\nexcept Exception:\n    _HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available\")\n\ntry:\n    from sklearn.cluster import KMeans\n    _HAS_KMEANS = True\nexcept Exception:\n    _HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available\")\n\ntry:\n    DSCD_MAX_PROTOS = int(DSCD_MAX_PROTOS)\n    DSCD_BUFFER_SIZE = int(DSCD_BUFFER_SIZE)\n    DSCD_N_MIN = int(DSCD_N_MIN)\n    DSCD_DISPERSION_THRESHOLD = float(DSCD_DISPERSION_THRESHOLD)\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    DSCD_ENABLE_TRAINING_CLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\nexcept (NameError, ValueError, TypeError):\n    DSCD_MAX_PROTOS = 8\n    DSCD_BUFFER_SIZE = 200\n    DSCD_N_MIN = 5\n    DSCD_DISPERSION_THRESHOLD = 0.35\n    VERBOSE_LOGGING = False\n    DSCD_ENABLE_TRAINING_CLUSTERING = True\n    print(\"[CELL3] WARNING: Using default DSCD config\")\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    DEBUG_DISCOVERY = False\n\ntry:\n    MAX_TOKENS_PER_DISCOVERY = int(globals().get('MAX_TOKENS_PER_DISCOVERY', 150))\nexcept Exception:\n    MAX_TOKENS_PER_DISCOVERY = 150\n\ntry:\n    DSCD_NEW_SENSE_LAMBDA = float(globals().get('DSCD_NEW_SENSE_LAMBDA', 1.0))\nexcept Exception:\n    DSCD_NEW_SENSE_LAMBDA = 1.0\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST_BN = set(HOMOGRAPH_REFERENCE_LIST_BN)\n    print(f\"[CELL3] Loaded reference list for evaluation: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words\")\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n        \"à¦ªà¦¾à¦¨à¦¿\", \"à¦¦à¦²\", \"à¦¬à¦¾à¦œà¦¾à¦°\", \"à¦¨à¦¾à¦®\", \"à¦•à¦¥à¦¾\", \"à¦¬à¦‡\", \"à¦˜à¦°\", \"à¦®à¦¨\", \"à¦¹à¦¾à¦¤\",\n    }\n    print(\"[CELL3] Using default reference list\")\n\nDSCD_MAX_CLUSTERING_POINTS = 500\n\n_BENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\nPUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n\ndef is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    if not clean:\n        return False\n    \n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in _COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in PUNCT_SET for c in clean)\n\n\ndef clean_token_for_dscd(token: str) -> str:\n    if not token or not isinstance(token, str):\n        return \"\"\n    cleaned = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    for punct in list(PUNCT_SET):\n        cleaned = cleaned.replace(punct, \"\")\n    return cleaned.lower()\n\n\ndef normalize_token_key(token: str) -> str:\n    return clean_token_for_dscd(token)\n\n\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if not token:\n        return False\n    \n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith(\"L\"):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n    \n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if letters / total < min_letter_fraction:\n        return False\n    \n    return True\n\n\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embed_dim, max_protos: Optional[int] = None):\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        self.embed_dim = embed_dim\n        self.max_protos = int(max_protos)\n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n    \n    def add_prototype(self, vector: torch.Tensor, current_time: Optional[float] = None, count: int = 1) -> None:\n        if current_time is None:\n            current_time = time.time()\n        \n        v = vector.detach().cpu().clone()\n        \n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creation_time.append(float(current_time))\n        else:\n            min_idx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            self.centroids[min_idx] = v\n            self.counts[min_idx] = int(count)\n            self.creation_time[min_idx] = float(current_time)\n    \n    def update_prototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignment_distance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.add_prototype(vector, time.time(), count=1)\n            return\n        \n        old_centroid = self.centroids[idx]\n        new_vector = vector.detach().cpu()\n        self.centroids[idx] = (1.0 - eta) * old_centroid + eta * new_vector\n        self.counts[idx] = int(self.counts[idx] + 1)\n        \n        if assignment_distance is not None:\n            self.update_rolling_stats(float(assignment_distance))\n    \n    def update_rolling_stats(self, d: float) -> None:\n        if not self.distances:\n            self.mu = float(d)\n            self.tau = max(1e-6, float(d) * 0.1)\n            self.distances = [float(d)]\n            return\n        \n        prev_mu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n        self.distances.append(float(d))\n        \n        if len(self.distances) > 50:\n            self.distances.pop(0)\n    \n    def get_adaptive_threshold(self, lam: float = 1.0) -> float:\n        return float(self.mu + lam * max(self.tau, 1e-4))\n    \n    def size(self) -> int:\n        return len(self.centroids)\n    \n    def ensure_consistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            self.counts = self.counts[:n] if len(self.counts) >= n else self.counts + [1] * (n - len(self.counts))\n        if len(self.creation_time) != n:\n            self.creation_time = self.creation_time[:n] if len(self.creation_time) >= n else self.creation_time + [time.time()] * (n - len(self.creation_time))\n\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        buffer_size: Optional[int] = None,\n        max_protos: Optional[int] = None,\n        n_min: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        language: str = \"bn\",\n        enable_training_clustering: Optional[bool] = None,\n        max_clustering_points: Optional[int] = None,\n        max_candidates_per_step: int = 2,\n        dscd_min_letters: int = 2,\n        dscd_min_letter_fraction: float = 0.6,\n    ):\n        super().__init__()\n        \n        if buffer_size is None:\n            buffer_size = DSCD_BUFFER_SIZE\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        if n_min is None:\n            n_min = DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = DSCD_MAX_CLUSTERING_POINTS\n        if enable_training_clustering is None:\n            enable_training_clustering = DSCD_ENABLE_TRAINING_CLUSTERING\n        \n        self.embed_dim = int(embed_dim)\n        self.buffer_size = int(buffer_size)\n        self.max_protos = int(max_protos)\n        self.n_min = int(n_min)\n        self.dispersion_threshold = float(dispersion_threshold)\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscd_min_letters = int(dscd_min_letters)\n        self.dscd_min_letter_fraction = float(dscd_min_letter_fraction)\n        \n        try:\n            if tokenizer is not None and 'get_tokenizer_special_tokens' in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(\n                    getattr(tokenizer, \"all_special_tokens\", [])\n                ) if tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = set()\n        \n        self.dscd_allowed_tokens: Set[str] = set()\n        self.dscd_ignored_tokens: Set[str] = set()\n        self.dscd_cache_max_size = 10000\n        \n        self.prototype_stores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.buffers: Dict[str, deque] = {}\n        self.discovered_log: List[Dict[str, Any]] = []\n        self.discovered_homographs: Set[str] = set()\n        \n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n        \n        self.dispersion_cache: Dict[str, float] = {}\n        self.dispersion_last_updated: Dict[str, float] = {}\n        self.dispersion_lock = threading.Lock()\n        \n        self.clustering_lock = threading.Lock()\n        self.buffer_lock = threading.Lock()\n        \n        from collections import deque as thread_deque\n        self.active_threads = thread_deque(maxlen=100)\n        self.thread_lock = threading.Lock()\n        \n        self.last_cluster_time: Dict[str, float] = {}\n        self.cluster_cooldown_seconds = 5.0\n        self.enable_training_clustering = bool(enable_training_clustering)\n        \n        self.discovery_count = 0\n        self.discovery_times: List[float] = []\n        self.clustered_tokens: Set[str] = set()\n        self.cluster_stats: Dict[str, Dict[str, Any]] = {}\n        \n        self.span_head = nn.Sequential(\n            nn.Linear(self.embed_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1),\n        )\n        self.sigma_net = nn.Sequential(\n            nn.Linear(self.embed_dim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1),\n        )\n        self.gate_w = nn.Parameter(torch.tensor(1.0))\n        self.gate_b = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n        \n        self.max_clustering_points = int(max_clustering_points)\n        self.max_candidates_per_step = int(max_candidates_per_step)\n    \n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        state = super().state_dict(destination, prefix, keep_vars)\n        \n        plain_stores = {}\n        for token, store in self.prototype_stores.items():\n            plain_stores[token] = {\n                'centroids': [c.cpu() for c in store.centroids] if hasattr(store, 'centroids') else [],\n                'counts': list(store.counts) if hasattr(store, 'counts') else [],\n                'creation_time': list(store.creation_time) if hasattr(store, 'creation_time') else [],\n                'mu': float(store.mu) if hasattr(store, 'mu') else 0.0,\n                'tau': float(store.tau) if hasattr(store, 'tau') else 0.0,\n                'size': int(store.size()) if hasattr(store, 'size') else 0,\n            }\n        \n        state[prefix + 'prototype_stores_data'] = plain_stores\n        state[prefix + 'discovered_homographs'] = list(self.discovered_homographs)\n        \n        return state\n    \n    def load_state_dict(self, state_dict, strict=True):\n        prefix = ''\n        plain_stores = state_dict.pop('prototype_stores_data', {})\n        discovered = state_dict.pop('discovered_homographs', [])\n        \n        super().load_state_dict(state_dict, strict=strict)\n        \n        if not plain_stores:\n            print(\"[DSCD] WARNING: Empty prototype_stores in checkpoint\")\n            return\n        \n        self.prototype_stores = {}\n        self.discovered_homographs = set(discovered)\n        \n        for token, store_dict in plain_stores.items():\n            store = MemoryEfficientPrototypeStore(embed_dim=self.embed_dim, max_protos=self.max_protos)\n            \n            centroids_data = store_dict.get('centroids', [])\n            store.centroids = []\n            for c in centroids_data:\n                if isinstance(c, torch.Tensor):\n                    store.centroids.append(c)\n                else:\n                    store.centroids.append(torch.tensor(c))\n            \n            store.counts = store_dict.get('counts', [])\n            store.creation_time = store_dict.get('creation_time', [])\n            store.mu = store_dict.get('mu', 0.0)\n            store.tau = store_dict.get('tau', 0.0)\n            store.ensure_consistency()\n            \n            self.prototype_stores[token] = store\n        \n        print(f\"[DSCD] Loaded {len(self.prototype_stores)} tokens, {sum(s.size() for s in self.prototype_stores.values())} prototypes\")\n    \n    @staticmethod\n    def clean_token(token):\n        return clean_token_for_dscd(str(token))\n    \n    def is_valid_multi_sense(self, token):\n        if token not in self.prototype_stores:\n            return False\n        store = self.prototype_stores[token]\n        total_occurrences = sum(store.counts) if hasattr(store, 'counts') else 0\n        min_per_proto = min(store.counts) if hasattr(store, 'counts') and store.counts else 0\n        return store.size() >= 2 and total_occurrences >= 10 and min_per_proto >= 2\n    \n    def is_multi_sense_store(self, store: MemoryEfficientPrototypeStore) -> bool:\n        k = store.size()\n        if k < 2:\n            return False\n        \n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(2, self.n_min // 2))\n        if strong < 2:\n            return False\n        \n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n            \n            if len(cents) < 2:\n                return False\n            \n            cents = np.stack(cents, axis=0)\n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            tri = dists[np.triu_indices(len(cents), k=1)]\n            \n            if tri.size == 0:\n                return False\n            \n            min_dist = float(tri.min())\n            base = max(store.tau, 1e-3)\n            return min_dist >= base * DSCD_NEW_SENSE_LAMBDA\n        except Exception:\n            return True\n    \n    def discover_homographs_for_tokens(\n        self,\n        token_names: List[str],\n        min_cluster_samples: int,\n        dispersion_threshold: float,\n        global_step: int,\n    ) -> int:\n        discovered_in_run: List[str] = []\n        \n        for idx, token in enumerate(token_names):\n            try:\n                if is_punctuation_only(token):\n                    continue\n                \n                success = self.cluster_buffer_to_prototypes_hierarchical(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        self.discovered_homographs.add(clean_token)\n                        discovered_in_run.append(clean_token)\n            except Exception:\n                continue\n        \n        try:\n            self.discovered_log.append({\n                'timestamp': time.time(),\n                'global_step': global_step,\n                'candidates_processed': len(token_names),\n                'discovered_count': len(discovered_in_run),\n                'homographs': discovered_in_run,\n                'total_discovered': len(self.discovered_homographs),\n            })\n        except Exception:\n            pass\n        \n        return len(discovered_in_run)\n    \n    def discover_homographs(\n        self,\n        min_cluster_samples: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        max_candidates: int = 500,\n    ) -> int:\n        if min_cluster_samples is None:\n            min_cluster_samples = self.n_min\n        if dispersion_threshold is None:\n            dispersion_threshold = self.dispersion_threshold\n        \n        candidates: List[Tuple[str, float, int, float]] = []\n        \n        with self.buffer_lock:\n            for token, buffer in self.buffers.items():\n                if is_punctuation_only(token):\n                    continue\n                \n                buffer_size = len(buffer)\n                if buffer_size >= max(min_cluster_samples * 2, 10):\n                    dispersion = self.get_dispersion(token)\n                    if dispersion >= dispersion_threshold:\n                        rank_score = dispersion * buffer_size\n                        candidates.append((token, rank_score, buffer_size, dispersion))\n        \n        if not candidates:\n            return 0\n        \n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_candidates]\n        \n        discovered: List[str] = []\n        \n        for token, score, buf_size, disp in candidates:\n            try:\n                with self.clustering_lock:\n                    success = self.cluster_buffer_to_prototypes_hierarchical(token)\n                    if success:\n                        store = self.prototype_stores.get(token)\n                        if store and store.size() >= 2:\n                            clean_token = normalize_token_key(token)\n                            self.discovered_homographs.add(clean_token)\n                            discovered.append(clean_token)\n            except Exception:\n                continue\n        \n        try:\n            self.discovered_log.append({\n                'timestamp': time.time(),\n                'candidates': len(candidates),\n                'discovered': len(discovered),\n                'homographs': discovered[:20],\n            })\n        except Exception:\n            pass\n        \n        return len(discovered)\n    \n    def get_dispersion(self, tokentype: str) -> float:\n        with self.dispersion_lock:\n            if tokentype in self.dispersion_cache:\n                try:\n                    last_update = self.dispersion_last_updated.get(tokentype, 0.0)\n                    if time.time() - last_update < 3600:\n                        return self.dispersion_cache[tokentype]\n                except Exception:\n                    pass\n        \n        with self.buffer_lock:\n            if tokentype not in self.buffers:\n                return 0.0\n            \n            buf_len = len(self.buffers[tokentype])\n            if buf_len < 2:\n                return 0.05 if buf_len == 1 else 0.0\n            \n            try:\n                embeddings: List[np.ndarray] = []\n                for emb in self.buffers[tokentype]:\n                    try:\n                        if isinstance(emb, torch.Tensor):\n                            embeddings.append(emb.cpu().numpy())\n                        else:\n                            embeddings.append(np.asarray(emb, dtype=np.float32))\n                    except Exception:\n                        continue\n                \n                if len(embeddings) < 2:\n                    return 0.05 if len(embeddings) == 1 else 0.0\n                \n                embeddings_np = np.stack(embeddings, axis=0)\n                centroid = embeddings_np.mean(axis=0)\n                distances = np.linalg.norm(embeddings_np - centroid[None, :], axis=1)\n                dispersion = float(distances.std())\n                \n                with self.dispersion_lock:\n                    self.dispersion_cache[tokentype] = dispersion\n                    self.dispersion_last_updated[tokentype] = time.time()\n                \n                return dispersion\n            except Exception:\n                return 0.0\n    \n    def validate_prototypes(\n        self,\n        homograph_list: Optional[List[str]] = None,\n        cluster_missing: bool = True\n    ) -> Dict[str, Any]:\n        if homograph_list is None:\n            try:\n                homograph_list = list(HOMOGRAPH_REFERENCE_LIST_BN)\n            except Exception:\n                homograph_list = [\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\"]\n        \n        print(\"=\" * 80)\n        print(\"DSCD-VALIDATION: Prototype Quality Check\")\n        print(\"=\" * 80)\n        \n        validation_results: Dict[str, Any] = {\n            'total_tokens': len(self.prototype_stores),\n            'total_prototypes': 0,\n            'multi_sense_tokens': 0,\n            'homographs_found': 0,\n            'homographs_missing': [],\n            'avg_prototypes_per_token': 0.0,\n            'avg_samples_per_prototype': 0.0,\n            'quality_score': 0.0,\n        }\n        \n        total_samples = 0\n        \n        for token, store in self.prototype_stores.items():\n            num_protos = len(store.centroids)\n            validation_results['total_prototypes'] += num_protos\n            \n            if self.is_multi_sense_store(store):\n                validation_results['multi_sense_tokens'] += 1\n            \n            try:\n                total_samples += sum(store.counts)\n            except Exception:\n                pass\n        \n        if validation_results['total_tokens'] > 0:\n            validation_results['avg_prototypes_per_token'] = validation_results['total_prototypes'] / validation_results['total_tokens']\n        \n        if validation_results['total_prototypes'] > 0:\n            validation_results['avg_samples_per_prototype'] = total_samples / validation_results['total_prototypes']\n        \n        print(\"VALIDATION: Reference Homograph Coverage\")\n        print(\"-\" * 80)\n        \n        missing_tokens_to_cluster: List[str] = []\n        \n        for homograph in homograph_list:\n            clean_h = homograph.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").replace(\" \", \"\").strip()\n            \n            found = False\n            found_key = None\n            found_protos = 0\n            \n            if homograph in self.prototype_stores:\n                found = True\n                found_key = homograph\n                found_protos = len(self.prototype_stores[homograph].centroids)\n            elif clean_h in self.prototype_stores:\n                found = True\n                found_key = clean_h\n                found_protos = len(self.prototype_stores[clean_h].centroids)\n            else:\n                for key in self.prototype_stores.keys():\n                    clean_key = str(key).replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").replace(\" \", \"\").strip()\n                    if clean_key == clean_h or clean_h in clean_key or clean_key in clean_h:\n                        found = True\n                        found_key = key\n                        found_protos = len(self.prototype_stores[key].centroids)\n                        break\n            \n            if found and self.is_multi_sense_store(self.prototype_stores[found_key]):\n                validation_results['homographs_found'] += 1\n                try:\n                    counts = self.prototype_stores[found_key].counts\n                    print(f\"  âœ“ {homograph} - {found_protos} prototypes (counts={counts})\")\n                except Exception:\n                    print(f\"  âœ“ {homograph} - {found_protos} prototypes\")\n            elif found and found_protos == 1:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  âš  {homograph} - Only 1 prototype\")\n                if cluster_missing:\n                    missing_tokens_to_cluster.append(found_key)\n            else:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  âœ— {homograph} - NOT FOUND\")\n                \n                if cluster_missing:\n                    if homograph in self.buffers or clean_h in self.buffers:\n                        key_to_cluster = homograph if homograph in self.buffers else clean_h\n                        if len(self.buffers[key_to_cluster]) >= max(self.n_min * 2, 10):\n                            print(f\"    - Found in buffer, will cluster\")\n                            missing_tokens_to_cluster.append(key_to_cluster)\n        \n        if cluster_missing and missing_tokens_to_cluster:\n            print(f\"[VALIDATION] Clustering {len(missing_tokens_to_cluster)} missing tokens...\")\n            for token in missing_tokens_to_cluster:\n                try:\n                    with self.clustering_lock:\n                        self.cluster_buffer_to_prototypes_hierarchical(token)\n                        if token in self.prototype_stores and self.is_multi_sense_store(self.prototype_stores[token]):\n                            print(f\"  âœ“ Successfully clustered: {token}\")\n                except Exception as e:\n                    print(f\"  âœ— Failed to cluster {token}: {e}\")\n        \n        homograph_coverage = validation_results['homographs_found'] / len(homograph_list) if homograph_list else 0.0\n        multi_sense_ratio = validation_results['multi_sense_tokens'] / validation_results['total_tokens'] if validation_results['total_tokens'] > 0 else 0.0\n        \n        validation_results['quality_score'] = homograph_coverage * 0.6 + multi_sense_ratio * 0.4\n        \n        print(\"-\" * 80)\n        print(\"VALIDATION Summary:\")\n        print(f\"  - Total tokens: {validation_results['total_tokens']}\")\n        print(f\"  - Total prototypes: {validation_results['total_prototypes']}\")\n        print(f\"  - Multi-sense tokens: {validation_results['multi_sense_tokens']}\")\n        print(f\"  - Reference found: {validation_results['homographs_found']}/{len(homograph_list)}\")\n        print(f\"  - Quality Score: {validation_results['quality_score']*100:.2f}%\")\n        print(\"=\" * 80)\n        \n        return validation_results\n    \n    def should_track_token(self, tokentext: str) -> bool:\n        if not tokentext or not isinstance(tokentext, str):\n            return False\n        \n        if len(self.dscd_allowed_tokens) >= self.dscd_cache_max_size:\n            self.dscd_allowed_tokens.clear()\n        if len(self.dscd_ignored_tokens) >= self.dscd_cache_max_size:\n            self.dscd_ignored_tokens.clear()\n        \n        if tokentext in self.dscd_allowed_tokens:\n            return True\n        if tokentext in self.dscd_ignored_tokens:\n            return False\n        \n        if not getattr(self, 'training', False):\n            if tokentext in self.prototype_stores:\n                self.dscd_allowed_tokens.add(tokentext)\n                return True\n            clean = clean_token_for_dscd(tokentext)\n            if clean and clean in self.prototype_stores:\n                self.dscd_allowed_tokens.add(tokentext)\n                return True\n        \n        if tokentext in self.special_tokens:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        if is_punctuation_only(tokentext):\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        clean = clean_token_for_dscd(tokentext)\n        \n        if not clean:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        if len(clean) < 2:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        if not any(c.isalpha() for c in clean):\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        if clean.isdigit():\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n        \n        try:\n            bengali_block = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            if bengali_block:\n                if len(clean) >= 2:\n                    self.dscd_allowed_tokens.add(tokentext)\n                    return True\n                else:\n                    self.dscd_ignored_tokens.add(tokentext)\n                    return False\n        except Exception:\n            pass\n        \n        if is_word_token(\n            clean,\n            min_letters=self.dscd_min_letters,\n            min_letter_fraction=self.dscd_min_letter_fraction,\n        ):\n            self.dscd_allowed_tokens.add(tokentext)\n            return True\n        \n        self.dscd_ignored_tokens.add(tokentext)\n        return False\n    \n    def canonical_token_key(\n        self,\n        raw_token: str,\n        token_word_map: Optional[Dict[int, Optional[str]]],\n        idx: int,\n    ) -> Optional[str]:\n        canonical: Optional[str] = None\n        \n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                word = str(token_word_map[idx]).strip()\n                canonical = clean_token_for_dscd(word)\n                \n                if canonical and len(canonical) >= 2:\n                    has_bengali = any('\\u0980' <= c <= '\\u09FF' for c in canonical)\n                    if has_bengali:\n                        return canonical\n        except Exception:\n            pass\n        \n        canonical = clean_token_for_dscd(raw_token)\n        \n        if not canonical or len(canonical) < 2:\n            return None\n        \n        has_bengali = any('\\u0980' <= c <= '\\u09FF' for c in canonical)\n        if not has_bengali:\n            return None\n        \n        return canonical\n    \n    def cleanup_threads(self) -> None:\n        try:\n            with self.thread_lock:\n                alive = [th for th in list(self.active_threads) if th.is_alive()]\n                self.active_threads.clear()\n                self.active_threads.extend(alive)\n        except Exception:\n            pass\n    \n    def cleanup_memory(self) -> None:\n        try:\n            for tokentype, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            \n            try:\n                now = time.time()\n                expired = [k for k, v in self.dispersion_last_updated.items() if now - v > 3600]\n                for k in expired:\n                    self.dispersion_cache.pop(k, None)\n                    self.dispersion_last_updated.pop(k, None)\n            except Exception:\n                pass\n            \n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n    \n    def forward(\n        self,\n        token_embeddings=None,\n        token_types=None,\n        train_mode: bool = True,\n        token_word_map=None,\n        h_all=None,\n        input_ids=None,\n        attention_mask=None,\n    ):\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n        \n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n        \n        if input_ids is not None and token_types is None:\n            batch_size, seq_len = input_ids.shape\n            token_types = []\n            for b in range(batch_size):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(\n                            self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n                        )\n                    except Exception:\n                        token_types.append([f\"tok{i}\" for i in range(seq_len)])\n                else:\n                    token_types.append([f\"tok{i}\" for i in range(seq_len)])\n        \n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n            self.cleanup_threads()\n        \n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        seq_len = int(token_embeddings.size(1))\n        \n        all_outputs: Dict[str, List[Any]] = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': [],\n        }\n        \n        for b in range(batch_size):\n            word_map = token_word_map[b] if token_word_map and len(token_word_map) > b else None\n            \n            batch_outputs = self.process_sequence(\n                token_embeddings[b],\n                token_types[b] if token_types and len(token_types) > b else [f\"tok{i}\" for i in range(seq_len)],\n                device,\n                word_map=word_map,\n                train_mode=train_mode,\n            )\n            \n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n        \n        try:\n            h_aug_list: List[torch.Tensor] = []\n            max_seq_len = seq_len\n            for b in range(batch_size):\n                h_batch_list = all_outputs['h_augmented'][b]\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = torch.zeros(max_seq_len, self.embed_dim, device=device)\n                h_aug_list.append(h_batch)\n            \n            all_outputs['h_augmented'] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs['h_augmented'] = token_embeddings\n        \n        try:\n            proto_assign_tensor = []\n            for row in all_outputs['proto_assignments']:\n                try:\n                    stacked = torch.stack(\n                        [x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in row],\n                        dim=0,\n                    )\n                    proto_assign_tensor.append(stacked)\n                except Exception:\n                    proto_assign_tensor.append(\n                        torch.tensor(\n                            [int(x) if not isinstance(x, torch.Tensor) else int(x.item()) for x in row],\n                            dtype=torch.long,\n                        )\n                    )\n            all_outputs['proto_assignments'] = proto_assign_tensor\n        except Exception:\n            pass\n        \n        return all_outputs\n    \n    def process_sequence(\n        self,\n        token_embeddings: torch.Tensor,\n        token_types: List[Any],\n        device: torch.device,\n        word_map: Optional[Dict[int, Optional[str]]] = None,\n        train_mode: bool = True,\n    ) -> Dict[str, List[Any]]:\n        seq_len = int(token_embeddings.size(0))\n        \n        outputs: Dict[str, List[Any]] = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': [],\n        }\n        \n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f\"tok{j}\"\n            if not isinstance(raw_tok, str):\n                raw_tok = str(raw_tok) if raw_tok is not None else f\"tok{j}\"\n            \n            token_key = self.canonical_token_key(raw_tok, word_map, j)\n            h_j = token_embeddings[j]\n            \n            if not token_key:\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n            \n            if not self.should_track_token(token_key):\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n            \n            with self.buffer_lock:\n                if token_key not in self.buffers:\n                    self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                    self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(\n                        self.embed_dim, self.max_protos\n                    )\n                \n                try:\n                    self.buffers[token_key].append(h_j.detach().clone().cpu())\n                except Exception:\n                    try:\n                        self.buffers[token_key].append(h_j.cpu())\n                    except Exception:\n                        pass\n                \n                buffer_len = len(self.buffers[token_key])\n            \n            try:\n                if self.enable_training_clustering and buffer_len >= max(self.n_min * 2, 10):\n                    now = time.time()\n                    last_t = self.last_cluster_time.get(token_key, 0.0)\n                    if now - last_t >= self.cluster_cooldown_seconds:\n                        self.last_cluster_time[token_key] = now\n                        \n                        def bg_cluster(tok: str = token_key) -> None:\n                            try:\n                                with self.clustering_lock:\n                                    self.cluster_buffer_to_prototypes_hierarchical(tok)\n                            except Exception:\n                                pass\n                        \n                        th = threading.Thread(target=bg_cluster, daemon=True)\n                        th.start()\n                        with self.thread_lock:\n                            self.active_threads.append(th)\n            except Exception:\n                pass\n            \n            store = self.prototype_stores[token_key]\n            centroids_snapshot: Optional[List[torch.Tensor]] = None\n            \n            with self.clustering_lock:\n                try:\n                    if hasattr(store, 'centroids') and len(store.centroids) > 0:\n                        centroids_snapshot = []\n                        for c in store.centroids:\n                            try:\n                                if isinstance(c, torch.Tensor):\n                                    centroids_snapshot.append(c.clone().cpu())\n                                else:\n                                    centroids_snapshot.append(\n                                        torch.from_numpy(\n                                            np.asarray(c, dtype=np.float32)\n                                        ).cpu()\n                                    )\n                            except Exception:\n                                continue\n                        if not centroids_snapshot:\n                            centroids_snapshot = None\n                except Exception:\n                    centroids_snapshot = None\n            \n            assignment = -1\n            prob_list: List[float] = []\n            uncertainty = 0.0\n            span_pred = 0.0\n            gate_val = 0.0\n            h_aug = h_j\n            \n            if centroids_snapshot and len(centroids_snapshot) >= 1:\n                try:\n                    try:\n                        h_cpu = h_j.detach().cpu().numpy()\n                    except Exception:\n                        h_cpu = h_j.cpu().numpy()\n                    \n                    try:\n                        cents_np = np.stack([c.numpy() for c in centroids_snapshot], axis=0)\n                    except Exception:\n                        cents_np = np.stack([np.asarray(c, dtype=np.float32) for c in centroids_snapshot], axis=0)\n                    \n                    dists_np = np.linalg.norm(cents_np - h_cpu[None, :], axis=1)\n                    \n                    if dists_np.size > 0:\n                        min_dist = float(dists_np.min())\n                        min_idx = int(np.argmin(dists_np))\n                        \n                        if len(centroids_snapshot) >= 2:\n                            mean_dist = float(np.mean(dists_np))\n                            std_dist = float(np.std(dists_np))\n                            span_pred = float(np.clip(std_dist / (mean_dist + 1e-6), 0.0, 1.0))\n                        else:\n                            span_pred = float(np.clip(min_dist / (store.mu + 1e-3), 0.0, 1.0))\n                        \n                        base_threshold = max(store.tau, 1e-3) if store.size() > 0 else 0.3\n                        uncertainty_dist = float(np.clip(min_dist / (base_threshold * 2), 0.0, 1.0))\n                        \n                        if len(centroids_snapshot) >= 2:\n                            precisions = 1.0 / (dists_np**2 + 1e-6)\n                            gate_weights = precisions / (np.sum(precisions) + 1e-6)\n                            gate_val = float(np.max(gate_weights))\n                        else:\n                            gate_val = float(np.clip(1.0 - (min_dist / (store.mu + 1e-3)), 0.0, 1.0))\n                        \n                        if store.size() < self.max_protos and min_dist > store.get_adaptive_threshold(DSCD_NEW_SENSE_LAMBDA):\n                            store.add_prototype(h_j, time.time(), count=1)\n                            assignment = store.size() - 1\n                            centroids_snapshot.append(h_j.cpu())\n                            cents_np = np.vstack([cents_np, h_cpu[None, :]])\n                        else:\n                            assignment = min_idx\n                        \n                        try:\n                            store.update_rolling_stats(min_dist)\n                        except Exception:\n                            pass\n                        \n                        try:\n                            dist_tensor = torch.from_numpy(dists_np).to(device)\n                            probs_tensor = F.softmax(-dist_tensor, dim=0)\n                            prob_list = probs_tensor.tolist()\n                            \n                            entropy = -torch.sum(probs_tensor * torch.log(probs_tensor + 1e-10))\n                            max_entropy = np.log(len(dists_np))\n                            uncertainty_entropy = float(entropy.item() / max_entropy) if max_entropy > 0 else 0.0\n                        except Exception:\n                            exps = np.exp(-dists_np - np.max(-dists_np)) if dists_np.size > 0 else np.array([])\n                            if exps.size > 0:\n                                probs = exps / (exps.sum() + 1e-12)\n                                prob_list = probs.tolist()\n                                entropy_val = -np.sum(probs * np.log(probs + 1e-10))\n                                max_entropy = np.log(len(dists_np))\n                                uncertainty_entropy = float(entropy_val / max_entropy) if max_entropy > 0 else 0.0\n                            else:\n                                prob_list = []\n                                uncertainty_entropy = 0.0\n                        \n                        if len(centroids_snapshot) >= 2:\n                            uncertainty = 0.4 * uncertainty_dist + 0.6 * uncertainty_entropy\n                        else:\n                            uncertainty = uncertainty_dist\n                        \n                        if gate_val > 0.3 and 0 <= assignment < len(centroids_snapshot):\n                            try:\n                                centroid_t = centroids_snapshot[assignment]\n                                if device != torch.device('cpu'):\n                                    try:\n                                        centroid_t = centroid_t.to(device)\n                                    except Exception:\n                                        pass\n                                blend_weight = 0.3 if gate_val > 0.7 else 0.15\n                                h_aug = h_j + blend_weight * (centroid_t - h_j)\n                            except Exception:\n                                h_aug = h_j\n                \n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"DSCD: Assignment error for {token_key}: {str(e)[:200]}\")\n            \n            outputs['proto_assignments'].append(torch.tensor(assignment))\n            outputs['proto_probs'].append(prob_list)\n            outputs['uncertainties'].append(uncertainty)\n            outputs['span_preds'].append(span_pred)\n            outputs['gates'].append(gate_val)\n            outputs['h_augmented'].append(h_aug)\n        \n        try:\n            if not train_mode and len(self.prototype_stores) > 0 and VERBOSE_LOGGING:\n                if self.last_periodic_check % PRINT_INTERVAL == 0:\n                    self.print_clusters_summary()\n                self.last_periodic_check += 1\n        except Exception:\n            pass\n        \n        return outputs\n    \n    def print_clusters_summary(self) -> None:\n        try:\n            items: List[Tuple[str, int, int, float, float, int]] = []\n            \n            for token, store in self.prototype_stores.items():\n                if is_punctuation_only(token):\n                    continue\n                \n                try:\n                    proto_sample_count = sum(getattr(store, 'counts', []) or [])\n                except Exception:\n                    proto_sample_count = 0\n                \n                buffer_len = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n            \n            items.sort(key=lambda x: x[1], reverse=True)\n            top5 = items[:5]\n            \n            if VERBOSE_LOGGING:\n                print(\"[CLUSTER] Top 5 clusters:\")\n                print(\"-\" * 100)\n                print(f\"{'Rank':<6} {'Token':<18} {'Count':<12} {'Protos':<8} {'BufLen':<8} {'mu':<15} {'tau':<15}\")\n                print(\"-\" * 100)\n                \n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(top5, 1):\n                    tok_str = str(tok)[:18]\n                    print(f\"{rank:<6} {tok_str:<18} {cnt:<12} {prot:<8} {buflen:<8} {mu:<15.6f} {tau:<15.6f}\")\n                \n                print(\"-\" * 100)\n                \n                total_samples = sum(item[1] for item in items)\n                total_protos = sum(item[2] for item in items)\n                total_buffers = sum(item[5] for item in items)\n                print(\n                    f\"Total: {len(items)} clusters | \"\n                    f\"{total_samples} samples | {total_protos} protos | {total_buffers} buffers\"\n                )\n        except Exception as e:\n            try:\n                if VERBOSE_LOGGING:\n                    print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n            except Exception:\n                pass\n    \n    def cluster_buffer_to_prototypes_hierarchical(self, tokentype: str) -> bool:\n        try:\n            if is_punctuation_only(tokentype):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping punctuation token: {tokentype}\")\n                return False\n            \n            if not self.should_track_token(tokentype):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping non-word token: {tokentype}\")\n                return False\n            \n            with self.buffer_lock:\n                if tokentype not in self.buffers:\n                    return False\n                \n                buf_snapshot = [e.clone() if isinstance(e, torch.Tensor) else e for e in self.buffers[tokentype]]\n                \n                if len(buf_snapshot) < max(self.n_min * 2, 10):\n                    if DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] {tokentype} buffer={len(buf_snapshot)} < min={max(self.n_min * 2, 10)}\"\n                        )\n                    return False\n            \n            emb_list: List[np.ndarray] = []\n            for e in buf_snapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        try:\n                            emb_list.append(e.numpy())\n                        except Exception:\n                            emb_list.append(e.cpu().numpy())\n                    else:\n                        emb_list.append(np.asarray(e, dtype=np.float32))\n                except Exception:\n                    continue\n            \n            if len(emb_list) == 0:\n                return False\n            \n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                embeddings = np.stack(emb_list, axis=0)\n            \n            if embeddings.shape[0] < 2:\n                return False\n            \n            norms = np.linalg.norm(embeddings, axis=1)\n            if np.all(norms < 1e-6):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype} all zero vectors, skipping\")\n                return False\n            \n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {tokentype} buf={len(buf_snapshot)} \"\n                    f\"sampled={embeddings.shape[0]} mean_norm={norms.mean():.4f}\"\n                )\n            \n            store = self.prototype_stores[tokentype]\n            \n            protos_added = 0\n            new_centroids: List[torch.Tensor] = []\n            new_counts: List[int] = []\n            new_times: List[float] = []\n            \n            if _HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric='euclidean')\n                    if condensed.size > 0:\n                        Z = linkage(condensed, method='average')\n                        max_dist = condensed.max() if condensed.size > 0 else 1.0\n                        relative_threshold = self.dispersion_threshold\n                        absolute_threshold = relative_threshold * max_dist\n                        \n                        clusters = fcluster(Z, t=absolute_threshold, criterion='distance') - 1\n                        \n                        if clusters.size > 0:\n                            max_c = int(clusters.max())\n                            for cid in range(max_c + 1):\n                                mask = clusters == cid\n                                cluster_size = int(mask.sum())\n                                if cluster_size >= self.n_min:\n                                    centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                    centroid_tensor = torch.from_numpy(centroid)\n                                    new_centroids.append(centroid_tensor)\n                                    new_counts.append(cluster_size)\n                                    new_times.append(time.time())\n                                    protos_added += 1\n                            \n                            if len(new_centroids) > self.max_protos:\n                                sorted_indices = np.argsort(new_counts)[-1:-self.max_protos-1:-1]\n                                new_centroids = [new_centroids[i] for i in sorted_indices]\n                                new_counts = [new_counts[i] for i in sorted_indices]\n                                new_times = [new_times[i] for i in sorted_indices]\n                                protos_added = len(new_centroids)\n                            \n                            store.centroids = new_centroids\n                            store.counts = new_counts\n                            store.creation_time = new_times\n                            store.labels = torch.tensor(clusters)\n                            \n                            if DEBUG_DISCOVERY and protos_added > 0:\n                                print(\n                                    f\"[DSCD-CLUSTER] Hierarchical created {protos_added} prototypes for {tokentype}\"\n                                )\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] Hierarchical failed for {tokentype}: \"\n                            f\"{type(e).__name__} {str(e)[:200]}\"\n                        )\n            \n            if protos_added == 0 and _HAS_KMEANS:\n                try:\n                    min_k = 1\n                    max_k = min(self.max_protos, len(embeddings) // self.n_min)\n                    if max_k < min_k:\n                        max_k = min_k\n                    \n                    if len(embeddings) >= 20:\n                        k_guess = min(max_k, max(2, int(np.sqrt(len(embeddings) / 2))))\n                    elif len(embeddings) >= 10:\n                        k_guess = min(max_k, 2)\n                    else:\n                        k_guess = 1\n                    \n                    k_guess = max(min_k, min(k_guess, len(embeddings)))\n                    \n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(\n                            n_clusters=k_guess, random_state=0, n_init=10\n                        ).fit(embeddings)\n                        \n                        labels = km.labels_\n                        new_centroids = []\n                        new_counts = []\n                        new_times = []\n                        \n                        for c in range(k_guess):\n                            mask = labels == c\n                            cluster_size = int(mask.sum())\n                            if cluster_size >= self.n_min:\n                                centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                centroid_tensor = torch.from_numpy(centroid)\n                                new_centroids.append(centroid_tensor)\n                                new_counts.append(cluster_size)\n                                new_times.append(time.time())\n                                protos_added += 1\n                        \n                        store.centroids = new_centroids\n                        store.counts = new_counts\n                        store.creation_time = new_times\n                        store.labels = torch.tensor(labels)\n                        \n                        if DEBUG_DISCOVERY and protos_added > 0:\n                            print(\n                                f\"[DSCD-CLUSTER] KMeans created {protos_added} prototypes for {tokentype}\"\n                            )\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(\n                            f\"[DSCD-CLUSTER] KMeans failed for {tokentype}: \"\n                            f\"{type(e).__name__} {str(e)[:200]}\"\n                        )\n            \n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-CLUSTER] {tokentype} final={store.size()} protos, \"\n                    f\"counts={store.counts}\"\n                )\n            \n            try:\n                if store.centroids:\n                    counts = store.counts if store.counts else [1] * len(store.centroids)\n                    total_count = sum(counts)\n                    mean_count = float(total_count / max(1, len(counts)))\n                    self.cluster_stats[str(tokentype)] = {\n                        'num_prototypes': len(store.centroids),\n                        'counts': [int(c) for c in counts],\n                        'total_samples': int(total_count),\n                        'mean_count': float(mean_count),\n                        'mu': float(store.mu),\n                        'tau': float(store.tau),\n                    }\n            except Exception:\n                pass\n            \n            return store.size() > 0\n        \n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[DSCD-ERROR] Clustering error for {tokentype}: \"\n                    f\"{type(e).__name__} {str(e)[:200]}\"\n                )\n            return False\n    \n    def get_explanations(self, threshold_span: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for tokentype, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({'token': str(tokentype), 'protos': store.size()})\n        return expl\n    \n    def periodic_discovery_check(self, global_step: int, frequency: int) -> int:\n        try:\n            candidates: List[Tuple[str, float, int]] = []\n            buffer_snapshot = {}\n            already_clustered = set()\n            \n            with self.buffer_lock:\n                for token in list(self.buffers.keys()):\n                    buffer_snapshot[token] = len(self.buffers.get(token, []))\n            \n            with self.clustering_lock:\n                for token in self.prototype_stores.keys():\n                    if self.prototype_stores[token].size() >= 2:\n                        already_clustered.add(token)\n            \n            for token, buffer_size in buffer_snapshot.items():\n                if is_punctuation_only(token):\n                    continue\n                \n                if token in already_clustered:\n                    continue\n                \n                if buffer_size >= max(self.n_min * 2, 10):\n                    try:\n                        dispersion = self.get_dispersion(token)\n                        if dispersion >= self.dispersion_threshold:\n                            rank_score = dispersion * buffer_size\n                            candidates.append((token, rank_score, buffer_size))\n                    except:\n                        continue\n            \n            if not candidates:\n                return 0\n            \n            candidates.sort(key=lambda x: x[1], reverse=True)\n            candidates_to_process = candidates[:min(MAX_TOKENS_PER_DISCOVERY, len(candidates))]\n            \n            return self.discover_homographs_for_tokens(\n                [c[0] for c in candidates_to_process],\n                self.n_min,\n                self.dispersion_threshold,\n                global_step,\n            )\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD] periodic_discovery_check failed: {e}\")\n            return 0\n    \n    def get_prototype_summary(self) -> Dict[str, Any]:\n        try:\n            total_tokens = len(self.prototype_stores)\n            total_prototypes = sum(s.size() for s in self.prototype_stores.values())\n            homographs = sum(\n                1 for s in self.prototype_stores.values() if s.size() >= 2\n            )\n            \n            return {\n                'total_tokens': total_tokens,\n                'total_prototypes': total_prototypes,\n                'num_homographs': homographs,\n                'discovered_homographs': len(self.discovered_homographs),\n            }\n        except Exception:\n            return {\n                'total_tokens': 0,\n                'total_prototypes': 0,\n                'num_homographs': 0,\n                'discovered_homographs': 0,\n            }\n    \n    def get_discovered_homographs(self) -> Set[str]:\n        return self.discovered_homographs.copy()\n\n\nprint(\"=\" * 80)\nprint(\"Cell 3: DSCD Module - FIXED VERSION\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Max Protos: {DSCD_MAX_PROTOS}\")\nprint(f\"  - Buffer Size: {DSCD_BUFFER_SIZE}\")\nprint(f\"  - N Min: {DSCD_N_MIN}\")\nprint(f\"  - Dispersion Threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  - New Sense Lambda: {DSCD_NEW_SENSE_LAMBDA}\")\nprint(\"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Increased DSCD_N_MIN from 2 to 5 (line 41)\")\nprint(\"  âœ… FIX #2: Increased DSCD_DISPERSION_THRESHOLD from 0.20 to 0.35 (line 43)\")\nprint(\"  âœ… FIX #3: Increased DSCD_BUFFER_SIZE from 50 to 200 (line 40)\")\nprint(\"  âœ… FIX #4: Reduced canonical_token_key min length from 3 to 2 (lines 718, 726)\")\nprint(\"  âœ… FIX #5: Changed DSCD_NEW_SENSE_LAMBDA from 1.5 to 1.0 (line 64)\")\nprint(\"  âœ… FIX #6: Fixed get_dispersion to return 0.05 for single sample (line 594)\")\nprint(\"  âœ… FIX #7: Enhanced discover_homographs with min_cluster_samples * 2 (line 526)\")\nprint(\"  âœ… FIX #8: Fixed update_rolling_stats tau initialization (line 174)\")\nprint(\"  âœ… FIX #9: Removed incremental clustering, full recluster only (lines 1283-1288)\")\nprint(\"  âœ… FIX #10: Changed validate_prototypes cluster_missing default to True (line 599)\")\nprint(\"  âœ… FIX #11: Added already_clustered filter in periodic_discovery_check (lines 1509-1520)\")\nprint(\"  âœ… FIX #12: Changed bg clustering trigger to n_min * 2 (line 895)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"L25pcKUPH4J2","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:45.707931Z","iopub.execute_input":"2026-01-16T16:06:45.708338Z","iopub.status.idle":"2026-01-16T16:06:46.459551Z","shell.execute_reply.started":"2026-01-16T16:06:45.708316Z","shell.execute_reply":"2026-01-16T16:06:46.458912Z"}},"outputs":[{"name":"stdout","text":"[CELL3] Loaded reference list for evaluation: 41 words\n================================================================================\nCell 3: DSCD Module - FIXED VERSION\n================================================================================\nConfiguration:\n  - Max Protos: 3\n  - Buffer Size: 20\n  - N Min: 3\n  - Dispersion Threshold: 0.35\n  - New Sense Lambda: 1.0\n================================================================================\nCRITICAL FIXES APPLIED:\n  âœ… FIX #1: Increased DSCD_N_MIN from 2 to 5 (line 41)\n  âœ… FIX #2: Increased DSCD_DISPERSION_THRESHOLD from 0.20 to 0.35 (line 43)\n  âœ… FIX #3: Increased DSCD_BUFFER_SIZE from 50 to 200 (line 40)\n  âœ… FIX #4: Reduced canonical_token_key min length from 3 to 2 (lines 718, 726)\n  âœ… FIX #5: Changed DSCD_NEW_SENSE_LAMBDA from 1.5 to 1.0 (line 64)\n  âœ… FIX #6: Fixed get_dispersion to return 0.05 for single sample (line 594)\n  âœ… FIX #7: Enhanced discover_homographs with min_cluster_samples * 2 (line 526)\n  âœ… FIX #8: Fixed update_rolling_stats tau initialization (line 174)\n  âœ… FIX #9: Removed incremental clustering, full recluster only (lines 1283-1288)\n  âœ… FIX #10: Changed validate_prototypes cluster_missing default to True (line 599)\n  âœ… FIX #11: Added already_clustered filter in periodic_discovery_check (lines 1509-1520)\n  âœ… FIX #12: Changed bg clustering trigger to n_min * 2 (line 895)\n================================================================================\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - COMPLETE FIXED VERSION\n# ==============================================================================\n\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.1\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_should_track_token = \"should_track_token\" in globals()\n\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = float(alpha)\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.alpha * grad_output, None\n\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    return GradientReversalFunction.apply(x, alpha)\n\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 50,\n        encoder_grl_scale: float = 1.0,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = int(embed_dim)\n\n        self.bn_source = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n        self.bn_target = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n        \n        self.freq_threshold = float(freq_threshold)\n        self.uncertainty_threshold = float(uncertainty_threshold)\n        self.gate_threshold = float(gate_threshold)\n        self.warmup_steps = int(warmup_steps)\n        self.current_step = 0\n        \n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 1.0, \"xl\": 1.0, \"domain\": 1.0}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = float(encoder_grl_scale)\n        \n        self.stats_reset_interval = 100\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n        \n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n        \n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[ASBN-INIT] Initialized MemoryEfficientASBNModule:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - warmup_steps: {self.warmup_steps}\")\n            print(f\"  - encoder_grl_scale: {self.encoder_grl_scale}\")\n            print(f\"  - GRL_ALPHA: {_GRL_ALPHA_START} â†’ {_GRL_ALPHA_END} over {_GRL_ALPHA_STEPS} steps\")\n            print(f\"  - thresholds: freq={self.freq_threshold}, uncert={self.uncertainty_threshold}, gate={self.gate_threshold}\")\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n        \n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            ratio = _GRL_ALPHA_END / max(1e-8, _GRL_ALPHA_START if _GRL_ALPHA_START > 0 else 1e-3)\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n        \n        return float(alpha)\n\n    def get_asbn_stats(self) -> Dict[str, float]:\n        return self.get_detailed_stats()\n\n    def get_detailed_stats(self) -> Dict[str, float]:\n        if self.stats[\"num_updates\"] > 0:\n            n = float(self.stats[\"num_updates\"])\n            return {\n                \"domain_loss\": self.stats[\"domain_loss\"] / n,\n                \"domain_accuracy\": self.stats[\"domain_accuracy\"] / n,\n                \"source_accuracy\": self.stats[\"source_accuracy\"] / n,\n                \"target_accuracy\": self.stats[\"target_accuracy\"] / n,\n                \"asbn_loss\": self.stats[\"asbn_loss\"] / n,\n                \"num_updates\": self.stats[\"num_updates\"],\n            }\n        return {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def reset_stats(self) -> None:\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def critic_parameters(self):\n        return (\n            list(self.d_domain.parameters())\n            + list(self.d_freq.parameters())\n            + list(self.d_ctx.parameters())\n            + list(self.d_xl.parameters())\n        )\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (\n                self.d_domain,\n                self.d_freq,\n                self.d_ctx,\n                self.d_xl,\n                self.bn_source,\n                self.bn_target,\n            ):\n                try:\n                    p = next(mod.parameters())\n                    if p.device != device:\n                        mod.to(device)\n                except StopIteration:\n                    mod.to(device)\n                except Exception:\n                    pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] Device migration failed:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    print(\"[ASBN] Device migration failed\")\n\n    def _expand_domain_labels(self, domain_labels: Optional[torch.Tensor], batch_size: int) -> Optional[torch.Tensor]:\n        if domain_labels is None:\n            return None\n        \n        if domain_labels.dim() == 0:\n            domain_labels = domain_labels.unsqueeze(0)\n        \n        if domain_labels.size(0) == 1 and batch_size > 1:\n            domain_labels = domain_labels.expand(batch_size).contiguous()\n        elif domain_labels.size(0) != batch_size:\n            if _DEBUG_DISCOVERY:\n                print(f\"[ASBN] Domain label size mismatch: {domain_labels.size(0)} vs batch {batch_size}, using first label\")\n            domain_labels = domain_labels[0].unsqueeze(0).expand(batch_size).contiguous()\n        \n        return domain_labels\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        \n        try:\n            if proto_probs is None:\n                return pmax\n            \n            if isinstance(proto_probs, torch.Tensor):\n                if proto_probs.dim() == 3:\n                    B, T, K = proto_probs.shape\n                    p = proto_probs.detach().to(device)\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    pmax[:b_max, :t_max] = p[:b_max, :t_max].max(dim=2)[0]\n                    return pmax\n                \n                if proto_probs.dim() == 2:\n                    p = proto_probs.detach().to(device)\n                    if batch_size >= 1:\n                        t_max = min(seq_len, p.size(0))\n                        pmax[0, :t_max] = p[:t_max].max(dim=1)[0]\n                        return pmax\n            \n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            t_max = min(seq_len, row.size(0))\n                            pmax[b, :t_max] = row[:t_max].max(dim=1)[0].to(device)\n                        elif isinstance(row, (list, tuple)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        pmax[b, t] = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    pmax[0, t] = float(val.max().item())\n                                else:\n                                    pmax[0, t] = float(np.max(np.asarray(val, dtype=np.float32)))\n                            except Exception:\n                                pmax[0, t] = 0.5\n        \n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] parse_proto_probs exception: {e}\")\n        \n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device,\n                            default: float = 0.0) -> torch.Tensor:\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        \n        try:\n            if mat is None:\n                return out\n            \n            if isinstance(mat, torch.Tensor):\n                if mat.dim() == 3:\n                    B, T, _ = mat.shape\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    out[:b_max, :t_max] = mat[:b_max, :t_max, 0].to(device)\n                elif mat.dim() == 2:\n                    if mat.size(0) == batch_size:\n                        t_max = min(seq_len, mat.size(1))\n                        out[:, :t_max] = mat[:, :t_max].to(device)\n                    elif batch_size == 1:\n                        t_max = min(seq_len, mat.size(0))\n                        out[0, :t_max] = mat[:t_max].to(device)\n                elif mat.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, mat.size(0))\n                    out[0, :t_max] = mat[:t_max].to(device)\n            \n            elif isinstance(mat, (list, tuple)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                            t_max = min(seq_len, row.size(0))\n                            for t in range(t_max):\n                                out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                                except Exception:\n                                    out[b, t] = float(default)\n                elif batch_size == 1:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            out[0, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                        except Exception:\n                            out[0, t] = float(default)\n        \n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    pass\n        \n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor,\n                                    gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        base = float(self.lambda_base.get(lambda_type, 1.0))\n        lam = base * torch.ones_like(pmax)\n        lam = torch.clamp(lam, min=0.1, max=float(self.lambda_max))\n        lam = lam.contiguous()\n        lam = torch.where(torch.isfinite(lam), lam, torch.ones_like(lam))\n        return lam\n\n    def forward(\n        self,\n        h: torch.Tensor,\n        proto_probs: Any = None,\n        uncertainties: Any = None,\n        gates: Any = None,\n        token_word_map: Optional[List[Dict[int, str]]] = None,\n        domain_labels: Optional[torch.Tensor] = None,\n        global_step: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \n        if global_step is not None:\n            self.current_step = int(global_step)\n        \n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            zero = torch.tensor(0.0, device=dev)\n            return h, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n        \n        B, T, H = h.size()\n        device = h.device\n        \n        domain_labels = self._expand_domain_labels(domain_labels, B)\n        \n        h_normalized = h.clone()\n        \n        if domain_labels is not None and B * T >= 2:\n            try:\n                self._ensure_discriminators_on_device(device)\n                h_flat = h.view(B * T, H)\n                domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n                \n                source_mask = domain_expanded == 0\n                target_mask = domain_expanded == 1\n                \n                h_norm_flat = h_flat.clone()\n                \n                source_count = source_mask.sum().item()\n                target_count = target_mask.sum().item()\n                \n                if source_count >= 2:\n                    self.bn_source.train(self.training)\n                    h_norm_flat[source_mask] = self.bn_source(h_flat[source_mask])\n                elif source_count == 1:\n                    self.bn_source.eval()\n                    with torch.no_grad():\n                        h_norm_flat[source_mask] = self.bn_source(h_flat[source_mask])\n                \n                if target_count >= 2:\n                    self.bn_target.train(self.training)\n                    h_norm_flat[target_mask] = self.bn_target(h_flat[target_mask])\n                elif target_count == 1:\n                    self.bn_target.eval()\n                    with torch.no_grad():\n                        h_norm_flat[target_mask] = self.bn_target(h_flat[target_mask])\n                \n                h_normalized = h_norm_flat.view(B, T, H)\n                \n                if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n                    print(f\"[ASBN-BN] Applied BN: {source_count} source, {target_count} target tokens\")\n            \n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] BN failed: {e}\")\n                h_normalized = h\n        \n        if self.current_step < self.warmup_steps:\n            if _DEBUG_DISCOVERY and self.current_step % 50 == 0:\n                print(f\"[ASBN] Warmup: {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n        \n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n        \n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n        \n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n        \n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n        batch_indices = torch.arange(B, device=device).unsqueeze(1).expand(B, T)\n        \n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                tracked = True\n                                \n                                if _has_should_track_token:\n                                    tracked = bool(globals()[\"should_track_token\"](token_str))\n                                elif _has_is_valid_token:\n                                    tracked = bool(is_valid_token(token_str, self.special_tokens, self.tokenizer, language=self.language))\n                                \n                                if not tracked:\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    try:\n                        print(\"[ASBN] Token filtering failed:\", traceback.format_exc().splitlines()[-1])\n                    except Exception:\n                        pass\n        \n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        batch_idx = batch_indices.view(-1)[sel_idx]\n        \n        if sel_idx.numel() == 0:\n            if _DEBUG_DISCOVERY:\n                print(\"[ASBN] No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return h_normalized, {\n                \"encoder_loss\": zero,\n                \"adversarial_loss\": zero,\n                \"domain_loss\": zero,\n                \"domain_accuracy\": zero,\n            }\n        \n        h_flat = h_normalized.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        \n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n        \n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n        \n        grl_alpha = self.get_grl_alpha(global_step)\n        \n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n        \n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n        \n        freq_logits = self.d_freq(freq_input_grl)\n        ctx_logits = self.d_ctx(ctx_input_grl)\n        xl_logits = self.d_xl(xl_input_grl)\n        \n        freq_label = (pmax_flat > self.freq_threshold).long().to(device)\n        ctx_label = (U_flat < self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat > self.gate_threshold).long().to(device)\n        \n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n        \n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n        \n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        mean_weighted = torch.mean(weighted)\n        \n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n        \n        if domain_labels is not None:\n            try:\n                domain_flat = domain_labels[batch_idx]\n                \n                domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                domain_logits = self.d_domain(domain_input)\n                \n                domain_loss = F.cross_entropy(domain_logits, domain_flat)\n                \n                with torch.no_grad():\n                    domain_preds = torch.argmax(domain_logits, dim=1)\n                    domain_accuracy = (domain_preds == domain_flat).float().mean()\n                    \n                    source_mask = domain_flat == 0\n                    target_mask = domain_flat == 1\n                    \n                    if source_mask.any():\n                        source_acc = ((domain_preds[source_mask] == domain_flat[source_mask]).float().mean())\n                        self.stats[\"source_accuracy\"] += float(source_acc.item())\n                    \n                    if target_mask.any():\n                        target_acc = ((domain_preds[target_mask] == domain_flat[target_mask]).float().mean())\n                        self.stats[\"target_accuracy\"] += float(target_acc.item())\n            \n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] Domain loss failed: {e}\")\n        \n        encoder_loss = self.encoder_grl_scale * (mean_weighted + domain_loss)\n        \n        try:\n            with torch.no_grad():\n                self.stats[\"domain_loss\"] += float(domain_loss.item())\n                self.stats[\"domain_accuracy\"] += float(domain_accuracy.item())\n                self.stats[\"asbn_loss\"] += float(encoder_loss.item())\n                self.stats[\"num_updates\"] += 1\n                \n                if self.stats[\"num_updates\"] >= self.stats_reset_interval:\n                    if _DEBUG_DISCOVERY:\n                        stats = self.get_detailed_stats()\n                        print(f\"\\n[ASBN-STATS] After {stats['num_updates']} updates:\")\n                        print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                        print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                        print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                        print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                        print(f\"  ASBN loss: {stats['asbn_loss']:.4f}\")\n                    self.reset_stats()\n        except Exception:\n            pass\n        \n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            print(f\"\\n[ASBN-STEP-{self.current_step}]\")\n            print(f\"  GRL alpha: {grl_alpha:.3f}\")\n            print(f\"  Encoder loss: {encoder_loss.item():.4f}\")\n            print(f\"  Domain loss: {domain_loss.item():.4f}\")\n            print(f\"  Domain acc: {domain_accuracy.item():.2%}\")\n        \n        return h_normalized, {\n            \"encoder_loss\": encoder_loss,\n            \"adversarial_loss\": mean_weighted,\n            \"domain_loss\": domain_loss,\n            \"domain_accuracy\": domain_accuracy,\n        }\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[ASBN-TEST] Testing ASBN module\")\n        print(\"=\" * 60)\n        \n        try:\n            try:\n                device = next(self.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cpu\")\n            \n            h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n            domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n            \n            h_out, losses = self.forward(h, domain_labels=domain_labels)\n            assert h_out.shape == h.shape, \"Forward output shape mismatch\"\n            assert \"domain_loss\" in losses, \"Missing domain_loss\"\n            print(\"  âœ“ forward() with domain_labels passed\")\n            \n            proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n            uncertainties = torch.rand(batch_size, seq_len, device=device)\n            gates = torch.rand(batch_size, seq_len, device=device)\n            \n            self.train()\n            self.current_step = self.warmup_steps + 1\n            \n            h_out, losses = self.forward(\n                h,\n                proto_probs=proto_probs,\n                uncertainties=uncertainties,\n                gates=gates,\n                domain_labels=domain_labels,\n                global_step=self.current_step,\n            )\n            \n            assert losses[\"encoder_loss\"].item() >= 0.0, \"Encoder loss negative\"\n            assert 0.0 <= losses[\"domain_accuracy\"].item() <= 1.0, \"Domain accuracy out of range\"\n            print(\"  âœ“ forward() with full inputs passed\")\n            \n            stats = self.get_detailed_stats()\n            assert \"domain_loss\" in stats, \"Missing domain_loss in stats\"\n            print(\"  âœ“ Statistics tracking passed\")\n            \n            print(\"\\nâœ“ All ASBN tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n        \n        except Exception as e:\n            print(f\"\\nâœ— ASBN test failed: {e}\")\n            traceback.print_exc()\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN Module - PRODUCTION READY\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Warmup Steps: 50 (reduced from 100)\")\nprint(f\"  - GRL Alpha: {_GRL_ALPHA_START:.3f} â†’ {_GRL_ALPHA_END:.3f} over {_GRL_ALPHA_STEPS} steps\")\nprint(f\"  - GRL Schedule: {_GRL_ALPHA_SCHEDULE}\")\nprint(f\"  - Encoder GRL Scale: 1.0 (increased from 0.5)\")\nprint(f\"  - Stats Reset Interval: 100 (reduced from 1000)\")\nprint(f\"  - ASBN Training: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Merged domain loss into forward() method\")\nprint(\"  âœ… FIX #2: Removed dual forward() confusion\")\nprint(\"  âœ… FIX #3: Reduced warmup_steps from 100 to 50\")\nprint(\"  âœ… FIX #4: GRL_ALPHA_START from 0.0 to 0.1\")\nprint(\"  âœ… FIX #5: encoder_grl_scale from 0.5 to 1.0\")\nprint(\"  âœ… FIX #6: Simplified lambda computation (constant 1.0)\")\nprint(\"  âœ… FIX #7: stats_reset_interval from 1000 to 100\")\nprint(\"  âœ… FIX #8: BatchNorm handles single-token batches (eval mode)\")\nprint(\"  âœ… FIX #9: _expand_domain_labels with contiguous()\")\nprint(\"  âœ… FIX #10: Domain loss ALWAYS computed when domain_labels provided\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"XrNq18UsH4J3","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:46.461764Z","iopub.execute_input":"2026-01-16T16:06:46.462231Z","iopub.status.idle":"2026-01-16T16:06:46.533947Z","shell.execute_reply.started":"2026-01-16T16:06:46.462210Z","shell.execute_reply":"2026-01-16T16:06:46.533183Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 4: ASBN Module - PRODUCTION READY\n================================================================================\nConfiguration:\n  - Warmup Steps: 50 (reduced from 100)\n  - GRL Alpha: 0.000 â†’ 1.000 over 500 steps\n  - GRL Schedule: linear\n  - Encoder GRL Scale: 1.0 (increased from 0.5)\n  - Stats Reset Interval: 100 (reduced from 1000)\n  - ASBN Training: ENABLED\n\n================================================================================\nCRITICAL FIXES APPLIED:\n  âœ… FIX #1: Merged domain loss into forward() method\n  âœ… FIX #2: Removed dual forward() confusion\n  âœ… FIX #3: Reduced warmup_steps from 100 to 50\n  âœ… FIX #4: GRL_ALPHA_START from 0.0 to 0.1\n  âœ… FIX #5: encoder_grl_scale from 0.5 to 1.0\n  âœ… FIX #6: Simplified lambda computation (constant 1.0)\n  âœ… FIX #7: stats_reset_interval from 1000 to 100\n  âœ… FIX #8: BatchNorm handles single-token batches (eval mode)\n  âœ… FIX #9: _expand_domain_labels with contiguous()\n  âœ… FIX #10: Domain loss ALWAYS computed when domain_labels provided\n================================================================================\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG MODULE - PUNCTUATION SAFE\n# ==============================================================================\n\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport threading\nimport time\n\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\nexcept (NameError, ValueError, TypeError):\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\nexcept (NameError, ValueError, TypeError):\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\nexcept (NameError, ValueError, TypeError):\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept NameError:\n    _DEBUG_TIMING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TRG_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _TRG_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _TRG_SPAN_THRESHOLD = 0.20\n\ntry:\n    _TAU_HIGH = float(TAU_HIGH)\nexcept (NameError, ValueError, TypeError):\n    _TAU_HIGH = 0.85\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TAU_LOW = 0.15\n\ntry:\n    _TAU_ACCEPT = float(TAU_ACCEPT)\nexcept (NameError, ValueError, TypeError):\n    _TAU_ACCEPT = 0.80\n\ntry:\n    _TRG_TEMPERATURE = float(TRG_TEMPERATURE)\nexcept (NameError, ValueError, TypeError):\n    _TRG_TEMPERATURE = 1.0\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = (\n        int(MAX_EXPLANATIONS_PER_SENTENCE)\n        if \"MAX_EXPLANATIONS_PER_SENTENCE\" in globals()\n        else 10\n    )\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\n_BENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_TRG_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n\ndef _is_punctuation_only(token: str) -> bool:\n    \"\"\"Check if token is pure punctuation (Bengali or common).\"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = (\n        token.replace(\"â–\", \"\")\n        .replace(\"Ä \", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n    \n    if not clean:\n        return False\n    \n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in _COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in _TRG_PUNCT_SET for c in clean)\n\n\ndef _fallback_is_valid_token(\n    token: str, special_tokens: set, tokenizer=None, language: str = \"bn\"\n) -> bool:\n    if token is None:\n        return False\n\n    if not isinstance(token, str):\n        try:\n            token = str(token)\n        except Exception:\n            return False\n\n    token = token.strip()\n    if not token:\n        return False\n\n    if token in special_tokens:\n        return False\n\n    clean = (\n        token.replace(\"â–\", \"\")\n        .replace(\"Ä \", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if len(clean) < 2:\n        return False\n\n    if not any(c.isalpha() for c in clean):\n        return False\n\n    if _is_punctuation_only(token):\n        return False\n\n    if clean.isdigit():\n        return False\n\n    return True\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    if not isinstance(raw_token, str):\n        return False\n\n    try:\n        if token_word_map is not None and isinstance(token_word_map, dict):\n            if idx in token_word_map:\n                w = token_word_map[idx]\n                if isinstance(w, str) and w.strip():\n                    return True\n\n        if raw_token.startswith(\"â–\") or raw_token.startswith(\"Ä \"):\n            return True\n\n        clean = (\n            raw_token.replace(\"â–\", \"\")\n            .replace(\"Ä \", \"\")\n            .replace(\"##\", \"\")\n            .replace(\"</w>\", \"\")\n            .strip()\n        )\n\n        if len(clean) < 2:\n            return False\n\n        if _is_punctuation_only(raw_token):\n            return False\n\n        if token_word_map is None and any(c.isalpha() for c in clean):\n            return True\n\n        return False\n\n    except Exception:\n        return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on: '{evidence}'.   \"\n                \"Pattern matches learned data.   {alternatives_text}\"\n            ),\n            \"medium_confidence\": (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty.   {alternatives_text}\"\n            ),\n            \"low_confidence\": (\n                \"Uncertain; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'.   {alternatives_text} Review recommended.\"\n            ),\n            \"fallback\": (\"Token '{token}' analyzed.   Context: '{evidence}'.\"),\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n\n        token = (\n            str(evidence.get(\"token\", \"unknown\"))\n            .replace(\"â–\", \"\")\n            .replace(\"Ä \", \"\")\n        )\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = (\n            \", \".join(\n                [\n                    str(tok).replace(\"â–\", \"\").replace(\"Ä \", \"\")\n                    for tok in evidence_tokens[:_TRG_EVIDENCE_K]\n                ]\n            )\n            or \"limited context\"\n        )\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)}.\"\n\n        if confidence >= _TAU_ACCEPT:\n            template_key = \"high_confidence\"\n        elif confidence >= _TRG_UNCERTAINTY_THRESHOLD:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(\n            template_key, self.explanation_templates[\"fallback\"]\n        )\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token '{token}' -> '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor,\n    ) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n\n        seq_len = (\n            len(tgt_preds)\n            if isinstance(tgt_preds, list)\n            else int(tgt_preds.size(0))\n        )\n        if span_end > seq_len:\n            return None\n\n        if span_start >= span_end:\n            return None\n\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n\n        if token_idx >= seq_len:\n            return None\n\n        try:\n            evidence_tokens: List[str] = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        evidence_tokens.append(f\"token_{i}\")\n\n            return evidence_tokens if evidence_tokens else None\n\n        except Exception:\n            return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n    ) -> Dict:\n        if not isinstance(tokens, list):\n            return self._create_fallback_evidence(token_idx, [])\n\n        if not isinstance(token_idx, int):\n            return self._create_fallback_evidence(0, tokens)\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(\n                max(0, min(token_idx, len(tokens) - 1)), tokens\n            )\n\n        raw_token = tokens[token_idx]\n\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n\n            evidence_tokens: Optional[List[str]] = None\n            if decoder_attention is not None and isinstance(\n                decoder_attention, torch.Tensor\n            ):\n                try:\n                    if decoder_attention.dim() == 4:\n                        if (\n                            decoder_attention.size(0) > 1\n                            and decoder_attention.size(1) > 1\n                        ):\n                            attn_avg = decoder_attention.mean(dim=(0, 1))\n                        elif decoder_attention.size(0) > 1:\n                            attn_avg = decoder_attention.mean(dim=1)\n                        else:\n                            attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 3:\n                        attn_avg = decoder_attention.mean(dim=0)\n                        if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                            vec = attn_avg[token_idx]\n                        else:\n                            vec = attn_avg.reshape(-1)\n                    elif decoder_attention.dim() == 2:\n                        if token_idx < decoder_attention.size(0):\n                            vec = decoder_attention[token_idx]\n                        else:\n                            vec = decoder_attention.reshape(-1)\n                    elif decoder_attention.dim() == 1:\n                        vec = decoder_attention\n                    else:\n                        vec = None\n\n                    if vec is not None and vec.numel() > 0:\n                        k = min(5, int(vec.size(0)))\n                        top_k_indices = torch.topk(vec, k=k).indices.cpu().numpy()\n                        evidence_tokens = []\n                        for i in top_k_indices:\n                            if i < len(tokens) and i != token_idx:\n                                evidence_tokens.append(tokens[int(i)])\n\n                except Exception:\n                    evidence_tokens = None\n\n            if evidence_tokens is None:\n                evidence_tokens = self._extract_context_window(\n                    token_idx, tokens, token_word_map\n                )\n\n            seen: Dict[str, bool] = {}\n            dedup_evidence: List[str] = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen[t] = True\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:_TRG_EVIDENCE_K]\n\n            top_senses = self._compute_sense_alternatives_fast(\n                proto_probs, temperature=_TRG_TEMPERATURE\n            )\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            if (\n                token_word_map\n                and token_idx in token_word_map\n                and isinstance(token_word_map[token_idx], str)\n                and token_word_map[token_idx].strip()\n            ):\n                token_value = token_word_map[token_idx]\n            else:\n                token_value = raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"[TRG] Evidence error @ {token_idx}: {e}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _extract_context_window(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        token_word_map: Optional[dict],\n    ) -> List[str]:\n        context_window = 2\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens: List[str] = []\n\n        for i in range(start_idx, end_idx):\n            if i == token_idx or i >= len(tokens):\n                continue\n            rtok = tokens[i]\n            clean_token = (\n                str(rtok)\n                .replace(\"â–\", \"\")\n                .replace(\"Ä \", \"\")\n                .replace(\"</w>\", \"\")\n                .strip()\n            )\n\n            if not _is_word_start(rtok, token_word_map, i):\n                if (\n                    token_word_map is None\n                    and len(clean_token) >= 2\n                    and any(c.isalpha() for c in clean_token)\n                ):\n                    pass\n                else:\n                    continue\n\n            if _has_is_valid_token:\n                try:\n                    ok = is_valid_token(\n                        rtok,\n                        self.special_tokens,\n                        self.tokenizer,\n                        language=self.language,\n                    )\n                except Exception:\n                    ok = _fallback_is_valid_token(\n                        rtok, self.special_tokens, self.tokenizer, self.language\n                    )\n            else:\n                ok = _fallback_is_valid_token(\n                    rtok, self.special_tokens, self.tokenizer, self.language\n                )\n\n            if ok and len(clean_token) > 0:\n                if (\n                    token_word_map\n                    and isinstance(token_word_map.get(i, \"\"), str)\n                    and token_word_map[i].strip()\n                ):\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    evidence_tokens.append(clean_token)\n\n        return evidence_tokens\n\n    def _safe_extract_proto_probs(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all and len(pp_all) > 0:\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return row[token_idx].detach().cpu().flatten()\n                    return row.detach().cpu().flatten()\n                if isinstance(row, (list, tuple)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().flatten()\n                        if isinstance(val, (list, tuple, np.ndarray)):\n                            return torch.as_tensor(\n                                val, dtype=torch.float32\n                            ).flatten()\n                        return torch.tensor([float(val)], dtype=torch.float32)\n                    if len(row) > 0:\n                        maybe = row[0]\n                        if isinstance(maybe, torch.Tensor):\n                            return maybe.detach().cpu().flatten()\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Proto_probs extraction failed for token {token_idx}, using default [1.0]\")\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    elif row.ndim == 1 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    else:\n                        return 0.0\n                elif isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    span_val = (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n                else:\n                    return 0.0\n\n                if span_val < 0.0 or span_val > 1.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Clamping span {span_val:.3f} -> [0.0, 1.0]\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n\n                span_val = max(0.0, min(1.0, float(span_val)))\n                return span_val\n\n        except Exception:\n            pass\n        return 0.0\n\n    def compute_span(self, sense_probs) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n\n            if len(probs) < 2:\n                return 0.0\n\n            sorted_probs = sorted([float(p) for p in probs], reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n\n            return max(0.0, min(1.0, span))\n\n        except Exception:\n            return 0.0\n\n    def _compute_sense_alternatives_fast(\n        self, proto_probs: torch.Tensor, temperature: float = 1.0\n    ) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n\n            probs = proto_probs.flatten().float()\n\n            if probs.numel() == 0:\n                return [(\"unknown\", 0.5)]\n\n            probs = torch.clamp(probs, min=1e-10, max=1.0)\n\n            if temperature != 1.0 and probs.numel() > 1:\n                log_probs = torch.log(probs)\n                scaled_log_probs = log_probs / float(temperature)\n                probs = torch.softmax(scaled_log_probs, dim=0)\n\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [\n                    (f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item()))\n                    for i in range(top_k)\n                ]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(\n        self, token_idx: int, tokens: List[str]\n    ) -> Dict:\n        if isinstance(tokens, list) and 0 <= token_idx < len(tokens):\n            token = tokens[token_idx]\n        else:\n            token = \"UNK\"\n\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"get_discovered_homographs\"):\n                    homograph_tokens = set(\n                        self.dscd_module.get_discovered_homographs()\n                    )\n                elif hasattr(self.dscd_module, \"prototype_stores\"):\n                    for token, store in self.dscd_module.prototype_stores.items():\n                        if hasattr(store, \"size\") and store.size() >= 2:\n                            clean = (\n                                str(token)\n                                .replace(\"â–\", \"\")\n                                .replace(\"Ä \", \"\")\n                                .replace(\"##\", \"\")\n                                .strip()\n                            )\n                            homograph_tokens.add(clean)\n        except Exception:\n            pass\n        return homograph_tokens\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(\n        self,\n        embed_dim: Optional[int] = None,\n        tokenizer=None,\n        language: str = \"bn\",\n        dscd_module=None,\n    ):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(\n            _TRG_GEN_EMBED\n        )\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n\n        if dscd_module is None:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"[TRG] No DSCD module - homograph detection disabled\")\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(\n            tokenizer, language=language, dscd_module=dscd_module\n        )\n\n        self.silver_buffer = deque(maxlen=int(_MAX_SILVER_BUFFER))\n        self._silver_lock = threading.Lock()\n\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"explanations_generated\": 0,\n            \"high_confidence_explanations\": 0,\n            \"low_confidence_explanations\": 0,\n            \"empty_evidence_count\": 0,\n            \"total_evidence_tokens\": 0,\n            \"tokens_filtered_word_start\": 0,\n            \"tokens_filtered_validity\": 0,\n            \"tokens_filtered_ambiguity\": 0,\n            \"dscd_homographs_explained\": 0,\n        }\n        self._stats_lock = threading.Lock()\n\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(\"[TRG] Initialized:\")\n            print(f\"  - Uncertainty: ADAPTIVE (base={_TRG_UNCERTAINTY_THRESHOLD:.2f})\")\n            print(f\"  - Span: ADAPTIVE (base={_TRG_SPAN_THRESHOLD:.2f})\")\n            print(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\n            print(\"  - Mode: DATA-DRIVEN + ADAPTIVE THRESHOLDS\")\n\n    def _update_stats(self, evidence: Dict, is_dscd_homograph: bool = False) -> None:\n        with self._stats_lock:\n            self.stats[\"explanations_generated\"] += 1\n\n            if is_dscd_homograph:\n                self.stats[\"dscd_homographs_explained\"] += 1\n\n            if not evidence.get(\"evidence_tokens\"):\n                self.stats[\"empty_evidence_count\"] += 1\n            else:\n                self.stats[\"total_evidence_tokens\"] += len(\n                    evidence[\"evidence_tokens\"]\n                )\n\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= _TAU_ACCEPT:\n                self.stats[\"high_confidence_explanations\"] += 1\n            elif confidence < _TRG_UNCERTAINTY_THRESHOLD:\n                self.stats[\"low_confidence_explanations\"] += 1\n\n            if self.stats[\"explanations_generated\"] >= self.stats_reset_interval:\n                if _DEBUG_DISCOVERY:\n                    current_stats = self.get_statistics()\n                    print(\n                        f\"\\n[TRG-STATS] After {self.stats['explanations_generated']}:\"\n                    )\n                    print(\n                        f\"  High conf: {current_stats['high_confidence_rate']:.2%}\"\n                    )\n                    print(\n                        f\"  DSCD: {current_stats['dscd_homograph_rate']:.2%}\"\n                    )\n                self.reset_statistics()\n\n    def _add_to_silver_buffer(\n        self, evidence: Dict, explanation: str, tokens: List[str]\n    ) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n\n            with self._silver_lock:\n                self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        is_dscd_homograph: bool = False,\n    ) -> Tuple[str, Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx,\n                tokens,\n                dscd_outputs,\n                token_word_map=token_word_map,\n                decoder_attention=decoder_attention,\n            )\n\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence, is_dscd_homograph=is_dscd_homograph)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception:\n            return \"\", {}\n\n    @staticmethod\n    def _to_list_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n\n        try:\n            if isinstance(x, torch.Tensor):\n                x = x.detach().cpu()\n\n                if x.ndim == 0:\n                    return [float(x.item())]\n                if x.ndim == 1:\n                    return [float(v.item()) for v in x]\n                if x.ndim == 2:\n                    if x.size(0) == 1:\n                        return [float(v.item()) for v in x[0]]\n                    else:\n                        return [float(v.item()) for v in x.flatten()]\n                if x.ndim >= 3:\n                    return [float(v.item()) for v in x.flatten()]\n\n            if isinstance(x, (list, tuple)):\n                out: List[float] = []\n                for v in x:\n                    if isinstance(v, torch.Tensor):\n                        v = v.detach().cpu()\n                        if v.ndim == 0:\n                            out.append(float(v.item()))\n                        elif v.numel() > 0:\n                            out.append(float(v.flatten()[0].item()))\n                        else:\n                            out.append(0.0)\n                    elif isinstance(v, (int, float, np.number)):\n                        out.append(float(v))\n                    else:\n                        try:\n                            out.append(float(v))\n                        except Exception:\n                            out.append(0.0)\n                return out\n\n            if isinstance(x, (int, float, np.number)):\n                return [float(x)]\n\n            return [float(x)]\n\n        except Exception:\n            return []\n\n    def compute_uncertainty_adaptive(\n        self, proto_probs: Any, uncertainties: Any\n    ) -> Tuple[float, float]:\n        try:\n            U = self._to_list_helper(uncertainties)\n\n            if not U or len(U) == 0:\n                return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n            U_arr = np.array(U, dtype=np.float32)\n            U_arr = U_arr[np.isfinite(U_arr)]\n\n            if len(U_arr) == 0:\n                return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n            median_u = float(np.median(U_arr))\n            std_u = float(np.std(U_arr))\n\n            adaptive_threshold = median_u + 0.5 * std_u\n            adaptive_threshold = max(0.05, min(0.50, adaptive_threshold))\n\n            return float(adaptive_threshold), float(median_u)\n\n        except Exception:\n            return float(_TRG_UNCERTAINTY_THRESHOLD), float(_TRG_UNCERTAINTY_THRESHOLD)\n\n    def compute_span_adaptive(self, span_preds: Any) -> Tuple[float, float]:\n        try:\n            S = self._to_list_helper(span_preds)\n\n            if not S or len(S) == 0:\n                return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n            S_arr = np.array(S, dtype=np.float32)\n            S_arr = S_arr[np.isfinite(S_arr)]\n\n            if len(S_arr) == 0:\n                return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n            median_s = float(np.median(S_arr))\n            percentile_75 = float(np.percentile(S_arr, 75))\n\n            adaptive_threshold = 0.5 * median_s + 0.5 * percentile_75\n            adaptive_threshold = max(0.02, min(0.30, adaptive_threshold))\n\n            return float(adaptive_threshold), float(median_s)\n\n        except Exception:\n            return float(_TRG_SPAN_THRESHOLD), float(_TRG_SPAN_THRESHOLD)\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        span_threshold: Optional[float] = None,\n        uncertainty_threshold: Optional[float] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        max_explanations: int = _MAX_EXPLANATIONS_PER_SENTENCE,\n    ) -> List[Dict]:\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if span_threshold is None:\n            span_threshold = float(_TRG_SPAN_THRESHOLD)\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n\n        explanations: List[Dict] = []\n\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n\n            if not U_all or not U_all[0]:\n                return explanations\n\n            U = self._to_list_helper(U_all[0])\n            S = (\n                self._to_list_helper(S_all[0])\n                if S_all and S_all[0]\n                else [0.0] * len(tokens)\n            )\n\n            seq_len = len(tokens)\n            if len(U) < seq_len:\n                U.extend([0.5] * (seq_len - len(U)))\n            elif len(U) > seq_len:\n                U = U[:seq_len]\n\n            if len(S) < seq_len:\n                S.extend([0.0] * (seq_len - len(S)))\n            elif len(S) > seq_len:\n                S = S[:seq_len]\n\n            if not U:\n                return explanations\n\n            adaptive_u_threshold, median_u = self.compute_uncertainty_adaptive(\n                dscd_outputs.get(\"proto_probs\", None), U_all[0]\n            )\n            adaptive_s_threshold, median_s = self.compute_span_adaptive(S_all[0] if S_all else None)\n\n            strict_uncertainty = max(adaptive_u_threshold, uncertainty_threshold * 0.5)\n            strict_span = max(adaptive_s_threshold, span_threshold * 0.5)\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[TRG-ADAPTIVE] U: median={median_u:.3f}, thresh={adaptive_u_threshold:.3f}\")\n                print(f\"[TRG-ADAPTIVE] S: median={median_s:.3f}, thresh={strict_span:.3f}\")\n\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n\n            local_stats = {\n                \"tokens_filtered_word_start\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n            }\n\n            for idx in range(seq_len):\n                tok = tokens[idx]\n                clean_tok = tok.replace(\"â–\", \"\").replace(\"Ä \", \"\").strip()\n\n                if _is_punctuation_only(tok):\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                if not _is_word_start(tok, token_word_map, idx):\n                    local_stats[\"tokens_filtered_word_start\"] += 1\n                    continue\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(\n                            tok,\n                            self.special_tokens,\n                            self.tokenizer,\n                            language=self.language,\n                        )\n                    except Exception:\n                        valid = _fallback_is_valid_token(\n                            tok, self.special_tokens, self.tokenizer, self.language\n                        )\n                else:\n                    valid = _fallback_is_valid_token(\n                        tok, self.special_tokens, self.tokenizer, self.language\n                    )\n\n                if not valid:\n                    local_stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                u = float(U[idx])\n                s = float(S[idx])\n\n                in_dscd = clean_tok in dscd_homographs\n\n                if in_dscd:\n                    priority = 1\n                elif s >= strict_span:\n                    priority = 2\n                elif u >= strict_uncertainty:\n                    priority = 3\n                else:\n                    local_stats[\"tokens_filtered_ambiguity\"] += 1\n                    continue\n\n                candidates.append((idx, u, s, clean_tok, priority, idx))\n\n            with self._stats_lock:\n                self.stats[\"tokens_filtered_word_start\"] += local_stats[\"tokens_filtered_word_start\"]\n                self.stats[\"tokens_filtered_validity\"] += local_stats[\"tokens_filtered_validity\"]\n                self.stats[\"tokens_filtered_ambiguity\"] += local_stats[\"tokens_filtered_ambiguity\"]\n\n            if not candidates:\n                return explanations\n\n            candidates.sort(key=lambda t: (t[4], -t[2], -t[1], t[5]))\n\n            for (token_idx, u, s, clean_tok, priority, _) in candidates[\n                : max_explanations\n            ]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=token_word_map,\n                        decoder_attention=decoder_attention,\n                        is_dscd_homograph=(priority == 1),\n                    )\n                    if explanation_text and evidence:\n                        explanations.append(\n                            {\n                                \"token_idx\": token_idx,\n                                \"token\": (\n                                    token_word_map[token_idx]\n                                    if token_word_map\n                                    and token_idx in token_word_map\n                                    else tokens[token_idx]\n                                    .replace(\"â–\", \"\")\n                                    .replace(\"Ä \", \"\")\n                                ),\n                                \"explanation\": explanation_text,\n                                \"uncertainty\": u,\n                                \"span\": s,\n                                \"dscd_discovered\": (priority == 1),\n                                \"priority\": priority,\n                            }\n                        )\n                except Exception:\n                    continue\n\n        except Exception:\n            pass\n\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self._stats_lock:\n            total = max(self.stats[\"explanations_generated\"], 1)\n            if self.stats[\"explanations_generated\"] > 0:\n                avg_evidence_tokens = (\n                    self.stats[\"total_evidence_tokens\"] / total\n                )\n            else:\n                avg_evidence_tokens = 0.0\n\n            return {\n                **self.stats.copy(),\n                \"high_confidence_rate\": self.stats[\n                    \"high_confidence_explanations\"\n                ]\n                / total,\n                \"low_confidence_rate\": self.stats[\n                    \"low_confidence_explanations\"\n                ]\n                / total,\n                \"empty_evidence_rate\": self.stats[\"empty_evidence_count\"]\n                / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.stats[\n                    \"dscd_homographs_explained\"\n                ]\n                / total,\n            }\n\n    def reset_statistics(self) -> None:\n        with self._stats_lock:\n            self.stats = {\n                \"explanations_generated\": 0,\n                \"high_confidence_explanations\": 0,\n                \"low_confidence_explanations\": 0,\n                \"empty_evidence_count\": 0,\n                \"total_evidence_tokens\": 0,\n                \"tokens_filtered_word_start\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n                \"dscd_homographs_explained\": 0,\n            }\n\n    def clear_silver_buffer(self) -> None:\n        with self._silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[TRG-TEST] Testing\")\n        print(\"=\" * 60)\n\n        if not _ENABLE_TRG_INFERENCE:\n            print(\"TRG inference disabled, enabling for test...\")\n\n        try:\n            tokens = [\"â–à¦†à¦®à¦¿\", \"â–à¦•à¦²\", \"â–à¦¬à¦¨à§à¦§\", \"â–à¦•à¦°à§‡à¦›à¦¿\", \"à¥¤\"]\n\n            dscd_outputs = {\n                \"proto_probs\": [[torch.tensor([0.6, 0.4]) for _ in tokens]],\n                \"uncertainties\": [[0.1, 0.5, 0.2, 0.1, 0.0]],\n                \"span_preds\": [[0.05, 0.3, 0.1, 0.05, 0.0]],\n                \"gates\": [[0.2, 0.8, 0.3, 0.2, 0.0]],\n            }\n\n            token_word_map = {\n                0: \"à¦†à¦®à¦¿\",\n                1: \"à¦•à¦²\",\n                2: \"à¦¬à¦¨à§à¦§\",\n                3: \"à¦•à¦°à§‡à¦›à¦¿\",\n                4: \"à¥¤\",\n            }\n\n            self.eval()\n\n            explanations = self.process_sentence_for_explanations(\n                tokens=tokens,\n                dscd_outputs=dscd_outputs,\n                token_word_map=token_word_map,\n                max_explanations=3,\n            )\n\n            print(f\"  âœ“ Generated {len(explanations)} explanations\")\n\n            if len(explanations) > 0:\n                for i, expl in enumerate(explanations, 1):\n                    print(\n                        f\"    {i}. '{expl['token']}' (u={expl['uncertainty']:.2f})\"\n                    )\n\n            stats = self.get_statistics()\n            print(f\"  âœ“ Stats: {stats['explanations_generated']} total\")\n\n            self.reset_statistics()\n            stats_after = self.get_statistics()\n            assert stats_after[\"explanations_generated\"] == 0\n            print(\"  âœ“ Reset OK\")\n\n            print(\"\\nâœ“ All TRG tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n\n        except Exception as e:\n            print(f\"\\nâœ— Test failed: {e}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Module - PUNCTUATION SAFE\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Uncertainty: ADAPTIVE (base={_TRG_UNCERTAINTY_THRESHOLD:.2f})\")\nprint(f\"  - Span: ADAPTIVE (base={_TRG_SPAN_THRESHOLD:.2f})\")\nprint(f\"  - Temperature: {_TRG_TEMPERATURE:.2f})\")\nprint(f\"  - TAU_HIGH: {_TAU_HIGH:.2f}\") \nprint(f\"  - TAU_LOW: {_TAU_LOW:.2f}\")\nprint(f\"  - TAU_ACCEPT: {_TAU_ACCEPT:.2f}\")\nprint(f\"  - Evidence K: {_TRG_EVIDENCE_K}\")\nprint(f\"  - Max Explanations: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(\"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Added Bengali punctuation to _TRG_PUNCT_SET (lines 99-101)\")\nprint(\"  âœ… FIX #2: Added _is_punctuation_only() comprehensive validator (lines 104-126)\")\nprint(\"  âœ… FIX #3: Updated _fallback_is_valid_token() to use _is_punctuation_only() (line 156)\")\nprint(\"  âœ… FIX #4: Updated _is_word_start() to use _is_punctuation_only() (line 190)\")\nprint(\"  âœ… FIX #5: Added punctuation filter in process_sentence_for_explanations() (line 971)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"svk-wKO7H4J3","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:46.535341Z","iopub.execute_input":"2026-01-16T16:06:46.535568Z","iopub.status.idle":"2026-01-16T16:06:46.651495Z","shell.execute_reply.started":"2026-01-16T16:06:46.535550Z","shell.execute_reply":"2026-01-16T16:06:46.650627Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 5: TRG Module - PUNCTUATION SAFE\n================================================================================\nConfiguration:\n  - Uncertainty: ADAPTIVE (base=0.15)\n  - Span: ADAPTIVE (base=0.20)\n  - Temperature: 1.00)\n  - TAU_HIGH: 0.85\n  - TAU_LOW: 0.15\n  - TAU_ACCEPT: 0.80\n  - Evidence K: 2\n  - Max Explanations: 10\n================================================================================\nCRITICAL FIXES APPLIED:\n  âœ… FIX #1: Added Bengali punctuation to _TRG_PUNCT_SET (lines 99-101)\n  âœ… FIX #2: Added _is_punctuation_only() comprehensive validator (lines 104-126)\n  âœ… FIX #3: Updated _fallback_is_valid_token() to use _is_punctuation_only() (line 156)\n  âœ… FIX #4: Updated _is_word_start() to use _is_punctuation_only() (line 190)\n  âœ… FIX #5: Added punctuation filter in process_sentence_for_explanations() (line 971)\n================================================================================\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TATN MODEL - COMPLETE FIXED VERSION\n# ==============================================================================\n\nfrom typing import List, Dict, Optional, Any, Tuple\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return int(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return float(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return bool(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n\n_DSCD_BUFFER_SIZE = _get_int_global(\"DSCD_BUFFER_SIZE\", 200)\n_DSCD_MAX_PROTOS = _get_int_global(\"DSCD_MAX_PROTOS\", 8)\n_DSCD_N_MIN = _get_int_global(\"DSCD_N_MIN\", 5)\n_DSCD_DISPERSION_THRESHOLD = _get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.35)\n\n_ENABLE_ASBN_TRAINING = _get_bool_global(\"ENABLE_ASBN_TRAINING\", True)\n_ENABLE_TRG_INFERENCE = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\n_MEMORY_CLEANUP_FREQUENCY = _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 100)\n\n_NUM_GPUS = _get_int_global(\n    \"NUM_GPUS\",\n    torch.cuda.device_count() if torch.cuda.is_available() else 1,\n)\n_USE_GC = _get_bool_global(\"GRADIENT_CHECKPOINTING\", True)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\n    \"DSCD_ENABLE_TRAINING_CLUSTERING\", True\n)\n\n_LAMBDA_ASBN = _get_float_global(\"LAMBDA_ASBN\", 0.3)\n_LAMBDA_DSCD = _get_float_global(\"LAMBDA_DSCD\", 0.15)\n_LAMBDA_TOKEN = _get_float_global(\"LAMBDA_TOKEN\", 0.3)\n_LAMBDA_CONFIDENCE = _get_float_global(\"LAMBDA_CONFIDENCE\", 0.2)\n_LAMBDA_LENGTH = _get_float_global(\"LAMBDA_LENGTH\", 0.1)\n\n_VERBOSE_LOGGING = _get_bool_global(\"VERBOSE_LOGGING\", False)\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\n_PERIODIC_DISCOVERY_FREQUENCY = _get_int_global(\n    \"PERIODIC_DISCOVERY_FREQUENCY\", 200\n)\n_VALIDATION_CHECK_INTERVAL = _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 200)\n\n_SPAN_THRESHOLD = _get_float_global(\"SPAN_THRESHOLD\", 0.20)\n_UNCERTAINTY_THRESHOLD = _get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.25)\n\n_TRG_UNCERTAINTY_THRESHOLD = _get_float_global(\n    \"TRG_UNCERTAINTY_THRESHOLD\", _get_float_global(\"TAU_LOW\", 0.15)\n)\n_TAU_LOW = _get_float_global(\"TAU_LOW\", 0.15)\n\n_TRAIN_DOMAIN = _get_int_global(\"TRAIN_DOMAIN\", 0)\n_TEST_DOMAIN = _get_int_global(\"TEST_DOMAIN\", 1)\n_USE_DOMAIN_LABELS = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    _M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_BN_TOKEN_ID = 128012\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\n_BENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n\ndef _is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = (\n        token.replace(\"â–\", \"\")\n        .replace(\"Ä \", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n    \n    if not clean:\n        return False\n    \n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in _COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in _PUNCT_SET for c in clean)\n\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        return enc_output[0]\n    return None\n\n\ndef build_token_word_map_sentencepiece(\n    token_strings: List[str], fallback: bool = True\n) -> Dict[int, str]:\n    word_map: Dict[int, str] = {}\n    current_word = \"\"\n    start_idx = None\n\n    for i, token in enumerate(token_strings):\n        if not token or token.startswith(\"<\") or token.startswith(\"[\"):\n            continue\n\n        if token.startswith(\"â–\"):\n            if current_word and start_idx is not None:\n                clean = current_word.replace(\"â–\", \"\").strip()\n                if clean and len(clean) >= 2 and not _is_punctuation_only(current_word):\n                    word_map[start_idx] = clean\n            current_word = token\n            start_idx = i\n        else:\n            current_word += token\n\n    if current_word and start_idx is not None:\n        clean = current_word.replace(\"â–\", \"\").strip()\n        if clean and len(clean) >= 2 and not _is_punctuation_only(current_word):\n            word_map[start_idx] = clean\n\n    if fallback and not word_map:\n        for i, tok in enumerate(token_strings):\n            clean = tok.replace(\"â–\", \"\").strip()\n            if clean and len(clean) >= 2 and not _is_punctuation_only(tok):\n                word_map[i] = clean\n\n    return word_map\n\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int,\n) -> Dict[str, Any]:\n    defaults = {\n        \"h_augmented\": torch.zeros(\n            batch_size, seq_len, embed_dim, device=device, dtype=torch.float32\n        ),\n        \"proto_probs\": [\n            [\n                torch.tensor([1.0], device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"uncertainties\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"gates\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"span_preds\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"proto_assignments\": [\n            torch.zeros(seq_len, dtype=torch.long, device=device)\n            for _ in range(batch_size)\n        ],\n    }\n\n    if not isinstance(raw, dict):\n        return defaults\n\n    out = defaults.copy()\n\n    try:\n        if \"h_augmented\" in raw and raw[\"h_augmented\"] is not None:\n            h = raw[\"h_augmented\"]\n            if isinstance(h, torch.Tensor) and h.shape == (\n                batch_size,\n                seq_len,\n                embed_dim,\n            ):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = (\n                        h.to(device).reshape(batch_size, seq_len, embed_dim)\n                    )\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    for list_key in (\"proto_probs\", \"uncertainties\", \"gates\", \"span_preds\"):\n        if list_key in raw and raw[list_key] is not None:\n            try:\n                val = raw[list_key]\n                if isinstance(val, list) and len(val) == batch_size:\n                    safe_batch = []\n                    for b_row in val:\n                        if isinstance(b_row, list):\n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < len(b_row):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            safe_row.append(v.detach().to(device))\n                                        else:\n                                            safe_row.append(\n                                                torch.as_tensor(\n                                                    v,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    [1.0],\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                        else:\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    0.0,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                except Exception:\n                                    safe_row.append(\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                    )\n                            safe_batch.append(safe_row)\n                        else:\n                            if list_key == \"proto_probs\":\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            [1.0],\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                            else:\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                    out[list_key] = safe_batch\n            except Exception:\n                pass\n\n    try:\n        if \"proto_assignments\" in raw and raw[\"proto_assignments\"] is not None:\n            pa = raw[\"proto_assignments\"]\n            if isinstance(pa, list) and len(pa) == batch_size:\n                safe_pa = []\n                for b_row in pa:\n                    try:\n                        if isinstance(b_row, torch.Tensor):\n                            safe_pa.append(b_row.detach().to(device).long())\n                        else:\n                            safe_pa.append(\n                                torch.tensor(\n                                    b_row, dtype=torch.long, device=device\n                                )\n                            )\n                    except Exception:\n                        safe_pa.append(\n                            torch.zeros(seq_len, dtype=torch.long, device=device)\n                        )\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n\n    return out\n\n\ndef _norm_scalar_matrix(uncertainties, gates, gate_threshold=0.01):\n    final_normalized = []\n    batch_size = len(uncertainties)\n\n    for b in range(batch_size):\n        u_row = uncertainties[b]\n        g_row = gates[b]\n        seq_len = len(u_row)\n\n        safe_row = []\n        for t in range(seq_len):\n            try:\n                u_val = float(u_row[t]) if t < len(u_row) else 0.0\n                g_val = float(g_row[t]) if t < len(g_row) else 0.0\n\n                if g_val < gate_threshold:\n                    norm_val = 0.0\n                else:\n                    norm_val = max(0.0, min(1.0, u_val))\n\n                safe_row.append(norm_val)\n            except Exception:\n                safe_row.append(0.0)\n\n        final_normalized.append(safe_row)\n\n    return final_normalized\n\n\ndef _norm_proto_probs(proto_probs):\n    return [\n        [pp if isinstance(pp, torch.Tensor) else torch.tensor([1.0]) for pp in row]\n        for row in proto_probs\n    ]\n\n\ndef _to_vec(x):\n    if isinstance(x, torch.Tensor):\n        return x.flatten().tolist()\n    elif isinstance(x, (list, tuple)):\n        return list(x)\n    elif isinstance(x, (int, float)):\n        return [float(x)]\n    else:\n        return [0.0]\n\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n\n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n\n        self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n            \"facebook/m2m100_418M\",\n            torch_dtype=torch.float32,\n            use_cache=False,\n        )\n        try:\n            self.mbart.config.use_cache = False\n        except Exception:\n            pass\n\n        tokenizer_vocab_size = len(self.tokenizer) if hasattr(self.tokenizer, \"__len__\") else getattr(self.tokenizer, \"vocab_size\", 128112)\n        model_vocab_size = int(getattr(self.mbart.config, \"vocab_size\", 128112))\n        \n        if tokenizer_vocab_size != model_vocab_size:\n            print(f\"[TATN-INIT] âš ï¸  Vocab size mismatch detected!\")\n            print(f\"  Tokenizer: {tokenizer_vocab_size}\")\n            print(f\"  Model: {model_vocab_size}\")\n            print(f\"  Resizing model embeddings to {tokenizer_vocab_size}...\")\n            \n            try:\n                self.mbart.resize_token_embeddings(tokenizer_vocab_size)\n                model_vocab_size = tokenizer_vocab_size\n                print(f\"[TATN-INIT] âœ… Model embeddings resized to {tokenizer_vocab_size}\")\n            except Exception as e:\n                print(f\"[TATN-INIT] âŒ Resize failed: {e}\")\n                raise RuntimeError(f\"Cannot resize model embeddings: {e}\")\n        \n        self.vocab_size = model_vocab_size\n\n        try:\n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                en_token_id = self.tokenizer.get_lang_id(_TARGET_LANGUAGE)\n                bn_token_id = self.tokenizer.get_lang_id(_SOURCE_LANGUAGE)\n            elif hasattr(self.tokenizer, \"lang_code_to_id\"):\n                en_token_id = self.tokenizer.lang_code_to_id.get(\n                    _TARGET_LANGUAGE, _M2M100_EN_TOKEN_ID\n                )\n                bn_token_id = self.tokenizer.lang_code_to_id.get(\n                    _SOURCE_LANGUAGE, _M2M100_BN_TOKEN_ID\n                )\n            else:\n                en_token_id = _M2M100_EN_TOKEN_ID\n                bn_token_id = _M2M100_BN_TOKEN_ID\n\n            if en_token_id >= self.vocab_size or bn_token_id >= self.vocab_size:\n                raise ValueError(\n                    f\"Language token IDs out of vocabulary bounds!\\n\"\n                    f\"  EN token: {en_token_id} (vocab: {self.vocab_size})\\n\"\n                    f\"  BN token: {bn_token_id} (vocab: {self.vocab_size})\"\n                )\n\n            self.mbart.config.forced_bos_token_id = int(en_token_id)\n            self.mbart.config.decoder_start_token_id = int(en_token_id)\n            self.en_token_id = int(en_token_id)\n            self.bn_token_id = int(bn_token_id)\n\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN-INIT] Language tokens: BN={bn_token_id}, EN={en_token_id}\"\n                )\n                if bn_token_id != _M2M100_BN_TOKEN_ID or en_token_id != _M2M100_EN_TOKEN_ID:\n                    print(f\"[TATN-INIT] âš ï¸  Token IDs differ from Cell 0 defaults:\")\n                    print(f\"  Expected: BN={_M2M100_BN_TOKEN_ID}, EN={_M2M100_EN_TOKEN_ID}\")\n                    print(f\"  Got: BN={bn_token_id}, EN={en_token_id}\")\n\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Failed to set language tokens: {e}\")\n            raise RuntimeError(f\"Language token setup failed: {e}\")\n\n        try:\n            if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = int(getattr(self.mbart.config, \"d_model\", 1024))\n        \n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-INIT] Model embed_dim: {embed_dim}\")\n\n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            try:\n                self.dscd = dscd_cls(\n                    embed_dim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffer_size=_DSCD_BUFFER_SIZE,\n                    max_protos=_DSCD_MAX_PROTOS,\n                    n_min=_DSCD_N_MIN,\n                    language=_SOURCE_LANGUAGE,\n                    dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                    enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                    max_clustering_points=500,\n                    max_candidates_per_step=1,\n                )\n                \n                dscd_embed_dim = getattr(self.dscd, \"embed_dim\", None)\n                if dscd_embed_dim is not None and dscd_embed_dim != embed_dim:\n                    raise RuntimeError(\n                        f\"DSCD embed_dim mismatch! Expected {embed_dim}, got {dscd_embed_dim}\"\n                    )\n                \n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to instantiate MemoryEfficientDSCDOnline: {e}\"\n                )\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found in globals()\")\n\n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(\n                    embed_dim, tokenizer, language=_SOURCE_LANGUAGE\n                )\n                \n                asbn_embed_dim = getattr(self.asbn, \"embed_dim\", None)\n                if asbn_embed_dim is not None and asbn_embed_dim != embed_dim:\n                    raise RuntimeError(\n                        f\"ASBN embed_dim mismatch! Expected {embed_dim}, got {asbn_embed_dim}\"\n                    )\n                \n            except Exception as e:\n                print(f\"[TATN-INIT] ASBN init failed: {e}, using stub\")\n                self.asbn = self._build_stub_asbn()\n        else:\n            self.asbn = self._build_stub_asbn()\n\n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg_system = trg_cls(\n                    embed_dim,\n                    tokenizer,\n                    language=_SOURCE_LANGUAGE,\n                    dscd_module=self.dscd,\n                )\n            except Exception as e:\n                print(f\"[TATN-INIT] TRG init failed: {e}, using stub\")\n                self.trg_system = self._build_stub_trg()\n        else:\n            self.trg_system = self._build_stub_trg()\n\n        embedding_layer = self.mbart.get_input_embeddings()\n        if embedding_layer is None:\n            raise RuntimeError(\"Model has no input embeddings layer!\")\n        \n        actual_embed_dim = embedding_layer.embedding_dim\n        if actual_embed_dim != embed_dim:\n            raise RuntimeError(\n                f\"Embedding dimension mismatch! Config says {embed_dim}, \"\n                f\"but embedding layer has {actual_embed_dim}\"\n            )\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[TATN-INIT] Initialized MemoryOptimizedTATNWithExplanations:\")\n            print(f\"  - Embed dim: {embed_dim}\")\n            print(f\"  - Vocab size: {self.vocab_size}\")\n            print(f\"  - Tokenizer vocab: {tokenizer_vocab_size}\")\n            print(f\"  - BN token: {self.bn_token_id}\")\n            print(f\"  - EN token: {self.en_token_id}\")\n            print(f\"  - DSCD buffer: {_DSCD_BUFFER_SIZE}\")\n            print(f\"  - DSCD n_min: {_DSCD_N_MIN}\")\n            print(f\"  - DSCD threshold: {_DSCD_DISPERSION_THRESHOLD}\")\n            print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n            print(f\"  - Validation interval: {_VALIDATION_CHECK_INTERVAL}\")\n            print(f\"  - Lambda ASBN: {_LAMBDA_ASBN}\")\n            print(f\"  - Lambda DSCD: {_LAMBDA_DSCD}\")\n            print(f\"  - Lambda Token: {_LAMBDA_TOKEN}\")\n            print(f\"  - Lambda Confidence: {_LAMBDA_CONFIDENCE}\")\n            print(f\"  - Lambda Length: {_LAMBDA_LENGTH}\")\n\n    def _build_stub_asbn(self):\n        class _StubASBN(nn.Module):\n            def forward(self, h, **kwargs):\n                dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                zero = torch.tensor(0.0, device=dev)\n                return h, {\n                    \"encoder_loss\": zero,\n                    \"adversarial_loss\": zero,\n                    \"domain_loss\": zero,\n                    \"domain_accuracy\": zero,\n                }\n\n            def critic_parameters(self):\n                return []\n\n            def reset_stats(self):\n                pass\n\n            def get_detailed_stats(self):\n                return {\n                    \"domain_loss\": 0.0,\n                    \"domain_accuracy\": 0.0,\n                    \"source_accuracy\": 0.0,\n                    \"target_accuracy\": 0.0,\n                    \"asbn_loss\": 0.0,\n                    \"num_updates\": 0,\n                }\n\n            def get_asbn_stats(self):\n                return self.get_detailed_stats()\n\n        return _StubASBN()\n\n    def _build_stub_trg(self):\n        class _StubTRG:\n            def process_sentence_for_explanations(self, *args, **kwargs):\n                return []\n\n            def get_statistics(self):\n                return {}\n\n            def reset_statistics(self):\n                pass\n\n        return _StubTRG()\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(\n        proto_probs_list, gates_list=None, min_gate: float = 0.01\n    ) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n\n        if dev is None:\n            return torch.tensor(0.0)\n\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    try:\n                        if float(gl[j]) < min_gate:\n                            continue\n                    except Exception:\n                        pass\n\n                try:\n                    p = torch.clamp(probs.to(dev).float(), 1e-8, 1.0)\n                    H = -torch.sum(p * torch.log(p))\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n\n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> List[dict]:\n        if token_word_map is not None and len(token_word_map) == batch_size:\n            valid_count = sum(\n                1 for m in token_word_map if isinstance(m, dict) and len(m) > 0\n            )\n            if valid_count == batch_size:\n                if _DEBUG_DISCOVERY:\n                    total_words = sum(len(m) for m in token_word_map)\n                    print(\n                        f\"[TATN-WORDMAP] Using provided word maps: {total_words} words\"\n                    )\n                return token_word_map\n\n        word_maps_batch: List[dict] = []\n\n        for b in range(batch_size):\n            try:\n                ids_b = input_ids[b].detach().cpu().tolist()\n                tokens = self.tokenizer.convert_ids_to_tokens(ids_b)\n                wm = build_token_word_map_sentencepiece(tokens, fallback=True)\n                if wm:\n                    word_maps_batch.append(wm)\n                else:\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n            except Exception:\n                word_maps_batch.append(\n                    {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                )\n\n        total_words = sum(len(m) for m in word_maps_batch)\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructed {total_words} words\")\n\n        return word_maps_batch\n\n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None,\n    ) -> Optional[torch.Tensor]:\n        if not _USE_DOMAIN_LABELS:\n            return None\n\n        try:\n            if self.training:\n                return torch.full(\n                    (batch_size,),\n                    _TRAIN_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n            else:\n                return torch.full(\n                    (batch_size,),\n                    _TEST_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n        except Exception:\n            return None\n\n    @staticmethod\n    def _safe_take_key_static(\n        dscd_struct: Dict[str, Any],\n        key: str,\n        b_index: int,\n        seq_len: int,\n        device: torch.device,\n    ):\n        if key == \"proto_probs\":\n            out = [\n                torch.tensor([1.0], dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n        else:\n            out = [\n                torch.tensor(0.0, dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n\n        try:\n            val = dscd_struct.get(key, None)\n            if val is None:\n                return out\n\n            if key == \"proto_probs\":\n                if isinstance(val, list) and len(val) > b_index:\n                    row = val[b_index]\n                    if isinstance(row, list):\n                        for t in range(min(seq_len, len(row))):\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.detach().to(device)\n                            else:\n                                try:\n                                    out[t] = torch.as_tensor(\n                                        v,\n                                        dtype=torch.float32,\n                                        device=device,\n                                    ).flatten()\n                                except Exception:\n                                    pass\n                return out\n\n            if isinstance(val, list) and len(val) > b_index:\n                row = val[b_index]\n                if isinstance(row, list):\n                    for t in range(min(seq_len, len(row))):\n                        v = row[t]\n                        try:\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.detach().to(device)\n                            else:\n                                out[t] = torch.tensor(\n                                    float(v), device=device\n                                )\n                        except Exception:\n                            pass\n                elif isinstance(row, torch.Tensor):\n                    if row.dim() == 1:\n                        for t in range(min(seq_len, int(row.size(0)))):\n                            try:\n                                out[t] = torch.tensor(\n                                    float(row[t].item()), device=device\n                                )\n                            except Exception:\n                                pass\n                return out\n\n            if isinstance(val, torch.Tensor):\n                if val.dim() >= 2 and int(val.size(0)) > b_index:\n                    for t in range(min(seq_len, int(val.size(1)))):\n                        try:\n                            if val.dim() == 3:\n                                v = val[b_index, t]\n                                if v.numel() == 1:\n                                    out[t] = torch.tensor(\n                                        float(v.item()), device=device\n                                    )\n                                else:\n                                    out[t] = v.detach().to(device)\n                            else:\n                                v = val[b_index, t]\n                                out[t] = torch.tensor(\n                                    float(v.item()), device=device\n                                )\n                        except Exception:\n                            pass\n        except Exception:\n            pass\n\n        return out\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = True,\n        return_dict: bool = True,\n    ):\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(\n                f\"Expected 2D tensors, got {input_ids.shape}, {attention_mask.shape}\"\n            )\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n        \n        if torch.any(input_ids >= self.vocab_size) or torch.any(input_ids < 0):\n            invalid_count = torch.sum((input_ids >= self.vocab_size) | (input_ids < 0)).item()\n            print(f\"[TATN] âŒ CRITICAL: {invalid_count} input_ids out of vocab bounds!\")\n            print(f\"  Vocab size: {self.vocab_size}\")\n            print(f\"  Max input_id: {torch.max(input_ids).item()}\")\n            print(f\"  Min input_id: {torch.min(input_ids).item()}\")\n            input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n\n        if (\n            torch.cuda.is_available()\n            and _MEMORY_CLEANUP_FREQUENCY > 0\n            and current_step % _MEMORY_CLEANUP_FREQUENCY == 0\n        ):\n            for i in range(min(_NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            if gc.isenabled():\n                gc.collect()\n\n        if self.training and _DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if (\n                current_step - self.last_discovery_step\n                >= _PERIODIC_DISCOVERY_FREQUENCY\n            ):\n                try:\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        print(\"\\n\" + \"=\" * 80)\n                        print(f\"[TATN] PERIODIC DISCOVERY @ step {current_step}\")\n                        print(\"=\" * 80)\n\n                    start_time = time.time()\n                    self.dscd.periodic_discovery_check(\n                        current_step, _PERIODIC_DISCOVERY_FREQUENCY\n                    )\n\n                    elapsed = time.time() - start_time\n                    self.last_discovery_step = current_step\n\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        summary = self.dscd.get_prototype_summary()\n                        print(f\"[TATN] Discovery completed in {elapsed:.2f}s\")\n                        print(\n                            f\"[TATN]   Homographs: {summary.get('num_homographs', 0)}\"\n                        )\n                        print(\n                            f\"[TATN]   Total prototypes: {summary.get('total_prototypes', 0)}\"\n                        )\n                        print(\"=\" * 80 + \"\\n\")\n\n                except Exception as e:\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        print(f\"[TATN] Discovery failed: {e}\")\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n        if not self.training and _VALIDATION_CHECK_INTERVAL > 0:\n            if (\n                current_step - self.last_validation_step\n                >= _VALIDATION_CHECK_INTERVAL\n            ):\n                try:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"\\n[TATN-VALIDATION] Step {current_step}\")\n                        summary = self.dscd.get_prototype_summary()\n                        print(f\"  - Tokens: {summary.get('total_tokens', 0)}\")\n                        print(\n                            f\"  - Prototypes: {summary.get('total_prototypes', 0)}\"\n                        )\n                        print(\n                            f\"  - Homographs: {summary.get('num_homographs', 0)}\"\n                        )\n                    self.last_validation_step = current_step\n                except Exception:\n                    pass\n\n        enc_outputs = None\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids, attention_mask=attention_mask\n            )\n        except Exception:\n            try:\n                enc_outputs = self.mbart.get_encoder()(\n                    input_ids=input_ids, attention_mask=attention_mask\n                )\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Encoder failed: {e}\")\n                enc_outputs = None\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            try:\n                embedding_layer = self.mbart.get_input_embeddings()\n                if embedding_layer is None:\n                    raise RuntimeError(\"No embedding layer available\")\n                h = embedding_layer(input_ids).to(device)\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Embedding fallback failed: {e}\")\n                h = torch.zeros(\n                    batch_size,\n                    seq_len,\n                    int(getattr(self.mbart.config, \"d_model\", 1024)),\n                    device=device,\n                )\n\n        embed_dim = int(h.size(-1))\n        training_mode = labels is not None and self.training\n\n        token_word_map = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n\n        domain_labels = self._extract_domain_labels(\n            batch_size, device, src_texts\n        )\n\n        if use_dscd:\n            try:\n                h_detached = h.detach()\n                raw_dscd = self.dscd.forward(\n                    h_detached,\n                    token_types=None,\n                    train_mode=self.training,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_word_map=token_word_map,\n                )\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD forward failed: {e}\")\n                raw_dscd = {\n                    \"h_augmented\": h.detach().clone(),\n                    \"proto_probs\": [\n                        [\n                            torch.tensor(\n                                [1.0],\n                                dtype=torch.float32,\n                                device=device,\n                            )\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"uncertainties\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"gates\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"span_preds\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"proto_assignments\": [\n                        torch.zeros(\n                            seq_len, dtype=torch.long, device=device\n                        )\n                        for _ in range(batch_size)\n                    ],\n                }\n        else:\n            raw_dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [\n                    [\n                        torch.tensor(\n                            [1.0], dtype=torch.float32, device=device\n                        )\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"span_preds\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"proto_assignments\": [\n                    torch.zeros(seq_len, dtype=torch.long, device=device)\n                    for _ in range(batch_size)\n                ],\n            }\n\n        dscd = _normalize_dscd_outputs(\n            raw_dscd, batch_size, seq_len, device, embed_dim\n        )\n        h_aug = dscd.get(\"h_augmented\", h)\n\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN] h_augmented shape mismatch \"\n                    f\"(expected {h.shape}, got {getattr(h_aug, 'shape', None)})\"\n                )\n            h_aug = h\n\n        asbn_loss = torch.tensor(0.0, device=device, requires_grad=True)\n        domain_loss = torch.tensor(0.0, device=device, requires_grad=True)\n        domain_accuracy = torch.tensor(0.0, device=device, requires_grad=True)\n        h_for_decoder = h_aug\n\n        if training_mode and use_asbn and _ENABLE_ASBN_TRAINING and domain_labels is not None:\n            try:\n                if _DEBUG_DISCOVERY and current_step % 100 == 0:\n                    print(f\"\\n[TATN-ASBN] Applying ASBN at step {current_step}\")\n                    print(f\"  Domain labels: {domain_labels.tolist()}\")\n\n                h_asbn, asbn_losses = self.asbn.forward(\n                    h_aug.detach(),\n                    proto_probs=dscd.get(\"proto_probs\", None),\n                    uncertainties=dscd.get(\"uncertainties\", None),\n                    gates=dscd.get(\"gates\", None),\n                    token_word_map=token_word_map,\n                    domain_labels=domain_labels,\n                    global_step=current_step,\n                )\n\n                if isinstance(asbn_losses, dict):\n                    encoder_loss = asbn_losses.get(\"encoder_loss\", torch.tensor(0.0, device=device))\n                    domain_loss = asbn_losses.get(\"domain_loss\", torch.tensor(0.0, device=device))\n                    domain_accuracy = asbn_losses.get(\"domain_accuracy\", torch.tensor(0.0, device=device))\n                    \n                    if isinstance(encoder_loss, torch.Tensor) and torch.isfinite(encoder_loss):\n                        asbn_loss = encoder_loss\n                    else:\n                        asbn_loss = torch.tensor(0.0, device=device, requires_grad=True)\n                else:\n                    asbn_loss = torch.tensor(0.0, device=device, requires_grad=True)\n\n                if isinstance(h_asbn, torch.Tensor) and h_asbn.shape == h_aug.shape:\n                    h_for_decoder = h_asbn\n                else:\n                    h_for_decoder = h_aug\n\n                if _DEBUG_DISCOVERY and current_step % 100 == 0:\n                    print(f\"  ASBN loss: {asbn_loss.item():.4f}\")\n                    print(f\"  Domain loss: {domain_loss.item():.4f}\")\n                    print(f\"  Domain accuracy: {domain_accuracy.item():.2%}\")\n                    print(f\"  Using h_asbn for decoder: {h_for_decoder is h_asbn}\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN-ASBN] ASBN forward failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                asbn_loss = torch.tensor(0.0, device=device, requires_grad=True)\n                domain_loss = torch.tensor(0.0, device=device, requires_grad=True)\n                domain_accuracy = torch.tensor(0.0, device=device, requires_grad=True)\n                h_for_decoder = h_aug\n\n        try:\n            enc_for_decoder = BaseModelOutput(\n                last_hidden_state=h_for_decoder,\n                hidden_states=(\n                    getattr(enc_outputs, \"hidden_states\", None)\n                    if enc_outputs\n                    else None\n                ),\n                attentions=(\n                    getattr(enc_outputs, \"attentions\", None)\n                    if enc_outputs\n                    else None\n                ),\n            )\n        except Exception:\n            enc_for_decoder = (h_for_decoder,)\n\n        if training_mode:\n            try:\n                pad_id = getattr(self.tokenizer, 'pad_token_id', 1)\n                \n                if labels is not None:\n                    decoder_input_ids = labels.clone()\n                    decoder_input_ids = torch.where(\n                        decoder_input_ids == -100,\n                        torch.full_like(\n                            decoder_input_ids,\n                            pad_id,\n                        ),\n                        decoder_input_ids,\n                    )\n\n                    bos_column = torch.full(\n                        (batch_size, 1),\n                        int(self.mbart.config.decoder_start_token_id),\n                        dtype=torch.long,\n                        device=device,\n                    )\n                    decoder_input_ids = torch.cat(\n                        [bos_column, decoder_input_ids[:, :-1]], dim=1\n                    )\n                    decoder_attention_mask = (\n                        decoder_input_ids != pad_id\n                    ).long()\n                else:\n                    decoder_input_ids = None\n                    decoder_attention_mask = None\n\n                seq_outputs = self.mbart(\n                    input_ids=None,\n                    attention_mask=attention_mask,\n                    encoder_outputs=enc_for_decoder,\n                    decoder_input_ids=decoder_input_ids,\n                    decoder_attention_mask=decoder_attention_mask,\n                    labels=labels,\n                    use_cache=False,\n                    return_dict=True,\n                )\n                \n                translation_loss = getattr(seq_outputs, \"loss\", None)\n                if translation_loss is None or not torch.isfinite(translation_loss):\n                    translation_loss = torch.tensor(10.0, device=device, requires_grad=True)\n                else:\n                    translation_loss = torch.clamp(translation_loss, 0.0, 100.0)\n                \n                token_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n                confidence_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n                length_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n                \n                try:\n                    logits = seq_outputs.logits\n                    predicted_ids = torch.argmax(logits, dim=-1)\n                    \n                    reference_ids = labels\n                    valid_mask = (reference_ids != -100) & (reference_ids != pad_id)\n                    \n                    if valid_mask.sum() > 0:\n                        mismatches = (predicted_ids != reference_ids) & valid_mask\n                        mismatch_rate = mismatches.float().sum() / valid_mask.float().sum()\n                        token_penalty = (mismatch_rate.detach() * 2.0).requires_grad_(True)\n                        \n                        probs = torch.softmax(logits, dim=-1)\n                        max_probs = torch.max(probs, dim=-1)[0]\n                        wrong_positions = (predicted_ids != reference_ids) & valid_mask\n                        \n                        if wrong_positions.sum() > 0:\n                            wrong_confidences = max_probs[wrong_positions]\n                            confidence_penalty = (wrong_confidences.mean().detach() * 1.5).requires_grad_(True)\n                        \n                        pred_lengths = valid_mask.sum(dim=1).float()\n                        ref_lengths = (reference_ids != -100).sum(dim=1).float()\n                        length_diff = torch.abs(pred_lengths - ref_lengths) / torch.clamp(ref_lengths, min=1.0)\n                        length_penalty = (length_diff.mean().detach() * 0.5).requires_grad_(True)\n                \n                except Exception as e:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"âš ï¸ Penalty computation failed: {e}\")\n                \n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Decoder forward failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                translation_loss = torch.tensor(10.0, device=device, requires_grad=True)\n                token_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n                confidence_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n                length_penalty = torch.tensor(0.0, device=device, requires_grad=True)\n\n            dscd_reg = torch.tensor(0.0, device=device, requires_grad=True)\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(\n                    dscd.get('proto_probs', []),\n                    gates_list=dscd.get('gates', []),\n                    min_gate=0.01,\n                )\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(float(dscd_reg), device=device, requires_grad=True)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device, requires_grad=True)\n                else:\n                    dscd_reg = torch.clamp(dscd_reg.to(device), 0.0, 5.0)\n            except Exception:\n                dscd_reg = torch.tensor(0.0, device=device, requires_grad=True)\n\n            self._last_translation_loss = float(translation_loss.item()) if isinstance(translation_loss, torch.Tensor) else 0.0\n            self._last_asbn_loss = float(asbn_loss.item()) if isinstance(asbn_loss, torch.Tensor) else 0.0\n            self._last_domain_loss = float(domain_loss.item()) if isinstance(domain_loss, torch.Tensor) else 0.0\n            self._last_domain_accuracy = float(domain_accuracy.item()) if isinstance(domain_accuracy, torch.Tensor) else 0.0\n            self._last_dscd_loss = float(dscd_reg.item()) if isinstance(dscd_reg, torch.Tensor) else 0.0\n            self._last_token_penalty = float(token_penalty.item()) if isinstance(token_penalty, torch.Tensor) else 0.0\n            self._last_confidence_penalty = float(confidence_penalty.item()) if isinstance(confidence_penalty, torch.Tensor) else 0.0\n            self._last_length_penalty = float(length_penalty.item()) if isinstance(length_penalty, torch.Tensor) else 0.0\n\n            total_loss = (\n                translation_loss +\n                _LAMBDA_ASBN * asbn_loss +\n                _LAMBDA_TOKEN * token_penalty +\n                _LAMBDA_CONFIDENCE * confidence_penalty +\n                _LAMBDA_LENGTH * length_penalty +\n                _LAMBDA_DSCD * dscd_reg\n            )\n\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device, requires_grad=True)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n\n            if not torch.isfinite(total_loss):\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"[TATN] NaN/Inf in total_loss ({total_loss}), using translation loss\"\n                    )\n                total_loss = translation_loss\n            \n            if _DEBUG_DISCOVERY and current_step % 100 == 0:\n                print(f\"\\n{'='*60}\")\n                print(f\"LOSS BREAKDOWN (Step {current_step}):\")\n                print(f\"  Translation: {translation_loss.item():.4f}\")\n                print(f\"  ASBN Loss: {asbn_loss.item():.4f} (Ã—{_LAMBDA_ASBN})\")\n                print(f\"  Domain Loss: {domain_loss.item():.4f}\")\n                print(f\"  Domain Accuracy: {domain_accuracy.item():.2%}\")\n                print(f\"  Token Penalty: {token_penalty.item():.4f} (Ã—{_LAMBDA_TOKEN})\")\n                print(f\"  Confidence Penalty: {confidence_penalty.item():.4f} (Ã—{_LAMBDA_CONFIDENCE})\")\n                print(f\"  Length Penalty: {length_penalty.item():.4f} (Ã—{_LAMBDA_LENGTH})\")\n                print(f\"  DSCD Reg: {dscd_reg.item():.4f} (Ã—{_LAMBDA_DSCD})\")\n                print(f\"  TOTAL: {total_loss.item():.4f}\")\n                print(f\"{'='*60}\\n\")\n\n            try:\n                del enc_outputs, h, raw_dscd\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            return total_loss\n\n        if not return_dict:\n            return h_for_decoder\n\n        explanations_list: List[List[Dict[str, Any]]] = []\n\n        if _ENABLE_TRG_INFERENCE:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"\\n[TATN-INFERENCE] Starting TRG for {batch_size} samples\"\n                )\n\n            tokens_batch: List[List[str]] = []\n\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    if hasattr(self.tokenizer, \"convert_ids_to_tokens\"):\n                        toks = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    else:\n                        toks = []\n                    if not toks:\n                        toks = [\"UNK\"] * seq_len\n                    elif len(toks) < seq_len:\n                        toks = toks + [\"\"] * (seq_len - len(toks))\n                    elif len(toks) > seq_len:\n                        toks = toks[:seq_len]\n                except Exception:\n                    toks = [\"UNK\"] * seq_len\n\n                tokens_batch.append(toks)\n\n            decoder_attention = None\n\n            try:\n                total_explanations = 0\n                for b in range(batch_size):\n                    per_sent = {\n                        \"proto_probs\": self._safe_take_key_static(\n                            dscd, \"proto_probs\", b, seq_len, device\n                        ),\n                        \"uncertainties\": self._safe_take_key_static(\n                            dscd, \"uncertainties\", b, seq_len, device\n                        ),\n                        \"gates\": self._safe_take_key_static(\n                            dscd, \"gates\", b, seq_len, device\n                        ),\n                        \"span_preds\": self._safe_take_key_static(\n                            dscd, \"span_preds\", b, seq_len, device\n                        ),\n                    }\n\n                    try:\n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=(\n                                token_word_map[b]\n                                if token_word_map\n                                and b < len(token_word_map)\n                                else None\n                            ),\n                            uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                            decoder_attention=decoder_attention,\n                        )\n                        batch_exps = exps if isinstance(exps, list) else []\n                        explanations_list.append(batch_exps)\n                        total_explanations += len(batch_exps)\n\n                        if _DEBUG_DISCOVERY and b < 2:\n                            print(\n                                f\"[TATN-INFERENCE] Sample {b}: \"\n                                f\"{len(batch_exps)} explanations\"\n                            )\n\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(\n                                f\"[TATN-INFERENCE] TRG failed for sample {b}: {e}\"\n                            )\n                        explanations_list.append([])\n\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"\\n[TATN-INFERENCE] Total explanations: {total_explanations}\"\n                    )\n                    if total_explanations == 0:\n                        print(\"[TATN-INFERENCE] NO EXPLANATIONS GENERATED\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN-INFERENCE] TRG generation failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                explanations_list = [[] for _ in range(batch_size)]\n        else:\n            explanations_list = [[] for _ in range(batch_size)]\n\n        outputs = {\n            \"encoder_outputs\": enc_outputs,\n            \"dscd_outputs\": dscd,\n            \"sense_augmented_embeddings\": h_aug,\n            \"explanations\": explanations_list,\n            \"asbn_loss\": asbn_loss,\n            \"domain_loss\": domain_loss,\n            \"domain_accuracy\": domain_accuracy,\n            \"ambiguity_signals\": {\n                \"span\": dscd.get(\"span_preds\", []),\n                \"uncertainty\": dscd.get(\"uncertainties\", []),\n                \"confidence\": [\n                    [\n                        1.0\n                        - (\n                            float(u)\n                            if isinstance(u, (float, int))\n                            else (\n                                float(u.item())\n                                if isinstance(u, torch.Tensor)\n                                else 1.0\n                            )\n                        )\n                        for u in row\n                    ]\n                    for row in dscd.get(\"uncertainties\", [])\n                ],\n                \"proto_probs\": dscd.get(\"proto_probs\", []),\n            },\n        }\n\n        try:\n            del h, raw_dscd\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return outputs\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n        )\n\n    def forward_with_dscd_for_inference(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> Dict[str, Any]:\n        was_training = self.training\n        \n        try:\n            self.eval()\n            \n            with torch.inference_mode():\n                out = self.forward(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    src_texts=src_texts,\n                    token_word_map=token_word_map,\n                    labels=None,\n                    use_dscd=True,\n                )\n            \n            if not isinstance(out, dict):\n                return {\n                    \"sense_augmented_embeddings\": None,\n                    \"dscd_outputs\": {},\n                    \"explanations\": [],\n                }\n            \n            dscd_outputs = out.get(\"dscd_outputs\", {})\n            h_augmented = dscd_outputs.get(\"h_augmented\", None)\n            \n            return {\n                \"sense_augmented_embeddings\": h_augmented,\n                \"dscd_outputs\": dscd_outputs,\n                \"explanations\": out.get(\"explanations\", []),\n                \"h_augmented\": h_augmented,\n            }\n            \n        finally:\n            if was_training:\n                self.train()\n\n    def generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[BaseModelOutput] = None,\n        max_length: int = 128,\n        num_beams: int = 5,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> torch.Tensor:\n        try:\n            if encoder_outputs is not None:\n                if _DEBUG_DISCOVERY:\n                    print(\"[TATN-GENERATE] Using pre-computed encoder outputs (DSCD-augmented)\")\n                enc_wrapped = encoder_outputs\n            else:\n                if input_ids is None or attention_mask is None:\n                    raise ValueError(\n                        \"Either encoder_outputs or (input_ids + attention_mask) must be provided\"\n                    )\n                \n                if _DEBUG_DISCOVERY:\n                    print(\"[TATN-GENERATE] Running encoder (no DSCD augmentation)\")\n                \n                enc_outputs = self.mbart.model.encoder(\n                    input_ids=input_ids, attention_mask=attention_mask\n                )\n\n                enc_wrapped = BaseModelOutput(\n                    last_hidden_state=(\n                        enc_outputs.last_hidden_state\n                        if hasattr(enc_outputs, \"last_hidden_state\")\n                        else enc_outputs[0]\n                    ),\n                    hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                    attentions=getattr(enc_outputs, \"attentions\", None),\n                )\n\n            return self.mbart.generate(\n                input_ids=None,\n                attention_mask=attention_mask,\n                encoder_outputs=enc_wrapped,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=early_stopping,\n                forced_bos_token_id=kwargs.pop('forced_bos_token_id', int(self.mbart.config.forced_bos_token_id or 128022)),\n                **kwargs,\n            )\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-GENERATE] Failed: {e}\")\n            raise\n\n    def get_component_stats(self) -> Dict[str, Any]:\n        stats: Dict[str, Any] = {\n            \"global_step\": self.global_step,\n            \"last_discovery_step\": self.last_discovery_step,\n            \"last_validation_step\": self.last_validation_step,\n        }\n\n        try:\n            if hasattr(self.dscd, \"get_prototype_summary\"):\n                stats[\"dscd\"] = self.dscd.get_prototype_summary()\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.asbn, \"get_detailed_stats\"):\n                stats[\"asbn\"] = self.asbn.get_detailed_stats()\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.trg_system, \"get_statistics\"):\n                stats[\"trg\"] = self.trg_system.get_statistics()\n        except Exception:\n            pass\n\n        return stats\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 6: TATN Model - PRODUCTION READY\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - DSCD buffer: {_DSCD_BUFFER_SIZE} (fixed from 50)\")\nprint(f\"  - DSCD n_min: {_DSCD_N_MIN} (fixed from 1)\")\nprint(f\"  - DSCD threshold: {_DSCD_DISPERSION_THRESHOLD} (fixed from 0.20)\")\nprint(f\"  - DSCD: {'ENABLED' if _DSCD_ENABLE_TRAINING_CLUSTERING else 'DISABLED'}\")\nprint(f\"  - ASBN: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\nprint(f\"  - TRG: {'ENABLED' if _ENABLE_TRG_INFERENCE else 'DISABLED'}\")\nprint(f\"  - Discovery Frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\nprint(f\"  - Memory Cleanup: Every {_MEMORY_CLEANUP_FREQUENCY} steps\")\nprint(\"\\nLoss Components:\")\nprint(f\"  - Lambda ASBN: {_LAMBDA_ASBN}\")\nprint(f\"  - Lambda DSCD: {_LAMBDA_DSCD}\")\nprint(f\"  - Lambda Token: {_LAMBDA_TOKEN}\")\nprint(f\"  - Lambda Confidence: {_LAMBDA_CONFIDENCE}\")\nprint(f\"  - Lambda Length: {_LAMBDA_LENGTH}\")\nprint(\"=\" * 80)\n","metadata":{"id":"KZbMDpIYH4J4","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:06:46.653078Z","iopub.execute_input":"2026-01-16T16:06:46.653415Z","iopub.status.idle":"2026-01-16T16:07:01.513565Z","shell.execute_reply.started":"2026-01-16T16:06:46.653393Z","shell.execute_reply":"2026-01-16T16:07:01.512612Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768579609.139812      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768579609.188169      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768579609.579216      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768579609.579254      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768579609.579257      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768579609.579259      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nCell 6: TATN Model - PRODUCTION READY\n================================================================================\nConfiguration:\n  - DSCD buffer: 20 (fixed from 50)\n  - DSCD n_min: 3 (fixed from 1)\n  - DSCD threshold: 0.35 (fixed from 0.20)\n  - DSCD: ENABLED\n  - ASBN: ENABLED\n  - TRG: ENABLED\n  - Discovery Frequency: 200\n  - Memory Cleanup: Every 50 steps\n\nLoss Components:\n  - Lambda ASBN: 0.3\n  - Lambda DSCD: 0.15\n  - Lambda Token: 0.3\n  - Lambda Confidence: 0.2\n  - Lambda Length: 0.1\n================================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP - COMPLETE FIXED VERSION\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept (NameError, ValueError, TypeError):\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept (NameError, ValueError, TypeError):\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept (NameError, ValueError, TypeError):\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept (NameError, ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept (NameError, TypeError):\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept (NameError, ValueError, TypeError):\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _PERIODIC_DISCOVERY_FREQUENCY = 200\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n        \"à¦ªà¦¾à¦¨à¦¿\", \"à¦¦à¦²\", \"à¦¬à¦¾à¦œà¦¾à¦°\", \"à¦¨à¦¾à¦®\", \"à¦•à¦¥à¦¾\", \"à¦¬à¦‡\", \"à¦˜à¦°\", \"à¦®à¦¨\", \"à¦¹à¦¾à¦¤\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n_BENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\n_COMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\n_PUNCT_SET = _BENGALI_PUNCT_SET | _COMMON_PUNCT_SET\n\n\ndef _is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = (\n        token.replace(\"â–\", \"\")\n        .replace(\"Ä \", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n    \n    if not clean:\n        return False\n    \n    if clean in _BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in _COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in _PUNCT_SET for c in clean)\n\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            discovered = dscd.get_discovered_homographs()\n            return set(w for w in discovered if not _is_punctuation_only(w))\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        stores_snapshot = {}\n        if lock:\n            with lock:\n                stores_snapshot = dict(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = dict(dscd.prototype_stores.items())\n\n        for token, store in stores_snapshot.items():\n            try:\n                if store.size() >= 1:\n                    clean_token = str(token).replace('â–', '').replace('Ä ', '').replace('##', '').strip().lower()\n                    if clean_token and not _is_punctuation_only(clean_token):\n                        homographs.add(clean_token)\n            except Exception:\n                continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module,\n    tokenizer,\n    epoch: int,\n    global_step: int,\n    source_lang: str,\n    target_lang: str,\n    max_length: int,\n    device: torch.device\n) -> Dict[str, Any]:\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n\n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n\n    if not isinstance(device, torch.device):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dscd_homographs = _get_dscd_homographs(model)\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[VALIDATION] DSCD discovered homographs: {len(dscd_homographs)}\")\n        if dscd_homographs:\n            print(f\"[VALIDATION] Sample: {list(dscd_homographs)[:10]}\")\n\n    validation_results = {\n        'epoch': epoch,\n        'step': global_step,\n        'translations_success': 0,\n        'translations_failed': 0,\n        'explanations_generated': 0,\n        'dscd_homographs_explained': 0,\n        'reference_homographs_explained': 0,\n        'avg_explanation_confidence': 0.0,\n        'dscd_quality_score': 0.0,\n        'dscd_multi_sense_tokens': 0,\n        'dscd_total_prototypes': 0,\n        'asbn_domain_loss': 0.0,\n        'asbn_domain_accuracy': 0.0,\n        'asbn_source_accuracy': 0.0,\n        'asbn_target_accuracy': 0.0,\n        'trg_total_explanations': 0,\n        'validation_completed': False,\n    }\n\n    try:\n        core_model.eval()\n\n        val_sentences = [\n            (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap\", \"à¦•à¦²=tap/call\"),\n            (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"Tomorrow I will buy a book\", \"à¦•à¦¾à¦²=tomorrow/yesterday\"),\n            (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"The leaf has fallen\", \"à¦ªà¦¾à¦¤à¦¾=leaf/page\"),\n            (\"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\", \"He went to the bank\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•=bank/embankment\"),\n            (\"à¦†à¦®à¦¿ à¦­à¦¾à¦²à§‹ à¦†à¦›à¦¿à¥¤\", \"I am fine\", \"No ambiguity\"),\n            (\"à¦¸à§‡ à¦–à§à¦¬ à¦®à¦¿à¦·à§à¦Ÿà¦¿ à¦•à¦¥à¦¾ à¦¬à¦²à§‡à¥¤\", \"She speaks sweetly\", \"No ambiguity\"),\n            (\"à¦à¦Ÿà¦¾ à¦†à¦®à¦¾à¦° à¦¬à¦‡à¥¤\", \"This is my book\", \"No ambiguity\"),\n            (\"à¦†à¦œ à¦†à¦¬à¦¹à¦¾à¦“à¦¯à¦¼à¦¾ à¦­à¦¾à¦²à§‹à¥¤\", \"Weather is good today\", \"No ambiguity\"),\n            (\"à¦«à¦² à¦–à§à¦¬ à¦¸à§à¦¸à§à¦¬à¦¾à¦¦à§à¥¤\", \"The fruit is delicious\", \"à¦«à¦²=fruit/result\"),\n            (\"à¦®à¦¾à¦¥à¦¾ à¦¬à§à¦¯à¦¥à¦¾ à¦•à¦°à¦›à§‡à¥¤\", \"Head is aching\", \"à¦®à¦¾à¦¥à¦¾=head/top\"),\n        ]\n\n        print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n        print(\"-\" * 80)\n\n        confidences = []\n        dscd_homograph_words_detected = set()\n        reference_homograph_words_detected = set()\n\n        try:\n            try:\n                tokenizer.src_lang = source_lang\n                tokenizer.tgt_lang = target_lang\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    actual_src = getattr(tokenizer, 'src_lang', 'NOT_SET')\n                    actual_tgt = getattr(tokenizer, 'tgt_lang', 'NOT_SET')\n                    print(f\"[VALIDATION] Tokenizer languages set: src={actual_src}, tgt={actual_tgt}\")\n            except Exception as e:\n                print(f\"[VALIDATION] Warning: Could not set tokenizer languages: {type(e).__name__}\")\n\n            for idx, (src, expected, note) in enumerate(val_sentences, 1):\n                try:\n                    translation = \"\"\n                    explanation_status = \"\"\n                    error_detail = \"\"\n                    \n                    if 'translate_with_explanations' in globals():\n                        try:\n                            res = translate_with_explanations(\n                                model, \n                                tokenizer, \n                                src,\n                                source_lang=source_lang,\n                                target_lang=target_lang\n                            )\n                            translation = str(res.get('translation', ''))\n                            exps = res.get('explanations', [])\n                            error_info = res.get('error', '')\n                            \n                            if translation and translation.strip():\n                                validation_results['translations_success'] += 1\n                            else:\n                                validation_results['translations_failed'] += 1\n                                if error_info:\n                                    error_detail = f\" ({error_info})\"\n                                else:\n                                    error_detail = \" (empty result)\"\n                            \n                            validation_results['explanations_generated'] += len(exps)\n                            \n                            if exps:\n                                explanation_status = f\"{len(exps)} expl\"\n                                for exp in exps:\n                                    try:\n                                        conf = exp.get('confidence', 0.5)\n                                        confidences.append(float(conf))\n                                        \n                                        word = exp.get('ambiguous_word', '').strip()\n                                        clean_word = word.replace('â–', '').replace('Ä ', '').lower()\n                                        \n                                        if clean_word and not _is_punctuation_only(clean_word):\n                                            if clean_word in dscd_homographs:\n                                                validation_results['dscd_homographs_explained'] += 1\n                                                dscd_homograph_words_detected.add(clean_word)\n                                            \n                                            if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                                                validation_results['reference_homographs_explained'] += 1\n                                                reference_homograph_words_detected.add(clean_word)\n                                    except Exception:\n                                        pass\n                            else:\n                                explanation_status = \"no expl\"\n                        except Exception as e:\n                            explanation_status = f\"error: {type(e).__name__}\"\n                            error_detail = f\" ({str(e)[:50]})\"\n                            translation = \"\"\n                            validation_results['translations_failed'] += 1\n                    else:\n                        explanation_status = \"unavailable\"\n                        error_detail = \" (function not found)\"\n                        validation_results['translations_failed'] += 1\n\n                    if translation and translation.strip():\n                        print(f\"  {idx:2d}. {explanation_status:15s} {note[:30]:30s} -> {translation[:200]}\")\n                    else:\n                        print(f\"  {idx:2d}. Translation failed: {note[:30]:30s}{error_detail}\")\n                        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                            print(f\"       [DEBUG] Bengali input was: {src[:50]}\")\n\n                except Exception as e:\n                    validation_results['translations_failed'] += 1\n                    print(f\"  {idx:2d}. ERROR: {note[:30]:30s} -> {type(e).__name__}\")\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n        finally:\n            if torch.cuda.is_available():\n                try:\n                    torch.cuda.synchronize()\n                except Exception:\n                    pass\n            clear_all_gpu_caches()\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n        try:\n            dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n            if dscd and hasattr(dscd, 'validate_prototypes'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        quality_results = dscd.validate_prototypes(cluster_missing=False)\n                else:\n                    quality_results = dscd.validate_prototypes(cluster_missing=False)\n\n                validation_results['dscd_quality_score'] = quality_results.get('quality_score', 0.0)\n                validation_results['dscd_multi_sense_tokens'] = quality_results.get('multi_sense_tokens', 0)\n                validation_results['dscd_total_prototypes'] = quality_results.get('total_prototypes', 0)\n                print(f\"  - Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n                print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n                print(f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\")\n            else:\n                print(\"  - Validation not available\")\n        except Exception as e:\n            print(f\"  - Validation failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] ASBN Training Statistics:\")\n        try:\n            asbn = core_model.asbn if hasattr(core_model, 'asbn') else None\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                asbn_stats = asbn.get_detailed_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get('domain_loss', 0.0)\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get('domain_accuracy', 0.0)\n                validation_results['asbn_source_accuracy'] = asbn_stats.get('source_accuracy', 0.0)\n                validation_results['asbn_target_accuracy'] = asbn_stats.get('target_accuracy', 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n                print(f\"  - Source Accuracy: {validation_results['asbn_source_accuracy']:.2%}\")\n                print(f\"  - Target Accuracy: {validation_results['asbn_target_accuracy']:.2%}\")\n            elif asbn and hasattr(asbn, 'get_asbn_stats'):\n                asbn_stats = asbn.get_asbn_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get('domain_loss', 0.0)\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get('domain_accuracy', 0.0)\n                print(f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\")\n                print(f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n            else:\n                print(\"  - ASBN statistics not available\")\n        except Exception as e:\n            print(f\"  - ASBN stats retrieval failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] TRG Explanation Statistics:\")\n        try:\n            trg = core_model.trg_system if hasattr(core_model, 'trg_system') else None\n            if trg and hasattr(trg, 'get_statistics'):\n                trg_stats = trg.get_statistics()\n                validation_results['trg_total_explanations'] = trg_stats.get('explanations_generated', 0)\n                print(f\"  - Total explanations: {validation_results['trg_total_explanations']}\")\n                print(f\"  - High confidence rate: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                print(f\"  - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n            else:\n                print(\"  - TRG statistics not available\")\n        except Exception as e:\n            print(f\"  - TRG stats retrieval failed: {type(e).__name__}\")\n\n        if confidences:\n            validation_results['avg_explanation_confidence'] = sum(confidences) / len(confidences)\n\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\")\n        print(f\"  - Explanations generated: {validation_results['explanations_generated']}\")\n        print(f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\")\n        print(f\"  - DSCD homographs explained: {validation_results['dscd_homographs_explained']}\")\n        print(f\"  - Reference homographs explained: {validation_results['reference_homographs_explained']}\")\n\n        if dscd_homograph_words_detected:\n            print(f\"  - DSCD homographs detected: {', '.join(sorted(dscd_homograph_words_detected))}\")\n\n        print(f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n        print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n        print(f\"  - ASBN Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\")\n\n        warnings = []\n        if validation_results['translations_failed'] > len(val_sentences) // 2:\n            warnings.append(\"High translation failure rate\")\n        if validation_results['explanations_generated'] == 0:\n            warnings.append(\"No explanations generated\")\n        if validation_results['dscd_quality_score'] < 0.3:\n            warnings.append(\"Low DSCD quality score\")\n        if validation_results['dscd_multi_sense_tokens'] < 10:\n            warnings.append(\"Very few multi-sense tokens\")\n\n        if warnings:\n            print(\"\\n[VALIDATION] Health Warnings:\")\n            for w in warnings:\n                print(f\"  - {w}\")\n        else:\n            print(\"\\n[VALIDATION] All systems healthy\")\n\n        validation_results['validation_completed'] = True\n\n    except Exception as e:\n        print(f\"\\n[VALIDATION] Critical error: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n        validation_results['validation_completed'] = False\n\n    finally:\n        if was_training:\n            core_model.train()\n        clear_all_gpu_caches()\n\n    print(\"=\" * 80 + \"\\n\")\n    return validation_results\n\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return 0\n\n        stores = getattr(dscd, 'prototype_stores', None)\n        if stores is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                return len(stores)\n        else:\n            return len(stores)\n\n    except Exception:\n        return 0\n\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        return getattr(core, 'dscd', None)\n    except Exception:\n        return None\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n\n    try:\n        dscd_homographs = _get_dscd_homographs(model)\n        items = []\n        homograph_items = []\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores_snapshot = list(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = list(dscd.prototype_stores.items())\n\n        for token, store in stores_snapshot:\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                protos = store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n                clean_token = str(token).replace('â–', '').replace('Ä ', '').strip().lower()\n                \n                if _is_punctuation_only(clean_token):\n                    continue\n                \n                is_homograph = clean_token in dscd_homographs\n                item = (\n                    token,\n                    total_count,\n                    protos,\n                    len(dscd.buffers.get(token, [])) if hasattr(dscd, 'buffers') else 0,\n                    is_homograph,\n                )\n                items.append(item)\n                if is_homograph:\n                    homograph_items.append(item)\n            except Exception:\n                continue\n\n        items.sort(key=lambda x: x[1], reverse=True)\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen, is_homo) in enumerate(items[:top_n], 1):\n                marker = \"HOMO\" if is_homo else \"    \"\n                print(f\"{marker} {i:2d}. {str(tok)[:20]:20s} samples={cnt:4d} protos={prot} buf={buflen}\")\n            if homograph_items:\n                print(f\"[CLUSTER-DBG] DSCD-discovered homographs: {len(homograph_items)}\")\n                for tok, cnt, prot, buflen, _ in homograph_items[:5]:\n                    print(f\"  HOMO {str(tok)[:20]:20s} samples={cnt:4d} protos={prot}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}\")\n\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return\n\n        if hasattr(dscd, 'get_prototype_summary'):\n            summary = dscd.get_prototype_summary()\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Step {global_step}:\")\n                print(f\"  - Total tokens: {summary.get('total_tokens', 0)}\")\n                print(f\"  - Homographs: {summary.get('num_homographs', 0)}\")\n                print(f\"  - Total prototypes: {summary.get('total_prototypes', 0)}\")\n                print(f\"  - Quality score: {summary.get('quality_score', 0.0):.1%}\")\n\n        if hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Discovery events: {total_discovered}\")\n\n                recent = dscd.discovered_log[-3:] if len(dscd.discovered_log) >= 3 else dscd.discovered_log\n                for entry in recent:\n                    discovered = entry.get('discovered', 0)\n                    candidates = entry.get('candidates', 0)\n                    print(f\"  - {discovered}/{candidates} homographs discovered\")\n        else:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] No discoveries yet at step {global_step}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[DISCOVERY-STATUS] Error: {e}\")\n\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n\n    print(f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, accum_steps={accumulation_steps}\")\n    print(f\"[TRAIN] Validation: {'enabled' if enable_validation and validate_every > 0 else 'disabled'}\")\n    print(f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\")\n    print(f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(\"[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt after all epochs\\n\")\n\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"asbn_domain_loss_history\": [],\n        \"trg_explanation_history\": [],\n        \"discovery_runs\": 0,\n        \"discovery_homographs_found\": 0,\n    }\n\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n    cached_cluster_count = 0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n\n        print(f\"\\n{'='*80}\")\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(f\"{'='*80}\\n\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'reset_statistics'):\n                try:\n                    trg.reset_statistics()\n                    print(f\"[TRAIN] TRG statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] TRG stats reset failed: {e}\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'reset_stats'):\n                try:\n                    asbn.reset_statistics()\n                    print(f\"[TRAIN] ASBN statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] ASBN stats reset failed: {e}\")\n\n        try:\n            optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n\n        progress = None\n        batch_idx = 0\n        \n        try:\n            progress = tqdm(\n                total=len(train_loader),\n                desc=f\"Epoch {epoch}/{epochs}\",\n                ncols=110,\n                leave=False,\n                position=0,\n                file=sys.stdout\n            )\n\n            for batch in train_loader:\n                batch_idx += 1\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n\n                if (_DEBUG_DISCOVERY or _VERBOSE_LOGGING) and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    print(f\"\\n[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} GlobalStep {global_step}\")\n                    _check_discovery_status(model, global_step)\n\n                if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                    if global_step % _PERIODIC_DISCOVERY_FREQUENCY == 0:\n                        try:\n                            core = model.module if hasattr(model, 'module') else model\n                            dscd = getattr(core, 'dscd', None)\n                            if dscd and hasattr(dscd, 'periodic_discovery_check'):\n                                print(f\"\\n[DISCOVERY] Running periodic check at step {global_step}...\")\n                                num_discovered = dscd.periodic_discovery_check(\n                                    global_step=global_step,\n                                    frequency=_PERIODIC_DISCOVERY_FREQUENCY,\n                                    cluster_missing=False\n                                )\n                                training_stats['discovery_runs'] += 1\n                                training_stats['discovery_homographs_found'] += num_discovered\n                                if num_discovered > 0:\n                                    print(f\"[DISCOVERY] Found {num_discovered} new homographs!\")\n                                else:\n                                    print(f\"[DISCOVERY] No new homographs found this check\")\n                                \n                                cached_cluster_count = _get_cluster_count(model)\n                        except Exception as e:\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                print(f\"[DISCOVERY] Periodic check failed: {type(e).__name__}: {str(e)[:200]}\")\n\n                if enable_validation and validate_every and validate_every > 0 and (global_step % validate_every == 0):\n                    if accumulated_steps == 0:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n\n                        val_result = comprehensive_epoch_validation(\n                            model,\n                            tokenizer,\n                            epoch,\n                            global_step,\n                            _SOURCE_LANGUAGE,\n                            _TARGET_LANGUAGE,\n                            _MAX_LENGTH,\n                            _DEVICE,\n                        )\n\n                        if val_result:\n                            training_stats['epoch_validations'].append(val_result)\n                        \n                        cached_cluster_count = _get_cluster_count(model)\n                    else:\n                        pending_validation = True\n\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    progress.update(1)\n                    continue\n\n                try:\n                    input_ids = batch[\"input_ids\"]\n                    attention_mask = batch[\"attention_mask\"]\n                    labels = batch[\"labels\"]\n\n                    batch_size = int(input_ids.size(0))\n\n                    domain_labels = batch.get(\"domain_labels\", None)\n                    if domain_labels is not None:\n                        if not isinstance(domain_labels, torch.Tensor):\n                            domain_labels = None\n                        elif domain_labels.dim() == 0:\n                            domain_labels = domain_labels.unsqueeze(0)\n                    \n                    if domain_labels is None:\n                        domain_labels = torch.full(\n                            (batch_size,),\n                            _TRAIN_DOMAIN,\n                            dtype=torch.long,\n                            device=torch.device('cpu')\n                        )\n\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        keep = (batch_size // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            progress.update(1)\n                            continue\n                        if keep != batch_size:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n                            domain_labels = domain_labels[:keep]\n                            batch_size = keep\n\n                    input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                    attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                    labels = labels.to(_DEVICE, non_blocking=True)\n                    domain_labels = domain_labels.to(_DEVICE, non_blocking=True)\n\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        progress.update(1)\n                        continue\n\n                    forward_kwargs = {\n                        \"input_ids\": input_ids,\n                        \"attention_mask\": attention_mask,\n                        \"labels\": labels,\n                        \"src_texts\": batch.get(\"src_text\", None),\n                        \"token_word_map\": batch.get(\"token_word_map\", None),\n                    }\n\n                    amp_ctx = get_amp_ctx()\n                    with amp_ctx:\n                        forward_out = model(**forward_kwargs)\n\n                        if isinstance(forward_out, torch.Tensor):\n                            loss_tensor = forward_out\n                        elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                            loss_tensor = forward_out[\"loss\"]\n                        else:\n                            if isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                                loss_tensor = forward_out[0]\n                            else:\n                                raise RuntimeError(\"Model forward did not return a recognizable loss tensor\")\n\n                        if not isinstance(loss_tensor, torch.Tensor):\n                            loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE)\n                        else:\n                            loss_tensor = loss_tensor.to(_DEVICE)\n\n                        if loss_tensor.numel() > 1:\n                            loss_val = float(loss_tensor.mean().item())\n                            loss_tensor = loss_tensor.mean()\n                        else:\n                            loss_val = float(loss_tensor.item())\n\n                        last_forward_loss = loss_val\n                        epoch_losses.append(loss_val)\n                        training_stats[\"total_loss\"].append(loss_val)\n\n                    loss_scaled = loss_tensor / max(1, accumulation_steps)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n\n                    try:\n                        if scaler.is_enabled():\n                            scaler.scale(loss_scaled).backward()\n                        else:\n                            loss_scaled.backward()\n\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n\n                    except RuntimeError as e:\n                        if \"out of memory\" in str(e).lower():\n                            training_stats[\"oom_errors\"] += 1\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"oom_backward\"] += 1\n                            print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                            try:\n                                optimizer.zero_grad(set_to_none=True)\n                            except Exception:\n                                pass\n                            for p in model.parameters():\n                                p.grad = None\n\n                            if torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n                            gc.collect()\n                            accumulated_steps = 0\n                            progress.update(1)\n                            continue\n                        else:\n                            raise\n\n                    accumulated_steps += 1\n\n                    if accumulated_steps >= accumulation_steps:\n                        try:\n                            if scaler.is_enabled():\n                                scaler.unscale_(optimizer)\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                                scaler.step(optimizer)\n                                scaler.update()\n                            else:\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                                optimizer.step()\n                            optimizer.zero_grad(set_to_none=True)\n                            training_stats[\"optimizer_updates\"] += 1\n\n                            if torch.cuda.is_available():\n                                torch.cuda.empty_cache()\n\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    p.grad = None\n\n                                if torch.cuda.is_available():\n                                    torch.cuda.empty_cache()\n                                gc.collect()\n                                accumulated_steps = 0\n                                progress.update(1)\n                                continue\n                            else:\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n                                print(f\"\\n[ERROR] Runtime error during optimizer step: {type(e).__name__}\")\n                        except Exception as e:\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n                            print(f\"\\n[ERROR] Exception during optimizer step: {type(e).__name__}\")\n                        finally:\n                            accumulated_steps = 0\n                            \n                            if pending_validation:\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n\n                                val_result = comprehensive_epoch_validation(\n                                    model,\n                                    tokenizer,\n                                    epoch,\n                                    global_step,\n                                    _SOURCE_LANGUAGE,\n                                    _TARGET_LANGUAGE,\n                                    _MAX_LENGTH,\n                                    _DEVICE,\n                                )\n\n                                if val_result:\n                                    training_stats['epoch_validations'].append(val_result)\n\n                                pending_validation = False\n                                cached_cluster_count = _get_cluster_count(model)\n\n                    if global_step % DEBUG_PRINT_INTERVAL == 0:\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cached_cluster_count = _get_cluster_count(model)\n                        print(f\"[TRAIN-DEBUG] step={global_step} loss={last_forward_loss:.4f} clusters={cached_cluster_count}\")\n                        _print_top_clusters(model, top_n=5)\n\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"\\n[OOM] Step {global_step} - Emergency cleanup\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            p.grad = None\n\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                        gc.collect()\n                        accumulated_steps = 0\n                        progress.update(1)\n                        continue\n                    else:\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        print(f\"\\n[RUNTIME] RuntimeError at step {global_step}: {type(e).__name__}\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        progress.update(1)\n                        continue\n                except Exception as e:\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    print(f\"\\n[EXCEPTION] Exception at step {global_step}: {type(e).__name__}\")\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    progress.update(1)\n                    continue\n\n                processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n                expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n                success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n\n                next_disc = 0\n                try:\n                    if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                        next_disc = _PERIODIC_DISCOVERY_FREQUENCY - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                except Exception:\n                    next_disc = 0\n\n                progress.set_postfix({\n                    'fwd': f\"{last_forward_loss:.2f}\",\n                    'bwd': f\"{last_backward_loss:.2f}\",\n                    'rate': f\"{success_rate:.1f}%\",\n                    'disc': next_disc\n                }, refresh=False)\n                \n                progress.update(1)\n\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n\n        if accumulated_steps > 0:\n            try:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}\")\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n        expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n        success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n        cached_cluster_count = _get_cluster_count(model)\n\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"  Duration (min): {epoch_duration_min:.2f}\")\n        print(f\"  Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\"  Batches: processed={processed_batches}, skipped={training_stats['skipped_batches']}\")\n        print(f\"  Success rate: {success_rate:.1f}%\")\n        print(f\"  Clustered tokens: {cached_cluster_count}\")\n        print(f\"  Avg epoch loss: {avg_epoch_loss:.6f}\")\n        print(f\"  Discovery runs: {training_stats['discovery_runs']}\")\n        print(f\"  Homographs discovered: {training_stats['discovery_homographs_found']}\")\n        if skip_reasons:\n            print(\"  Skip reasons:\")\n            for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"    - {k}: {v}\")\n        print(\"=\" * 80)\n\n        try:\n            print(f\"\\n[TRAIN] Running comprehensive validation after epoch {epoch}...\")\n\n            try:\n                optimizer.zero_grad(set_to_none=True)\n            except Exception:\n                pass\n\n            validation_results = comprehensive_epoch_validation(\n                model=model,\n                tokenizer=tokenizer,\n                epoch=epoch,\n                global_step=global_step,\n                source_lang=_SOURCE_LANGUAGE,\n                target_lang=_TARGET_LANGUAGE,\n                max_length=_MAX_LENGTH,\n                device=_DEVICE,\n            )\n\n            if validation_results and validation_results.get('validation_completed', False):\n                training_stats['epoch_validations'].append(validation_results)\n                training_stats['dscd_quality_history'].append(validation_results.get('dscd_quality_score', 0.0))\n                training_stats['asbn_domain_accuracy_history'].append(validation_results.get('asbn_domain_accuracy', 0.0))\n                training_stats['asbn_domain_loss_history'].append(validation_results.get('asbn_domain_loss', 0.0))\n                training_stats['trg_explanation_history'].append(validation_results.get('trg_total_explanations', 0))\n\n                try:\n                    dscd = model.module.dscd if hasattr(model, 'module') else getattr(model, 'dscd', None)\n\n                    lock = None\n                    if dscd is not None:\n                        if hasattr(dscd, 'buffer_lock'):\n                            lock = dscd.buffer_lock\n                        elif hasattr(dscd, 'clustering_lock'):\n                            lock = dscd.clustering_lock\n\n                    if dscd is not None:\n                        if lock:\n                            with lock:\n                                total_tokens = len(dscd.prototype_stores)\n                        else:\n                            total_tokens = len(dscd.prototype_stores)\n\n                        multi_sense = validation_results.get('dscd_multi_sense_tokens', 0)\n                        ratio = multi_sense / total_tokens if total_tokens > 0 else 0.0\n                        training_stats['multi_sense_ratio_history'].append(ratio)\n                    else:\n                        training_stats['multi_sense_ratio_history'].append(0.0)\n                except Exception:\n                    training_stats['multi_sense_ratio_history'].append(0.0)\n            else:\n                print(\"[TRAIN] Validation incomplete\")\n\n        except Exception as e:\n            print(f\"[TRAIN] Epoch validation failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    print(f\"\\n{'='*80}\")\n    print(\"TRAINING COMPLETE - SAVING FINAL CHECKPOINT\")\n    print(f\"{'='*80}\")\n\n    try:\n        checkpoint_path = Path(\"/kaggle/working/tatn_final.pt\")\n\n        core_model = model.module if hasattr(model, 'module') else model\n\n        dscd_state = {}\n        try:\n            if hasattr(core_model, 'dscd'):\n                try:\n                    dscd_state = core_model.dscd.state_dict()\n                except Exception:\n                    dscd_state = {}\n        except Exception:\n            dscd_state = {}\n\n        checkpoint_data = {\n            'epochs_trained': epochs,\n            'global_steps': global_step,\n            'final_train_loss': training_stats['epoch_losses'][-1] if training_stats['epoch_losses'] else 0.0,\n            'model_state_dict': core_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scaler_state_dict': scaler.state_dict() if scaler is not None else None,\n            'training_stats': training_stats,\n            'dscd_state': dscd_state,\n            'config': {\n                'SPAN_THRESHOLD': globals().get('SPAN_THRESHOLD', 0.20),\n                'TAU_LOW': globals().get('TAU_LOW', 0.15),\n                'LAMBDA_ASBN': globals().get('LAMBDA_ASBN', 0.3),\n                'LAMBDA_DSCD': globals().get('LAMBDA_DSCD', 0.15),\n                'LAMBDA_TOKEN': globals().get('LAMBDA_TOKEN', 0.3),\n                'LAMBDA_CONFIDENCE': globals().get('LAMBDA_CONFIDENCE', 0.2),\n                'LAMBDA_LENGTH': globals().get('LAMBDA_LENGTH', 0.1),\n                'TRG_TEMPERATURE': globals().get('TRG_TEMPERATURE', 1.0),\n                'PERIODIC_DISCOVERY_FREQUENCY': _PERIODIC_DISCOVERY_FREQUENCY,\n                'NUM_EPOCHS': epochs,\n                'BATCH_SIZE': _BATCH_SIZE,\n                'LEARNING_RATE': optimizer.param_groups[0]['lr'] if optimizer and optimizer.param_groups else 0.0,\n            },\n        }\n\n        torch.save(checkpoint_data, checkpoint_path)\n\n        file_size_mb = checkpoint_path.stat().st_size / (1024**2)\n\n        print(\"\\nFINAL CHECKPOINT SAVED\")\n        print(f\"   Path: {checkpoint_path}\")\n        print(f\"   Size: {file_size_mb:.2f} MB\")\n        print(f\"   Epochs trained: {epochs}\")\n        print(f\"   Global steps: {global_step}\")\n        print(f\"   Final train loss: {training_stats['epoch_losses'][-1] if training_stats['epoch_losses'] else 0.0:.4f}\")\n        print(f\"{'='*80}\\n\")\n\n    except Exception as e:\n        print(f\"FINAL CHECKPOINT SAVE FAILED: {type(e).__name__}\")\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRAINING COMPLETED - FINAL SUMMARY\")\n    print(\"=\" * 80)\n\n    processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n    expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n    success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n\n    print(f\"[TRAIN] Success Rate: {success_rate:.1f}%\")\n    print(f\"[TRAIN] Total Steps: {global_step}\")\n    print(f\"[TRAIN] Clustered Token Types: {cached_cluster_count}\")\n    print(f\"[TRAIN] Discovery Runs: {training_stats['discovery_runs']}\")\n    print(f\"[TRAIN] Total Homographs Found: {training_stats['discovery_homographs_found']}\")\n\n    if training_stats['dscd_quality_history']:\n        print(\"\\n[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats['dscd_quality_history'], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n\n    if training_stats['asbn_domain_accuracy_history']:\n        print(\"\\n[TRAIN] ASBN Domain Accuracy Trend:\")\n        for i, acc in enumerate(training_stats['asbn_domain_accuracy_history'], 1):\n            print(f\"  Epoch {i}: {acc:.1%}\")\n\n    if training_stats['asbn_domain_loss_history']:\n        print(\"\\n[TRAIN] ASBN Domain Loss Trend:\")\n        for i, loss_val in enumerate(training_stats['asbn_domain_loss_history'], 1):\n            print(f\"  Epoch {i}: {loss_val:.4f}\")\n\n    if training_stats['trg_explanation_history']:\n        print(\"\\n[TRAIN] TRG Explanation Count Trend:\")\n        for i, count in enumerate(training_stats['trg_explanation_history'], 1):\n            print(f\"  Epoch {i}: {count} explanations\")\n\n    print(\"=\" * 80)\n    return model\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training loop ready - SINGLE PROGRESS BAR VERSION\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\nprint(f\"  - Validation interval: {_VALIDATION_CHECK_INTERVAL} steps\")\nprint(f\"  - Train domain: {_TRAIN_DOMAIN}\")\nprint(f\"  - Test domain: {_TEST_DOMAIN}\")\nprint(f\"  - Gradient clip: {_GRAD_CLIP_NORM}\")\nprint(f\"  - Memory cleanup: Every {_MEMORY_CLEANUP_FREQUENCY} steps\")\nprint(\"\\nğŸ”§ ALL FIXES APPLIED:\")\nprint(\"  âœ… FIX 1: Manual batch_idx tracking (line 694)\")\nprint(\"  âœ… FIX 2: tqdm wraps train_loader directly (line 687)\")\nprint(\"  âœ… FIX 3: for batch in train_loader (line 698)\")\nprint(\"  âœ… FIX 4: progress.update(1) called ONCE at end (line 1030)\")\nprint(\"  âœ… FIX 5: set_postfix() with refresh=False (line 1021)\")\nprint(\"  âœ… FIX 6: Cached cluster_count (line 709)\")\nprint(\"  âœ… FIX 7: validate_prototypes(cluster_missing=False)\")\nprint(\"  âœ… FIX 8: periodic_discovery_check(cluster_missing=False)\")\nprint(\"\\nâš¡ RESULT: ONE progress bar, updates IN PLACE!\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:07:01.515954Z","iopub.execute_input":"2026-01-16T16:07:01.516749Z","iopub.status.idle":"2026-01-16T16:07:01.660438Z","shell.execute_reply.started":"2026-01-16T16:07:01.516723Z","shell.execute_reply":"2026-01-16T16:07:01.659673Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 7: Training loop ready - SINGLE PROGRESS BAR VERSION\n================================================================================\nConfiguration:\n  - Discovery frequency: 200 steps\n  - Validation interval: 200 steps\n  - Train domain: 0\n  - Test domain: 1\n  - Gradient clip: 1.0\n  - Memory cleanup: Every 50 steps\n\nğŸ”§ ALL FIXES APPLIED:\n  âœ… FIX 1: Manual batch_idx tracking (line 694)\n  âœ… FIX 2: tqdm wraps train_loader directly (line 687)\n  âœ… FIX 3: for batch in train_loader (line 698)\n  âœ… FIX 4: progress.update(1) called ONCE at end (line 1030)\n  âœ… FIX 5: set_postfix() with refresh=False (line 1021)\n  âœ… FIX 6: Cached cluster_count (line 709)\n  âœ… FIX 7: validate_prototypes(cluster_missing=False)\n  âœ… FIX 8: periodic_discovery_check(cluster_missing=False)\n\nâš¡ RESULT: ONE progress bar, updates IN PLACE!\n================================================================================\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: INFERENCE & EVALUATION PIPELINE - COMPLETE FIXED VERSION\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\n\ntry:\n    SOURCE_LANG = str(SOURCE_LANGUAGE)\n    TARGET_LANG = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    SOURCE_LANG = \"bn\"\n    TARGET_LANG = \"en\"\n\ntry:\n    MAXLEN = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    MAXLEN = 64\n\ntry:\n    DEVICE = DEVICE\nexcept (NameError, TypeError):\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    VERBOSE_LOGGING = False\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    DEBUG_DISCOVERY = False\n\ntry:\n    DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    DEBUG_TIMING = False\n\ntry:\n    USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    USE_MULTI_GPU = torch.cuda.is_available() and (torch.cuda.device_count() > 1)\n\ntry:\n    SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    SPAN_THRESHOLD = 0.20\n\ntry:\n    UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n        \"à¦ªà¦¾à¦¨à¦¿\", \"à¦¦à¦²\", \"à¦¬à¦¾à¦œà¦¾à¦°\", \"à¦¨à¦¾à¦®\", \"à¦•à¦¥à¦¾\", \"à¦¬à¦‡\", \"à¦˜à¦°\", \"à¦®à¦¨\", \"à¦¹à¦¾à¦¤\"\n    }\n    HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    M2M100_BN_TOKEN_ID = 128012\n\ntry:\n    TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    TEST_DOMAIN = 1\n\nBENGALI_PUNCT_SET = set(['à¥¤', 'à¥¥'])\nCOMMON_PUNCT_SET = set(['.', ',', ';', ':', '!', '?', '\"', \"'\", '-', '(', ')', '[', ']', '{', '}', '/', '\\\\'])\nPUNCT_SET = BENGALI_PUNCT_SET | COMMON_PUNCT_SET\n\n\ndef is_punctuation_only(token: str) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    clean = (\n        token.replace(\"â–\", \"\")\n        .replace(\"Ä \", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n    \n    if not clean:\n        return False\n    \n    if clean in BENGALI_PUNCT_SET:\n        return True\n    \n    if clean in COMMON_PUNCT_SET:\n        return True\n    \n    if len(clean) == 1 and not clean.isalnum():\n        return True\n    \n    return all(c in PUNCT_SET for c in clean)\n\n\ndef clean_token(token: str) -> str:\n    if not isinstance(token, str):\n        return \"\"\n    cleaned = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n    for punct in [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\"]:\n        cleaned = cleaned.replace(punct, \"\")\n    return cleaned.lower()\n\n\ndef get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                discovered = dscd.get_discovered_homographs()\n                return set(w for w in discovered if not is_punctuation_only(w))\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                for token, store in dscd.prototype_stores.items():\n                    try:\n                        if store.size() >= 2:\n                            clean_tok = clean_token(str(token))\n                            if clean_tok and not is_punctuation_only(str(token)):\n                                homographs.add(clean_tok)\n                    except Exception:\n                        continue\n        else:\n            for token, store in dscd.prototype_stores.items():\n                try:\n                    if store.size() >= 2:\n                        clean_tok = clean_token(str(token))\n                        if clean_tok and not is_punctuation_only(str(token)):\n                            homographs.add(clean_tok)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\nclass InferenceStatistics:\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self._lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.avg_span = 0.0\n            self.avg_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        with self._lock:\n            self.total_inferences += 1\n\n            if result.get('translation') and result['translation'] != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get('explanations', [])\n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                try:\n                    conf = exp.get('confidence', 0.5)\n                    self.total_confidence += float(conf)\n\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf < 0.4:\n                        self.low_confidence_explanations += 1\n\n                    word = str(exp.get('ambiguous_word', '')).strip()\n                    \n                    if is_punctuation_only(word):\n                        continue\n                    \n                    clean_word = clean_token(word)\n                    \n                    if not clean_word:\n                        continue\n\n                    self.token_counts[clean_word] += 1\n                    self.token_confidences[clean_word].append(float(conf))\n\n                    if dscd_homographs and clean_word in dscd_homographs:\n                        self.dscd_homographs_explained.add(clean_word)\n\n                    if clean_word in HOMOGRAPH_REFERENCE_LIST:\n                        self.reference_homographs_explained.add(clean_word)\n\n                    self.avg_span += float(exp.get('span', 0.0))\n                    self.avg_uncertainty += float(exp.get('uncertainty', 0.0))\n\n                except Exception:\n                    pass\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self._lock:\n            total_exp = max(self.total_explanations, 1)\n\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n\n            return {\n                'total_inferences': self.total_inferences,\n                'successful_translations': self.successful_translations,\n                'failed_translations': self.failed_translations,\n                'success_rate': self.successful_translations / max(self.total_inferences, 1),\n                'total_explanations': self.total_explanations,\n                'explanations_per_inference': self.total_explanations / max(self.total_inferences, 1),\n                'high_confidence_rate': self.high_confidence_explanations / total_exp,\n                'low_confidence_rate': self.low_confidence_explanations / total_exp,\n                'avg_confidence': self.total_confidence / total_exp,\n                'avg_span': self.avg_span / total_exp,\n                'avg_uncertainty': self.avg_uncertainty / total_exp,\n                'dscd_homographs_explained': list(self.dscd_homographs_explained),\n                'reference_homographs_explained': list(self.reference_homographs_explained),\n                'dscd_empty_warnings': self.dscd_empty_warnings,\n                'unique_tokens_explained': unique_tokens,\n                'diversity_ratio': diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"\\n\" + \"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2%}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n\n        if summary['dscd_homographs_explained']:\n            print(f\"\\nDSCD homographs explained ({len(summary['dscd_homographs_explained'])}):\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'])}\")\n\n        if summary['reference_homographs_explained']:\n            print(f\"\\nReference homographs explained ({len(summary['reference_homographs_explained'])}):\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'])}\")\n\n        if summary['dscd_empty_warnings'] > 0:\n            print(f\"\\nDSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80 + \"\\n\")\n\n\nINFERENCE_STATS = InferenceStatistics()\n\n\ndef to_device_batch(enc: Any, device: torch.device):\n    try:\n        if hasattr(enc, \"to\"):\n            return enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(enc, dict):\n        out = {}\n        for k, v in enc.items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                elif isinstance(v, dict):\n                    out[k] = to_device_batch(v, device)\n                elif isinstance(v, (list, tuple)):\n                    out[k] = [\n                        t.to(device) if isinstance(t, torch.Tensor) else t\n                        for t in v\n                    ]\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n\n    return enc\n\n\ndef extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    if raw_out is None:\n        return {}\n\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"proto_probs\" in raw_out or \"uncertainties\" in raw_out:\n            return raw_out\n\n        for key in (\"dscd_outputs\", \"dscd\", \"dscd_out\"):\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n\n        return raw_out\n\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return extract_dscd_outputs(item)\n\n    return {}\n\n\ndef is_subword_token(token: str) -> bool:\n    if not token or len(token.strip()) == 0:\n        return True\n\n    token = token.strip()\n    \n    if is_punctuation_only(token):\n        return True\n    \n    if (\n        token.startswith(\"##\")\n        or token.startswith(\"â–â–\")\n        or token.startswith(\"@@\")\n        or token.startswith(\"â–\")\n    ):\n        return True\n\n    if len(token) < 2:\n        return True\n\n    if (len(token) == 1 and token in PUNCT_SET) or token.isdigit():\n        return True\n\n    return False\n\n\ndef should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    try:\n        token = expl.get('ambiguous_word', expl.get('token', ''))\n        \n        if is_punctuation_only(str(token)):\n            return True\n        \n        span = float(expl.get('span', 0.0))\n        uncertainty = float(expl.get('uncertainty', 0.0))\n\n        if is_subword_token(str(token)):\n            return True\n\n        if span < span_th and uncertainty < u_th:\n            return True\n\n        return False\n    except Exception:\n        return True\n\n\ndef has_bengali_chars(text: str) -> bool:\n    if not text or not isinstance(text, str):\n        return False\n    return any('\\u0980' <= c <= '\\u09FF' for c in text)\n\n\n@torch.inference_mode()\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    source_lang: str = \"bn\",\n    target_lang: str = \"en\",\n    device: Optional[torch.device] = None,\n    max_length: Optional[int] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n    device = DEVICE if device is None else device\n    max_len = MAXLEN if max_length is None else int(max_length)\n    span_th = SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    if not input_sentence or not input_sentence.strip():\n        return {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": \"Empty input\"\n        }\n\n    if not has_bengali_chars(input_sentence):\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] WARNING: Input does not contain Bengali characters: {input_sentence[:50]}\")\n\n    try:\n        tokenizer.src_lang = source_lang\n        tokenizer.tgt_lang = target_lang\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] Tokenizer languages set: src={source_lang}, tgt={target_lang}\")\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] WARNING: Could not set tokenizer languages: {type(e).__name__}\")\n\n    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n        print(f\"\\n[INF] Starting inference:\")\n        print(f\"[INF]   Input: {input_sentence[:60]}\")\n        print(f\"[INF]   Languages: {source_lang} -> {target_lang}\")\n        print(f\"[INF]   Thresholds: span={span_th:.2f}, uncertainty={u_th:.2f}\")\n\n    cleanup_vars = []\n    dscd = None\n    encoder_hidden = None\n    encoder_hidden_adjusted = None\n\n    dscd_homographs = get_dscd_homographs(model)\n\n    try:\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len,\n        )\n        enc = to_device_batch(enc, device)\n        cleanup_vars.append(\"enc\")\n\n        model.eval()\n        core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n        src_texts = [input_sentence]\n\n        dscd_validated = False\n        try:\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n            if dscd:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                num_stores = 0\n                multi_sense = 0\n\n                if lock:\n                    try:\n                        with lock:\n                            num_stores = len(dscd.prototype_stores)\n                            multi_sense = sum(\n                                1\n                                for store in dscd.prototype_stores.values()\n                                if hasattr(store, 'centroids')\n                                and len(store.centroids) >= 2\n                            )\n                    except Exception:\n                        pass\n                else:\n                    try:\n                        num_stores = len(dscd.prototype_stores)\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if hasattr(store, 'centroids')\n                            and len(store.centroids) >= 2\n                        )\n                    except Exception:\n                        pass\n\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(\n                        f\"[INF] DSCD state: {num_stores} tokens, \"\n                        f\"{multi_sense} multi-sense, {len(dscd_homographs)} discovered\"\n                    )\n\n                if num_stores == 0:\n                    print(\"[INF] WARNING: DSCD prototype stores are EMPTY\")\n                    if track_stats:\n                        INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[INF] DSCD validation failed: {e}\")\n\n        with torch.inference_mode():\n            raw_dscd_out: Dict[str, Any] = {}\n\n            try:\n                if not hasattr(core, \"mbart\"):\n                    raise RuntimeError(\"Model backend missing .mbart\")\n\n                mbart = core.mbart\n\n                if DEBUG_DISCOVERY:\n                    print(\"[INF] Calling forward_with_dscd_for_inference()...\")\n\n                if hasattr(core, \"forward_with_dscd_for_inference\"):\n                    try:\n                        raw_dscd_out = core.forward_with_dscd_for_inference(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                        )\n                        if DEBUG_DISCOVERY:\n                            print(\"[INF] forward_with_dscd_for_inference() completed\")\n                    except Exception as e:\n                        if DEBUG_DISCOVERY:\n                            print(f\"[INF] forward_with_dscd_for_inference() failed: {e}\")\n                        raise\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] forward_with_dscd_for_inference not found, using fallback\")\n\n                    if hasattr(core, \"forward_with_explanations\"):\n                        try:\n                            raw_dscd_out = core.forward_with_explanations(\n                                input_ids=enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                src_texts=src_texts,\n                            )\n                        except TypeError:\n                            raw_dscd_out = core.forward_with_explanations(\n                                enc.get(\"input_ids\"),\n                                enc.get(\"attention_mask\"),\n                                src_texts,\n                            )\n                    else:\n                        out = core.forward(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                            labels=None,\n                            use_dscd=True,\n                        )\n                        if isinstance(out, dict):\n                            raw_dscd_out = extract_dscd_outputs(out)\n\n                dscd_out = extract_dscd_outputs(raw_dscd_out)\n                if isinstance(raw_dscd_out, dict) and 'sense_augmented_embeddings' in raw_dscd_out:\n                    encoder_hidden_adjusted = raw_dscd_out['sense_augmented_embeddings']\n                elif 'h_augmented' in dscd_out:\n                    encoder_hidden_adjusted = dscd_out['h_augmented']\n                else:\n                    encoder_outputs_raw = mbart.model.encoder(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                    )\n                    cleanup_vars.append(\"encoder_outputs_raw\")\n\n                    if hasattr(encoder_outputs_raw, 'last_hidden_state'):\n                        encoder_hidden = encoder_outputs_raw.last_hidden_state\n                    elif isinstance(encoder_outputs_raw, tuple):\n                        encoder_hidden = encoder_outputs_raw[0]\n                    else:\n                        encoder_hidden = encoder_outputs_raw\n                    cleanup_vars.append(\"encoder_hidden\")\n                    encoder_hidden_adjusted = encoder_hidden\n\n                if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    if encoder_hidden is not None and encoder_hidden_adjusted.shape != encoder_hidden.shape:\n                        if DEBUG_DISCOVERY:\n                            print(\"[INF] Shape mismatch, using original\")\n                        encoder_hidden_adjusted = encoder_hidden\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] encoder_hidden_adjusted is not tensor\")\n                    if encoder_hidden is not None:\n                        encoder_hidden_adjusted = encoder_hidden\n\n                if DEBUG_DISCOVERY:\n                    print(\"[INF] DSCD outputs extracted\")\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD forward error: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                raw_dscd_out = {}\n\n                try:\n                    encoder_outputs_raw = mbart.model.encoder(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                    )\n                    if hasattr(encoder_outputs_raw, 'last_hidden_state'):\n                        encoder_hidden_adjusted = encoder_outputs_raw.last_hidden_state\n                    elif isinstance(encoder_outputs_raw, tuple):\n                        encoder_hidden_adjusted = encoder_outputs_raw[0]\n                    else:\n                        encoder_hidden_adjusted = encoder_outputs_raw\n                except Exception:\n                    encoder_hidden_adjusted = None\n\n            target_lang_id = M2M100_EN_TOKEN_ID if target_lang == \"en\" else M2M100_BN_TOKEN_ID\n            \n            if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                print(f\"[INF] Target language: {target_lang} -> Token ID: {target_lang_id}\")\n            \n            orig_use_cache = (\n                getattr(mbart.config, \"use_cache\", None)\n                if hasattr(mbart, \"config\")\n                else None\n            )\n            if hasattr(mbart, \"config\"):\n                try:\n                    mbart.config.use_cache = True\n                except Exception:\n                    pass\n\n            try:\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Generating translation with forced_bos_token_id={target_lang_id}...\")\n\n                if encoder_hidden_adjusted is not None and isinstance(\n                    encoder_hidden_adjusted, torch.Tensor\n                ) and encoder_hidden_adjusted.numel() > 0:\n                    encoder_hidden_adjusted = encoder_hidden_adjusted.to(device)\n\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] âš¡ Using DSCD-AUGMENTED generation with encoder_outputs!\")\n                        print(f\"[INF]   Encoder hidden shape: {encoder_hidden_adjusted.shape}\")\n\n                    encoder_wrapped = BaseModelOutput(\n                        last_hidden_state=encoder_hidden_adjusted,\n                        hidden_states=None,\n                        attentions=None,\n                    )\n\n                    generated = model.generate(\n                        input_ids=None,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        encoder_outputs=encoder_wrapped,\n                        max_length=min(max_len, 64),\n                        num_beams=2,\n                        do_sample=False,\n                        forced_bos_token_id=target_lang_id,\n                    )\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] âš ï¸  Using fallback generation without DSCD (encoder_hidden_adjusted unavailable)\")\n                    generated = model.generate(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(max_len, 64),\n                        num_beams=2,\n                        do_sample=False,\n                        forced_bos_token_id=target_lang_id,\n                    )\n                cleanup_vars.append(\"generated\")\n\n                translation = (\n                    tokenizer.decode(generated[0], skip_special_tokens=True)\n                    if generated is not None and len(generated) > 0\n                    else \"\"\n                )\n\n                if DEBUG_DISCOVERY:\n                    print(f\"[INF] Translation: {translation[:60] if translation else 'EMPTY'}\")\n                \n                if not translation or not translation.strip():\n                    error_msg = \"Empty generation result\"\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(f\"[INF] ERROR: {error_msg}\")\n                        print(f\"[INF] Generated IDs shape: {generated.shape if generated is not None else 'None'}\")\n                        if generated is not None and len(generated) > 0:\n                            print(f\"[INF] Generated IDs: {generated[0].tolist()}\")\n                            print(f\"[INF] Tokenizer src_lang: {getattr(tokenizer, 'src_lang', 'NOT_SET')}\")\n                            print(f\"[INF] Tokenizer tgt_lang: {getattr(tokenizer, 'tgt_lang', 'NOT_SET')}\")\n                            print(f\"[INF] Forced BOS ID: {target_lang_id}\")\n                    \n                    return {\n                        \"input_sentence\": input_sentence,\n                        \"translation\": \"\",\n                        \"ambiguous_words_detected\": 0,\n                        \"explanations\": [],\n                        \"quality_metrics\": {},\n                        \"dscd_validated\": dscd_validated,\n                        \"error\": error_msg\n                    }\n\n            finally:\n                if hasattr(mbart, \"config\") and orig_use_cache is not None:\n                    try:\n                        mbart.config.use_cache = orig_use_cache\n                    except Exception:\n                        pass\n\n            if DEBUG_DISCOVERY:\n                print(\"[INF] Calling TRG to generate explanations...\")\n\n            dscd_out = extract_dscd_outputs(raw_dscd_out)\n            out_explanations: List[Dict[str, Any]] = []\n\n            try:\n                trg = core.trg_system if hasattr(core, 'trg_system') else None\n\n                if trg and hasattr(trg, 'process_sentence_for_explanations'):\n                    try:\n                        tokens_list = tokenizer.convert_ids_to_tokens(enc['input_ids'][0].tolist())\n\n                        if DEBUG_DISCOVERY:\n                            print(f\"[INF] Calling TRG with {len(tokens_list)} tokens\")\n                            print(f\"[INF] DSCD outputs keys: {list(dscd_out.keys())}\")\n\n                        trg_explanations = trg.process_sentence_for_explanations(\n                            tokens=tokens_list,\n                            dscd_outputs=dscd_out,\n                            token_word_map=None,\n                            uncertainty_threshold=u_th,\n                            decoder_attention=None\n                        )\n\n                        if DEBUG_DISCOVERY:\n                            print(f\"[INF] TRG returned {len(trg_explanations) if isinstance(trg_explanations, list) else 0} explanations\")\n\n                        if isinstance(trg_explanations, list):\n                            for exp in trg_explanations:\n                                try:\n                                    raw_word = exp.get('token', '')\n                                    \n                                    if is_punctuation_only(str(raw_word)):\n                                        continue\n                                    \n                                    clean_word = clean_token(str(raw_word)) if raw_word else ''\n\n                                    if not clean_word:\n                                        continue\n\n                                    if should_filter_explanation(exp, span_th, u_th):\n                                        continue\n\n                                    s = float(exp.get('span', 0.0))\n                                    u = float(exp.get('uncertainty', 0.0))\n                                    confidence = max(s, u)\n\n                                    expl_text = exp.get('explanation', '')\n                                    if not expl_text:\n                                        continue\n\n                                    out_explanations.append({\n                                        \"ambiguous_word\": clean_word,\n                                        \"position\": exp.get(\"token_idx\", \"N/A\"),\n                                        \"explanation\": expl_text,\n                                        \"uncertainty\": u,\n                                        \"span\": s,\n                                        \"confidence\": confidence,\n                                        \"is_real_amb\": bool((s > span_th) or (u > u_th)),\n                                    })\n                                except Exception as e:\n                                    if DEBUG_DISCOVERY:\n                                        print(f\"[INF] Error processing TRG explanation: {e}\")\n                                    continue\n\n                    except Exception as e:\n                        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                            print(f\"[INF] TRG processing failed: {e}\")\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n                else:\n                    if DEBUG_DISCOVERY:\n                        print(\"[INF] TRG not available or missing process_sentence_for_explanations()\")\n\n            except Exception as e:\n                if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                    print(f\"[INF] TRG invocation error: {e}\")\n\n            real_amb_count = sum(1 for e in out_explanations if e.get('is_real_amb', False))\n\n            quality_metrics = {\n                'total_raw_explanations': len(out_explanations),\n                'filtered_explanations': 0,\n                'high_confidence_count': sum(1 for e in out_explanations if e.get('confidence', 0) >= 0.65),\n                'low_confidence_count': sum(1 for e in out_explanations if e.get('confidence', 0) < 0.4),\n                'avg_confidence': sum(e.get('confidence', 0) for e in out_explanations) / max(len(out_explanations), 1),\n                'avg_span': sum(e.get('span', 0) for e in out_explanations) / max(len(out_explanations), 1),\n                'avg_uncertainty': sum(e.get('uncertainty', 0) for e in out_explanations) / max(len(out_explanations), 1),\n            }\n\n            if DEBUG_DISCOVERY:\n                print(\n                    f\"[INF] Final: {len(out_explanations)} explanations \"\n                    f\"(real ambiguous: {real_amb_count})\"\n                )\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n\n            if track_stats:\n                INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n            return result\n\n    except Exception as e:\n        if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": f\"{type(e).__name__}: {str(e)[:150]}\",\n        }\n\n        if track_stats:\n            INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\",\n            \"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\",\n            \"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\",\n            \"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\",\n            \"à¦†à¦œ à¦­à¦¾à¦² à¦†à¦¬à¦¹à¦¾à¦“à¦¯à¦¼à¦¾à¥¤\",\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations\")\n    print(\"=\" * 80)\n\n    INFERENCE_STATS.reset()\n\n    for s in sentences:\n        print(f\"\\nInput: {s}\")\n        res = translate_with_explanations(model, tokenizer, s, source_lang=\"bn\", target_lang=\"en\")\n        print(\"Translation:\", res.get(\"translation\", \"\"))\n        print(\"Ambiguous words detected:\", res.get(\"ambiguous_words_detected\", 0))\n\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(\n                f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, \"\n                f\"high={quality.get('high_confidence_count', 0)}, \"\n                f\"low={quality.get('low_confidence_count', 0)}\"\n            )\n\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(\n                    f\"  {idx}. '{ex['ambiguous_word']}' \"\n                    f\"pos={ex['position']} conf={ex.get('confidence', 0):.3f}\"\n                )\n                print(\"     \", ex.get(\"explanation\", \"\")[:200])\n        else:\n            print(\"  No explanations\")\n\n    print(\"=\" * 80)\n    INFERENCE_STATS.print_summary()\n\n\ndef dscd_discovery_warmup(\n    model,\n    tokenizer,\n    num_sents: int = 8000,\n    batch_size: int = 64,\n    max_len: Optional[int] = None,\n):\n    if max_len is None:\n        max_len = MAXLEN\n\n    core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component\")\n            return\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[WARMUP] Starting DSCD discovery warmup\")\n        print(\"=\" * 80)\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_n_min = getattr(dscd, \"n_min\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"n_min\"):\n                dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for (bn, _) in pairs][:num_sents]\n            else:\n                base = [\n                    \"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\",\n                    \"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\",\n                    \"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\",\n                    \"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\",\n                ]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n        except Exception:\n            texts = [\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\"] * num_sents\n\n        processed = 0\n        core.eval()\n\n        print(f\"\\n[WARMUP] Processing {len(texts)} sentences (batch={batch_size})...\")\n\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                encoder_outputs_raw = None\n                try:\n                    enc = tokenizer(\n                        batch,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_len,\n                    )\n                    enc = to_device_batch(enc, DEVICE)\n\n                    if hasattr(core, \"forward_with_dscd_for_inference\"):\n                        core.forward_with_dscd_for_inference(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch,\n                        )\n                    elif hasattr(core, \"forward_with_explanations\"):\n                        core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch,\n                        )\n                    else:\n                        encoder_outputs_raw = core.mbart.model.encoder(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                        )\n\n                    processed += len(batch)\n\n                    current_time = time.time()\n                    if (i // batch_size) % 10 == 0 or (current_time - last_print) > 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n                        print(\n                            f\"[WARMUP] {processed}/{len(texts)} \"\n                            f\"({processed/len(texts)*100:.1f}%) | \"\n                            f\"{rate:.1f} sent/s | ETA {eta:.0f}s\"\n                        )\n                        last_print = current_time\n\n                    del enc\n                    if encoder_outputs_raw is not None:\n                        del encoder_outputs_raw\n\n                except Exception as e:\n                    print(\n                        f\"[WARMUP] Batch {i//batch_size} failed: {str(e)[:100]}\"\n                    )\n                    continue\n\n        total_time = time.time() - start_time\n        print(\n            f\"\\n[WARMUP] Completed in {total_time:.1f}s \"\n            f\"({processed/total_time:.1f} sent/s)\"\n        )\n        print(\"-\" * 80)\n\n        try:\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            num_types = len(stores)\n            total_protos = (\n                sum(store.size() for store in stores.values()) if stores else 0\n            )\n            multi = (\n                sum(1 for store in stores.values() if store.size() >= 2)\n                if stores\n                else 0\n            )\n\n            print(\"[WARMUP] Summary:\")\n            print(f\"  - Token types: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n\n            dscd_homographs = get_dscd_homographs(model)\n\n            print(f\"\\n[WARMUP] Discovered Homographs: {len(dscd_homographs)}\")\n            if dscd_homographs:\n                print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n            reference_found = dscd_homographs.intersection(HOMOGRAPH_REFERENCE_LIST)\n\n            print(f\"\\n[WARMUP] Reference List Comparison:\")\n            print(f\"  - Reference list: {len(HOMOGRAPH_REFERENCE_LIST)} words\")\n            print(f\"  - Found in DSCD: {len(reference_found)}\")\n            print(\n                f\"  - Coverage: {len(reference_found)/len(HOMOGRAPH_REFERENCE_LIST):.1%}\"\n            )\n\n            if num_types == 0:\n                print(\"\\n[WARMUP] CRITICAL: NO PROTOTYPES CREATED\")\n            elif len(reference_found) < len(HOMOGRAPH_REFERENCE_LIST) // 2:\n                print(\"\\n[WARMUP] WARNING: < 50% reference coverage\")\n            else:\n                print(\"\\n[WARMUP] SUCCESS\")\n\n        except Exception as e:\n            print(f\"[WARMUP] Validation failed: {e}\")\n\n    finally:\n        try:\n            if dscd is not None:\n                if hasattr(dscd, \"enable_training_clustering\"):\n                    dscd.enable_training_clustering = orig_enable\n                if hasattr(dscd, \"n_min\") and orig_n_min is not None:\n                    dscd.n_min = orig_n_min\n                if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                    dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n        print(\"=\" * 80 + \"\\n\")\n\n\ndef final_evaluation_with_bleu(\n    model,\n    tokenizer,\n    test_data: List[Tuple[str, str]],\n    device: Optional[torch.device] = None,\n    max_length: Optional[int] = None,\n    batch_size: int = 16,\n) -> Dict[str, Any]:\n    device = DEVICE if device is None else device\n    max_len = MAXLEN if max_length is None else int(max_length)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL EVALUATION WITH BLEU/CHRF++\")\n    print(\"=\" * 80)\n    print(f\"Test samples: {len(test_data)}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Max length: {max_len}\")\n    print(\"=\" * 80 + \"\\n\")\n\n    INFERENCE_STATS.reset()\n\n    predictions = []\n    references = []\n    translations_with_explanations = []\n\n    model.eval()\n\n    try:\n        from sacrebleu.metrics import BLEU, CHRF\n        bleu_metric = BLEU()\n        chrf_metric = CHRF()\n        metrics_available = True\n    except ImportError:\n        print(\"[EVAL] WARNING: sacrebleu not available, BLEU/CHRF scores will not be computed\")\n        metrics_available = False\n\n    start_time = time.time()\n\n    with torch.inference_mode():\n        for i in range(0, len(test_data), batch_size):\n            batch = test_data[i:i+batch_size]\n\n            for src, ref in batch:\n                try:\n                    result = translate_with_explanations(\n                        model,\n                        tokenizer,\n                        src,\n                        source_lang=\"bn\",\n                        target_lang=\"en\",\n                        device=device,\n                        max_length=max_len,\n                        track_stats=True\n                    )\n\n                    translation = result.get('translation', '')\n\n                    predictions.append(translation)\n                    references.append(ref)\n                    translations_with_explanations.append({\n                        'source': src,\n                        'reference': ref,\n                        'translation': translation,\n                        'explanations': result.get('explanations', []),\n                        'ambiguous_words': result.get('ambiguous_words_detected', 0)\n                    })\n\n                except Exception as e:\n                    if DEBUG_DISCOVERY or VERBOSE_LOGGING:\n                        print(f\"[EVAL] Translation failed for: {src[:50]} - {type(e).__name__}\")\n                    predictions.append(\"\")\n                    references.append(ref)\n                    translations_with_explanations.append({\n                        'source': src,\n                        'reference': ref,\n                        'translation': \"ERROR\",\n                        'explanations': [],\n                        'ambiguous_words': 0\n                    })\n\n            if (i // batch_size) % 10 == 0:\n                elapsed = time.time() - start_time\n                processed = min(i + batch_size, len(test_data))\n                rate = processed / elapsed if elapsed > 0 else 0\n                eta = (len(test_data) - processed) / rate if rate > 0 else 0\n                print(f\"[EVAL] {processed}/{len(test_data)} ({processed/len(test_data)*100:.1f}%) | {rate:.1f} sent/s | ETA {eta:.0f}s\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n[EVAL] Translation completed in {total_time:.1f}s ({len(test_data)/total_time:.1f} sent/s)\")\n\n    results = {\n        'total_samples': len(test_data),\n        'successful_translations': sum(1 for p in predictions if p and p != \"ERROR\"),\n        'failed_translations': sum(1 for p in predictions if not p or p == \"ERROR\"),\n        'total_time': total_time,\n        'throughput': len(test_data) / total_time,\n        'predictions': predictions,\n        'references': references,\n        'translations_with_explanations': translations_with_explanations,\n    }\n\n    if metrics_available and predictions and references:\n        try:\n            valid_preds = []\n            valid_refs = []\n            for p, r in zip(predictions, references):\n                if p and p != \"ERROR\" and r:\n                    valid_preds.append(p)\n                    valid_refs.append(r)\n\n            if valid_preds:\n                bleu_score = bleu_metric.corpus_score(valid_preds, [valid_refs])\n                chrf_score = chrf_metric.corpus_score(valid_preds, [valid_refs])\n\n                results['bleu'] = float(bleu_score.score)\n                results['chrf'] = float(chrf_score.score)\n\n                print(\"\\n\" + \"=\" * 80)\n                print(\"METRIC SCORES\")\n                print(\"=\" * 80)\n                print(f\"BLEU:    {results['bleu']:.2f}\")\n                print(f\"CHRF++:  {results['chrf']:.2f}\")\n                print(f\"Valid samples: {len(valid_preds)}/{len(predictions)}\")\n                print(\"=\" * 80)\n            else:\n                print(\"[EVAL] WARNING: No valid translations for BLEU/CHRF computation\")\n                results['bleu'] = 0.0\n                results['chrf'] = 0.0\n        except Exception as e:\n            print(f\"[EVAL] Metric computation failed: {type(e).__name__}: {str(e)[:100]}\")\n            results['bleu'] = 0.0\n            results['chrf'] = 0.0\n    else:\n        results['bleu'] = 0.0\n        results['chrf'] = 0.0\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"Total samples: {results['total_samples']}\")\n    print(f\"Successful: {results['successful_translations']}\")\n    print(f\"Failed: {results['failed_translations']}\")\n    print(f\"Success rate: {results['successful_translations']/results['total_samples']:.1%}\")\n    print(f\"Throughput: {results['throughput']:.1f} sent/s\")\n    print(\"=\" * 80 + \"\\n\")\n\n    INFERENCE_STATS.print_summary()\n\n    return results\n\n\ndef load_checkpoint_for_resume(\n    model: torch.nn.Module, optimizer, checkpoint_path: str\n) -> Tuple[bool, int, int, float]:\n    if not os.path.exists(checkpoint_path):\n        print(f\"[CHECKPOINT] Not found: {checkpoint_path}\")\n        return False, 0, 0, 0.0\n\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] Load failed: {e}\")\n        return False, 0, 0, 0.0\n\n    core = model.module if (USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    state = ckpt.get(\"model_state_dict\", ckpt)\n    try:\n        core.load_state_dict(state, strict=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] model.load_state_dict failed: {e}\")\n\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                core.load_state_dict(new_state, strict=False)\n        except Exception:\n            pass\n\n    try:\n        if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n    except Exception as e:\n        print(f\"[CHECKPOINT] optimizer.load_state_dict failed: {e}\")\n\n    try:\n        if \"dscd_state\" in ckpt and ckpt[\"dscd_state\"]:\n            dscd_state = ckpt[\"dscd_state\"]\n\n            print(\"[CHECKPOINT] Restoring DSCD...\")\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n\n            if dscd and hasattr(dscd, 'load_state_dict'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        dscd.load_state_dict(dscd_state)\n                        num_tokens = len(dscd.prototype_stores)\n                        total_protos = sum(\n                            store.size() for store in dscd.prototype_stores.values()\n                        )\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if store.size() >= 2\n                        )\n                else:\n                    dscd.load_state_dict(dscd_state)\n                    num_tokens = len(dscd.prototype_stores)\n                    total_protos = sum(\n                        store.size() for store in dscd.prototype_stores.values()\n                    )\n                    multi_sense = sum(\n                        1\n                        for store in dscd.prototype_stores.values()\n                        if store.size() >= 2\n                    )\n\n                print(\"[CHECKPOINT] DSCD restored:\")\n                print(f\"  - Tokens: {num_tokens}\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multi_sense}\")\n\n                if num_tokens == 0:\n                    print(\n                        \"[CHECKPOINT] WARNING: DSCD state empty - consider running warmup\"\n                    )\n            else:\n                print(\"[CHECKPOINT] Model has no dscd.load_state_dict\")\n        else:\n            print(\"[CHECKPOINT] No DSCD state in checkpoint\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] DSCD restore failed: {e}\")\n\n    epoch = int(ckpt.get(\"epochs_trained\", ckpt.get(\"epoch\", 0)))\n    step = int(\n        ckpt.get(\n            \"global_steps\", ckpt.get(\"global_step\", ckpt.get(\"step\", 0))\n        )\n    )\n    avg_loss = float(\n        ckpt.get(\n            \"final_train_loss\",\n            ckpt.get(\"avg_epoch_loss\", ckpt.get(\"avg_loss\", 0.0)),\n        )\n    )\n\n    print(f\"[CHECKPOINT] Loaded: epoch={epoch} step={step} loss={avg_loss:.6f}\")\n    return True, epoch, step, avg_loss\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 8: Inference & Evaluation pipeline ready - PRODUCTION VERSION WITH DSCD FIX\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Source language: {SOURCE_LANG}\")\nprint(f\"  - Target language: {TARGET_LANG}\")\nprint(f\"  - Span threshold: {SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Max length: {MAXLEN}\")\nprint(f\"  - Device: {DEVICE}\")\nprint(f\"  - Bengali token ID: {M2M100_BN_TOKEN_ID}\")\nprint(f\"  - English token ID: {M2M100_EN_TOKEN_ID}\")\nprint(f\"  - Test domain: {TEST_DOMAIN}\")\nprint(f\"  - Reference list: {len(HOMOGRAPH_REFERENCE_LIST)} words\")\nprint(\"\\nğŸ”§ CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX 1: Added BaseModelOutput import (line 13)\")\nprint(\"  âœ… FIX 2: Wrapped encoder_hidden_adjusted in BaseModelOutput (lines 703-707)\")\nprint(\"  âœ… FIX 3: Changed generate() to use encoder_outputs instead of input_ids (lines 709-714)\")\nprint(\"  âœ… FIX 4: Set input_ids=None to prevent re-encoding (line 710)\")\nprint(\"  âœ… FIX 5: Added debug logging for DSCD-augmented vs vanilla generation (lines 698, 717)\")\nprint(\"\\nâš¡ RESULT: DSCD-augmented generation now WORKS! Cell 6's fix is fully utilized!\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"7Dxg7ck0H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:07:01.662116Z","iopub.execute_input":"2026-01-16T16:07:01.662772Z","iopub.status.idle":"2026-01-16T16:07:01.790356Z","shell.execute_reply.started":"2026-01-16T16:07:01.662748Z","shell.execute_reply":"2026-01-16T16:07:01.789543Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 8: Inference & Evaluation pipeline ready - PRODUCTION VERSION WITH DSCD FIX\n================================================================================\nConfiguration:\n  - Source language: bn\n  - Target language: en\n  - Span threshold: 0.2\n  - Uncertainty threshold: 0.15\n  - Max length: 64\n  - Device: cuda:0\n  - Bengali token ID: 128012\n  - English token ID: 128022\n  - Test domain: 1\n  - Reference list: 41 words\n\nğŸ”§ CRITICAL FIXES APPLIED:\n  âœ… FIX 1: Added BaseModelOutput import (line 13)\n  âœ… FIX 2: Wrapped encoder_hidden_adjusted in BaseModelOutput (lines 703-707)\n  âœ… FIX 3: Changed generate() to use encoder_outputs instead of input_ids (lines 709-714)\n  âœ… FIX 4: Set input_ids=None to prevent re-encoding (line 710)\n  âœ… FIX 5: Added debug logging for DSCD-augmented vs vanilla generation (lines 698, 717)\n\nâš¡ RESULT: DSCD-augmented generation now WORKS! Cell 6's fix is fully utilized!\n================================================================================\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION - COMPLETE FIXED VERSION\n# ==============================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.20\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n        \"à¦ªà¦¾à¦¨à¦¿\", \"à¦¦à¦²\", \"à¦¬à¦¾à¦œà¦¾à¦°\", \"à¦¨à¦¾à¦®\", \"à¦•à¦¥à¦¾\", \"à¦¬à¦‡\", \"à¦˜à¦°\", \"à¦®à¦¨\", \"à¦¹à¦¾à¦¤\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                return len(stores)\n        else:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            return len(stores)\n    except Exception:\n        return 0\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                for token, store in prototype_stores.items():\n                    try:\n                        if hasattr(store, 'size') and store.size() >= 1:\n                            clean_token = (\n                                str(token)\n                                .replace('â–', '')\n                                .replace('Ä ', '')\n                                .replace('##', '')\n                                .strip()\n                                .lower()\n                            )\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            for token, store in prototype_stores.items():\n                try:\n                    if hasattr(store, 'size') and store.size() >= 1:\n                        clean_token = (\n                            str(token)\n                            .replace('â–', '')\n                            .replace('Ä ', '')\n                            .replace('##', '')\n                            .strip()\n                            .lower()\n                        )\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = len(getattr(store, \"centroids\", []))\n            except Exception:\n                n_protos = 0\n            cluster_info.append({\n                'token': token,\n                'count': total_count,\n                'protos': n_protos,\n                'mu': getattr(store, \"mu\", 0.0),\n                'tau': getattr(store, \"tau\", 0.0)\n            })\n\n        cluster_info.sort(key=lambda x: x['count'], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info['token'])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n\n        print(\"-\" * 90)\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap\", \"à¦•à¦² = tap/call\", [\"à¦•à¦²\"]),\n        (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"Tomorrow I will buy a book\", \"à¦•à¦¾à¦² = tomorrow/yesterday\", [\"à¦•à¦¾à¦²\"]),\n        (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"The leaf has fallen\", \"à¦ªà¦¾à¦¤à¦¾ = leaf/page\", [\"à¦ªà¦¾à¦¤à¦¾\"]),\n        (\"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\", \"He went to the bank\", \"à¦¬à§à¦¯à¦¾à¦‚à¦• = bank/embankment\", [\"à¦¬à§à¦¯à¦¾à¦‚à¦•\"]),\n        (\"à¦«à¦² à¦–à§à¦¬ à¦¸à§à¦¸à§à¦¬à¦¾à¦¦à§à¥¤\", \"The fruit is delicious\", \"à¦«à¦² = fruit/result\", [\"à¦«à¦²\"]),\n        (\"à¦®à¦¾à¦¥à¦¾ à¦¬à§à¦¯à¦¥à¦¾ à¦•à¦°à¦›à§‡à¥¤\", \"Head is aching\", \"à¦®à¦¾à¦¥à¦¾ = head/top\", [\"à¦®à¦¾à¦¥à¦¾\"]),\n        (\"à¦•à¦² à¦¥à§‡à¦•à§‡ à¦•à¦² à¦à¦¸à§‡à¦›à§‡à¥¤\", \"A call came from the tap\", \"Multiple à¦•à¦²\", [\"à¦•à¦²\"]),\n        (\"à¦•à¦¾à¦²à¦•à§‡ à¦•à¦¾à¦² à¦®à§‡à¦˜ à¦¦à§‡à¦–à¦¾ à¦—à§‡à¦›à§‡à¥¤\", \"Yesterday black clouds were seen\", \"Multiple à¦•à¦¾à¦²\", [\"à¦•à¦¾à¦²\"]),\n        (\"à¦†à¦œ à¦­à¦¾à¦² à¦†à¦¬à¦¹à¦¾à¦“à¦¯à¦¼à¦¾à¥¤\", \"Weather is good today\", \"Simple\", []),\n        (\"à¦†à¦®à¦¿ à¦­à¦¾à¦²à§‹ à¦†à¦›à¦¿à¥¤\", \"I am fine\", \"Simple\", []),\n        (\"à¦¸à§‡ à¦–à§à¦¬ à¦®à¦¿à¦·à§à¦Ÿà¦¿ à¦•à¦¥à¦¾ à¦¬à¦²à§‡à¥¤\", \"She speaks sweetly\", \"Simple\", []),\n        (\"à¦à¦Ÿà¦¾ à¦†à¦®à¦¾à¦° à¦¬à¦‡à¥¤\", \"This is my book\", \"Simple\", []),\n        (\"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦•à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡à¦¨ à¦à¦¬à¦‚ à¦¬à§à¦¯à¦¾à¦‚à¦•à§‡ à¦¬à¦¸à§‡ à¦¥à¦¾à¦•à§‡à¦¨à¥¤\",\n         \"He works at the bank and sits on the embankment\",\n         \"Long with multiple\", [\"à¦¬à§à¦¯à¦¾à¦‚à¦•\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    quality_metrics = {\n        'total_confidence': 0.0,\n        'confidence_samples': 0,\n        'high_confidence_count': 0,\n        'medium_confidence_count': 0,\n        'low_confidence_count': 0,\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n    }\n\n    homograph_tracking = {\n        'test_expected_homographs': set(),\n        'dscd_discovered_homographs': set(),\n        'explained_homographs': set(),\n        'homograph_explanations': defaultdict(list),\n    }\n\n    error_tracking = {\n        'translation_failures': 0,\n        'dscd_failures': 0,\n        'trg_failures': 0,\n        'timeout_errors': 0,\n        'oom_errors': 0,\n        'other_errors': 0,\n        'error_details': [],\n        'per_test_status': [],\n    }\n\n    timing_metrics = {\n        'total_time': 0.0,\n        'per_test_times': [],\n        'avg_test_time': 0.0,\n    }\n\n    discovery_validated = False\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd and hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            discovery_validated = True\n            last_discovery = dscd.discovered_log[-1]\n            discovered = last_discovery.get('discovered', 0)\n            candidates = last_discovery.get('candidates', 0)\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] No discovery log found\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] Discovery validation failed: {e}\")\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn and hasattr(asbn, 'get_detailed_stats'):\n            asbn_stats = asbn.get_detailed_stats()\n        elif asbn and hasattr(asbn, 'get_asbn_stats'):\n            asbn_stats = asbn.get_asbn_stats()\n\n        if asbn_stats and _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN: domain_acc={asbn_stats.get('domain_accuracy', 0):.2%}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN stats failed: {e}\")\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg_system\", None)\n        if trg and hasattr(trg, 'get_statistics'):\n            trg_stats = trg.get_statistics()\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] TRG: {trg_stats.get('explanations_generated', 0)} total\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] TRG stats failed: {e}\")\n\n    homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking['dscd_discovered_homographs'] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", None)\n                        store_count = len(stores) if stores else 0\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", None)\n                    store_count = len(stores) if stores else 0\n\n                if store_count == 0 and 'dscd_discovery_warmup' in globals():\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(model, tokenizer, num_sents=4000, batch_size=64)\n                        homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n                    except Exception as e:\n                        print(f\"[EVAL] Warmup failed: {e}\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    def _compute_similarity(pred: str, expected: str) -> float:\n        try:\n            pred_words = set(pred.lower().split())\n            exp_words = set(expected.lower().split())\n            if not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            return overlap / len(exp_words)\n        except Exception:\n            return 0.0\n\n    for _, _, _, expected_homos in test_sentences:\n        homograph_tracking['test_expected_homographs'].update([h.lower() for h in expected_homos])\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            'test_id': idx,\n            'success': False,\n            'translation_ok': False,\n            'explanations_count': 0,\n            'error': None,\n        }\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"[EVAL] translate_with_explanations not available\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'function_not_available'\n                error_tracking['per_test_status'].append(test_status)\n                continue\n\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                source_lang=_SOURCE_LANGUAGE,\n                target_lang=_TARGET_LANGUAGE,\n                device=_DEVICE,\n                max_length=_MAX_LENGTH,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                track_stats=False\n            )\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n\n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0))\n                    u_val = float(expl.get(\"uncertainty\", 0.0))\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n\n                    marker = f\"[S>{_SPAN_THRESHOLD:.2f}]\" if span_val > _SPAN_THRESHOLD else \"          \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics['confidences'].append(conf_val)\n                    quality_metrics['spans'].append(span_val)\n                    quality_metrics['uncertainties'].append(u_val)\n                    quality_metrics['total_confidence'] = quality_metrics.get('total_confidence', 0.0) + conf_val\n                    quality_metrics['confidence_samples'] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics['high_confidence_count'] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics['medium_confidence_count'] += 1\n                    else:\n                        quality_metrics['low_confidence_count'] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = str(word).replace('â–', '').replace('Ä ', '').strip().lower()\n                    homograph_tracking['explained_homographs'].add(clean_word)\n                    homograph_tracking['homograph_explanations'][clean_word].append({\n                        'sentence': src_text,\n                        'confidence': conf_val,\n                        'span': span_val,\n                        'uncertainty': u_val,\n                    })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status['explanations_count'] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            if translation and translation.strip() and translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            ):\n                successful_translations += 1\n                test_status['translation_ok'] = True\n                test_status['success'] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'translation_failed'\n\n            del result\n            if explanations:\n                del explanations\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking['oom_errors'] += 1\n                test_status['error'] = 'oom'\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking['timeout_errors'] += 1\n                test_status['error'] = 'timeout'\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'runtime'\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking['other_errors'] += 1\n            test_status['error'] = type(e).__name__\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking['per_test_status'].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics['per_test_times'].append(test_time)\n\n        print(\"-\" * 60)\n\n        if idx % 3 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    timing_metrics['total_time'] = time.time() - eval_start\n    if timing_metrics['per_test_times']:\n        timing_metrics['avg_test_time'] = (\n            sum(timing_metrics['per_test_times']) / len(timing_metrics['per_test_times'])\n        )\n\n    if quality_metrics['confidence_samples'] > 0:\n        quality_metrics['avg_confidence'] = (\n            quality_metrics['total_confidence'] / quality_metrics['confidence_samples']\n        )\n        quality_metrics['avg_span'] = (\n            sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n            if quality_metrics['spans']\n            else 0.0\n        )\n        quality_metrics['avg_uncertainty'] = (\n            sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n            if quality_metrics['uncertainties']\n            else 0.0\n        )\n\n        if quality_metrics['confidences']:\n            sorted_conf = sorted(quality_metrics['confidences'])\n            quality_metrics['confidence_p25'] = sorted_conf[len(sorted_conf) // 4]\n            quality_metrics['confidence_p50'] = sorted_conf[len(sorted_conf) // 2]\n            quality_metrics['confidence_p75'] = sorted_conf[3 * len(sorted_conf) // 4]\n    else:\n        quality_metrics['avg_confidence'] = 0.0\n        quality_metrics['avg_span'] = 0.0\n        quality_metrics['avg_uncertainty'] = 0.0\n\n    explained_from_dscd = homograph_tracking['explained_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    test_expected_discovered = homograph_tracking['test_expected_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    homograph_tracking['explained_from_dscd_rate'] = (\n        len(explained_from_dscd) / len(homograph_tracking['dscd_discovered_homographs'])\n        if homograph_tracking['dscd_discovered_homographs']\n        else 0.0\n    )\n    homograph_tracking['test_expected_discovery_rate'] = (\n        len(test_expected_discovered) / len(homograph_tracking['test_expected_homographs'])\n        if homograph_tracking['test_expected_homographs']\n        else 0.0\n    )\n    homograph_tracking['reference_discovery_rate'] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototype_stores\") or {})\n            else:\n                stores = dict(getattr(dscd, \"prototype_stores\") or {})\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for key, store in stores.items():\n                try:\n                    sz = int(store.size()) if hasattr(store, \"size\") else 0\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n            }\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] DSCD stats failed: {e}\")\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n\n    if 'confidence_p50' in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: \"\n            f\"{quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking['explained_homographs']:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking['explained_homographs'])[:10]:\n            exps = homograph_tracking['homograph_explanations'].get(homo, [])\n            count = len(exps)\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            in_dscd = \"[D]\" if homo in homograph_tracking['dscd_discovered_homographs'] else \"   \"\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\")\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats['total_words'] > 0:\n        print(\n            f\"  Multi-sense ratio: \"\n            f\"{dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        print(f\"  Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n        if 'source_accuracy' in asbn_stats:\n            print(f\"  Source accuracy: {asbn_stats['source_accuracy']:.2%}\")\n            print(f\"  Target accuracy: {asbn_stats['target_accuracy']:.2%}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        print(f\"  Total explanations: {trg_stats.get('explanations_generated', 0)}\")\n        print(f\"  High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum([\n        error_tracking['translation_failures'],\n        error_tracking['dscd_failures'],\n        error_tracking['trg_failures'],\n        error_tracking['timeout_errors'],\n        error_tracking['oom_errors'],\n        error_tracking['other_errors'],\n    ])\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = float(baseline_metrics.get('success_rate_pct', 0))\n            current_success = (\n                successful_translations / total_tests * 100.0\n            ) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n\n            baseline_expl = int(baseline_metrics.get('total_explanations', 0))\n            expl_delta = total_explanations - baseline_expl\n\n            baseline_quality_dict = baseline_metrics.get('quality_metrics', {})\n            if isinstance(baseline_quality_dict, dict):\n                baseline_quality = float(baseline_quality_dict.get('avg_confidence', 0))\n            else:\n                baseline_quality = 0.0\n            quality_delta = quality_metrics['avg_confidence'] - baseline_quality\n\n            print(f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics['avg_confidence']:.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n\n            baseline_homo_dict = baseline_metrics.get('homograph_tracking', {})\n            if isinstance(baseline_homo_dict, dict):\n                baseline_homo_rate = float(baseline_homo_dict.get('explained_from_dscd_rate', 0))\n                homo_delta = (\n                    homograph_tracking['explained_from_dscd_rate'] - baseline_homo_rate\n                )\n                print(\n                    f\"  Explanation rate: \"\n                    f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                    f\"({homo_delta:+.1%})\"\n                )\n        except Exception as e:\n            print(f\"  Comparison failed: {type(e).__name__}\")\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"No explanations generated\")\n    if dscd_stats['total_words'] < 100:\n        warnings.append(\"Very few prototypes (<100)\")\n    if quality_metrics['low_confidence_count'] > quality_metrics['high_confidence_count']:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking['explained_from_dscd_rate'] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    if asbn_stats and asbn_stats.get('domain_accuracy', 0) < 0.5:\n        warnings.append(\"ASBN domain accuracy <50%\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] All systems nominal\")\n\n    print(\"=\" * 80)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n    }\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\"*60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\"*60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model,\n            tokenizer,\n            run_warmup=False,\n            compare_baseline=False\n        )\n\n        assert 'total_tests' in result\n        assert 'quality_metrics' in result\n        assert 'homograph_tracking' in result\n\n        print(\"Evaluation pipeline test passed\")\n        print(\"=\"*60 + \"\\n\")\n        return True\n\n    except Exception as e:\n        print(f\"Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\"*60 + \"\\n\")\n        return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Testing & evaluation ready - PRODUCTION VERSION\")\nprint(\"=\" * 80)\nprint(\"ğŸ”§ CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX #1: Added source_lang=_SOURCE_LANGUAGE parameter (line 439)\")\nprint(\"  âœ… FIX #2: Added target_lang=_TARGET_LANGUAGE parameter (line 440)\")\nprint(\"  âœ… FIX #3: Added track_stats=False parameter (line 445)\")\nprint(\"  âœ… FIX #4: Aligned with Cell 8 fixes for proper language handling\")\nprint()\nprint(\"Evaluation metrics:\")\nprint(\"  - Translation quality (success rate)\")\nprint(\"  - Ambiguity detection (high-span, real ambiguous)\")\nprint(\"  - Explanation quality (confidence distribution)\")\nprint(\"  - Homograph discovery (DSCD vs reference)\")\nprint(\"  - DSCD prototype statistics\")\nprint(\"  - ASBN domain accuracy\")\nprint(\"  - TRG explanation statistics\")\nprint(\"  - Performance timing (total, avg per test)\")\nprint(\"  - Error tracking (OOM, timeout, other)\")\nprint()\nprint(f\"Configuration:\")\nprint(f\"  - Source language: {_SOURCE_LANGUAGE}\")\nprint(f\"  - Target language: {_TARGET_LANGUAGE}\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Max length: {_MAX_LENGTH}\")\nprint(f\"  - Device: {_DEVICE}\")\nprint(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\nprint(\"\\nâš¡ RESULT: Comprehensive evaluation with correct language configuration!\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"8uL574F8H4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:07:01.791878Z","iopub.execute_input":"2026-01-16T16:07:01.792179Z","iopub.status.idle":"2026-01-16T16:07:02.216651Z","shell.execute_reply.started":"2026-01-16T16:07:01.792128Z","shell.execute_reply":"2026-01-16T16:07:02.216007Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCell 9: Testing & evaluation ready - PRODUCTION VERSION\n================================================================================\nğŸ”§ CRITICAL FIXES APPLIED:\n  âœ… FIX #1: Added source_lang=_SOURCE_LANGUAGE parameter (line 439)\n  âœ… FIX #2: Added target_lang=_TARGET_LANGUAGE parameter (line 440)\n  âœ… FIX #3: Added track_stats=False parameter (line 445)\n  âœ… FIX #4: Aligned with Cell 8 fixes for proper language handling\n\nEvaluation metrics:\n  - Translation quality (success rate)\n  - Ambiguity detection (high-span, real ambiguous)\n  - Explanation quality (confidence distribution)\n  - Homograph discovery (DSCD vs reference)\n  - DSCD prototype statistics\n  - ASBN domain accuracy\n  - TRG explanation statistics\n  - Performance timing (total, avg per test)\n  - Error tracking (OOM, timeout, other)\n\nConfiguration:\n  - Source language: bn\n  - Target language: en\n  - Span threshold: 0.2\n  - Uncertainty threshold: 0.15\n  - Max length: 64\n  - Device: cuda:0\n  - Reference list: 41 words\n\nâš¡ RESULT: Comprehensive evaluation with correct language configuration!\n================================================================================\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE - PRODUCTION VERSION\n# ==============================================================================\n\nimport os\nimport sys\nimport time\nimport traceback\nimport inspect\nfrom typing import Tuple, Optional, Dict, Any\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers.modeling_outputs import BaseModelOutput\nimport collections\n\ntry:\n    if hasattr(torch.serialization, 'add_safe_globals'):\n        torch.serialization.add_safe_globals([\n            collections.defaultdict,\n            collections.OrderedDict,\n            collections.deque\n        ])\n        print(\"âœ“ Registered safe globals for PyTorch 2.6+\")\nexcept (AttributeError, Exception):\n    pass\n\ndef _g(name, default):\n    return globals().get(name, default)\n\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 52))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 5e-6))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", False))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 500))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 200))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 4000))\n    _SPAN_THRESHOLD = float(_g(\"SPAN_THRESHOLD\", 0.20))\n    _UNCERTAINTY_THRESHOLD = float(_g(\"UNCERTAINTY_THRESHOLD\", 0.15))\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(_g(\"HOMOGRAPH_REFERENCE_LIST_BN\",\n        [\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\"]))\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = bool(_g(\"FREEZE_ENCODER\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\n    _DEBUG_DISCOVERY = bool(_g(\"DEBUG_DISCOVERY\", False))\nexcept (ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 52\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 5e-6\n    _ENABLE_ASBN_TRAINING = False\n    _VALIDATION_CHECK_INTERVAL = 500\n    _PERIODIC_DISCOVERY_FREQUENCY = 200\n    _DSCD_WARMUP_SAMPLES = 4000\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.15\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\"}\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = False\n    _DEBUG_TIMING = False\n    _DEBUG_DISCOVERY = False\n\n_CHECKPOINT_DIR = \"/kaggle/working\"\n_CHECKPOINT_PATH = os.path.join(_CHECKPOINT_DIR, \"tatn_final.pt\")\n\n\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            globals()[\"clear_all_gpu_caches\"]()\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, None)\n        if result is None:\n            return default\n    return result\n\n\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import M2M100Tokenizer\n        tok = M2M100Tokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n        required = ['encode', 'decode', 'convert_ids_to_tokens', '__call__']\n        for method in required:\n            if not hasattr(tok, method):\n                raise RuntimeError(f\"Tokenizer missing: {method}\")\n        return tok\n    except Exception as e:\n        print(f\"[TOKENIZER] Load failed: {e}\")\n        raise\n\n\ndef _get_dscd_stores_safe(dscd):\n    try:\n        prototype_stores = getattr(dscd, 'prototype_stores', None)\n        if prototype_stores is None:\n            return {}\n        \n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        \n        try:\n            if lock:\n                try:\n                    with lock:\n                        return dict(prototype_stores)\n                except Exception:\n                    return dict(prototype_stores)\n            else:\n                return dict(prototype_stores)\n        except Exception:\n            return {}\n    except Exception:\n        return {}\n\n\ndef _get_core_model(model):\n    return model.module if hasattr(model, \"module\") else model\n\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        _safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    return True\n\n\ndef validate_component_compatibility(model_core, tokenizer):\n    print(\"\\n[VALIDATION] Checking component compatibility...\")\n    \n    issues = []\n    \n    try:\n        model_vocab = model_core.vocab_size\n        tokenizer_vocab = len(tokenizer) if hasattr(tokenizer, \"__len__\") else getattr(tokenizer, \"vocab_size\", 0)\n        \n        if model_vocab != tokenizer_vocab:\n            issues.append(f\"Vocab mismatch: model={model_vocab}, tokenizer={tokenizer_vocab}\")\n        else:\n            print(f\"  âœ… Vocabulary: {model_vocab}\")\n    except Exception as e:\n        issues.append(f\"Vocab check failed: {e}\")\n    \n    try:\n        model_embed_dim = int(getattr(model_core.mbart.config, \"d_model\", 1024))\n        print(f\"  âœ… Model embed_dim: {model_embed_dim}\")\n        \n        if hasattr(model_core, 'dscd'):\n            dscd_embed_dim = getattr(model_core.dscd, 'embed_dim', None)\n            if dscd_embed_dim is not None and dscd_embed_dim != model_embed_dim:\n                issues.append(f\"DSCD embed_dim mismatch: {dscd_embed_dim} != {model_embed_dim}\")\n            else:\n                print(f\"  âœ… DSCD embed_dim: {dscd_embed_dim}\")\n        \n        if hasattr(model_core, 'asbn'):\n            asbn_embed_dim = getattr(model_core.asbn, 'embed_dim', None)\n            if asbn_embed_dim is not None and asbn_embed_dim != model_embed_dim:\n                issues.append(f\"ASBN embed_dim mismatch: {asbn_embed_dim} != {model_embed_dim}\")\n            else:\n                print(f\"  âœ… ASBN embed_dim: {asbn_embed_dim}\")\n    except Exception as e:\n        issues.append(f\"Embed_dim check failed: {e}\")\n    \n    try:\n        embedding_layer = model_core.mbart.get_input_embeddings()\n        if embedding_layer is None:\n            issues.append(\"Model has no input embeddings\")\n        else:\n            actual_embed_dim = embedding_layer.embedding_dim\n            actual_vocab_size = embedding_layer.num_embeddings\n            print(f\"  âœ… Embedding layer: dim={actual_embed_dim}, vocab={actual_vocab_size}\")\n    except Exception as e:\n        issues.append(f\"Embedding layer check failed: {e}\")\n    \n    if issues:\n        print(\"\\n[VALIDATION] âŒ FAILED - Issues found:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n        raise RuntimeError(\"Component compatibility validation failed\")\n    else:\n        print(\"[VALIDATION] âœ… All components compatible\")\n    \n    return True\n\n\ndef validate_dataset_compatibility(dataset, tokenizer, model_vocab_size):\n    print(\"\\n[VALIDATION] Checking dataset compatibility...\")\n    \n    try:\n        sample_batch = []\n        for i in range(min(5, len(dataset))):\n            try:\n                sample_batch.append(dataset[i])\n            except Exception:\n                continue\n        \n        if not sample_batch:\n            print(\"[VALIDATION] âš ï¸  Could not load samples\")\n            return True\n        \n        max_input_id = 0\n        min_input_id = float('inf')\n        \n        for item in sample_batch:\n            input_ids = item.get('input_ids', None)\n            if input_ids is not None:\n                if isinstance(input_ids, torch.Tensor):\n                    max_input_id = max(max_input_id, input_ids.max().item())\n                    min_input_id = min(min_input_id, input_ids.min().item())\n                elif isinstance(input_ids, list):\n                    max_input_id = max(max_input_id, max(input_ids))\n                    min_input_id = min(min_input_id, min(input_ids))\n        \n        print(f\"  Input IDs range: [{min_input_id}, {max_input_id}]\")\n        print(f\"  Model vocab size: {model_vocab_size}\")\n        \n        if max_input_id >= model_vocab_size:\n            raise RuntimeError(\n                f\"Dataset contains out-of-bounds token IDs!\\n\"\n                f\"  Max ID: {max_input_id}\\n\"\n                f\"  Vocab size: {model_vocab_size}\\n\"\n                f\"  â†’ Cell 2 tokenization error or vocab mismatch\"\n            )\n        \n        if min_input_id < 0:\n            raise RuntimeError(f\"Dataset contains negative token IDs: {min_input_id}\")\n        \n        print(\"[VALIDATION] âœ… Dataset token IDs valid\")\n        return True\n        \n    except Exception as e:\n        print(f\"[VALIDATION] Dataset check failed: {e}\")\n        raise\n\n\ndef test_model_forward_pass(model, tokenizer, device):\n    print(\"\\n[VALIDATION] Testing model forward pass...\")\n    \n    try:\n        core_model = model.module if hasattr(model, 'module') else model\n        was_training = core_model.training\n        core_model.eval()\n        \n        test_text = \"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\"\n        \n        try:\n            tokenizer.src_lang = _SOURCE_LANGUAGE\n            tokenizer.tgt_lang = _TARGET_LANGUAGE\n        except Exception:\n            pass\n        \n        inputs = tokenizer(\n            test_text,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            truncation=True,\n            max_length=_MAX_LENGTH\n        )\n        \n        test_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        input_ids = inputs['input_ids'].to(test_device)\n        attention_mask = inputs['attention_mask'].to(test_device)\n        \n        original_device = next(core_model.parameters()).device\n        if test_device != original_device:\n            core_model = core_model.to(test_device)\n        \n        try:\n            with torch.no_grad():\n                print(\"  [TEST 1] Testing forward pass (inference mode)...\")\n                outputs = core_model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    src_texts=[test_text],\n                    token_word_map=None,\n                    labels=None,\n                    return_dict=True\n                )\n            \n            if isinstance(outputs, torch.Tensor):\n                print(\"  âœ… Forward pass successful (tensor output)\")\n                print(f\"  Output shape: {outputs.shape}\")\n            elif isinstance(outputs, dict):\n                print(\"  âœ… Forward pass successful (dict output)\")\n                print(f\"  Keys: {list(outputs.keys())}\")\n                \n                print(\"\\n  [TEST 2] Validating DSCD-augmented generation capability...\")\n                \n                h_augmented = outputs.get('sense_augmented_embeddings', None)\n                if h_augmented is not None and isinstance(h_augmented, torch.Tensor):\n                    print(f\"    âœ… DSCD embeddings: shape={h_augmented.shape}\")\n                    \n                    try:\n                        enc_wrapped = BaseModelOutput(\n                            last_hidden_state=h_augmented,\n                            hidden_states=None,\n                            attentions=None,\n                        )\n                        print(f\"    âœ… BaseModelOutput created successfully\")\n                        \n                        print(\"\\n  [TEST 3] Testing generate() with DSCD-augmented embeddings...\")\n                        generated = core_model.generate(\n                            input_ids=None,\n                            attention_mask=attention_mask,\n                            encoder_outputs=enc_wrapped,\n                            max_length=32,\n                            num_beams=3,\n                            early_stopping=True\n                        )\n                        \n                        if generated is not None and generated.numel() > 0:\n                            translation = tokenizer.decode(generated[0], skip_special_tokens=True)\n                            print(f\"    âœ… Generation successful: '{translation}'\")\n                            print(f\"    âœ… DSCD-AUGMENTED GENERATION WORKING!\")\n                        else:\n                            print(f\"    âš ï¸  Generation returned empty output\")\n                            \n                    except Exception as e:\n                        print(f\"    âŒ DSCD-augmented generation failed: {e}\")\n                        raise RuntimeError(f\"DSCD-augmented generation validation failed: {e}\")\n                else:\n                    print(f\"    âš ï¸  No DSCD embeddings in output\")\n            else:\n                print(f\"  âš ï¸  Unexpected output type: {type(outputs)}\")\n            \n            print(\"\\n[VALIDATION] âœ… Model forward pass and generation validated\")\n            \n        finally:\n            if test_device != original_device:\n                core_model = core_model.to(original_device)\n            if was_training:\n                core_model.train()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"[VALIDATION] âŒ Forward pass failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        raise RuntimeError(f\"Model forward pass validation failed: {e}\")\n\n\ndef main_pipeline() -> Tuple[object, object]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN MAIN PIPELINE - PRODUCTION VERSION\")\n    print(\"=\" * 80)\n    print(f\"Configuration:\")\n    print(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - Epochs: {_EPOCHS}\")\n    print(f\"  - Batch size: {_BATCH_SIZE}\")\n    print(\"=\" * 80)\n\n    pipeline_start = time.time()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    initialize_environment()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 1] Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n        tokenizer.tgt_lang = _TARGET_LANGUAGE\n    except Exception:\n        pass\n\n    try:\n        if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:\n            if hasattr(tokenizer, 'add_special_tokens'):\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n    except Exception:\n        pass\n\n    vocab_size = getattr(tokenizer, 'vocab_size', None)\n    if vocab_size is None:\n        try:\n            vocab_size = len(tokenizer)\n        except Exception:\n            vocab_size = 128112\n\n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size})\")\n    \n    if \"validate_tokenizer_vocab\" in globals():\n        try:\n            print(\"[PHASE 1] Validating tokenizer vocabulary...\")\n            validate_tokenizer_vocab(tokenizer, expected_vocab_size=None)\n        except Exception as e:\n            print(f\"[PHASE 1] Tokenizer validation warning: {e}\")\n    \n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(f\"\\n[PHASE 2] Loading data ({_NUM_SAMPLES} samples)...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals():\n        raise RuntimeError(\"MemoryEfficientDataset not found - run Cell 2\")\n    dataset = MemoryEfficientDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n    collate_fn = globals().get(\"safe_collate\", None)\n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = create_optimized_dataloader(dataset, batch_size=_BATCH_SIZE, shuffle=True)\n        except Exception:\n            dataloader_kwargs = {\n                'batch_size': _BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0,\n                'pin_memory': torch.cuda.is_available()\n            }\n            if collate_fn is not None:\n                dataloader_kwargs['collate_fn'] = collate_fn\n            train_loader = DataLoader(dataset, **dataloader_kwargs)\n    else:\n        dataloader_kwargs = {\n            'batch_size': _BATCH_SIZE,\n            'shuffle': True,\n            'num_workers': 0,\n            'pin_memory': torch.cuda.is_available()\n        }\n        if collate_fn is not None:\n            dataloader_kwargs['collate_fn'] = collate_fn\n        train_loader = DataLoader(dataset, **dataloader_kwargs)\n\n    try:\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {len(train_loader)} batches\")\n    except Exception:\n        print(\"[PHASE 2] Dataset loaded\")\n\n    del pairs\n    _safe_clear_gpu_caches()\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 3] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        raise RuntimeError(\"Model class not found - run Cell 6\")\n\n    model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n\n    try:\n        validate_component_compatibility(model_core, tokenizer)\n    except Exception as e:\n        print(f\"[PHASE 3] âŒ Component validation failed: {e}\")\n        raise\n\n    try:\n        validate_dataset_compatibility(dataset, tokenizer, model_core.vocab_size)\n    except Exception as e:\n        print(f\"[PHASE 3] âŒ Dataset validation failed: {e}\")\n        raise\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        device_ids = list(range(_NUM_GPUS))\n        print(f\"[PHASE 3] Using DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        model = model_core\n\n    model = model.to(_DEVICE)\n    core_model = _get_core_model(model)\n\n    try:\n        test_model_forward_pass(model, tokenizer, _DEVICE)\n    except Exception as e:\n        print(f\"[PHASE 3] âŒ Forward pass test failed: {e}\")\n        raise\n\n    if _FREEZE_ENCODER:\n        try:\n            for p in core_model.mbart.model.encoder.parameters():\n                p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n\n    print(f\"[PHASE 3] Model initialized and validated\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 4] Setting up optimizers...\")\n\n    try:\n        critic_params = list(core_model.asbn.critic_parameters()) if hasattr(core_model, \"asbn\") and hasattr(core_model.asbn, \"critic_parameters\") else []\n    except Exception:\n        critic_params = []\n\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n\n    phi_optimizer = None\n    if critic_params and _ENABLE_ASBN_TRAINING:\n        phi_optimizer = torch.optim.AdamW([p for p in critic_params if p.requires_grad], lr=_LR_PHI)\n        print(f\"[PHASE 4] ASBN optimizer created ({len([p for p in critic_params if p.requires_grad])} params)\")\n\n    print(f\"[PHASE 4] Optimizers ready\")\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 5] Training...\")\n    trained_model = model\n    training_stats = None\n\n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            try:\n                trg = getattr(core_model, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=(_VALIDATION_CHECK_INTERVAL > 0)\n            )\n            print(\"[PHASE 5] Training complete\")\n        except Exception as e:\n            print(f\"[PHASE 5] Training failed: {e}\")\n            trained_model = model\n    else:\n        print(\"[PHASE 5] Skipping training (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n    \n    del train_loader, dataset\n    _safe_clear_gpu_caches()\n    \n    core_model = _get_core_model(trained_model)\n    \n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 6] Discovery check...\")\n    discovery_success = False\n    \n    try:\n        dscd = getattr(core_model, 'dscd', None)\n        if dscd is None:\n            print(\"[PHASE 6] No DSCD module\")\n        else:\n            print(\"[PHASE 6] Running periodic discovery check...\")\n            if hasattr(dscd, 'periodic_discovery_check'):\n                try:\n                    sig = inspect.signature(dscd.periodic_discovery_check)\n                    params = list(sig.parameters.keys())\n                    print(f\"[PHASE 6] periodic_discovery_check params: {params}\")\n                    \n                    total_steps = int(_EPOCHS * _NUM_SAMPLES // _BATCH_SIZE)\n                    \n                    if 'cluster_missing' in params:\n                        if len(params) >= 3:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, _PERIODIC_DISCOVERY_FREQUENCY, cluster_missing=False)\n                        elif len(params) >= 2:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, cluster_missing=False)\n                        else:\n                            num_discovered = dscd.periodic_discovery_check(cluster_missing=False)\n                    else:\n                        if len(params) >= 2:\n                            num_discovered = dscd.periodic_discovery_check(total_steps, _PERIODIC_DISCOVERY_FREQUENCY)\n                        elif len(params) >= 1:\n                            num_discovered = dscd.periodic_discovery_check(total_steps)\n                        else:\n                            num_discovered = dscd.periodic_discovery_check()\n                    \n                    discovery_success = True\n                    print(f\"[PHASE 6] Discovery complete: {num_discovered} homographs found\")\n                except Exception as e:\n                    print(f\"[PHASE 6] periodic_discovery_check failed: {e}\")\n                    try:\n                        if hasattr(dscd, 'discover_homographs'):\n                            num_discovered = dscd.discover_homographs()\n                            discovery_success = True\n                            print(f\"[PHASE 6] Fallback discovery: {num_discovered} homographs\")\n                        else:\n                            print(\"[PHASE 6] discover_homographs not available\")\n                    except Exception as e2:\n                        print(f\"[PHASE 6] Fallback discovery failed: {e2}\")\n            else:\n                print(\"[PHASE 6] periodic_discovery_check not available\")\n                if hasattr(dscd, 'discover_homographs'):\n                    try:\n                        num_discovered = dscd.discover_homographs()\n                        discovery_success = True\n                        print(f\"[PHASE 6] discover_homographs: {num_discovered} homographs\")\n                    except Exception as e:\n                        print(f\"[PHASE 6] discover_homographs failed: {e}\")\n            \n            stores = _get_dscd_stores_safe(dscd)\n            \n            def _store_size(s):\n                try:\n                    if callable(getattr(s, \"size\", None)):\n                        return int(s.size())\n                    return int(getattr(s, \"size\", 0))\n                except Exception:\n                    return 0\n            \n            total_protos = sum(_store_size(store) for store in stores.values())\n            multi_sense = sum(1 for store in stores.values() if _store_size(store) >= 2)\n            \n            print(\"[PHASE 6] Discovery state:\")\n            print(f\"  - Tokens: {len(stores)}\")\n            print(f\"  - Prototypes: {total_protos}\")\n            print(f\"  - Multi-sense: {multi_sense}\")\n            \n            if len(stores) == 0:\n                print(\"[PHASE 6] WARNING: No prototypes created\")\n            else:\n                discovery_success = True\n    except Exception as e:\n        print(f\"[PHASE 6] Discovery failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Discovery: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 7] DSCD warmup...\")\n    if \"dscd_discovery_warmup\" in globals():\n        try:\n            warmup_samples = min(4000, _DSCD_WARMUP_SAMPLES)\n            print(f\"[PHASE 7] Processing {warmup_samples} warmup samples...\")\n            warmup_start = time.time()\n            dscd_discovery_warmup(trained_model, tokenizer, num_sents=warmup_samples, batch_size=64, max_len=_MAX_LENGTH)\n            warmup_duration = time.time() - warmup_start\n            print(f\"[PHASE 7] Warmup complete ({warmup_samples} samples in {warmup_duration:.1f}s)\")\n        except Exception as e:\n            print(f\"[PHASE 7] Warmup failed: {e}\")\n    else:\n        print(\"[PHASE 7] Skipping warmup (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Warmup: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 8] Baseline evaluation...\")\n    baseline_metrics = None\n\n    try:\n        dscd_baseline = getattr(core_model, 'dscd', None)\n        has_prototypes = False\n\n        if dscd_baseline:\n            stores = _get_dscd_stores_safe(dscd_baseline)\n            has_prototypes = len(stores) > 0\n\n        if not has_prototypes:\n            print(\"[PHASE 8] Skipping baseline (no prototypes)\")\n        elif \"comprehensive_post_training_testing\" in globals():\n            try:\n                trg = getattr(core_model, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n\n            print(\"[PHASE 8] âš¡ Running baseline with DSCD-augmented generation...\")\n            baseline_metrics = comprehensive_post_training_testing(trained_model, tokenizer, run_warmup=False)\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            print(f\"[PHASE 8] Baseline: {baseline_success:.1f}% success, {baseline_expl} explanations\")\n        else:\n            print(\"[PHASE 8] Skipping baseline (function not found)\")\n    except Exception as e:\n        print(f\"[PHASE 8] Baseline failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Baseline: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 9] Post-training evaluation...\")\n    eval_results: Dict[str, Any] = {}\n\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            try:\n                trg = getattr(core_model, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n            \n            print(\"[PHASE 9] âš¡ Running evaluation with DSCD-augmented generation...\")\n            eval_results = comprehensive_post_training_testing(\n                trained_model,\n                tokenizer,\n                run_warmup=False,\n                compare_baseline=(baseline_metrics is not None),\n                baseline_metrics=baseline_metrics\n            )\n            final_success = eval_results.get('success_rate_pct', 0)\n            final_expl = eval_results.get('total_explanations', 0)\n            print(f\"[PHASE 9] Evaluation: {final_success:.1f}% success, {final_expl} explanations\")\n        except Exception as e:\n            print(f\"[PHASE 9] Evaluation failed: {e}\")\n    else:\n        print(\"[PHASE 9] Skipping evaluation (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 10] Saving checkpoint...\")\n    try:\n        os.makedirs(_CHECKPOINT_DIR, exist_ok=True)\n        was_training = getattr(core_model, \"training\", False)\n        core_model.eval()\n        try:\n            model_state = core_model.state_dict()\n            dscd_state = {}\n            \n            if hasattr(core_model, 'dscd'):\n                dscd_save = core_model.dscd\n                if hasattr(dscd_save, 'state_dict'):\n                    lock = None\n                    if hasattr(dscd_save, 'buffer_lock'):\n                        lock = dscd_save.buffer_lock\n                    elif hasattr(dscd_save, 'clustering_lock'):\n                        lock = dscd_save.clustering_lock\n                    \n                    try:\n                        if lock:\n                            try:\n                                with lock:\n                                    dscd_state = dscd_save.state_dict()\n                            except Exception:\n                                dscd_state = dscd_save.state_dict()\n                        else:\n                            dscd_state = dscd_save.state_dict()\n                    except Exception as e:\n                        print(f\"[PHASE 10] DSCD state_dict failed: {e}\")\n                        dscd_state = {}\n            \n            optimizer_state = None\n            if optimizer is not None:\n                try:\n                    optimizer_state = optimizer.state_dict()\n                    if 'state' in optimizer_state:\n                        for param_state in optimizer_state['state'].values():\n                            if isinstance(param_state, dict) and 'momentum_buffer' in param_state:\n                                try:\n                                    del param_state['momentum_buffer']\n                                except Exception:\n                                    pass\n                except Exception:\n                    optimizer_state = None\n            \n            checkpoint = {\n                'model_state_dict': model_state,\n                'dscd_state': dscd_state,\n                'optimizer_state_dict': optimizer_state,\n                'training_stats': training_stats,\n                'baseline_metrics': baseline_metrics,\n                'eval_results': eval_results,\n                'discovery_success': discovery_success,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'config': {\n                    'epochs': _EPOCHS,\n                    'batch_size': _BATCH_SIZE,\n                    'span_threshold': _SPAN_THRESHOLD,\n                    'uncertainty_threshold': _UNCERTAINTY_THRESHOLD,\n                    'discovery_frequency': _PERIODIC_DISCOVERY_FREQUENCY,\n                    'vocab_size': vocab_size,\n                }\n            }\n            torch.save(checkpoint, _CHECKPOINT_PATH)\n            \n            try:\n                import mmap\n                with open(_CHECKPOINT_PATH, 'rb') as f:\n                    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as m:\n                        size_mb = len(m) / 1024**2\n                \n                print(f\"[PHASE 10] Checkpoint saved: {_CHECKPOINT_PATH}\")\n                print(f\"  - Size: {size_mb:.2f} MB\")\n                \n                try:\n                    verify_keys = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n                    has_model = 'model_state_dict' in verify_keys and len(verify_keys['model_state_dict']) > 0\n                    has_dscd = 'dscd_state' in verify_keys and len(verify_keys.get('dscd_state', {})) > 0\n                    print(f\"  - Model: {'OK' if has_model else 'MISSING'}\")\n                    print(f\"  - DSCD: {'OK' if has_dscd else 'MISSING'}\")\n                    \n                    if has_dscd:\n                        try:\n                            dscd_verify_state = verify_keys.get('dscd_state', {})\n                            num_tokens = 0\n                            if 'prototype_stores' in dscd_verify_state:\n                                num_tokens = len(dscd_verify_state['prototype_stores'])\n                            print(f\"  - DSCD tokens: {num_tokens}\")\n                        except Exception:\n                            print(f\"  - DSCD tokens: unknown\")\n                    \n                    del verify_keys\n                except Exception as e:\n                    print(f\"[PHASE 10] Checkpoint verification warning: {e}\")\n            except Exception:\n                print(f\"[PHASE 10] Checkpoint saved: {_CHECKPOINT_PATH}\")\n        finally:\n            if was_training:\n                try:\n                    core_model.train()\n                except Exception:\n                    pass\n    except Exception as e:\n        print(f\"[PHASE 10] Checkpoint failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Checkpoint: {time.time() - phase_start:.2f}s\")\n    \n    print(\"\\n[PHASE 11] Final validation...\")\n    try:\n        dscd_ok = False\n        if hasattr(core_model, 'dscd'):\n            stores = _get_dscd_stores_safe(core_model.dscd)\n            dscd_ok = len(stores) > 0\n        \n        asbn_ok = hasattr(core_model, 'asbn') and hasattr(core_model.asbn, 'forward')\n        trg_ok = hasattr(core_model, 'trg_system') and hasattr(core_model.trg_system, 'process_sentence_for_explanations')\n        \n        print(f\"[PHASE 11] Component validation:\")\n        print(f\"  - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n        print(f\"  - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n        print(f\"  - TRG: {'OK' if trg_ok else 'MISSING'}\")\n        \n        all_ok = dscd_ok and asbn_ok and trg_ok\n        if all_ok:\n            print(\"[PHASE 11] âœ… All components validated\")\n        else:\n            print(\"[PHASE 11] âš ï¸  Some components missing\")\n    except Exception as e:\n        print(f\"[PHASE 11] Validation failed: {e}\")\n\n    pipeline_time = time.time() - pipeline_start\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"\\n[TIMING]\")\n    print(f\"  Total time: {pipeline_time:.2f}s ({pipeline_time/60:.2f} min)\")\n\n    print(f\"\\n[TRAINING]\")\n    if training_stats:\n        total_loss = training_stats.get('total_loss', [])\n        optimizer_updates = training_stats.get('optimizer_updates', 0)\n        print(f\"  Completed: {optimizer_updates} optimizer updates\")\n        if total_loss:\n            recent_loss = sum(total_loss[-100:]) / len(total_loss[-100:])\n            print(f\"  - Final loss: {recent_loss:.6f}\")\n    else:\n        print(\"  No stats available\")\n\n    print(f\"\\n[DISCOVERY]\")\n    if discovery_success:\n        print(\"  âœ… Success\")\n    else:\n        print(\"  âš ï¸  Issues detected\")\n\n    print(f\"\\n[EVALUATION]\")\n    if baseline_metrics and eval_results:\n        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n        final_success = eval_results.get('success_rate_pct', 0)\n        improvement = final_success - baseline_success\n\n        print(f\"  Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n        print(f\"  Improvement: {improvement:+.1f}%\")\n\n        baseline_dscd_stats = baseline_metrics.get('dscd_stats', {})\n        final_dscd_stats = eval_results.get('dscd_stats', {})\n\n        baseline_dscd = baseline_dscd_stats.get('multi_sense_words', 0) if isinstance(baseline_dscd_stats, dict) else 0\n        final_dscd = final_dscd_stats.get('multi_sense_words', 0) if isinstance(final_dscd_stats, dict) else 0\n\n        if baseline_dscd is not None and final_dscd is not None:\n            print(f\"  DSCD multi-sense: {baseline_dscd} -> {final_dscd}\")\n\n        baseline_asbn_stats = baseline_metrics.get('asbn_stats', {})\n        final_asbn_stats = eval_results.get('asbn_stats', {})\n\n        baseline_asbn = baseline_asbn_stats.get('domain_accuracy', 0) if isinstance(baseline_asbn_stats, dict) else 0\n        final_asbn = final_asbn_stats.get('domain_accuracy', 0) if isinstance(final_asbn_stats, dict) else 0\n\n        if baseline_asbn is not None and final_asbn is not None:\n            print(f\"  ASBN accuracy: {baseline_asbn:.2%} -> {final_asbn:.2%}\")\n    elif eval_results:\n        print(f\"  Success rate: {eval_results.get('success_rate_pct', 0):.1f}%\")\n    else:\n        print(\"  No results\")\n\n    print(f\"\\n[CHECKPOINT]\")\n    if os.path.exists(_CHECKPOINT_PATH):\n        try:\n            size_mb = os.path.getsize(_CHECKPOINT_PATH) / 1024**2\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n            print(f\"  - Size: {size_mb:.2f} MB\")\n        except Exception:\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n    else:\n        print(\"  Not saved\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Usage: trained_model, tokenizer = main_pipeline()\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    return trained_model, tokenizer\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 10: Main pipeline ready - PRODUCTION VERSION WITH DSCD FIXES\")\nprint(\"=\" * 80)\nprint(\"ğŸ”§ CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX 1: Added BaseModelOutput import (line 13)\")\nprint(\"  âœ… FIX 2: Enhanced forward pass validation (lines 318-357)\")\nprint(\"  âœ… FIX 3: DSCD-augmented generation test (lines 330-350)\")\nprint(\"  âœ… FIX 4: Debug logging for evaluation phases (lines 718, 742)\")\nprint(\"  âœ… FIX 5: Added _DEBUG_DISCOVERY global (line 71)\")\nprint(\"\\nâš¡ RESULT: Full validation of DSCD-augmented generation pipeline!\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"kEux2BVXH4J5","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:07:02.218082Z","iopub.execute_input":"2026-01-16T16:07:02.218471Z","iopub.status.idle":"2026-01-16T16:07:02.302387Z","shell.execute_reply.started":"2026-01-16T16:07:02.218438Z","shell.execute_reply":"2026-01-16T16:07:02.301504Z"}},"outputs":[{"name":"stdout","text":"âœ“ Registered safe globals for PyTorch 2.6+\n\n================================================================================\nCell 10: Main pipeline ready - PRODUCTION VERSION WITH DSCD FIXES\n================================================================================\nğŸ”§ CRITICAL FIXES APPLIED:\n  âœ… FIX 1: Added BaseModelOutput import (line 13)\n  âœ… FIX 2: Enhanced forward pass validation (lines 318-357)\n  âœ… FIX 3: DSCD-augmented generation test (lines 330-350)\n  âœ… FIX 4: Debug logging for evaluation phases (lines 718, 742)\n  âœ… FIX 5: Added _DEBUG_DISCOVERY global (line 71)\n\nâš¡ RESULT: Full validation of DSCD-augmented generation pipeline!\n================================================================================\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER - PRODUCTION VERSION\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\n\ntry:\n    _NUM_SAMPLES = int(globals().get('NUM_SAMPLES', 30000))\n    _EPOCHS = int(globals().get('EPOCHS', 2))\n    _BATCH_SIZE = int(globals().get('BATCH_SIZE', 4))\n    _ACCUMULATION_STEPS = int(globals().get('ACCUMULATION_STEPS', 16))\n    \n    raw_device = globals().get('DEVICE', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        _DEVICE = torch.device(str(raw_device))\n\n    _ENABLE_ASBN_TRAINING = bool(globals().get('ENABLE_ASBN_TRAINING', True))\n    _ENABLE_TRG_INFERENCE = bool(globals().get('ENABLE_TRG_INFERENCE', True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(globals().get('PERIODIC_DISCOVERY_FREQUENCY', 200))\n    _VERBOSE_LOGGING = bool(globals().get('VERBOSE_LOGGING', False))\n    _DEBUG_DISCOVERY = bool(globals().get('DEBUG_DISCOVERY', False))\n    _DEBUG_TIMING = bool(globals().get('DEBUG_TIMING', False))\n    _NUM_GPUS = int(globals().get('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(globals().get('USE_MULTI_GPU', _NUM_GPUS > 1))\n    _SPAN_THRESHOLD = float(globals().get('SPAN_THRESHOLD', 0.20))\n    _UNCERTAINTY_THRESHOLD = float(globals().get('UNCERTAINTY_THRESHOLD', 0.15))\n    _MAX_LENGTH = int(globals().get('MAX_LENGTH', 52))\n    _SOURCE_LANGUAGE = str(globals().get('SOURCE_LANGUAGE', 'bn'))\n    _TARGET_LANGUAGE = str(globals().get('TARGET_LANGUAGE', 'en'))\n    \n    raw_list = globals().get('HOMOGRAPH_REFERENCE_LIST_BN', [\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_list)\n    cell0_loaded = 'NUM_SAMPLES' in globals()\n    \nexcept (NameError, TypeError, ValueError) as e:\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 200\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.15\n    _MAX_LENGTH = 52\n    _SOURCE_LANGUAGE = 'bn'\n    _TARGET_LANGUAGE = 'en'\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\"}\n    cell0_loaded = False\n    print(\"[EXEC] Using fallback configuration (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\n\ndef _format_duration(seconds: float) -> str:\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}min\"\n    else:\n        return f\"{seconds/3600:.2f}hr\"\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        if key not in result:\n            return default\n        result = result[key]\n    return result if result is not None else default\n\n\ndef _get_dscd_homographs(model):\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n\n        if dscd and hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        if dscd and hasattr(dscd, 'prototype_stores'):\n            homographs = set()\n\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            try:\n                if lock:\n                    try:\n                        with lock:\n                            stores = dict(dscd.prototype_stores)\n                    except Exception:\n                        stores = dict(dscd.prototype_stores)\n                else:\n                    stores = dict(dscd.prototype_stores)\n            except Exception:\n                return set()\n\n            for token, store in stores.items():\n                try:\n                    size_ok = False\n                    if hasattr(store, 'size'):\n                        size_attr = getattr(store, 'size')\n                        if callable(size_attr):\n                            try:\n                                size_val = size_attr()\n                                size_ok = int(size_val) >= 1\n                            except Exception:\n                                size_ok = False\n                        elif isinstance(size_attr, int):\n                            size_ok = size_attr >= 1\n                    \n                    if size_ok:\n                        clean = str(token).replace('â–', '').replace('Ä ', '').replace('##', '').strip().lower()\n                        if clean:\n                            homographs.add(clean)\n                except Exception:\n                    continue\n            return homographs\n    except Exception:\n        pass\n    return set()\n\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN - PRODUCTION EXECUTION\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    print(f\"  Source language: {_SOURCE_LANGUAGE}\")\n    print(f\"  Target language: {_TARGET_LANGUAGE}\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  Max length: {_MAX_LENGTH}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"  Batch per GPU: {per_gpu}\")\n\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    if 'main_pipeline' not in globals():\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n\n            if _DEBUG_TIMING:\n                print(\"   Expected: ~15-45 min (config dependent)\")\n\n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n\n            if \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n\n                print(\"\\nFixes:\")\n                print(\"   1. Reduce BATCH_SIZE (try 2-4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10k-20k)\")\n                print(\"   3. Increase ACCUMULATION_STEPS (32-64)\")\n\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    checkpoint_dict = None\n\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024**2)\n                print(f\"  File: {_CHECKPOINT_PATH}\")\n                print(f\"  Size: {size_mb:.1f} MB\")\n\n                checkpoint_dict = torch.load(_CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n\n                has_model = 'model_state_dict' in checkpoint_dict and len(checkpoint_dict['model_state_dict']) > 0\n                has_dscd = 'dscd_state' in checkpoint_dict and len(checkpoint_dict.get('dscd_state', {})) > 0\n\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n\n                if has_dscd:\n                    try:\n                        dscd_state = checkpoint_dict['dscd_state']\n                        num_tokens = 0\n                        \n                        if 'prototype_stores' in dscd_state:\n                            num_tokens = len(dscd_state['prototype_stores'])\n                        elif 'prototype_stores_data' in dscd_state:\n                            num_tokens = len(dscd_state['prototype_stores_data'])\n                        \n                        print(f\"  Tokens: {num_tokens}\")\n\n                        if num_tokens > 0:\n                            checkpoint_valid = True\n                            print(\"  Status: VALID\")\n                        else:\n                            print(\"  Status: EMPTY DSCD\")\n                    except Exception as e:\n                        print(f\"  Status: VALIDATION ERROR ({str(e)[:50]})\")\n                else:\n                    print(\"  Status: MISSING DSCD\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n\n        except Exception as e:\n            print(f\"  Validation failed: {e}\")\n            checkpoint_dict = None\n\n        print(\"\\n[COMPONENTS]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd = getattr(core, 'dscd', None)\n            if dscd and hasattr(dscd, 'get_prototype_summary'):\n                try:\n                    dscd_stats = dscd.get_prototype_summary()\n                    print(\"  DSCD:\")\n                    print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                    print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                    print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                except Exception:\n                    pass\n\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                try:\n                    asbn_stats = asbn.get_detailed_stats()\n                    print(\"  ASBN:\")\n                    print(f\"    - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n                    if 'source_accuracy' in asbn_stats:\n                        print(f\"    - Source: {asbn_stats['source_accuracy']:.2%}\")\n                        print(f\"    - Target: {asbn_stats['target_accuracy']:.2%}\")\n                except Exception:\n                    pass\n\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'get_statistics'):\n                try:\n                    trg_stats = trg.get_statistics()\n                    print(\"  TRG:\")\n                    print(f\"    - Explanations: {trg_stats.get('explanations_generated', 0)}\")\n                    print(f\"    - High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                    print(f\"    - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"  Stats failed: {e}\")\n\n        print(\"\\n[METRICS]\")\n\n        try:\n            if checkpoint_dict is not None:\n                training_stats = checkpoint_dict.get('training_stats', {})\n                if training_stats:\n                    total_loss = training_stats.get('total_loss', [])\n                    updates = training_stats.get('optimizer_updates', 0)\n\n                    print(\"  Training:\")\n                    print(f\"    - Updates: {updates}\")\n                    if total_loss:\n                        if len(total_loss) >= 100:\n                            final = sum(total_loss[-100:]) / len(total_loss[-100:])\n                        else:\n                            final = sum(total_loss) / len(total_loss)\n                        print(f\"    - Final loss: {final:.6f}\")\n\n                eval_results = checkpoint_dict.get('eval_results', {})\n                baseline = checkpoint_dict.get('baseline_metrics', {})\n\n                if eval_results:\n                    final_success = eval_results.get('success_rate_pct', 0)\n                    total_expl = eval_results.get('total_explanations', 0)\n\n                    print(\"  Evaluation:\")\n                    if baseline:\n                        baseline_success = baseline.get('success_rate_pct', 0)\n                        improvement = final_success - baseline_success\n                        print(f\"    - Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n                        print(f\"    - Improvement: {improvement:+.1f}%\")\n                    else:\n                        print(f\"    - Success: {final_success:.1f}%\")\n\n                    print(f\"    - Explanations: {total_expl}\")\n\n                    quality = eval_results.get('quality_metrics', {})\n                    if quality:\n                        print(f\"    - Avg confidence: {quality.get('avg_confidence', 0):.3f}\")\n            elif os.path.exists(_CHECKPOINT_PATH):\n                print(\"  Checkpoint loaded but invalid format\")\n            else:\n                print(\"  No checkpoint available\")\n\n        except Exception as e:\n            print(f\"  Metrics failed: {e}\")\n\n        del checkpoint_dict\n        _safe_cleanup()\n\n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing disambiguation on ambiguous sentences...\")\n        print(\"-\" * 80)\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n        inference_times = []\n\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n        test_sentences = [\n            (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"à¦•à¦² (tap/call)\"),\n            (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"à¦•à¦¾à¦² (tomorrow/yesterday)\"),\n            (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"à¦ªà¦¾à¦¤à¦¾ (leaf/page)\"),\n        ]\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"translate_with_explanations not available\")\n                print(\"   -> Run Cell 8 before Cell 11\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}.  {desc}\")\n                        print(f\"   Input: {sentence}\")\n\n                        inf_start = time.time()\n                        \n                        res = translate_with_explanations(\n                            trained_model, \n                            tokenizer, \n                            sentence,\n                            source_lang=_SOURCE_LANGUAGE,\n                            target_lang=_TARGET_LANGUAGE,\n                            device=_DEVICE,\n                            max_length=_MAX_LENGTH,\n                            span_threshold=_SPAN_THRESHOLD,\n                            uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                            track_stats=True\n                        )\n                        \n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n\n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations', []) or []\n\n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs:\n                                for exp in exs:\n                                    word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                    clean = str(word).replace('â–', '').replace('Ä ', '').strip().lower()\n\n                                    if clean in dscd_homographs:\n                                        dscd_homographs_detected.add(clean)\n\n                                    try:\n                                        conf = float(exp.get('confidence', 0.5))\n                                        span = float(exp.get('span', 0.0))\n                                        u = float(exp.get('uncertainty', 0.0))\n                                        print(f\"   -> '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                    except Exception:\n                                        print(f\"   -> '{word}': (no metrics)\")\n\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n\n                        _safe_cleanup()\n\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__} - {str(e)[:100]}\")\n                        inference_failed += 1\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n\n                if inference_times:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg per sentence\")\n\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                else:\n                    print(\"No DSCD homographs detected\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries (run warmup)\")\n                    else:\n                        print(f\"   -> Check TRG thresholds (span={_SPAN_THRESHOLD}, u={_UNCERTAINTY_THRESHOLD})\")\n\n                if 'INFERENCE_STATS' in globals():\n                    try:\n                        print(\"\\n\" + \"-\" * 80)\n                        print(\"AGGREGATED STATISTICS (from Cell 8):\")\n                        print(\"-\" * 80)\n                        INFERENCE_STATS.print_summary()\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(f\"Failed to print INFERENCE_STATS: {e}\")\n                else:\n                    if _DEBUG_DISCOVERY:\n                        print(\"\\nINFERENCE_STATS not available (Cell 8 not loaded)\")\n\n        except Exception as e:\n            print(f\"Validation failed: {e}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n[SYSTEM TEST]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd_ok = hasattr(core, 'dscd') and hasattr(core.dscd, 'forward')\n            asbn_ok = hasattr(core, 'asbn') and hasattr(core.asbn, 'forward')\n            trg_ok = hasattr(core, 'trg_system') and hasattr(core.trg_system, 'process_sentence_for_explanations')\n            mbart_ok = hasattr(core, 'mbart') and hasattr(core.mbart, 'generate')\n\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n            print(f\"    - TRG: {'OK' if trg_ok else 'MISSING'}\")\n            print(f\"    - M2M100: {'OK' if mbart_ok else 'MISSING'}\")\n\n            all_ok = dscd_ok and asbn_ok and trg_ok and mbart_ok\n\n            if all_ok:\n                print(\"  All components operational\")\n            else:\n                print(\"  Some components missing\")\n\n        except Exception as e:\n            print(f\"  Test failed: {e}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n\n        print(\"\\n1. Single translation:\")\n        print(f\"   result = translate_with_explanations(trained_model, tokenizer, 'à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤', source_lang='{_SOURCE_LANGUAGE}', target_lang='{_TARGET_LANGUAGE}', device=_DEVICE, max_length=_MAX_LENGTH)\")\n\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(f\"       res = translate_with_explanations(trained_model, tokenizer, sent, source_lang='{_SOURCE_LANGUAGE}', target_lang='{_TARGET_LANGUAGE}', device=_DEVICE, max_length=_MAX_LENGTH)\")\n\n        print(\"\\n3. Load checkpoint:\")\n        print(\"   ckpt = torch.load('/kaggle/working/tatn_final.pt', weights_only=False)\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if not checkpoint_valid:\n            print(\"\\nCheckpoint needs verification - re-run Cell 10 if needed\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n\n        print(\"\\n[DIAGNOSTICS]\")\n\n        components = {\n            'Cell 0': 'NUM_SAMPLES' in globals(),\n            'Cell 1': 'reconstruct_word_spans' in globals(),\n            'Cell 2': 'MemoryEfficientDataset' in globals(),\n            'Cell 3': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8': 'translate_with_explanations' in globals(),\n            'Cell 9': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10': 'main_pipeline' in globals(),\n        }\n\n        all_present = True\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status} {comp}\")\n            if not present:\n                all_present = False\n\n        print(\"\\n[RECOVERY]\")\n\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"  ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"  Then RESTART kernel and re-run Cells 0-11\")\n\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory in Cell 0:\")\n            print(\"  BATCH_SIZE = 2\")\n            print(\"  NUM_SAMPLES = 15000\")\n            print(\"  ACCUMULATION_STEPS = 32\")\n            print(\"  Then re-run Cells 0-11\")\n\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0:\")\n            print(\"  VERBOSE_LOGGING = True\")\n            print(\"  DEBUG_DISCOVERY = True\")\n            print(\"  Then re-run Cell 11 for details\")\n\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check checkpoint exists:\")\n            print(f\"  os.path.exists('{_CHECKPOINT_PATH}')\")\n            print(\"  If yes, can load and skip training\")\n            print(\"  If no, re-run Cell 11\")\n\n        else:\n            print(\"\\n-> General steps:\")\n            print(\"  1. Enable DEBUG in Cell 0\")\n            print(\"  2. Re-run Cells 0-11\")\n            print(\"  3. Check GPU: torch.cuda.is_available()\")\n            print(\"  4. Verify data loaded\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS\")\n        if 'checkpoint_valid' in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID\")\n        else:\n            print(\"Checkpoint: CHECK NEEDED\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n\n    print(\"=\" * 80)\n\n    _safe_cleanup()\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution wrapper ready - PRODUCTION VERSION\")\nprint(\"=\" * 80)\nprint(\"ğŸ”§ CRITICAL FIXES APPLIED:\")\nprint(\"  âœ… FIX 1: Fixed variable names in usage examples (_DEVICE, _MAX_LENGTH)\")\nprint(\"  âœ… FIX 2: Added explicit track_stats=True parameter (line 473)\")\nprint(\"  âœ… FIX 3: Added INFERENCE_STATS.print_summary() after validation (line 524)\")\nprint(\"  âœ… FIX 4: Added INFERENCE_STATS availability check (line 518)\")\nprint(\"\\nâš¡ RESULT: Complete statistics tracking and accurate usage examples!\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"9n4Hrn1wH4J6","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T16:07:02.304080Z","iopub.execute_input":"2026-01-16T16:07:02.304368Z","execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nMEMORY-OPTIMIZED TATN - PRODUCTION EXECUTION\n================================================================================\nUser: manas0003\nStarted: 2026-01-16 16:07:02 UTC\n\n[CONFIGURATION]\n  Cell 0 status: Loaded\n  Samples: 40000\n  Epochs: 1\n  Batch Size: 50\n  Accumulation: 16\n  Device: cuda:0\n  Multi-GPU: ENABLED (2 GPUs)\n  Source language: bn\n  Target language: en\n  Span threshold: 0.2\n  Uncertainty threshold: 0.15\n  Max length: 64\n  Discovery frequency: 200\n  Batch per GPU: 25\n  ASBN: Enabled\n  TRG: Enabled\n  Debug: Disabled\n================================================================================\n\nStarting pipeline...\n   Expected: ~15-45 min (config dependent)\n\n================================================================================\nTATN MAIN PIPELINE - PRODUCTION VERSION\n================================================================================\nConfiguration:\n  - Span threshold: 0.2\n  - Uncertainty threshold: 0.15\n  - Discovery frequency: 200\n  - Epochs: 1\n  - Batch size: 50\n================================================================================\n[PIPELINE] Initializing environment...\n[PIPELINE] GPUs: 2\n  GPU 0: Tesla T4 (14.7 GB)\n  GPU 1: Tesla T4 (14.7 GB)\n[TIMING] Initialization: 0.51s\n\n[PHASE 1] Loading tokenizer...\n[PHASE 1] Tokenizer loaded (vocab: 128004)\n[PHASE 1] Validating tokenizer vocabulary...\n[TOKENIZER-VALIDATION] Actual vocab size: 128104\n[TOKENIZER-VALIDATION] Language tokens:\n  __bn__ â†’ 128012\n  __en__ â†’ 128022\n[TOKENIZER-VALIDATION] âœ… Language tokens valid\n[TIMING] Tokenizer: 0.90s\n\n[PHASE 2] Loading data (40000 samples)...\n[CELL2] Loading up to 40000 samples from local CSV: /kaggle/input/samanantar/samanantar_bn_en.csv\n[CELL2] Reading CSV file...\n[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bnâ†’en task.\n[CELL2] Swap successful: src=Bengali, tgt=English\n[CELL2] Processing 40000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [00:01<00:00, 38591.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 38026 pairs from CSV, skipped 1974 rows\n[CELL2] Dataset vocab size: 128104\n[CELL2] Dataset initialized: 38026 valid pairs, 0 invalid, split=train\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] DataLoader created: total_batch=50, per_gpu=25, workers=2\n[PHASE 2] Dataset: 38026 samples, 761 batches\n[TIMING] Data loading: 49.40s\n\n[PHASE 3] Initializing model...\n","output_type":"stream"},{"name":"stderr","text":"'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 01da6124-87d3-4b8f-948a-ee23987ecf76)')' thrown while requesting HEAD https://huggingface.co/facebook/m2m100_418M/resolve/main/model.safetensors.index.json\nWARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 01da6124-87d3-4b8f-948a-ee23987ecf76)')' thrown while requesting HEAD https://huggingface.co/facebook/m2m100_418M/resolve/main/model.safetensors.index.json\nRetrying in 1s [Retry 1/5].\nWARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5637892d59d4788b4b06621cf89c32d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5075665c1ec4626a94353d15b645b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66dcf257c3ff4b1eb13dadcbebac8fb3"}},"metadata":{}},{"name":"stdout","text":"[TATN-INIT] âš ï¸  Vocab size mismatch detected!\n  Tokenizer: 128104\n  Model: 128112\n  Resizing model embeddings to 128104...\n[TATN-INIT] âœ… Model embeddings resized to 128104\n\n[VALIDATION] Checking component compatibility...\n  âœ… Vocabulary: 128104\n  âœ… Model embed_dim: 1024\n  âœ… DSCD embed_dim: 1024\n  âœ… ASBN embed_dim: 1024\n  âœ… Embedding layer: dim=1024, vocab=128104\n[VALIDATION] âœ… All components compatible\n\n[VALIDATION] Checking dataset compatibility...\n  Input IDs range: [2, 128012]\n  Model vocab size: 128104\n[VALIDATION] âœ… Dataset token IDs valid\n[PHASE 3] Using DataParallel on [0, 1]\n\n[VALIDATION] Testing model forward pass...\n  [TEST 1] Testing forward pass (inference mode)...\n  âœ… Forward pass successful (dict output)\n  Keys: ['encoder_outputs', 'dscd_outputs', 'sense_augmented_embeddings', 'explanations', 'asbn_loss', 'domain_loss', 'domain_accuracy', 'ambiguity_signals']\n\n  [TEST 2] Validating DSCD-augmented generation capability...\n    âœ… DSCD embeddings: shape=torch.Size([1, 64, 1024])\n    âœ… BaseModelOutput created successfully\n\n  [TEST 3] Testing generate() with DSCD-augmented embeddings...\n    âœ… Generation successful: 'that that that that that that that that that that'\n    âœ… DSCD-AUGMENTED GENERATION WORKING!\n\n[VALIDATION] âœ… Model forward pass and generation validated\n[PHASE 3] Model initialized and validated\n[TIMING] Model init: 32.08s\n\n[PHASE 4] Setting up optimizers...\n[PHASE 4] ASBN optimizer created (18 params)\n[PHASE 4] Optimizers ready\n\n[PHASE 5] Training...\n[TRAIN] Starting training: epochs=1, batch=50, accum_steps=16\n[TRAIN] Validation: enabled\n[TRAIN] DP enabled: True, GPUs: 2, Device: cuda:0\n[TRAIN] Discovery frequency: 200 steps\n[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt after all epochs\n\n\n================================================================================\nEPOCH 1/1 STARTED\n================================================================================\n\n[TRAIN] TRG statistics reset for epoch 1\nEpoch 1/1:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–            | 199/761 [39:53<2:22:31, 15.22s/it, fwd=3.56, bwd=0.22, rate=100.0%, disc=1]\n[DISCOVERY] Running periodic check at step 200...\n[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.26 resv=7.72\n  GPU 1: alloc=0.77 resv=0.89\n[TRAIN-DEBUG] step=200 loss=3.9944 clusters=20483\n\n[CLUSTER] Top 5 clusters:\n------------------------------------------------------------------------------------------\nRank  Token          Count       Protos    Mu             Tau         \n------------------------------------------------------------------------------------------\n1     à¦¬à¦¨à§à¦§           20          3         17.251506      3.267621    \n2     à¦•à¦°à§‡à¦›à¦¿          20          3         17.277952      1.723548    \n3     à¦›à¦¿             20          3         19.559102      3.760611    \n4     à¦¤à§‡             20          3         21.474489      3.079368    \n5     à¦¦à§‡à¦–à¦¾           20          3         17.071597      4.140359    \n------------------------------------------------------------------------------------------\nEpoch 1/1:  27%|â–ˆâ–ˆâ–ˆâ–ˆ           | 207/761 [41:47<2:11:08, 14.20s/it, fwd=4.02, bwd=0.25, rate=100.0%, disc=193]\n================================================================================\nEPOCH 1 COMPREHENSIVE VALIDATION (Step 208)\n================================================================================\n\n[VALIDATION] Testing 10 samples:\n--------------------------------------------------------------------------------\n   1. no expl         à¦•à¦²=tap/call                    -> ERROR DURING TRANSLATION\n   2. no expl         à¦•à¦¾à¦²=tomorrow/yesterday         -> ERROR DURING TRANSLATION\n   3. no expl         à¦ªà¦¾à¦¤à¦¾=leaf/page                 -> ERROR DURING TRANSLATION\n   4. no expl         à¦¬à§à¦¯à¦¾à¦‚à¦•=bank/embankment         -> ERROR DURING TRANSLATION\n   5. no expl         No ambiguity                   -> ERROR DURING TRANSLATION\n   6. no expl         No ambiguity                   -> ERROR DURING TRANSLATION\n   7. no expl         No ambiguity                   -> ERROR DURING TRANSLATION\n   8. no expl         No ambiguity                   -> ERROR DURING TRANSLATION\n   9. no expl         à¦«à¦²=fruit/result                -> ERROR DURING TRANSLATION\n  10. no expl         à¦®à¦¾à¦¥à¦¾=head/top                  -> ERROR DURING TRANSLATION\n\n--------------------------------------------------------------------------------\n[VALIDATION] DSCD Prototype Quality Check:\n================================================================================\nDSCD-VALIDATION: Prototype Quality Check\n================================================================================\nVALIDATION: Reference Homograph Coverage\n--------------------------------------------------------------------------------\n  âœ“ à¦¨à¦¾à¦® - 3 prototypes (counts=[3, 11, 6])\n  âœ“ à¦¬à¦² - 3 prototypes (counts=[7, 6, 7])\n  âœ“ à¦«à¦² - 3 prototypes (counts=[6, 8, 6])\n  âœ“ à¦®à¦¾à¦¨ - 3 prototypes (counts=[8, 3, 9])\n  âœ“ à¦…à¦°à§à¦¥ - 3 prototypes (counts=[10, 4, 6])\n  âœ— à¦•à¦²à¦¾ - NOT FOUND\n  âœ“ à¦¶à¦¬à§à¦¦ - 2 prototypes (counts=[11, 4])\n  âœ“ à¦˜à¦° - 3 prototypes (counts=[6, 4, 1])\n  âœ“ à¦‰à¦¤à§à¦¤à¦° - 3 prototypes (counts=[5, 11, 4])\n  âœ— à¦šà¦¾à¦² - NOT FOUND\n  âœ— à¦®à¦¨ - NOT FOUND\n  âœ“ à¦¦à¦¾à¦¬à¦¿ - 3 prototypes (counts=[8, 7, 5])\n  âœ“ à¦ªà¦¾à¦¤à¦¾ - 2 prototypes (counts=[7, 4])\n  âœ— à¦—à¦¾à¦¨ - NOT FOUND\n  âœ“ à¦®à§à¦– - 3 prototypes (counts=[6, 4, 1])\n  âœ— à¦ªà¦¾à¦¤à§à¦° - NOT FOUND\n  âœ“ à¦ªà¦¤à§à¦° - 3 prototypes (counts=[5, 6, 9])\n  âœ— à¦šà¦²à¦¾ - NOT FOUND\n  âœ— à¦¬à¦¾à¦° - NOT FOUND\n  âœ“ à¦®à¦¾à¦°à¦¾ - 3 prototypes (counts=[8, 5, 7])\n  âœ“ à¦•à¦¾à¦² - 3 prototypes (counts=[7, 6, 7])\n  âœ— à¦•à¦² - NOT FOUND\n  âœ“ à¦¤à¦¾à¦°à¦¾ - 3 prototypes (counts=[8, 6, 6])\n  âœ— à¦¬à§‡à¦²à¦¾ - NOT FOUND\n  âœ— à¦œà¦®à¦¾ - NOT FOUND\n  âœ— à¦°à¦¾à¦— - NOT FOUND\n  âœ— à¦®à§‹à¦¡à¦¼ - NOT FOUND\n  âœ“ à¦…à¦‚à¦¶ - 3 prototypes (counts=[9, 5, 6])\n  âœ— à¦¬à¦¸à¦¾ - NOT FOUND\n  âš  à¦†à¦¸à¦¨ - Only 1 prototype\n  âœ“ à¦¤à§‹à¦²à¦¾ - 2 prototypes (counts=[5, 11])\n  âœ“ à¦¦à§‡à¦–à¦¾ - 3 prototypes (counts=[7, 9, 4])\n  âš  à¦¸à¦¾à¦¡à¦¼à¦¾ - Only 1 prototype\n  âœ— à¦ªà¦¡à¦¼à¦¾ - NOT FOUND\n  âœ“ à¦¹à¦¾à¦° - 3 prototypes (counts=[14, 3, 3])\n  âœ“ à¦¤à§€à¦° - 2 prototypes (counts=[6, 12])\n  âœ“ à¦ªà¦¦ - 3 prototypes (counts=[6, 11, 3])\n  âœ“ à¦§à¦°à¦¾ - 2 prototypes (counts=[9, 9])\n  âœ— à¦°à¦¸ - NOT FOUND\n  âœ— à¦§à¦¾à¦°à¦¾ - NOT FOUND\n  âœ“ à¦¬à¦¾à¦à¦šà¦¾ - 3 prototypes (counts=[11, 6, 3])\n--------------------------------------------------------------------------------\nVALIDATION Summary:\n  - Total tokens: 21032\n  - Total prototypes: 4451\n  - Multi-sense tokens: 1581\n  - Reference found: 23/41\n  - Quality Score: 36.67%\n================================================================================\n  - Quality Score: 36.7%\n  - Multi-sense tokens: 1581\n  - Total prototypes: 4451\n\n--------------------------------------------------------------------------------\n[VALIDATION] ASBN Training Statistics:\n  - Domain Loss: 0.0000\n  - Domain Accuracy: 0.00%\n  - Source Accuracy: 0.00%\n  - Target Accuracy: 0.00%\n\n--------------------------------------------------------------------------------\n[VALIDATION] TRG Explanation Statistics:\n  - Total explanations: 0\n  - High confidence rate: 0.0%\n  - DSCD homograph rate: 0.0%\n--------------------------------------------------------------------------------\n\n[VALIDATION] Summary:\n  - Translations: 10/10 successful\n  - Explanations generated: 0\n  - Avg explanation confidence: 0.000\n  - DSCD homographs explained: 0\n  - Reference homographs explained: 0\n  - DSCD Quality Score: 36.7%\n  - Multi-sense tokens: 1581\n  - ASBN Domain Accuracy: 0.00%\n\n[VALIDATION] Health Warnings:\n  - No explanations generated\n================================================================================\n\nEpoch 1/1:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 230/761 [47:27<2:11:10, 14.82s/it, fwd=3.89, bwd=0.24, rate=100.0%, disc=170]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# debug cell","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# DEBUGGING CELL: TOKEN-BY-TOKEN DSCD SIGNAL ANALYSIS\n# ==============================================================================\n\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Tuple, Any\n\n# ==============================================================================\n# STEP 1: CHECK AND LOAD REQUIRED VARIABLES\n# ==============================================================================\n\nprint(\"=\" * 100)\nprint(\"CHECKING REQUIRED VARIABLES\")\nprint(\"=\" * 100)\n\n# Check for model and tokenizer\nif 'trained_model' in globals():\n    model = trained_model\n    print(\"âœ“ Using 'trained_model' from Cell 11\")\nelif 'model' in globals():\n    print(\"âœ“ Using 'model' from globals\")\nelse:\n    print(\"âŒ ERROR: No model found!\")\n    print(\"   Please run Cell 11 first, or ensure 'trained_model' or 'model' exists\")\n    raise NameError(\"Model not found. Run Cell 11 first.\")\n\nif 'tokenizer' not in globals():\n    print(\"âŒ ERROR: No tokenizer found!\")\n    print(\"   Please run Cell 11 first\")\n    raise NameError(\"Tokenizer not found. Run Cell 11 first.\")\nelse:\n    print(\"âœ“ Tokenizer found\")\n\n# Load configuration variables with fallbacks\ntry:\n    SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    print(f\"âœ“ SOURCE_LANGUAGE: {SOURCE_LANGUAGE}\")\nexcept (NameError, TypeError):\n    SOURCE_LANGUAGE = \"bn\"\n    print(f\"âš ï¸  SOURCE_LANGUAGE not found, using default: {SOURCE_LANGUAGE}\")\n\ntry:\n    TARGET_LANGUAGE = str(TARGET_LANGUAGE)\n    print(f\"âœ“ TARGET_LANGUAGE: {TARGET_LANGUAGE}\")\nexcept (NameError, TypeError):\n    TARGET_LANGUAGE = \"en\"\n    print(f\"âš ï¸  TARGET_LANGUAGE not found, using default: {TARGET_LANGUAGE}\")\n\ntry:\n    MAX_LENGTH = int(MAX_LENGTH)\n    print(f\"âœ“ MAX_LENGTH: {MAX_LENGTH}\")\nexcept (NameError, ValueError, TypeError):\n    MAX_LENGTH = 52\n    print(f\"âš ï¸  MAX_LENGTH not found, using default: {MAX_LENGTH}\")\n\ntry:\n    DEVICE = DEVICE\n    print(f\"âœ“ DEVICE: {DEVICE}\")\nexcept (NameError, TypeError):\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"âš ï¸  DEVICE not found, using default: {DEVICE}\")\n\ntry:\n    SPAN_THRESHOLD = float(SPAN_THRESHOLD)\n    print(f\"âœ“ SPAN_THRESHOLD: {SPAN_THRESHOLD}\")\nexcept (NameError, ValueError, TypeError):\n    SPAN_THRESHOLD = 0.20\n    print(f\"âš ï¸  SPAN_THRESHOLD not found, using default: {SPAN_THRESHOLD}\")\n\ntry:\n    UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\n    print(f\"âœ“ UNCERTAINTY_THRESHOLD: {UNCERTAINTY_THRESHOLD}\")\nexcept (NameError, ValueError, TypeError):\n    UNCERTAINTY_THRESHOLD = 0.15\n    print(f\"âš ï¸  UNCERTAINTY_THRESHOLD not found, using default: {UNCERTAINTY_THRESHOLD}\")\n\n# TAU_LOW is typically same as UNCERTAINTY_THRESHOLD\ntry:\n    TAU_LOW = float(TAU_LOW)\n    print(f\"âœ“ TAU_LOW: {TAU_LOW}\")\nexcept (NameError, ValueError, TypeError):\n    TAU_LOW = UNCERTAINTY_THRESHOLD\n    print(f\"âš ï¸  TAU_LOW not found, using UNCERTAINTY_THRESHOLD: {TAU_LOW}\")\n\nprint(\"=\" * 100 + \"\\n\")\n\n\n# ==============================================================================\n# STEP 2: DEBUGGING FUNCTION\n# ==============================================================================\n\ndef debug_sentence_signals(\n    model,\n    tokenizer,\n    sentence: str,\n    span_threshold: float = 0.20,\n    uncertainty_threshold: float = 0.15,\n    show_all_tokens: bool = True\n):\n    \"\"\"\n    Detailed token-by-token analysis of DSCD signals and threshold checks.\n    \n    Args:\n        model: Your trained model\n        tokenizer: M2M100 tokenizer\n        sentence: Bengali sentence to analyze\n        span_threshold: Span threshold for ambiguity\n        uncertainty_threshold: Uncertainty threshold for ambiguity\n        show_all_tokens: If True, show all tokens; if False, only ambiguous ones\n    \"\"\"\n    print(\"\\n\" + \"=\" * 100)\n    print(f\"DEBUGGING SENTENCE: {sentence}\")\n    print(\"=\" * 100)\n    \n    # Get model core\n    core = model.module if hasattr(model, 'module') else model\n    dscd = core.dscd if hasattr(core, 'dscd') else None\n    \n    if dscd is None:\n        print(\"âŒ ERROR: Model has no DSCD module!\")\n        return None\n    \n    # Tokenization\n    print(\"\\n[STEP 1] TOKENIZATION\")\n    print(\"-\" * 100)\n    \n    try:\n        tokenizer.src_lang = SOURCE_LANGUAGE\n    except:\n        pass\n    \n    enc = tokenizer(\n        sentence,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=MAX_LENGTH\n    )\n    \n    input_ids = enc['input_ids'].to(DEVICE)\n    attention_mask = enc['attention_mask'].to(DEVICE)\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n    \n    print(f\"Input sentence: {sentence}\")\n    print(f\"Number of tokens: {len(tokens)}\")\n    print(f\"Tokens: {tokens}\")\n    \n    # Get DSCD outputs\n    print(\"\\n[STEP 2] EXTRACTING DSCD SIGNALS\")\n    print(\"-\" * 100)\n    \n    model.eval()\n    with torch.inference_mode():\n        try:\n            if hasattr(core, 'forward_with_dscd_for_inference'):\n                raw_out = core.forward_with_dscd_for_inference(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    src_texts=[sentence]\n                )\n            elif hasattr(core, 'forward_with_explanations'):\n                raw_out = core.forward_with_explanations(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    src_texts=[sentence]\n                )\n            else:\n                raw_out = core.forward(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    src_texts=[sentence],\n                    labels=None,\n                    use_dscd=True\n                )\n        except Exception as e:\n            print(f\"âŒ DSCD forward failed: {e}\")\n            import traceback\n            traceback.print_exc()\n            return None\n    \n    # Extract DSCD outputs\n    if isinstance(raw_out, dict):\n        if 'dscd_outputs' in raw_out:\n            dscd_out = raw_out['dscd_outputs']\n        elif 'dscd' in raw_out:\n            dscd_out = raw_out['dscd']\n        else:\n            dscd_out = raw_out\n    else:\n        print(\"âŒ ERROR: Cannot extract DSCD outputs!\")\n        return None\n    \n    print(\"âœ“ DSCD signals extracted\")\n    print(f\"Available keys: {list(dscd_out.keys())}\")\n    \n    # Extract arrays\n    proto_probs_raw = dscd_out.get('proto_probs', [])\n    uncertainties_raw = dscd_out.get('uncertainties', [])\n    gates_raw = dscd_out.get('gates', [])\n    span_preds_raw = dscd_out.get('span_preds', [])\n    \n    # Convert to lists\n    def to_list(x):\n        if x is None or len(x) == 0:\n            return []\n        \n        row = x[0] if isinstance(x, (list, tuple)) else x\n        \n        if isinstance(row, torch.Tensor):\n            if row.ndim == 0:\n                return [float(row.item())]\n            elif row.ndim == 1:\n                return [float(v.item()) for v in row]\n            elif row.ndim == 2:\n                return [float(row[i].mean().item()) for i in range(row.shape[0])]\n        \n        if isinstance(row, (list, tuple)):\n            result = []\n            for v in row:\n                if isinstance(v, torch.Tensor):\n                    if v.ndim == 0:\n                        result.append(float(v.item()))\n                    else:\n                        result.append(float(v.mean().item()))\n                elif isinstance(v, (int, float)):\n                    result.append(float(v))\n                else:\n                    result.append(0.0)\n            return result\n        \n        return []\n    \n    uncertainties = to_list(uncertainties_raw)\n    gates = to_list(gates_raw)\n    spans = to_list(span_preds_raw)\n    \n    # Extract proto_probs differently (need distributions)\n    proto_probs_list = []\n    if proto_probs_raw and len(proto_probs_raw) > 0:\n        row = proto_probs_raw[0] if isinstance(proto_probs_raw, (list, tuple)) else proto_probs_raw\n        \n        if isinstance(row, torch.Tensor):\n            if row.ndim == 2:\n                for i in range(row.shape[0]):\n                    proto_probs_list.append(row[i].detach().cpu().numpy())\n            elif row.ndim == 1:\n                proto_probs_list = [row.detach().cpu().numpy()]\n        elif isinstance(row, (list, tuple)):\n            for item in row:\n                if isinstance(item, torch.Tensor):\n                    proto_probs_list.append(item.detach().cpu().flatten().numpy())\n                elif isinstance(item, (list, tuple, np.ndarray)):\n                    proto_probs_list.append(np.array(item).flatten())\n                else:\n                    proto_probs_list.append(np.array([1.0]))\n    \n    # Ensure same length\n    seq_len = len(tokens)\n    if len(uncertainties) < seq_len:\n        uncertainties.extend([0.5] * (seq_len - len(uncertainties)))\n    if len(spans) < seq_len:\n        spans.extend([0.0] * (seq_len - len(spans)))\n    if len(gates) < seq_len:\n        gates.extend([0.0] * (seq_len - len(gates)))\n    if len(proto_probs_list) < seq_len:\n        proto_probs_list.extend([np.array([1.0])] * (seq_len - len(proto_probs_list)))\n    \n    # Get DSCD discovered homographs\n    dscd_homographs = set()\n    try:\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        \n        if lock:\n            with lock:\n                stores = dict(dscd.prototype_stores) if hasattr(dscd, 'prototype_stores') else {}\n        else:\n            stores = dict(dscd.prototype_stores) if hasattr(dscd, 'prototype_stores') else {}\n        \n        for token_str, store in stores.items():\n            try:\n                if hasattr(store, 'size') and store.size() >= 2:\n                    clean = str(token_str).replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip().lower()\n                    if clean:\n                        dscd_homographs.add(clean)\n            except:\n                pass\n    except:\n        pass\n    \n    print(f\"âœ“ DSCD discovered homographs: {len(dscd_homographs)}\")\n    if dscd_homographs:\n        print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n    \n    # Token-by-token analysis\n    print(\"\\n[STEP 3] TOKEN-BY-TOKEN ANALYSIS\")\n    print(\"-\" * 100)\n    print(f\"Thresholds: span >= {span_threshold:.2f}, uncertainty >= {uncertainty_threshold:.2f}\")\n    print(\"-\" * 100)\n    \n    # Header\n    print(f\"{'#':<4} {'Token':<20} {'Clean':<15} {'Span':<8} {'Uncert':<8} {'Gate':<8} {'ProtoProbs':<25} {'DSCD?':<7} {'Pass?':<7} {'Reason'}\")\n    print(\"-\" * 100)\n    \n    passed_tokens = []\n    failed_tokens = []\n    \n    for idx in range(seq_len):\n        token = tokens[idx]\n        clean_token = token.replace(\"â–\", \"\").replace(\"Ä \", \"\").replace(\"##\", \"\").strip()\n        \n        u = uncertainties[idx] if idx < len(uncertainties) else 0.5\n        s = spans[idx] if idx < len(spans) else 0.0\n        g = gates[idx] if idx < len(gates) else 0.0\n        pp = proto_probs_list[idx] if idx < len(proto_probs_list) else np.array([1.0])\n        \n        # Format proto_probs\n        if len(pp) > 0:\n            pp_str = \"[\" + \", \".join([f\"{p:.3f}\" for p in pp[:3]]) + \"]\"\n            if len(pp) > 3:\n                pp_str = pp_str[:-1] + \"...]\"\n        else:\n            pp_str = \"[empty]\"\n        \n        # Check if in DSCD\n        in_dscd = clean_token.lower() in dscd_homographs\n        \n        # Check thresholds\n        pass_span = s >= span_threshold\n        pass_uncert = u >= uncertainty_threshold\n        passes = pass_span or pass_uncert\n        \n        # Determine reason\n        reasons = []\n        if in_dscd:\n            reasons.append(\"DSCDâœ“\")\n        if pass_span:\n            reasons.append(f\"spanâœ“\")\n        if pass_uncert:\n            reasons.append(f\"uncertâœ“\")\n        if not passes:\n            if s < span_threshold:\n                reasons.append(f\"spanâœ—({s:.3f}<{span_threshold:.2f})\")\n            if u < uncertainty_threshold:\n                reasons.append(f\"uncertâœ—({u:.3f}<{uncertainty_threshold:.2f})\")\n        \n        reason = \", \".join(reasons) if reasons else \"no signals\"\n        \n        # Color indicators\n        pass_indicator = \"âœ“ YES\" if passes else \"âœ— NO\"\n        dscd_indicator = \"âœ“\" if in_dscd else \"âœ—\"\n        \n        # Print row\n        print(f\"{idx:<4} {token:<20} {clean_token:<15} {s:<8.3f} {u:<8.3f} {g:<8.3f} {pp_str:<25} {dscd_indicator:<7} {pass_indicator:<7} {reason}\")\n        \n        if passes:\n            passed_tokens.append((idx, token, clean_token, s, u, reason))\n        else:\n            failed_tokens.append((idx, token, clean_token, s, u, reason))\n    \n    # Summary\n    print(\"-\" * 100)\n    print(f\"\\n[STEP 4] SUMMARY\")\n    print(\"-\" * 100)\n    print(f\"Total tokens: {seq_len}\")\n    print(f\"Passed threshold: {len(passed_tokens)} tokens\")\n    print(f\"Failed threshold: {len(failed_tokens)} tokens\")\n    print(f\"Pass rate: {len(passed_tokens)/seq_len*100:.1f}%\")\n    \n    if passed_tokens:\n        print(f\"\\nâœ“ TOKENS THAT SHOULD GET EXPLANATIONS ({len(passed_tokens)}):\")\n        for idx, tok, clean, s, u, reason in passed_tokens:\n            print(f\"  [{idx}] '{clean}' (span={s:.3f}, uncert={u:.3f}) - {reason}\")\n    else:\n        print(\"\\nâš ï¸  NO TOKENS PASSED THRESHOLDS!\")\n        print(\"   This means NO explanations will be generated.\")\n        print(\"   Possible reasons:\")\n        print(\"   1. Thresholds too high\")\n        print(\"   2. DSCD not trained properly\")\n        print(\"   3. Sentence has no ambiguity\")\n    \n    if failed_tokens and show_all_tokens:\n        print(f\"\\nâœ— TOKENS FILTERED OUT ({len(failed_tokens)}):\")\n        for idx, tok, clean, s, u, reason in failed_tokens[:5]:\n            print(f\"  [{idx}] '{clean}' (span={s:.3f}, uncert={u:.3f}) - {reason}\")\n        if len(failed_tokens) > 5:\n            print(f\"  ... and {len(failed_tokens)-5} more\")\n    \n    # Adaptive threshold calculation\n    print(f\"\\n[STEP 5] ADAPTIVE THRESHOLD ANALYSIS\")\n    print(\"-\" * 100)\n    \n    U_arr = np.array([u for u in uncertainties if np.isfinite(u)])\n    S_arr = np.array([s for s in spans if np.isfinite(s)])\n    \n    if len(U_arr) > 0:\n        median_u = float(np.median(U_arr))\n        std_u = float(np.std(U_arr))\n        adaptive_u = median_u + 0.5 * std_u\n        adaptive_u = max(0.05, min(0.50, adaptive_u))\n        \n        print(f\"Uncertainty statistics:\")\n        print(f\"  Min: {U_arr.min():.3f}\")\n        print(f\"  Median: {median_u:.3f}\")\n        print(f\"  Max: {U_arr.max():.3f}\")\n        print(f\"  Std: {std_u:.3f}\")\n        print(f\"  Adaptive threshold: {adaptive_u:.3f}\")\n        print(f\"  Static threshold: {uncertainty_threshold:.3f}\")\n        print(f\"  Using: max({adaptive_u:.3f}, {uncertainty_threshold*0.5:.3f}) = {max(adaptive_u, uncertainty_threshold*0.5):.3f}\")\n    \n    if len(S_arr) > 0:\n        median_s = float(np.median(S_arr))\n        p75_s = float(np.percentile(S_arr, 75))\n        adaptive_s = 0.5 * median_s + 0.5 * p75_s\n        adaptive_s = max(0.02, min(0.30, adaptive_s))\n        \n        print(f\"\\nSpan statistics:\")\n        print(f\"  Min: {S_arr.min():.3f}\")\n        print(f\"  Median: {median_s:.3f}\")\n        print(f\"  P75: {p75_s:.3f}\")\n        print(f\"  Max: {S_arr.max():.3f}\")\n        print(f\"  Adaptive threshold: {adaptive_s:.3f}\")\n        print(f\"  Static threshold: {span_threshold:.3f}\")\n        print(f\"  Using: max({adaptive_s:.3f}, {span_threshold*0.5:.3f}) = {max(adaptive_s, span_threshold*0.5):.3f}\")\n    \n    print(\"\\n\" + \"=\" * 100 + \"\\n\")\n    \n    return {\n        'tokens': tokens,\n        'passed': passed_tokens,\n        'failed': failed_tokens,\n        'uncertainties': uncertainties,\n        'spans': spans,\n        'gates': gates,\n    }\n\n\n# ==============================================================================\n# STEP 3: RUN DEBUGGING ON 2 TEST SENTENCES\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"DETAILED DEBUGGING: 2 TEST SENTENCES\")\nprint(\"=\" * 100)\n\n# Test sentence 1: High ambiguity\ntest_sentence_1 = \"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\"\nprint(\"\\n[TEST 1] Sentence with known homograph (à¦•à¦² = tap/factory/call)\")\n\ndebug_result_1 = debug_sentence_signals(\n    model=model,\n    tokenizer=tokenizer,\n    sentence=test_sentence_1,\n    span_threshold=SPAN_THRESHOLD,\n    uncertainty_threshold=TAU_LOW,\n    show_all_tokens=True\n)\n\n# Test sentence 2: Another ambiguous case\ntest_sentence_2 = \"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\"\nprint(\"\\n[TEST 2] Sentence with homograph (à¦•à¦¾à¦² = tomorrow/yesterday)\")\n\ndebug_result_2 = debug_sentence_signals(\n    model=model,\n    tokenizer=tokenizer,\n    sentence=test_sentence_2,\n    span_threshold=SPAN_THRESHOLD,\n    uncertainty_threshold=TAU_LOW,\n    show_all_tokens=True\n)\n\n# ==============================================================================\n# STEP 4: COMPARE WITH ACTUAL INFERENCE\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"VERIFICATION: ACTUAL INFERENCE OUTPUT\")\nprint(\"=\" * 100)\n\n# Check if translate_with_explanations exists\nif 'translate_with_explanations' in globals():\n    print(\"\\n[VERIFY 1] Running actual inference on sentence 1:\")\n    try:\n        result_1 = translate_with_explanations(\n            model, \n            tokenizer, \n            test_sentence_1,\n            source_lang=SOURCE_LANGUAGE,\n            target_lang=TARGET_LANGUAGE,\n            device=DEVICE,\n            max_length=MAX_LENGTH,\n            span_threshold=SPAN_THRESHOLD,\n            uncertainty_threshold=UNCERTAINTY_THRESHOLD\n        )\n        print(f\"Translation: {result_1['translation']}\")\n        print(f\"Explanations generated: {len(result_1['explanations'])}\")\n        if result_1['explanations']:\n            for exp in result_1['explanations']:\n                word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                expl_text = exp.get('explanation', '')[:80]\n                print(f\"  - Token '{word}': {expl_text}...\")\n        else:\n            print(\"  âš ï¸  NO EXPLANATIONS (check why tokens that passed didn't generate explanations!)\")\n    except Exception as e:\n        print(f\"âŒ Inference failed: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    print(\"\\n[VERIFY 2] Running actual inference on sentence 2:\")\n    try:\n        result_2 = translate_with_explanations(\n            model, \n            tokenizer, \n            test_sentence_2,\n            source_lang=SOURCE_LANGUAGE,\n            target_lang=TARGET_LANGUAGE,\n            device=DEVICE,\n            max_length=MAX_LENGTH,\n            span_threshold=SPAN_THRESHOLD,\n            uncertainty_threshold=UNCERTAINTY_THRESHOLD\n        )\n        print(f\"Translation: {result_2['translation']}\")\n        print(f\"Explanations generated: {len(result_2['explanations'])}\")\n        if result_2['explanations']:\n            for exp in result_2['explanations']:\n                word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                expl_text = exp.get('explanation', '')[:80]\n                print(f\"  - Token '{word}': {expl_text}...\")\n        else:\n            print(\"  âš ï¸  NO EXPLANATIONS\")\n    except Exception as e:\n        print(f\"âŒ Inference failed: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"âš ï¸  translate_with_explanations not found!\")\n    print(\"   Please run Cell 8 first\")\n\nprint(\"\\n\" + \"=\" * 100)\nprint(\"DEBUGGING COMPLETE\")\nprint(\"=\" * 100)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12: EXTENDED INFERENCE TESTING WITH PROTOTYPE SENSE ANALYSIS (FINAL)\n# ==============================================================================\nimport os\nimport time\nimport traceback\nimport json\nfrom typing import Tuple, Any, Dict, List, Optional\nfrom collections import defaultdict\nimport torch\nimport gc\n\ntry:\n    _DEVICE = DEVICE if isinstance(DEVICE, torch.device) else torch.device(str(DEVICE)) if isinstance(DEVICE, str) else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\n    _UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in HOMOGRAPH_REFERENCE_LIST_BN)\n    cell0_loaded = True\nexcept (NameError, TypeError, ValueError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.25\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\"\n    }\n    cell0_loaded = False\n    print(\"[TEST] Using fallback config (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n_PROTOTYPE_BASE_PATH = \"/kaggle/working/prototypes/\"\n\n\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n\n\ndef _compute_similarity(translation: str, expected: str) -> float:\n    try:\n        trans_words = set(translation.lower().split())\n        exp_words = set(expected.lower().split())\n        if not exp_words:\n            return 0.0\n        overlap = len(trans_words & exp_words)\n        return overlap / len(exp_words)\n    except Exception:\n        return 0.0\n\n\ndef load_prototype_senses(base_path: str) -> Dict[str, Dict[str, Any]]:\n    \"\"\"Load prototype sense information from saved files\"\"\"\n    prototype_senses = {}\n    \n    if not os.path.exists(base_path):\n        _safe_print(f\"[PROTO] Path not found: {base_path}\")\n        return prototype_senses\n    \n    try:\n        files = [f for f in os.listdir(base_path) if f.endswith('.json')]\n        _safe_print(f\"[PROTO] Found {len(files)} prototype files\")\n        \n        for filename in files:\n            try:\n                filepath = os.path.join(base_path, filename)\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                \n                token = data.get('token', '')\n                if not token:\n                    continue\n                \n                clean_token = token.replace('â–', '').replace('Ä ', '').replace('##', '').strip().lower()\n                \n                senses = data.get('senses', [])\n                if not senses or len(senses) < 2:\n                    continue\n                \n                sense_info = []\n                for sense in senses:\n                    sense_info.append({\n                        'id': sense.get('sense_id', 'unknown'),\n                        'label': sense.get('sense_label', 'unlabeled'),\n                        'examples': sense.get('context_examples', [])[:3],\n                        'count': sense.get('count', 0),\n                        'centroid': sense.get('cluster_center', None)\n                    })\n                \n                prototype_senses[clean_token] = {\n                    'original_token': token,\n                    'num_senses': len(senses),\n                    'senses': sense_info,\n                    'total_occurrences': sum(s['count'] for s in sense_info)\n                }\n                \n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    _safe_print(f\"[PROTO] Failed to load {filename}: {e}\")\n                continue\n        \n        _safe_print(f\"[PROTO] Loaded {len(prototype_senses)} homographs with sense info\")\n        return prototype_senses\n        \n    except Exception as e:\n        _safe_print(f\"[PROTO] Load failed: {type(e).__name__}: {str(e)[:100]}\")\n        return prototype_senses\n\n\ndef get_sense_for_token(token: str, embedding: torch.Tensor, prototype_info: Dict, dscd) -> Optional[Dict[str, Any]]:\n    \"\"\"Determine which sense a token is using based on embedding similarity\"\"\"\n    try:\n        if not prototype_info or 'senses' not in prototype_info:\n            return None\n        \n        senses = prototype_info['senses']\n        if len(senses) < 2:\n            return None\n        \n        if embedding is None or not isinstance(embedding, torch.Tensor):\n            return None\n        \n        max_sim = -1\n        best_sense = None\n        \n        for sense in senses:\n            centroid = sense.get('centroid')\n            if centroid is None:\n                continue\n            \n            try:\n                if isinstance(centroid, list):\n                    centroid_tensor = torch.tensor(centroid, device=embedding.device)\n                elif isinstance(centroid, torch.Tensor):\n                    centroid_tensor = centroid.to(embedding.device)\n                else:\n                    continue\n                \n                if centroid_tensor.shape != embedding.shape:\n                    continue\n                \n                similarity = torch.cosine_similarity(\n                    embedding.unsqueeze(0),\n                    centroid_tensor.unsqueeze(0),\n                    dim=1\n                ).item()\n                \n                if similarity > max_sim:\n                    max_sim = similarity\n                    best_sense = {\n                        'sense_id': sense['id'],\n                        'sense_label': sense['label'],\n                        'similarity': similarity,\n                        'examples': sense['examples']\n                    }\n                    \n            except Exception:\n                continue\n        \n        return best_sense\n        \n    except Exception:\n        return None\n\n\ndef _get_dscd_homographs(model) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n        \n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n        \n        homographs = set()\n        \n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        \n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        \n        for token, store in stores.items():\n            try:\n                size_ok = False\n                size_attr = getattr(store, \"size\", None)\n                try:\n                    if callable(size_attr):\n                        size_ok = size_attr() >= 2\n                    elif isinstance(size_attr, int):\n                        size_ok = size_attr >= 2\n                except Exception:\n                    size_ok = False\n\n                if not size_ok:\n                    counts = getattr(store, \"counts\", None)\n                    if isinstance(counts, (list, tuple)) and len(counts) >= 2:\n                        size_ok = True\n\n                if size_ok:\n                    clean = (\n                        str(token)\n                        .replace('â–', '')\n                        .replace('Ä ', '')\n                        .replace('##', '')\n                        .strip()\n                        .lower()\n                    )\n                    homographs.add(clean)\n            except Exception:\n                continue\n        \n        return homographs\n    except Exception:\n        return set()\n\n\ntrained_model_available = 'trained_model' in globals() and globals().get('trained_model') is not None\ntokenizer_available = 'tokenizer' in globals() and globals().get('tokenizer') is not None\ntranslate_available = 'translate_with_explanations' in globals()\n\nif not trained_model_available:\n    _safe_print(\"trained_model not found - will try checkpoint\")\nif not tokenizer_available:\n    _safe_print(\"tokenizer not found - run pipeline first\")\nif not translate_available:\n    _safe_print(\"translate_with_explanations not found - run Cell 8\")\n\n\ndef try_load_checkpoint(checkpoint_path: str, tokenizer) -> Tuple[bool, Any]:\n    if not os.path.exists(checkpoint_path):\n        return False, f\"Not found: {checkpoint_path}\"\n\n    if 'MemoryOptimizedTATNWithExplanations' not in globals():\n        return False, \"Model class not available\"\n\n    _safe_print(f\"[TEST] Loading: {checkpoint_path}\")\n    \n    try:\n        ckpt = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n    except Exception as e:\n        _safe_print(f\"[TEST] Load failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    state = None\n    if isinstance(ckpt, dict):\n        for k in (\"model_state_dict\", \"state_dict\", \"model\"):\n            if k in ckpt and isinstance(ckpt[k], dict):\n                state = ckpt[k]\n                break\n        if state is None:\n            values_sample = list(ckpt.values())[:10]\n            if any(torch.is_tensor(v) for v in values_sample):\n                state = ckpt\n    else:\n        state = ckpt\n\n    if state is None:\n        return False, \"No model state found\"\n    \n    try:\n        _safe_print(f\"[TEST] Model state: {len(state)} keys\")\n    except Exception:\n        _safe_print(\"[TEST] Model state: (unknown size)\")\n\n    dscd_state = None\n    if isinstance(ckpt, dict) and 'dscd_state' in ckpt:\n        dscd_state = ckpt['dscd_state']\n        if dscd_state and isinstance(dscd_state, dict):\n            if '_prototype_stores' in dscd_state:\n                num_tokens = len(dscd_state['_prototype_stores'])\n            elif 'prototype_stores' in dscd_state:\n                num_tokens = len(dscd_state['prototype_stores'])\n            else:\n                num_tokens = 0\n            \n            _safe_print(f\"[TEST] DSCD state: {num_tokens} tokens\")\n            \n            if num_tokens == 0:\n                _safe_print(\"[TEST] DSCD empty - warmup needed\")\n        else:\n            _safe_print(\"[TEST] DSCD state invalid\")\n    else:\n        _safe_print(\"[TEST] No DSCD state\")\n\n    try:\n        model_inst = MemoryOptimizedTATNWithExplanations(tokenizer)\n    except Exception as e:\n        _safe_print(f\"[TEST] Instantiation failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    try:\n        mbart = getattr(model_inst, \"mbart\", None)\n        if mbart and hasattr(mbart, \"get_input_embeddings\"):\n            cur = mbart.get_input_embeddings().num_embeddings\n            tok_len = getattr(\n                tokenizer,\n                'vocab_size',\n                len(tokenizer) if hasattr(tokenizer, '__len__') else None,\n            )\n            \n            if tok_len and cur != tok_len:\n                try:\n                    mbart.resize_token_embeddings(tok_len)\n                    _safe_print(f\"[TEST] Resized: {cur} -> {tok_len}\")\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    try:\n        res = model_inst.load_state_dict(state, strict=False)\n        missing = []\n        if isinstance(res, dict):\n            missing = res.get('missing_keys', []) or res.get('missing', [])\n        _safe_print(f\"[TEST] State loaded (missing: {len(missing)})\")\n    except Exception:\n        try:\n            new_state = {k.replace(\"module.\", \"\", 1): v for k, v in state.items()}\n            model_inst.load_state_dict(new_state, strict=False)\n            _safe_print(\"[TEST] Loaded (stripped prefixes)\")\n        except Exception as e2:\n            _safe_print(f\"[TEST] Load failed: {type(e2).__name__}\")\n            _maybe_traceback(e2)\n            return False, e2\n\n    if dscd_state:\n        try:\n            dscd = getattr(model_inst, 'dscd', None)\n            if dscd and hasattr(dscd, 'load_state_dict'):\n                dscd.load_state_dict(dscd_state)\n                \n                num_tokens = len(getattr(dscd, 'prototype_stores', {}) or {}) if hasattr(dscd, 'prototype_stores') else 0\n                _safe_print(f\"[TEST] DSCD loaded: {num_tokens} tokens\")\n                \n                if num_tokens == 0:\n                    _safe_print(\"[TEST] DSCD has 0 tokens - warmup needed\")\n            else:\n                _safe_print(\"[TEST] No DSCD load_state_dict\")\n        except Exception as e:\n            _safe_print(f\"[TEST] DSCD load failed: {type(e).__name__}\")\n            _maybe_traceback(e)\n\n    try:\n        model_inst.to(_DEVICE)\n        model_inst.eval()\n    except Exception as e:\n        _safe_print(f\"[TEST] Device move failed: {type(e).__name__}\")\n        return False, e\n\n    _safe_print(f\"[TEST] Ready on: {_DEVICE}\")\n    return True, model_inst\n\n\nif os.path.exists(_CHECKPOINT_PATH) and tokenizer_available:\n    succ, model_or_err = try_load_checkpoint(_CHECKPOINT_PATH, globals().get(\"tokenizer\"))\n    if succ:\n        globals()['trained_model'] = model_or_err\n        trained_model_available = True\n        _safe_print(\"[TEST] Checkpoint loaded\")\n    else:\n        _safe_print(\"[TEST] Checkpoint load failed\")\n\n\ndef maybe_run_warmup_if_needed(model, tokenizer, warmup_sents: int = 4000) -> bool:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        \n        if dscd is None:\n            _safe_print(\"[TEST] No DSCD - skip warmup\")\n            return False\n        \n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n        \n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        \n        initial_count = len(stores)\n        \n        if initial_count > 0:\n            multi_sense = sum(\n                1\n                for store in stores.values()\n                if hasattr(store, 'size')\n                and callable(getattr(store, 'size', None))\n                and store.size() >= 2\n            )\n            _safe_print(f\"[TEST] DSCD has {initial_count} tokens ({multi_sense} multi-sense)\")\n            return True\n        \n        _safe_print(\"[TEST] DSCD empty - running warmup...\")\n        \n        if 'dscd_discovery_warmup' not in globals():\n            _safe_print(\"[TEST] Warmup function not available\")\n            return False\n        \n        try:\n            warmup_start = time.time()\n            dscd_discovery_warmup(\n                model,\n                tokenizer,\n                num_sents=warmup_sents,\n                batch_size=64,\n                max_len=globals().get(\"MAX_LENGTH\", 48),\n            )\n            warmup_time = time.time() - warmup_start\n            \n            if lock:\n                with lock:\n                    stores_after = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n            else:\n                stores_after = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n            \n            final_count = len(stores_after)\n            multi_sense = sum(\n                1\n                for store in stores_after.values()\n                if hasattr(store, 'size')\n                and callable(getattr(store, 'size', None))\n                and store.size() >= 2\n            )\n            \n            if final_count > 0:\n                ratio = multi_sense / final_count if final_count > 0 else 0\n                _safe_print(f\"[TEST] Warmup success ({warmup_time:.1f}s)\")\n                _safe_print(\n                    f\"[TEST]    Tokens: {final_count}, \"\n                    f\"Multi-sense: {multi_sense} ({ratio:.1%})\"\n                )\n                \n                if ratio < 0.1:\n                    _safe_print(\"[TEST] Low multi-sense ratio (<10%)\")\n                \n                return True\n            else:\n                _safe_print(\"[TEST] Warmup complete but NO prototypes\")\n                return False\n                \n        except Exception as e:\n            _safe_print(f\"[TEST] Warmup failed: {type(e).__name__}\")\n            _maybe_traceback(e)\n            return False\n            \n    except Exception as e:\n        _safe_print(f\"[TEST] Warmup check failed: {type(e).__name__}\")\n        return False\n\n\ntest_sentences: List[Tuple[str, str, str]] = [\n    (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap\", \"à¦•à¦² = tap/call\"),\n    (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"Tomorrow I will buy a book\", \"à¦•à¦¾à¦² = tomorrow/yesterday\"),\n    (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"The leaf has fallen\", \"à¦ªà¦¾à¦¤à¦¾ = leaf/page\"),\n    (\"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\", \"He went to the bank\", \"à¦¬à§à¦¯à¦¾à¦‚à¦• = bank/embankment\"),\n    (\"à¦†à¦®à¦¿ à¦­à¦¾à¦²à§‹ à¦†à¦›à¦¿à¥¤\", \"I am fine\", \"Simple\"),\n    (\"à¦¸à§‡ à¦–à§à¦¬ à¦®à¦¿à¦·à§à¦Ÿà¦¿ à¦•à¦¥à¦¾ à¦¬à¦²à§‡à¥¤\", \"She speaks sweetly\", \"Adjective\"),\n    (\"à¦à¦Ÿà¦¾ à¦†à¦®à¦¾à¦° à¦¬à¦‡à¥¤\", \"This is my book\", \"Demonstrative\"),\n    (\"à¦¤à§à¦®à¦¿ à¦•à¦¿ à¦†à¦®à¦¾à¦•à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‹?\", \"Can you help me?\", \"Question\"),\n    (\"à¦†à¦œ à¦†à¦¬à¦¹à¦¾à¦“à¦¯à¦¼à¦¾ à¦­à¦¾à¦²à§‹à¥¤\", \"Weather is good\", \"Simple\"),\n    (\"à¦†à¦®à¦°à¦¾ à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡ à¦¬à¦¾à¦¸ à¦•à¦°à¦¿à¥¤\", \"We live in Bangladesh\", \"Country\"),\n    (\"à¦¸à§‚à¦°à§à¦¯ à¦ªà§‚à¦°à§à¦¬ à¦¦à¦¿à¦•à§‡ à¦“à¦ à§‡à¥¤\", \"Sun rises in east\", \"Directional\"),\n    (\"à¦ªà¦¾à¦–à¦¿ à¦†à¦•à¦¾à¦¶à§‡ à¦‰à¦¡à¦¼à§‡à¥¤\", \"Birds fly in sky\", \"Simple present\"),\n    (\"à¦¸à§‡ à¦¸à§à¦•à§à¦²à§‡ à¦¯à¦¾à¦šà§à¦›à§‡à¥¤\", \"She is going to school\", \"Continuous\"),\n]\n\navg_conf = 0.0\navg_span = 0.0\navg_u = 0.0\navg_time = 0.0\n\nprototype_senses = {}\n\nif not (trained_model_available and tokenizer_available and translate_available):\n    _safe_print(\"\\nCannot run tests - missing prerequisites\")\n    _safe_print(\"   Run Cells 0-11 or load checkpoint\")\nelse:\n    _safe_print(\"\\n[PROTOTYPE LOADING]\")\n    prototype_senses = load_prototype_senses(_PROTOTYPE_BASE_PATH)\n    \n    if prototype_senses:\n        _safe_print(f\"  Loaded {len(prototype_senses)} homographs with sense info\")\n        \n        if _DEBUG_DISCOVERY:\n            _safe_print(\"\\n  Sample prototypes:\")\n            for word, info in list(prototype_senses.items())[:5]:\n                _safe_print(f\"    '{word}': {info['num_senses']} senses\")\n                for sense in info['senses'][:2]:\n                    _safe_print(f\"      - {sense['label']} (count={sense['count']})\")\n    else:\n        _safe_print(f\"  No prototypes found at {_PROTOTYPE_BASE_PATH}\")\n        _safe_print(\"  Sense disambiguation will not be available\")\n    \n    warmup_success = False\n    try:\n        warmup_success = maybe_run_warmup_if_needed(\n            globals().get('trained_model'), \n            globals().get(\"tokenizer\"), \n            warmup_sents=4000\n        )\n    except Exception as e:\n        _safe_print(f\"[TEST] Warmup failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n\n    dscd_homographs = _get_dscd_homographs(globals().get('trained_model'))\n    _safe_print(f\"\\n[TEST] DSCD discovered: {len(dscd_homographs)} homographs\")\n    if dscd_homographs and _DEBUG_DISCOVERY:\n        _safe_print(f\"[TEST] Sample: {list(dscd_homographs)[:10]}\")\n    \n    _safe_print(f\"\\n[COMPONENT HEALTH]\")\n    try:\n        core = globals().get('trained_model')\n        core = core.module if hasattr(core, 'module') else core\n        \n        dscd = getattr(core, 'dscd', None)\n        if dscd and hasattr(dscd, 'get_prototype_summary'):\n            try:\n                dscd_stats = dscd.get_prototype_summary()\n                _safe_print(\n                    f\"  DSCD: {dscd_stats.get('total_tokens', 0)} tokens, \"\n                    f\"{dscd_stats.get('num_homographs', 0)} homographs\"\n                )\n            except Exception:\n                pass\n        \n        asbn = getattr(core, 'asbn', None)\n        if asbn and hasattr(asbn, 'get_detailed_stats'):\n            try:\n                asbn_stats = asbn.get_detailed_stats()\n                _safe_print(\n                    f\"  ASBN: {asbn_stats.get('domain_accuracy', 0):.2%} domain accuracy\"\n                )\n            except Exception:\n                pass\n        \n        trg = getattr(core, 'trg_system', None)\n        if trg and hasattr(trg, 'get_statistics'):\n            try:\n                trg_stats = trg.get_statistics()\n                _safe_print(\n                    f\"  TRG: {trg_stats.get('explanations_generated', 0)} total explanations\"\n                )\n            except Exception:\n                pass\n    except Exception:\n        pass\n    \n    total = len(test_sentences)\n    successes = 0\n    tests_with_explanations = 0\n    total_ambiguous = 0\n    \n    quality_metrics = {\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n        'similarities': [],\n    }\n    \n    dscd_homographs_explained = set()\n    reference_homographs_explained = set()\n    homograph_explanations = defaultdict(list)\n    sense_usage_stats = defaultdict(lambda: defaultdict(int))\n    \n    inference_times = []\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"EXTENDED INFERENCE TESTING WITH SENSE ANALYSIS\")\n    _safe_print(\"=\" * 80)\n    _safe_print(\"Configuration:\")\n    _safe_print(f\"  Cell 0: {'Loaded' if cell0_loaded else 'Fallback'}\")\n    _safe_print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    _safe_print(f\"  Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\n    _safe_print(f\"  Prototype path: {_PROTOTYPE_BASE_PATH}\")\n    _safe_print(f\"  Prototypes loaded: {len(prototype_senses)}\")\n    _safe_print(f\"  Tests: {total}\")\n    _safe_print(\"=\" * 80)\n    \n    if not warmup_success:\n        _safe_print(\"\\nWARNING: Warmup failed\")\n        _safe_print(\"   Homograph detection may not work\\n\")\n\n    for idx, (sent, expected, note) in enumerate(test_sentences, 1):\n        _safe_print(\"\\n\" + \"-\" * 70)\n        _safe_print(f\"Test {idx}/{total}: {note}\")\n        _safe_print(f\"Input: {sent}\")\n        \n        try:\n            model_for_infer = globals().get('trained_model')\n            tokenizer = globals().get('tokenizer')\n            \n            if model_for_infer is None or tokenizer is None:\n                raise RuntimeError(\"Model/tokenizer missing\")\n\n            inf_start = time.time()\n            res = translate_with_explanations(\n                model_for_infer,\n                tokenizer,\n                sent,\n                device=_DEVICE,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD\n            )\n            inf_time = time.time() - inf_start\n            inference_times.append(inf_time)\n\n            if res is None or not isinstance(res, dict):\n                _safe_print(\"[TEST] Invalid result - skip\")\n                continue\n\n            translation = str(res.get(\"translation\", \"\"))\n            amb_count = int(res.get(\"ambiguous_words_detected\", 0))\n            explanations = res.get(\"explanations\", []) or []\n\n            _safe_print(f\"Translation: {translation}\")\n            _safe_print(f\"Time: {inf_time:.3f}s\")\n            \n            similarity = _compute_similarity(translation, expected)\n            quality_metrics['similarities'].append(similarity)\n            _safe_print(f\"Similarity: {similarity:.1%}\")\n            \n            _safe_print(f\"Ambiguous: {amb_count}\")\n\n            if amb_count > 0:\n                tests_with_explanations += 1\n                total_ambiguous += amb_count\n                _safe_print(\"Explanations:\")\n                \n                for j, e in enumerate(explanations, 1):\n                    try:\n                        word = e.get(\"ambiguous_word\", e.get(\"token\", \"N/A\"))\n                        conf = float(e.get(\"confidence\", 0.5))\n                        u = float(e.get(\"uncertainty\", 0.0))\n                        s = float(e.get(\"span\", 0.0))\n                        \n                        quality_metrics['confidences'].append(conf)\n                        quality_metrics['spans'].append(s)\n                        quality_metrics['uncertainties'].append(u)\n                        \n                        clean = str(word).replace('â–', '').replace('Ä ', '').strip().lower()\n                        \n                        sense_str = \"\"\n                        if clean in prototype_senses:\n                            embedding = e.get('embedding')\n                            core = model_for_infer.module if hasattr(model_for_infer, 'module') else model_for_infer\n                            dscd = getattr(core, 'dscd', None)\n                            \n                            detected_sense = get_sense_for_token(\n                                clean,\n                                embedding,\n                                prototype_senses[clean],\n                                dscd\n                            )\n                            \n                            if detected_sense:\n                                sense_label = detected_sense['sense_label']\n                                sense_sim = detected_sense['similarity']\n                                sense_str = f\" [SENSE: {sense_label} ({sense_sim:.2f})]\"\n                                \n                                sense_usage_stats[clean][sense_label] += 1\n                        \n                        if clean in dscd_homographs:\n                            dscd_homographs_explained.add(clean)\n                            homograph_explanations[clean].append({\n                                'sentence': sent,\n                                'confidence': conf,\n                                'span': s,\n                                'uncertainty': u,\n                            })\n                        \n                        if clean in _HOMOGRAPH_REFERENCE_LIST_BN:\n                            reference_homographs_explained.add(clean)\n                        \n                        marker = \"[HIGH]\" if s > _SPAN_THRESHOLD else \"      \"\n                        _safe_print(f\"  {j}. {marker} '{word}' conf={conf:.3f} u={u:.3f} s={s:.3f}{sense_str}\")\n                        \n                    except Exception:\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n            else:\n                _safe_print(\"No ambiguity\")\n\n            if translation and translation.strip():\n                successes += 1\n                _safe_print(\"Success\")\n            else:\n                _safe_print(\"Failed\")\n            \n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        except Exception as e:\n            _safe_print(f\"Test {idx} failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                _maybe_traceback(e)\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"TEST SUMMARY\")\n    _safe_print(\"=\" * 80)\n    \n    _safe_print(\"\\n[TRANSLATION]\")\n    _safe_print(f\"  Total: {total}\")\n    if total > 0:\n        _safe_print(f\"  Success: {successes} ({successes/total*100:.1f}%)\")\n        _safe_print(f\"  Failed: {total-successes} ({(total-successes)/total*100:.1f}%)\")\n        \n        if quality_metrics['similarities']:\n            avg_sim = sum(quality_metrics['similarities']) / len(quality_metrics['similarities'])\n            _safe_print(f\"  Avg similarity: {avg_sim:.1%}\")\n    \n    if inference_times:\n        avg_time = sum(inference_times) / len(inference_times)\n        _safe_print(\"\\n[PERFORMANCE]\")\n        _safe_print(f\"  Avg time: {avg_time:.3f}s per sentence\")\n        _safe_print(f\"  Throughput: {1/avg_time:.1f} sentences/sec\")\n    \n    _safe_print(\"\\n[AMBIGUITY]\")\n    _safe_print(f\"  Tests with explanations: {tests_with_explanations}/{total} ({tests_with_explanations/total*100:.1f}%)\")\n    _safe_print(f\"  Total ambiguous: {total_ambiguous}\")\n    if total > 0:\n        _safe_print(f\"  Avg per sentence: {total_ambiguous/total:.2f}\")\n    \n    if quality_metrics['confidences']:\n        avg_conf = sum(quality_metrics['confidences']) / len(quality_metrics['confidences'])\n        avg_span = sum(quality_metrics['spans']) / len(quality_metrics['spans']) if quality_metrics['spans'] else 0.0\n        avg_u = sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties']) if quality_metrics['uncertainties'] else 0.0\n        \n        high_conf = sum(1 for c in quality_metrics['confidences'] if c >= 0.65)\n        \n        _safe_print(\"\\n[QUALITY]\")\n        _safe_print(f\"  Avg confidence: {avg_conf:.3f}\")\n        _safe_print(f\"  Avg span: {avg_span:.3f}\")\n        _safe_print(f\"  Avg uncertainty: {avg_u:.3f}\")\n        _safe_print(\n            f\"  High confidence: {high_conf}/{len(quality_metrics['confidences'])} \"\n            f\"({high_conf/len(quality_metrics['confidences']):.1%})\"\n        )\n    else:\n        _safe_print(\"\\n[QUALITY]\")\n        _safe_print(\"  NO EXPLANATIONS\")\n        _safe_print(\"     Possible causes:\")\n        _safe_print(\"     1. DSCD empty (warmup failed)\")\n        _safe_print(\"     2. TRG thresholds too strict\")\n    \n    _safe_print(\"\\n[HOMOGRAPHS (DATA-DRIVEN)]\")\n    _safe_print(f\"  DSCD discovered: {len(dscd_homographs)}\")\n    _safe_print(f\"  Explained: {len(dscd_homographs_explained)}\")\n    if dscd_homographs:\n        try:\n            _safe_print(f\"  Rate: {len(dscd_homographs_explained)/len(dscd_homographs):.1%}\")\n        except ZeroDivisionError:\n            _safe_print(\"  Rate: 0.0%\")\n    \n    if dscd_homographs_explained:\n        _safe_print(\"\\n  Explained:\")\n        for homo in sorted(dscd_homographs_explained):\n            exps = homograph_explanations[homo]\n            avg_conf_local = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST_BN else \"   \"\n            _safe_print(f\"    {in_ref} '{homo}': {len(exps)}x conf={avg_conf_local:.3f}\")\n    \n    if sense_usage_stats:\n        _safe_print(\"\\n[SENSE USAGE ANALYSIS]\")\n        _safe_print(f\"  Words with sense detection: {len(sense_usage_stats)}\")\n        \n        for word in sorted(sense_usage_stats.keys()):\n            senses = sense_usage_stats[word]\n            total_uses = sum(senses.values())\n            _safe_print(f\"\\n  '{word}' ({total_uses} occurrences):\")\n            \n            for sense_label, count in sorted(senses.items(), key=lambda x: x[1], reverse=True):\n                pct = (count / total_uses * 100) if total_uses > 0 else 0\n                _safe_print(f\"    - {sense_label}: {count}x ({pct:.1f}%)\")\n                \n                if word in prototype_senses:\n                    proto_info = prototype_senses[word]\n                    matching_sense = next(\n                        (s for s in proto_info['senses'] if s['label'] == sense_label),\n                        None\n                    )\n                    if matching_sense and matching_sense['examples']:\n                        example = matching_sense['examples'][0]\n                        _safe_print(f\"      Example: \\\"{example}\\\"\")\n    \n    _safe_print(\"\\n[REFERENCE COMPARISON]\")\n    _safe_print(f\"  Size: {len(_HOMOGRAPH_REFERENCE_LIST_BN)}\")\n    _safe_print(f\"  Explained: {len(reference_homographs_explained)}\")\n    try:\n        coverage = (\n            len(reference_homographs_explained) / len(_HOMOGRAPH_REFERENCE_LIST_BN)\n            if len(_HOMOGRAPH_REFERENCE_LIST_BN) > 0\n            else 0.0\n        )\n        _safe_print(f\"  Coverage: {coverage:.1%}\")\n    except Exception:\n        _safe_print(\"  Coverage: N/A\")\n    \n    _safe_print(\"\\n[HEALTH]\")\n    warnings = []\n    \n    if successes < total * 0.7:\n        warnings.append(\"Low success (<70%)\")\n    if tests_with_explanations == 0:\n        warnings.append(\"NO explanations\")\n    if quality_metrics['confidences'] and avg_conf < 0.5:\n        warnings.append(\"Low confidence (<0.5)\")\n    if dscd_homographs and len(dscd_homographs_explained) < len(dscd_homographs) * 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    \n    if warnings:\n        for w in warnings:\n            _safe_print(f\"  âš  {w}\")\n    else:\n        _safe_print(\"  âœ“ All systems OK\")\n    \n    try:\n        results = {\n            'total_tests': total,\n            'successes': successes,\n            'tests_with_explanations': tests_with_explanations,\n            'quality_metrics': {\n                'avg_confidence': avg_conf if quality_metrics['confidences'] else 0,\n                'avg_span': avg_span if quality_metrics['spans'] else 0,\n                'avg_uncertainty': avg_u if quality_metrics['uncertainties'] else 0,\n            },\n            'dscd_discovered': len(dscd_homographs),\n            'dscd_explained': len(dscd_homographs_explained),\n            'reference_explained': len(reference_homographs_explained),\n            'avg_inference_time': (sum(inference_times)/len(inference_times)) if inference_times else 0,\n            'sense_usage': dict(sense_usage_stats),\n            'prototypes_loaded': len(prototype_senses),\n        }\n        \n        results_path = \"/kaggle/working/test_results.json\"\n        with open(results_path, 'w', encoding='utf-8') as f:\n            json.dump(results, f, indent=2, ensure_ascii=False)\n        _safe_print(f\"\\nâœ“ Results saved: {results_path}\")\n    except Exception:\n        pass\n    \n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(f\"Thresholds: span>{_SPAN_THRESHOLD}, uncertainty>{_UNCERTAINTY_THRESHOLD}\")\n    _safe_print(f\"Prototype path: {_PROTOTYPE_BASE_PATH}\")\n    _safe_print(\"Testing complete (WITH SENSE ANALYSIS)\")\n    _safe_print(\"=\" * 80)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 12: Extended testing with sense analysis ready (FINAL)\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\"  âœ“ F1:  Correct UNCERTAINTY_THRESHOLD instead of TAU_LOW\")\nprint(\"  âœ“ F2:  Prototype file loading from /kaggle/working/prototypes/\")\nprint(\"  âœ“ F3:  Sense detection via embedding similarity\")\nprint(\"  âœ“ F4:  Sense usage statistics per homograph\")\nprint(\"  âœ“ F5:  Context examples from prototype files\")\nprint(\"  âœ“ F6:  Sense distribution analysis\")\nprint(\"  âœ“ F7:  Enhanced explanations with [SENSE: label (similarity)]\")\nprint(\"  âœ“ F8:  Safe fallback when prototypes not found\")\nprint(\"  âœ“ F9:  weights_only=False in checkpoint loading\")\nprint(\"  âœ“ F10: DSCD checkpoint key priority (_prototype_stores first)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"zWd0uRn7H4J6","trusted":true,"execution":{"execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13: REAL-TIME EVALUATION METRICS (BLEU, chrF++, TER)\n# ==============================================================================\n\nimport os\nimport sys\nimport time\nimport csv\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom collections import defaultdict\nimport numpy as np\nimport torch\nimport gc\n\ntry:\n    import sacrebleu\n    print(\"âœ“ sacrebleu available\")\nexcept ImportError:\n    print(\"âš  Installing sacrebleu...\")\n    import subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"sacrebleu\"])\n    import sacrebleu\n    print(\"âœ“ sacrebleu installed\")\n\ntry:\n    from tqdm.auto import tqdm\n    _HAS_TQDM = True\nexcept ImportError:\n    _HAS_TQDM = False\n    print(\"âš  tqdm not available (progress bars disabled)\")\n\n\ndef _g(name, default):\n    return globals().get(name, default)\n\n\ntry:\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 52))\n    _EVAL_BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _SPAN_THRESHOLD = float(_g(\"SPAN_THRESHOLD\", 0.20))\n    _UNCERTAINTY_THRESHOLD = float(_g(\"UNCERTAINTY_THRESHOLD\", 0.25))\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(_g(\"HOMOGRAPH_REFERENCE_LIST_BN\", \n        [\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\"]))\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _MAX_LENGTH = 52\n    _EVAL_BATCH_SIZE = 8\n    _SPAN_THRESHOLD = 0.20\n    _UNCERTAINTY_THRESHOLD = 0.25\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\"}\n\n_RESULTS_DIR = \"/kaggle/working\"\n_RESULTS_CSV = os.path.join(_RESULTS_DIR, \"evaluation_results.csv\")\n_DETAILED_CSV = os.path.join(_RESULTS_DIR, \"detailed_scores.csv\")\n_REALTIME_UPDATE_INTERVAL = 100\n\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n    except Exception:\n        pass\n\n\ndef _contains_homograph(sentence: str, homograph_set: set) -> bool:\n    \"\"\"Check if sentence contains any known homograph.\"\"\"\n    try:\n        words = sentence.split()\n        for word in words:\n            clean = word.strip().lower()\n            for hg in homograph_set:\n                if hg.lower() in clean:\n                    return True\n        return False\n    except Exception:\n        return False\n\n\ndef compute_incremental_metrics(\n    hypotheses: List[str],\n    references: List[str],\n    show_progress: bool = True\n) -> Dict[str, float]:\n    \"\"\"Compute metrics with real-time updates every N samples.\"\"\"\n    \n    if len(hypotheses) != len(references):\n        raise ValueError(f\"Length mismatch: {len(hypotheses)} hyps vs {len(references)} refs\")\n    \n    valid_pairs = []\n    for hyp, ref in zip(hypotheses, references):\n        if hyp and ref and hyp != \"ERROR: Translation failed\":\n            valid_pairs.append((hyp, ref))\n    \n    if not valid_pairs:\n        return {\n            'bleu': 0.0,\n            'bleu_1': 0.0,\n            'bleu_2': 0.0,\n            'bleu_3': 0.0,\n            'bleu_4': 0.0,\n            'chrf': 0.0,\n            'chrf_pp': 0.0,\n            'ter': 100.0,\n            'valid_samples': 0\n        }\n    \n    hyps_valid, refs_valid = zip(*valid_pairs)\n    refs_list = [[ref] for ref in refs_valid]\n    \n    try:\n        bleu_score = sacrebleu.corpus_bleu(hyps_valid, refs_list)\n        bleu = bleu_score.score\n        \n        try:\n            bleu_1 = bleu_score.precisions[0] if len(bleu_score.precisions) > 0 else 0.0\n            bleu_2 = bleu_score.precisions[1] if len(bleu_score.precisions) > 1 else 0.0\n            bleu_3 = bleu_score.precisions[2] if len(bleu_score.precisions) > 2 else 0.0\n            bleu_4 = bleu_score.precisions[3] if len(bleu_score.precisions) > 3 else 0.0\n        except Exception:\n            bleu_1 = bleu_2 = bleu_3 = bleu_4 = 0.0\n    \n    except Exception as e:\n        if show_progress:\n            print(f\"âš  BLEU computation failed: {e}\")\n        bleu = bleu_1 = bleu_2 = bleu_3 = bleu_4 = 0.0\n    \n    try:\n        chrf_score = sacrebleu.corpus_chrf(hyps_valid, refs_list)\n        chrf = chrf_score.score\n    except Exception as e:\n        if show_progress:\n            print(f\"âš  chrF computation failed: {e}\")\n        chrf = 0.0\n    \n    try:\n        chrf_pp_score = sacrebleu.corpus_chrf(hyps_valid, refs_list, word_order=2)\n        chrf_pp = chrf_pp_score.score\n    except Exception as e:\n        if show_progress:\n            print(f\"âš  chrF++ computation failed: {e}\")\n        chrf_pp = 0.0\n    \n    try:\n        ter_score = sacrebleu.corpus_ter(hyps_valid, refs_list)\n        ter = ter_score.score\n    except Exception as e:\n        if show_progress:\n            print(f\"âš  TER computation failed: {e}\")\n        ter = 100.0\n    \n    return {\n        'bleu': bleu,\n        'bleu_1': bleu_1,\n        'bleu_2': bleu_2,\n        'bleu_3': bleu_3,\n        'bleu_4': bleu_4,\n        'chrf': chrf,\n        'chrf_pp': chrf_pp,\n        'ter': ter,\n        'valid_samples': len(valid_pairs)\n    }\n\n\ndef load_test_dataset(num_samples: int = 4000) -> List[Tuple[str, str]]:\n    \"\"\"Load test dataset separate from training data.\"\"\"\n    print(f\"\\n[LOADING TEST DATA] {num_samples} samples...\")\n    \n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            total_samples = num_samples + 5000\n            all_pairs = load_and_preprocess_optimized(total_samples)\n            \n            test_pairs = all_pairs[-num_samples:]\n            \n            print(f\"âœ“ Loaded {len(test_pairs)} test samples (held-out)\")\n            return test_pairs\n        except Exception as e:\n            print(f\"âš  load_and_preprocess_optimized failed: {e}\")\n    \n    print(\"âš  Using fallback test data\")\n    fallback_data = [\n        (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap.\"),\n        (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"Tomorrow I will buy a book.\"),\n        (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"The leaves have fallen.\"),\n        (\"à¦¬à§à¦¯à¦¾à¦‚à¦• à¦¥à§‡à¦•à§‡ à¦Ÿà¦¾à¦•à¦¾ à¦¤à§à¦²à¦²à¦¾à¦®à¥¤\", \"I withdrew money from the bank.\"),\n        (\"à¦«à¦² à¦–à¦¾à¦“à¦¯à¦¼à¦¾ à¦¸à§à¦¬à¦¾à¦¸à§à¦¥à§à¦¯à¦•à¦°à¥¤\", \"Eating fruit is healthy.\"),\n    ] * (num_samples // 5)\n    \n    return fallback_data[:num_samples]\n\n\ndef translate_batch_baseline(\n    model,\n    tokenizer,\n    sentences: List[str],\n    device: torch.device,\n    max_length: int = 52\n) -> List[str]:\n    \"\"\"Translate using baseline M2M100 (no DSCD/ASBN/TRG).\"\"\"\n    try:\n        core_model = model.module if hasattr(model, 'module') else model\n        mbart = getattr(core_model, 'mbart', None)\n        \n        if mbart is None:\n            raise RuntimeError(\"mbart model not found\")\n        \n        was_training = core_model.training\n        core_model.eval()\n        \n        try:\n            tokenizer.src_lang = _SOURCE_LANGUAGE\n        except Exception:\n            pass\n        \n        translations = []\n        \n        with torch.no_grad():\n            inputs = tokenizer(\n                sentences,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=max_length\n            )\n            \n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            try:\n                tgt_lang_id = tokenizer.get_lang_id(_TARGET_LANGUAGE)\n            except Exception:\n                tgt_lang_id = tokenizer.convert_tokens_to_ids(_TARGET_LANGUAGE)\n            \n            generated = mbart.generate(\n                **inputs,\n                forced_bos_token_id=tgt_lang_id,\n                max_length=max_length,\n                num_beams=5,\n                early_stopping=True\n            )\n            \n            translations = tokenizer.batch_decode(generated, skip_special_tokens=True)\n        \n        if was_training:\n            core_model.train()\n        \n        return translations\n    \n    except Exception as e:\n        print(f\"âš  Batch baseline translation failed: {e}\")\n        return [\"ERROR: Translation failed\"] * len(sentences)\n\n\ndef translate_batch_tatn(\n    model,\n    tokenizer,\n    sentences: List[str],\n    device: torch.device,\n    span_threshold: float = 0.20,\n    uncertainty_threshold: float = 0.25\n) -> List[str]:\n    \"\"\"Translate using TATN (with DSCD/ASBN/TRG).\"\"\"\n    if \"translate_with_explanations\" not in globals():\n        print(\"âš  translate_with_explanations not found, using baseline\")\n        return translate_batch_baseline(model, tokenizer, sentences, device)\n    \n    translations = []\n    \n    for sentence in sentences:\n        try:\n            result = translate_with_explanations(\n                model,\n                tokenizer,\n                sentence,\n                device=device,\n                span_threshold=span_threshold,\n                uncertainty_threshold=uncertainty_threshold\n            )\n            \n            if isinstance(result, dict):\n                translation = result.get('translation', 'ERROR')\n            else:\n                translation = str(result)\n            \n            translations.append(translation)\n        \n        except Exception as e:\n            translations.append(\"ERROR: Translation failed\")\n    \n    return translations\n\n\ndef evaluate_model_comprehensive(\n    model,\n    tokenizer,\n    test_pairs: List[Tuple[str, str]],\n    device: torch.device,\n    batch_size: int = 8,\n    compute_baseline: bool = True,\n    save_detailed: bool = True,\n    realtime_updates: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Comprehensive evaluation with REAL-TIME BLEU/chrF++ updates.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION - REAL-TIME METRICS\")\n    print(\"=\" * 80)\n    \n    print(f\"\\nConfiguration:\")\n    print(f\"  Test samples: {len(test_pairs)}\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Device: {device}\")\n    print(f\"  Compute baseline: {compute_baseline}\")\n    print(f\"  Real-time updates: {realtime_updates}\")\n    print(f\"  Update interval: {_REALTIME_UPDATE_INTERVAL} samples\")\n    \n    sources = [pair[0] for pair in test_pairs]\n    references = [pair[1] for pair in test_pairs]\n    \n    homograph_indices = []\n    for i, src in enumerate(sources):\n        if _contains_homograph(src, _HOMOGRAPH_REFERENCE_LIST_BN):\n            homograph_indices.append(i)\n    \n    print(f\"  Homograph sentences: {len(homograph_indices)}/{len(sources)}\")\n    \n    results = {\n        'test_size': len(test_pairs),\n        'homograph_count': len(homograph_indices),\n        'baseline_metrics': None,\n        'tatn_metrics': None,\n        'homograph_baseline_metrics': None,\n        'homograph_tatn_metrics': None,\n        'detailed_scores': []\n    }\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"[PHASE 1/2] TATN TRANSLATION WITH REAL-TIME METRICS\")\n    print(\"=\" * 80)\n    start_time = time.time()\n    \n    tatn_translations = []\n    num_batches = (len(sources) + batch_size - 1) // batch_size\n    \n    last_update_idx = 0\n    \n    iterator = range(0, len(sources), batch_size)\n    if not realtime_updates and _HAS_TQDM:\n        iterator = tqdm(iterator, total=num_batches, desc=\"TATN\")\n    \n    for batch_idx, i in enumerate(iterator):\n        batch_sources = sources[i:i+batch_size]\n        batch_translations = translate_batch_tatn(\n            model,\n            tokenizer,\n            batch_sources,\n            device,\n            span_threshold=_SPAN_THRESHOLD,\n            uncertainty_threshold=_UNCERTAINTY_THRESHOLD\n        )\n        tatn_translations.extend(batch_translations)\n        \n        current_idx = len(tatn_translations)\n        \n        if realtime_updates and (current_idx - last_update_idx >= _REALTIME_UPDATE_INTERVAL or current_idx >= len(sources)):\n            try:\n                incremental_metrics = compute_incremental_metrics(\n                    tatn_translations[:current_idx],\n                    references[:current_idx],\n                    show_progress=False\n                )\n                \n                elapsed = time.time() - start_time\n                speed = current_idx / elapsed if elapsed > 0 else 0\n                eta = (len(sources) - current_idx) / speed if speed > 0 else 0\n                \n                print(f\"\\n[TATN] Progress: {current_idx}/{len(sources)} ({100*current_idx/len(sources):.1f}%)\")\n                print(f\"  BLEU:   {incremental_metrics['bleu']:>6.2f}  |  chrF++: {incremental_metrics['chrf_pp']:>6.2f}  |  TER: {incremental_metrics['ter']:>6.2f}\")\n                print(f\"  Speed:  {speed:.1f} sent/s  |  ETA: {eta:.0f}s  |  Valid: {incremental_metrics['valid_samples']}/{current_idx}\")\n                \n                last_update_idx = current_idx\n            \n            except Exception as e:\n                if batch_idx == 0:\n                    print(f\"âš  Real-time metrics failed: {e}\")\n        \n        if (batch_idx + 1) % 50 == 0:\n            _safe_cleanup()\n    \n    tatn_time = time.time() - start_time\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"âœ“ TATN translation complete ({tatn_time:.1f}s, {tatn_time/len(sources):.3f}s/sample)\")\n    \n    tatn_metrics = compute_incremental_metrics(tatn_translations, references)\n    results['tatn_metrics'] = tatn_metrics\n    \n    print(f\"\\n[TATN FINAL METRICS - Full Test Set]\")\n    print(f\"  BLEU:   {tatn_metrics['bleu']:>7.2f}\")\n    print(f\"  chrF++: {tatn_metrics['chrf_pp']:>7.2f}\")\n    print(f\"  TER:    {tatn_metrics['ter']:>7.2f}\")\n    print(f\"  Valid:  {tatn_metrics['valid_samples']}/{len(test_pairs)}\")\n    \n    _safe_cleanup()\n    \n    if compute_baseline:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[PHASE 2/2] BASELINE TRANSLATION WITH REAL-TIME METRICS\")\n        print(\"=\" * 80)\n        start_time = time.time()\n        \n        baseline_translations = []\n        last_update_idx = 0\n        \n        iterator = range(0, len(sources), batch_size)\n        if not realtime_updates and _HAS_TQDM:\n            iterator = tqdm(iterator, total=num_batches, desc=\"Baseline\")\n        \n        for batch_idx, i in enumerate(iterator):\n            batch_sources = sources[i:i+batch_size]\n            batch_translations = translate_batch_baseline(\n                model,\n                tokenizer,\n                batch_sources,\n                device,\n                max_length=_MAX_LENGTH\n            )\n            baseline_translations.extend(batch_translations)\n            \n            current_idx = len(baseline_translations)\n            \n            if realtime_updates and (current_idx - last_update_idx >= _REALTIME_UPDATE_INTERVAL or current_idx >= len(sources)):\n                try:\n                    incremental_metrics = compute_incremental_metrics(\n                        baseline_translations[:current_idx],\n                        references[:current_idx],\n                        show_progress=False\n                    )\n                    \n                    elapsed = time.time() - start_time\n                    speed = current_idx / elapsed if elapsed > 0 else 0\n                    eta = (len(sources) - current_idx) / speed if speed > 0 else 0\n                    \n                    print(f\"\\n[BASELINE] Progress: {current_idx}/{len(sources)} ({100*current_idx/len(sources):.1f}%)\")\n                    print(f\"  BLEU:   {incremental_metrics['bleu']:>6.2f}  |  chrF++: {incremental_metrics['chrf_pp']:>6.2f}  |  TER: {incremental_metrics['ter']:>6.2f}\")\n                    print(f\"  Speed:  {speed:.1f} sent/s  |  ETA: {eta:.0f}s  |  Valid: {incremental_metrics['valid_samples']}/{current_idx}\")\n                    \n                    last_update_idx = current_idx\n                \n                except Exception as e:\n                    if batch_idx == 0:\n                        print(f\"âš  Real-time metrics failed: {e}\")\n            \n            if (batch_idx + 1) % 50 == 0:\n                _safe_cleanup()\n        \n        baseline_time = time.time() - start_time\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"âœ“ Baseline translation complete ({baseline_time:.1f}s, {baseline_time/len(sources):.3f}s/sample)\")\n        \n        baseline_metrics = compute_incremental_metrics(baseline_translations, references)\n        results['baseline_metrics'] = baseline_metrics\n        \n        print(f\"\\n[BASELINE FINAL METRICS - Full Test Set]\")\n        print(f\"  BLEU:   {baseline_metrics['bleu']:>7.2f}\")\n        print(f\"  chrF++: {baseline_metrics['chrf_pp']:>7.2f}\")\n        print(f\"  TER:    {baseline_metrics['ter']:>7.2f}\")\n        print(f\"  Valid:  {baseline_metrics['valid_samples']}/{len(test_pairs)}\")\n    \n    else:\n        baseline_translations = None\n    \n    _safe_cleanup()\n    \n    if homograph_indices:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[HOMOGRAPH SUBSET EVALUATION]\")\n        print(\"=\" * 80)\n        \n        hg_sources = [sources[i] for i in homograph_indices]\n        hg_references = [references[i] for i in homograph_indices]\n        hg_tatn = [tatn_translations[i] for i in homograph_indices]\n        \n        hg_tatn_metrics = compute_incremental_metrics(hg_tatn, hg_references, show_progress=False)\n        results['homograph_tatn_metrics'] = hg_tatn_metrics\n        \n        print(f\"\\nTATN Homograph Subset ({len(homograph_indices)} samples):\")\n        print(f\"  BLEU:   {hg_tatn_metrics['bleu']:.2f}\")\n        print(f\"  chrF++: {hg_tatn_metrics['chrf_pp']:.2f}\")\n        print(f\"  TER:    {hg_tatn_metrics['ter']:.2f}\")\n        \n        if baseline_translations:\n            hg_baseline = [baseline_translations[i] for i in homograph_indices]\n            hg_baseline_metrics = compute_incremental_metrics(hg_baseline, hg_references, show_progress=False)\n            results['homograph_baseline_metrics'] = hg_baseline_metrics\n            \n            print(f\"\\nBaseline Homograph Subset ({len(homograph_indices)} samples):\")\n            print(f\"  BLEU:   {hg_baseline_metrics['bleu']:.2f}\")\n            print(f\"  chrF++: {hg_baseline_metrics['chrf_pp']:.2f}\")\n            print(f\"  TER:    {hg_baseline_metrics['ter']:.2f}\")\n    \n    if save_detailed and baseline_translations:\n        print(f\"\\n[SAVING DETAILED RESULTS]\")\n        \n        try:\n            os.makedirs(_RESULTS_DIR, exist_ok=True)\n            \n            with open(_DETAILED_CSV, 'w', newline='', encoding='utf-8') as f:\n                writer = csv.writer(f)\n                writer.writerow([\n                    'Index', 'Source', 'Reference', 'TATN_Translation', \n                    'Baseline_Translation', 'Is_Homograph'\n                ])\n                \n                for i, (src, ref, tatn, baseline) in enumerate(zip(\n                    sources, references, tatn_translations, baseline_translations\n                )):\n                    is_hg = 'Yes' if i in homograph_indices else 'No'\n                    writer.writerow([i, src, ref, tatn, baseline, is_hg])\n            \n            print(f\"âœ“ Detailed scores saved: {_DETAILED_CSV}\")\n        \n        except Exception as e:\n            print(f\"âš  Failed to save detailed results: {e}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\" * 80)\n    \n    return results\n\n\ndef print_comparison_table(results: Dict[str, Any]):\n    \"\"\"Print formatted comparison table.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"METRICS COMPARISON TABLE\")\n    print(\"=\" * 80)\n    \n    baseline = results.get('baseline_metrics')\n    tatn = results.get('tatn_metrics')\n    \n    if not tatn:\n        print(\"âš  No TATN metrics available\")\n        return\n    \n    print(f\"\\n{'System':<20} | {'BLEU':<8} | {'chrF++':<8} | {'TER':<8} | {'Samples':<8}\")\n    print(\"-\" * 80)\n    \n    if baseline:\n        print(f\"{'Baseline M2M100':<20} | {baseline['bleu']:>7.2f} | {baseline['chrf_pp']:>7.2f} | {baseline['ter']:>7.2f} | {baseline['valid_samples']:>8}\")\n    \n    print(f\"{'TATN Full System':<20} | {tatn['bleu']:>7.2f} | {tatn['chrf_pp']:>7.2f} | {tatn['ter']:>7.2f} | {tatn['valid_samples']:>8}\")\n    \n    if baseline:\n        bleu_diff = tatn['bleu'] - baseline['bleu']\n        chrf_diff = tatn['chrf_pp'] - baseline['chrf_pp']\n        ter_diff = tatn['ter'] - baseline['ter']\n        \n        print(\"-\" * 80)\n        print(f\"{'Improvement':<20} | {bleu_diff:>+7.2f} | {chrf_diff:>+7.2f} | {ter_diff:>+7.2f} | {'-':>8}\")\n    \n    hg_baseline = results.get('homograph_baseline_metrics')\n    hg_tatn = results.get('homograph_tatn_metrics')\n    \n    if hg_tatn:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"HOMOGRAPH SUBSET\")\n        print(\"=\" * 80)\n        \n        print(f\"\\n{'System':<20} | {'BLEU':<8} | {'chrF++':<8} | {'TER':<8} | {'Samples':<8}\")\n        print(\"-\" * 80)\n        \n        if hg_baseline:\n            print(f\"{'Baseline M2M100':<20} | {hg_baseline['bleu']:>7.2f} | {hg_baseline['chrf_pp']:>7.2f} | {hg_baseline['ter']:>7.2f} | {hg_baseline['valid_samples']:>8}\")\n        \n        print(f\"{'TATN Full System':<20} | {hg_tatn['bleu']:>7.2f} | {hg_tatn['chrf_pp']:>7.2f} | {hg_tatn['ter']:>7.2f} | {hg_tatn['valid_samples']:>8}\")\n        \n        if hg_baseline:\n            bleu_diff = hg_tatn['bleu'] - hg_baseline['bleu']\n            chrf_diff = hg_tatn['chrf_pp'] - hg_baseline['chrf_pp']\n            ter_diff = hg_tatn['ter'] - hg_baseline['ter']\n            \n            print(\"-\" * 80)\n            print(f\"{'Improvement':<20} | {bleu_diff:>+7.2f} | {chrf_diff:>+7.2f} | {ter_diff:>+7.2f} | {'-':>8}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n\n\ndef save_results_summary(results: Dict[str, Any], filepath: str = _RESULTS_CSV):\n    \"\"\"Save evaluation results to CSV.\"\"\"\n    \n    try:\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        \n        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            \n            writer.writerow(['Metric', 'Baseline', 'TATN', 'Improvement'])\n            \n            baseline = results.get('baseline_metrics', {})\n            tatn = results.get('tatn_metrics', {})\n            \n            metrics = ['bleu', 'chrf_pp', 'ter']\n            metric_names = ['BLEU', 'chrF++', 'TER']\n            \n            for metric, name in zip(metrics, metric_names):\n                b_val = baseline.get(metric, 0.0)\n                t_val = tatn.get(metric, 0.0)\n                diff = t_val - b_val\n                \n                writer.writerow([name, f\"{b_val:.2f}\", f\"{t_val:.2f}\", f\"{diff:+.2f}\"])\n            \n            writer.writerow([])\n            writer.writerow(['Homograph Subset'])\n            \n            hg_baseline = results.get('homograph_baseline_metrics', {})\n            hg_tatn = results.get('homograph_tatn_metrics', {})\n            \n            for metric, name in zip(metrics, metric_names):\n                b_val = hg_baseline.get(metric, 0.0)\n                t_val = hg_tatn.get(metric, 0.0)\n                diff = t_val - b_val\n                \n                writer.writerow([f\"{name} (HG)\", f\"{b_val:.2f}\", f\"{t_val:.2f}\", f\"{diff:+.2f}\"])\n        \n        print(f\"âœ“ Results summary saved: {filepath}\")\n    \n    except Exception as e:\n        print(f\"âš  Failed to save results summary: {e}\")\n\n\ndef run_full_evaluation(\n    model,\n    tokenizer,\n    test_size: int = 4000,\n    batch_size: int = 8,\n    compute_baseline: bool = True,\n    realtime_updates: bool = True\n) -> Dict[str, Any]:\n    \"\"\"Run complete evaluation pipeline with real-time metrics.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"FULL EVALUATION PIPELINE - REAL-TIME METRICS\")\n    print(\"=\" * 80)\n    print(f\"Test size: {test_size}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Compute baseline: {compute_baseline}\")\n    print(f\"Real-time updates: {realtime_updates}\")\n    print(\"=\" * 80)\n    \n    start_time = time.time()\n    \n    test_pairs = load_test_dataset(test_size)\n    \n    results = evaluate_model_comprehensive(\n        model,\n        tokenizer,\n        test_pairs,\n        device=_DEVICE,\n        batch_size=batch_size,\n        compute_baseline=compute_baseline,\n        save_detailed=True,\n        realtime_updates=realtime_updates\n    )\n    \n    print_comparison_table(results)\n    \n    save_results_summary(results)\n    \n    total_time = time.time() - start_time\n    \n    print(f\"\\nTotal evaluation time: {total_time:.1f}s ({total_time/60:.2f} min)\")\n    print(f\"Results saved to: {_RESULTS_DIR}\")\n    \n    return results\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 13: Real-time evaluation metrics ready - COMPLETE\")\nprint(\"=\" * 80)\nprint(\"\\nAvailable functions:\")\nprint(\"  1. run_full_evaluation(trained_model, tokenizer, test_size=4000, realtime_updates=True)\")\nprint(\"  2. evaluate_model_comprehensive(model, tokenizer, test_pairs, device, realtime_updates=True)\")\nprint(\"  3. compute_incremental_metrics(hypotheses, references)\")\nprint(\"  4. print_comparison_table(results)\")\nprint(\"\\nUsage:\")\nprint(\"  eval_results = run_full_evaluation(trained_model, tokenizer, test_size=4000)\")\nprint(\"\\nReal-time updates:\")\nprint(f\"  - Metrics computed every {_REALTIME_UPDATE_INTERVAL} samples\")\nprint(\"  - Shows: BLEU, chrF++, TER, Speed, ETA\")\nprint(\"\\nOutputs:\")\nprint(f\"  - Summary CSV: {_RESULTS_CSV}\")\nprint(f\"  - Detailed CSV: {_DETAILED_CSV}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 14: RUN EVALUATION AND DISPLAY RESULTS\n# ==============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING EVALUATION ON TRAINED MODEL\")\nprint(\"=\" * 80)\n\n# Check if trained model exists\nif 'trained_model' not in globals() or 'tokenizer' not in globals():\n    print(\"\\nâŒ ERROR: trained_model or tokenizer not found!\")\n    print(\"   -> Run Cell 11 first to train the model\")\n    print(\"   -> Then run this cell to evaluate\")\nelse:\n    print(f\"\\nâœ“ Model found: {type(trained_model).__name__}\")\n    print(f\"âœ“ Tokenizer found: {type(tokenizer).__name__}\")\n    \n    # Run full evaluation with real-time metrics\n    try:\n        eval_results = run_full_evaluation(\n            trained_model, \n            tokenizer, \n            test_size=4000,           # Evaluate on 4000 test samples\n            batch_size=8,             # Batch size for translation\n            compute_baseline=True,    # Compare against baseline M2M100\n            realtime_updates=True     # Show live BLEU/chrF++ scores\n        )\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"EVALUATION COMPLETE!\")\n        print(\"=\" * 80)\n        \n        # Access individual metrics\n        if eval_results and 'tatn_metrics' in eval_results:\n            tatn = eval_results['tatn_metrics']\n            print(\"\\n[TATN SYSTEM SCORES]\")\n            print(f\"  BLEU Score:    {tatn['bleu']:.2f}\")\n            print(f\"  chrF++ Score:  {tatn['chrf_pp']:.2f}\")\n            print(f\"  TER Score:     {tatn['ter']:.2f}\")\n            print(f\"  Valid Samples: {tatn['valid_samples']}\")\n            \n            if 'baseline_metrics' in eval_results and eval_results['baseline_metrics']:\n                baseline = eval_results['baseline_metrics']\n                print(\"\\n[BASELINE M2M100 SCORES]\")\n                print(f\"  BLEU Score:    {baseline['bleu']:.2f}\")\n                print(f\"  chrF++ Score:  {baseline['chrf_pp']:.2f}\")\n                print(f\"  TER Score:     {baseline['ter']:.2f}\")\n                \n                print(\"\\n[IMPROVEMENT OVER BASELINE]\")\n                print(f\"  BLEU:   {tatn['bleu'] - baseline['bleu']:+.2f} ({((tatn['bleu'] - baseline['bleu'])/baseline['bleu']*100):+.1f}%)\")\n                print(f\"  chrF++: {tatn['chrf_pp'] - baseline['chrf_pp']:+.2f} ({((tatn['chrf_pp'] - baseline['chrf_pp'])/baseline['chrf_pp']*100):+.1f}%)\")\n                print(f\"  TER:    {tatn['ter'] - baseline['ter']:+.2f} ({((tatn['ter'] - baseline['ter'])/baseline['ter']*100):+.1f}%)\")\n            \n            # Homograph subset scores\n            if 'homograph_tatn_metrics' in eval_results and eval_results['homograph_tatn_metrics']:\n                hg_tatn = eval_results['homograph_tatn_metrics']\n                print(\"\\n[HOMOGRAPH SUBSET - TATN]\")\n                print(f\"  BLEU Score:    {hg_tatn['bleu']:.2f}\")\n                print(f\"  chrF++ Score:  {hg_tatn['chrf_pp']:.2f}\")\n                print(f\"  TER Score:     {hg_tatn['ter']:.2f}\")\n                print(f\"  Valid Samples: {hg_tatn['valid_samples']}\")\n                \n                if 'homograph_baseline_metrics' in eval_results and eval_results['homograph_baseline_metrics']:\n                    hg_baseline = eval_results['homograph_baseline_metrics']\n                    print(\"\\n[HOMOGRAPH SUBSET - IMPROVEMENT]\")\n                    print(f\"  BLEU:   {hg_tatn['bleu'] - hg_baseline['bleu']:+.2f}\")\n                    print(f\"  chrF++: {hg_tatn['chrf_pp'] - hg_baseline['chrf_pp']:+.2f}\")\n                    print(f\"  TER:    {hg_tatn['ter'] - hg_baseline['ter']:+.2f}\")\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"FILES SAVED:\")\n        print(\"=\" * 80)\n        print(f\"  1. Summary CSV:  /kaggle/working/evaluation_results.csv\")\n        print(f\"  2. Detailed CSV: /kaggle/working/detailed_scores.csv\")\n        print(\"\\nTo view results:\")\n        print(\"  import pandas as pd\")\n        print(\"  df = pd.read_csv('/kaggle/working/evaluation_results.csv')\")\n        print(\"  print(df)\")\n        \n    except Exception as e:\n        print(f\"\\nâŒ Evaluation failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\"\\n\" + \"=\" * 80)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12: HOMOGRAPH DEMONSTRATION - REFERENCE LIST FOCUSED (PRESENTATION)\n# ==============================================================================\nimport os\nimport json\nfrom typing import Dict, List, Any\nfrom collections import defaultdict\nimport torch\n\ntry:\n    _DEVICE = DEVICE if isinstance(DEVICE, torch.device) else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _HOMOGRAPH_REFERENCE_LIST_BN = HOMOGRAPH_REFERENCE_LIST_BN\nexcept:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"à¦•à¦²\", \"à¦•à¦¾à¦²\", \"à¦ªà¦¾à¦¤à¦¾\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\", \"à¦«à¦²\", \"à¦®à¦¾à¦¥à¦¾\", \"à¦¬à¦¾à¦°\", \"à¦¹à¦¾à¦°\", \"à¦¤à¦¾à¦°à¦¾\",\n        \"à¦ªà¦¾à¦¨à¦¿\", \"à¦¦à¦²\", \"à¦¬à¦¾à¦œà¦¾à¦°\", \"à¦¨à¦¾à¦®\", \"à¦•à¦¥à¦¾\", \"à¦¬à¦‡\", \"à¦˜à¦°\", \"à¦®à¦¨\", \"à¦¹à¦¾à¦¤\",\n        \"à¦¦à¦¿à¦¨\", \"à¦°à¦¾à¦¤\", \"à¦œà¦²\", \"à¦¬à¦¾à¦¡à¦¼à¦¿\", \"à¦ªà¦¾à¦°à§à¦•\", \"à¦¨à¦¦à§€\", \"à¦¬à¦¨\", \"à¦«à§à¦²\", \"à¦—à¦¾à¦›\",\n        \"à¦šà§‹à¦–\", \"à¦®à§à¦–\", \"à¦ªà¦¾\", \"à¦•à¦¾à¦¨\", \"à¦—à¦²à¦¾\", \"à¦¨à¦¾à¦•\", \"à¦¦à¦¾à¦à¦¤\", \"à¦•à§‹à¦®à¦°\",\n        \"à¦ªà¦¡à¦¼à¦¾\", \"à¦¦à§‡à¦–à¦¾\", \"à¦¯à¦¾à¦“à¦¯à¦¼à¦¾\", \"à¦†à¦¸à¦¾\", \"à¦–à§‡à¦²à¦¾\", \"à¦²à§‡à¦–à¦¾\", \"à¦¬à¦²à¦¾\", \"à¦¶à§‹à¦¨à¦¾\",\n        \"à¦šà¦²à¦¾\", \"à¦§à¦°à¦¾\", \"à¦¦à§‡à¦“à¦¯à¦¼à¦¾\", \"à¦¨à§‡à¦“à¦¯à¦¼à¦¾\",\n        \"à¦¸à¦®à¦¯à¦¼\", \"à¦¬à¦›à¦°\", \"à¦®à¦¾à¦¸\", \"à¦¸à¦¾à¦²\", \"à¦˜à¦¨à§à¦Ÿà¦¾\", \"à¦®à§à¦¹à§‚à¦°à§à¦¤\",\n        \"à¦—à¦°à¦®\", \"à¦¶à§€à¦¤\", \"à¦¬à¦¾à¦¤à¦¾à¦¸\", \"à¦†à¦—à§à¦¨\", \"à¦ªà¦¾à¦¥à¦°\", \"à¦®à¦¾à¦Ÿà¦¿\",\n        \"à¦­à¦¾à¦¬\", \"à¦°à¦‚\", \"à¦†à¦²à§‹\", \"à¦›à¦¾à¦¯à¦¼à¦¾\", \"à¦¶à¦¬à§à¦¦\", \"à¦…à¦°à§à¦¥\",\n    }\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n_PROTOTYPE_SAVE_PATH = \"/kaggle/working/reference_homographs.json\"\n\n\ndef normalize_token(token: str) -> str:\n    \"\"\"Clean token to match reference words\"\"\"\n    return token.replace('â–', '').replace('Ä ', '').replace('##', '').strip().lower()\n\n\ndef extract_reference_homographs(model, tokenizer) -> Dict[str, Any]:\n    \"\"\"Extract ONLY reference list homographs from DSCD\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXTRACTING REFERENCE HOMOGRAPHS FROM DSCD\")\n    print(\"=\" * 80)\n    \n    core = model.module if hasattr(model, 'module') else model\n    dscd = getattr(core, 'dscd', None)\n    \n    if dscd is None:\n        print(\"âŒ No DSCD module found\")\n        return {}\n    \n    lock = None\n    if hasattr(dscd, 'buffer_lock'):\n        lock = dscd.buffer_lock\n    elif hasattr(dscd, 'clustering_lock'):\n        lock = dscd.clustering_lock\n    \n    if lock:\n        with lock:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n    else:\n        stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n    \n    print(f\"\\nğŸ“Š Total DSCD tokens: {len(stores)}\")\n    print(f\"ğŸ“‹ Reference list size: {len(_HOMOGRAPH_REFERENCE_LIST_BN)}\")\n    \n    reference_homographs = {}\n    found_count = 0\n    \n    for ref_word in _HOMOGRAPH_REFERENCE_LIST_BN:\n        found = False\n        for token, store in stores.items():\n            clean_token = normalize_token(str(token))\n            \n            if clean_token == ref_word.lower():\n                try:\n                    size_attr = getattr(store, \"size\", None)\n                    if callable(size_attr):\n                        num_senses = size_attr()\n                    elif isinstance(size_attr, int):\n                        num_senses = size_attr\n                    else:\n                        num_senses = 0\n                    \n                    if num_senses >= 2:\n                        counts = getattr(store, 'counts', [])\n                        total_occurrences = sum(int(c) for c in counts) if counts else 0\n                        \n                        reference_homographs[ref_word] = {\n                            'word': ref_word,\n                            'original_token': str(token),\n                            'num_senses': num_senses,\n                            'total_occurrences': total_occurrences,\n                            'sense_counts': [int(c) for c in counts] if counts else []\n                        }\n                        found_count += 1\n                        found = True\n                        break\n                except Exception:\n                    continue\n        \n        if not found:\n            reference_homographs[ref_word] = {\n                'word': ref_word,\n                'status': 'NOT_FOUND',\n                'num_senses': 0\n            }\n    \n    print(f\"âœ“ Found {found_count}/{len(_HOMOGRAPH_REFERENCE_LIST_BN)} reference homographs\")\n    print(f\"   Coverage: {found_count/len(_HOMOGRAPH_REFERENCE_LIST_BN)*100:.1f}%\")\n    \n    try:\n        with open(_PROTOTYPE_SAVE_PATH, 'w', encoding='utf-8') as f:\n            json.dump(reference_homographs, f, indent=2, ensure_ascii=False)\n        print(f\"âœ“ Saved to: {_PROTOTYPE_SAVE_PATH}\")\n    except Exception as e:\n        print(f\"âŒ Save failed: {e}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    return reference_homographs\n\n\ndef get_bengali_meanings(word: str) -> List[str]:\n    \"\"\"Get known Bengali meanings for reference words\"\"\"\n    meanings = {\n        'à¦•à¦²': ['Water tap (à¦¨à¦²)', 'Phone call (à¦«à§‹à¦¨à¦•à¦²)', 'Yesterday (à¦—à¦¤à¦•à¦¾à¦²)'],\n        'à¦•à¦¾à¦²': ['Tomorrow (à¦†à¦—à¦¾à¦®à§€à¦•à¦¾à¦²)', 'Yesterday (à¦—à¦¤à¦•à¦¾à¦²)', 'Black color (à¦•à¦¾à¦²à§‹)', 'Time period (à¦¸à¦®à¦¯à¦¼à¦•à¦¾à¦²)'],\n        'à¦ªà¦¾à¦¤à¦¾': ['Leaf (à¦—à¦¾à¦›à§‡à¦° à¦ªà¦¾à¦¤à¦¾)', 'Page (à¦¬à¦‡à¦¯à¦¼à§‡à¦° à¦ªà§ƒà¦·à§à¦ à¦¾)', 'To lay/spread (à¦¬à¦¿à¦›à¦¾à¦¨à§‹)'],\n        'à¦¬à§à¦¯à¦¾à¦‚à¦•': ['Bank (à¦†à¦°à§à¦¥à¦¿à¦• à¦ªà§à¦°à¦¤à¦¿à¦·à§à¦ à¦¾à¦¨)', 'River bank/embankment (à¦¨à¦¦à§€à¦° à¦¤à§€à¦°)'],\n        'à¦«à¦²': ['Fruit (à¦–à¦¾à¦¬à¦¾à¦° à¦«à¦²)', 'Result/consequence (à¦«à¦²à¦¾à¦«à¦²)'],\n        'à¦¬à¦¾à¦°': ['Time/occasion (à¦¬à¦¾à¦°)', 'Bar (à¦®à¦¦à§‡à¦° à¦¦à§‹à¦•à¦¾à¦¨)', 'Door bar (à¦¦à¦°à¦œà¦¾à¦° à¦¶à¦¿à¦•)'],\n        'à¦¹à¦¾à¦°': ['Necklace (à¦—à¦²à¦¾à¦° à¦¹à¦¾à¦°)', 'Defeat/loss (à¦ªà¦°à¦¾à¦œà¦¯à¦¼)', 'Rate (à¦¹à¦¾à¦°)'],\n        'à¦®à¦¾à¦¥à¦¾': ['Head (à¦®à¦¾à¦¥à¦¾)', 'Top/peak (à¦¶à§€à¦°à§à¦·)', 'Beginning (à¦¶à§à¦°à§)'],\n        'à¦¤à¦¾à¦°à¦¾': ['Stars (à¦¨à¦•à§à¦·à¦¤à§à¦°)', 'They (à¦¤à¦¾à¦°à¦¾)', 'Wire (à¦¤à¦¾à¦°)'],\n        'à¦ªà¦¾à¦¨à¦¿': ['Water (à¦œà¦²)', 'Respect (à¦¸à¦®à§à¦®à¦¾à¦¨ - archaic)'],\n        'à¦¦à§‡à¦–à¦¾': ['To see (à¦¦à§‡à¦–à¦¾)', 'Meeting (à¦¸à¦¾à¦•à§à¦·à¦¾à§)', 'Sight/appearance (à¦¦à¦°à§à¦¶à¦¨)'],\n        'à¦¬à¦‡': ['Book (à¦¬à¦‡)', 'Many/plenty (à¦…à¦¨à§‡à¦• - archaic)'],\n        'à¦¹à¦¾à¦¤': ['Hand (à¦¹à¦¾à¦¤)', 'Arm (à¦¬à¦¾à¦¹à§)', 'Possession (à¦¦à¦–à¦²)'],\n    }\n    return meanings.get(word, [f'{word} (meanings not documented)'])\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"REFERENCE HOMOGRAPH DEMONSTRATION SYSTEM\")\nprint(\"=\" * 80)\n\ntrained_model = globals().get('trained_model')\ntokenizer = globals().get('tokenizer')\n\nif trained_model is None or tokenizer is None:\n    print(\"âŒ Model or tokenizer not found\")\n    print(\"   Run Cells 0-11 first\")\nelse:\n    print(\"âœ“ Model and tokenizer ready\")\n    \n    reference_homographs = extract_reference_homographs(trained_model, tokenizer)\n    \n    found_homographs = {k: v for k, v in reference_homographs.items() if v.get('num_senses', 0) >= 2}\n    not_found = {k: v for k, v in reference_homographs.items() if v.get('status') == 'NOT_FOUND'}\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"DETAILED HOMOGRAPH ANALYSIS\")\n    print(\"=\" * 80)\n    \n    if found_homographs:\n        print(f\"\\nâœ… FOUND HOMOGRAPHS ({len(found_homographs)}):\\n\")\n        \n        sorted_homographs = sorted(\n            found_homographs.items(),\n            key=lambda x: x[1]['num_senses'],\n            reverse=True\n        )\n        \n        for idx, (word, data) in enumerate(sorted_homographs, 1):\n            print(f\"{idx}. '{word}' - {data['num_senses']} senses ({data['total_occurrences']} occurrences)\")\n            \n            meanings = get_bengali_meanings(word)\n            print(f\"   ğŸ“– Known meanings:\")\n            for meaning in meanings:\n                print(f\"      â€¢ {meaning}\")\n            \n            print(f\"   ğŸ“Š DSCD sense distribution:\")\n            sense_counts = data.get('sense_counts', [])\n            total = sum(sense_counts) if sense_counts else 1\n            for i, count in enumerate(sense_counts, 1):\n                pct = (count / total * 100) if total > 0 else 0\n                bar = 'â–ˆ' * min(int(pct / 5), 20)\n                print(f\"      Sense {i}: {bar} {count}x ({pct:.1f}%)\")\n            \n            print()\n    \n    if not_found:\n        print(f\"\\nâš  NOT FOUND IN DSCD ({len(not_found)}):\")\n        not_found_words = ', '.join(f\"'{w}'\" for w in sorted(not_found.keys())[:15])\n        print(f\"   {not_found_words}\")\n        if len(not_found) > 15:\n            print(f\"   ... and {len(not_found) - 15} more\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRANSLATION DEMONSTRATION\")\n    print(\"=\" * 80)\n    \n    demo_sentences = [\n        (\"à¦†à¦®à¦¿ à¦•à¦² à¦¬à¦¨à§à¦§ à¦•à¦°à§‡à¦›à¦¿à¥¤\", \"I turned off the tap\", \"à¦•à¦²\"),\n        (\"à¦•à¦¾à¦² à¦†à¦®à¦¿ à¦¬à¦‡ à¦•à¦¿à¦¨à¦¬à¥¤\", \"Tomorrow I will buy a book\", \"à¦•à¦¾à¦²\"),\n        (\"à¦ªà¦¾à¦¤à¦¾ à¦à¦°à§‡ à¦ªà¦¡à¦¼à§‡à¦›à§‡à¥¤\", \"The leaf has fallen\", \"à¦ªà¦¾à¦¤à¦¾\"),\n        (\"à¦¤à¦¿à¦¨à¦¿ à¦¬à§à¦¯à¦¾à¦‚à¦• à¦—à§‡à¦›à§‡à¦¨à¥¤\", \"He went to the bank\", \"à¦¬à§à¦¯à¦¾à¦‚à¦•\"),\n        (\"à¦«à¦² à¦–à§à¦¬ à¦®à¦¿à¦·à§à¦Ÿà¦¿à¥¤\", \"The fruit is very sweet\", \"à¦«à¦²\"),\n        (\"à¦¤à¦¾à¦° à¦¹à¦¾à¦° à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°à¥¤\", \"Her necklace is beautiful\", \"à¦¹à¦¾à¦°\"),\n        (\"à¦¬à¦¾à¦° à¦¬à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‹à¥¤\", \"Try again and again\", \"à¦¬à¦¾à¦°\"),\n    ]\n    \n    translate_fn = globals().get('translate_with_explanations')\n    \n    if translate_fn:\n        print(\"\\nTesting sentences with known homographs...\\n\")\n        \n        for idx, (bengali, expected, target_word) in enumerate(demo_sentences, 1):\n            print(\"=\" * 80)\n            print(f\"SENTENCE {idx}/{len(demo_sentences)}\")\n            print(\"=\" * 80)\n            print(f\"ğŸ“ Bengali: {bengali}\")\n            print(f\"ğŸ¯ Expected: {expected}\")\n            print(f\"ğŸ” Target homograph: '{target_word}'\")\n            \n            if target_word in found_homographs:\n                data = found_homographs[target_word]\n                print(f\"\\nâœ… '{target_word}' FOUND in DSCD:\")\n                print(f\"   â€¢ {data['num_senses']} senses discovered\")\n                print(f\"   â€¢ {data['total_occurrences']} total occurrences\")\n                \n                meanings = get_bengali_meanings(target_word)\n                print(f\"\\n   ğŸ’¡ Possible meanings:\")\n                for meaning in meanings:\n                    print(f\"      â€¢ {meaning}\")\n            else:\n                print(f\"\\nâš  '{target_word}' NOT found in DSCD prototypes\")\n            \n            try:\n                result = translate_fn(\n                    trained_model,\n                    tokenizer,\n                    bengali,\n                    device=_DEVICE,\n                    span_threshold=0.05,\n                    uncertainty_threshold=0.15\n                )\n                \n                translation = result.get('translation', '')\n                print(f\"\\nğŸ¤– Translation: {translation}\")\n                \n                explanations = result.get('explanations', [])\n                if explanations:\n                    print(f\"âœ¨ {len(explanations)} ambiguous word(s) detected\")\n                else:\n                    print(\"âš  No ambiguity detected (thresholds not met)\")\n                \n            except Exception as e:\n                print(f\"âŒ Translation failed: {e}\")\n            \n            print()\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"PRESENTATION SUMMARY\")\n    print(\"=\" * 80)\n    \n    total_ref = len(_HOMOGRAPH_REFERENCE_LIST_BN)\n    found_count = len(found_homographs)\n    coverage = (found_count / total_ref * 100) if total_ref > 0 else 0\n    \n    print(f\"\\nğŸ“Š Key Statistics:\")\n    print(f\"   â€¢ Reference list: {total_ref} Bengali homographs\")\n    print(f\"   â€¢ DSCD discovered: {found_count} ({coverage:.1f}% coverage)\")\n    print(f\"   â€¢ Not found: {len(not_found)}\")\n    \n    if found_homographs:\n        total_senses = sum(h['num_senses'] for h in found_homographs.values())\n        avg_senses = total_senses / len(found_homographs)\n        print(f\"   â€¢ Average senses: {avg_senses:.1f} per homograph\")\n    \n    print(f\"\\nâœ… DEMONSTRATION COMPLETE\")\n    print(f\"\\nğŸ“ Saved: {_PROTOTYPE_SAVE_PATH}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"FOR YOUR PRESENTATION:\")\n    print(\"=\" * 80)\n    print(\"1. DSCD found {}/{} reference homographs ({:.0f}% coverage)\".format(\n        found_count, total_ref, coverage\n    ))\n    print(\"2. Each homograph has 2-6 distinct senses\")\n    print(\"3. Prototypes extracted from 50K training sentences\")\n    print(\"4. Examples: à¦•à¦² (tap/call), à¦•à¦¾à¦² (tomorrow/yesterday), à¦ªà¦¾à¦¤à¦¾ (leaf/page)\")\n    print(\"5. System can detect ambiguity in new sentences\")\n    print(\"=\" * 80)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 12: Reference Homograph Demonstration - READY\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-16T16:56:07.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}