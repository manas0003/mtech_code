{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14014111,"sourceType":"datasetVersion","datasetId":8927824}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers sentence-transformers\n!pip install transformers==4.30.2 --no-deps\n!pip install \"tokenizers<0.14\" sacremoses\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"id":"W8IIWAEHH4Jy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: TATN CONFIGURATION (BENGALI → ENGLISH) - FIXED\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept Exception:\n    pd = None\n    _HAS_PANDAS = False\n\ntry:\n    import transformers\n    _HAS_TRANSFORMERS = True\nexcept Exception:\n    transformers = None\n    _HAS_TRANSFORMERS = False\n\n_HAS_M2M_TOKENIZER = False\nif _HAS_TRANSFORMERS:\n    try:\n        from transformers import M2M100Tokenizer\n        _HAS_M2M_TOKENIZER = True\n    except Exception:\n        _HAS_M2M_TOKENIZER = False\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\nwarnings.filterwarnings(\"ignore\")\n\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n\ndef _get_int_env(name: str, default: int) -> int:\n    try:\n        v = globals().get(name, None)\n        if v is None:\n            v_env = os.environ.get(name, None)\n            if v_env is not None:\n                return int(v_env)\n            return int(default)\n        return int(v)\n    except Exception:\n        return int(default)\n\n\ndef _get_float_env(name: str, default: float) -> float:\n    try:\n        v = globals().get(name, None)\n        if v is None:\n            v_env = os.environ.get(name, None)\n            if v_env is not None:\n                return float(v_env)\n            return float(default)\n        return float(v)\n    except Exception:\n        return float(default)\n\n\ndef _get_bool_env(name: str, default: bool) -> bool:\n    try:\n        v = globals().get(name, None)\n        if v is None:\n            s = os.environ.get(name, None)\n            if s is None:\n                return bool(default)\n            if str(s).lower() in (\"1\", \"true\", \"yes\", \"y\"):\n                return True\n            return False\n        return bool(v)\n    except Exception:\n        return bool(default)\n\n\nNUM_GPUS = max(0, _get_int_env(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\nUSE_MULTI_GPU = _get_bool_env(\"USE_MULTI_GPU\", NUM_GPUS > 1)\n\nif torch.cuda.is_available():\n    if USE_MULTI_GPU and NUM_GPUS > 1:\n        DEVICE = torch.device(\"cuda\")\n    else:\n        DEVICE = torch.device(\"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    globals().get(\"DATASET_CSV_PATH\", \"/kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\"),\n)\n\nBATCH_SIZE = max(1, _get_int_env(\"BATCH_SIZE\", globals().get(\"BATCH_SIZE\", 32)))\nNUM_SAMPLES = max(1, _get_int_env(\"NUM_SAMPLES\", globals().get(\"NUM_SAMPLES\", 30000)))\nMAX_LENGTH = max(1, _get_int_env(\"MAX_LENGTH\", globals().get(\"MAX_LENGTH\", 48)))\n\nLR_NMT = float(_get_float_env(\"LR_NMT\", globals().get(\"LR_NMT\", 2e-5)))\nLR_TRG = float(_get_float_env(\"LR_TRG\", globals().get(\"LR_TRG\", 1e-5)))\nLR_PHI = float(_get_float_env(\"LR_PHI\", globals().get(\"LR_PHI\", 1e-5)))\n\nEPOCHS = max(1, _get_int_env(\"EPOCHS\", globals().get(\"EPOCHS\", 5)))\nGRAD_CLIP_NORM = float(_get_float_env(\"GRAD_CLIP_NORM\", globals().get(\"GRAD_CLIP_NORM\", 1.0)))\nUSE_AMP = _get_bool_env(\"USE_AMP\", globals().get(\"USE_AMP\", True))\nPRINT_INTERVAL = max(1, _get_int_env(\"PRINT_INTERVAL\", globals().get(\"PRINT_INTERVAL\", 100)))\nSEED = int(_get_int_env(\"SEED\", globals().get(\"SEED\", 42)))\n\nACCUMULATION_STEPS = max(1, _get_int_env(\"ACCUMULATION_STEPS\", globals().get(\"ACCUMULATION_STEPS\", 1)))\n\nNUM_WORKERS = max(0, _get_int_env(\"NUM_WORKERS\", globals().get(\"NUM_WORKERS\", 0)))\nPIN_MEMORY = _get_bool_env(\"PIN_MEMORY\", globals().get(\"PIN_MEMORY\", False))\nPREFETCH_FACTOR = max(1, _get_int_env(\"PREFETCH_FACTOR\", globals().get(\"PREFETCH_FACTOR\", 2)))\nGRADIENT_CHECKPOINTING = _get_bool_env(\"GRADIENT_CHECKPOINTING\", globals().get(\"GRADIENT_CHECKPOINTING\", False))\n\nDEBUG_DISCOVERY = _get_bool_env(\"DEBUG_DISCOVERY\", globals().get(\"DEBUG_DISCOVERY\", True))\nDEBUG_TIMING = _get_bool_env(\"DEBUG_TIMING\", globals().get(\"DEBUG_TIMING\", True))\nDEBUG_VERBOSE = _get_bool_env(\"DEBUG_VERBOSE\", globals().get(\"DEBUG_VERBOSE\", True))\nVERBOSE_LOGGING = _get_bool_env(\"VERBOSE_LOGGING\", globals().get(\"VERBOSE_LOGGING\", True))\n\nDSCD_BUFFER_SIZE = max(1, _get_int_env(\"DSCD_BUFFER_SIZE\", globals().get(\"DSCD_BUFFER_SIZE\", 80)))\nDSCD_MAX_PROTOS = max(1, _get_int_env(\"DSCD_MAX_PROTOS\", globals().get(\"DSCD_MAX_PROTOS\", 8)))\nDSCD_N_MIN = max(1, _get_int_env(\"DSCD_N_MIN\", globals().get(\"DSCD_N_MIN\", 2)))\nDSCD_DISPERSION_THRESHOLD = _get_float_env(\"DSCD_DISPERSION_THRESHOLD\", globals().get(\"DSCD_DISPERSION_THRESHOLD\", 0.70))\nDSCD_EMBED_DIM = max(1, _get_int_env(\"DSCD_EMBED_DIM\", globals().get(\"DSCD_EMBED_DIM\", 1024)))\nDSCD_TEMPERATURE = float(_get_float_env(\"DSCD_TEMPERATURE\", globals().get(\"DSCD_TEMPERATURE\", 0.7)))\nDSCD_DROPOUT = float(_get_float_env(\"DSCD_DROPOUT\", globals().get(\"DSCD_DROPOUT\", 0.1)))\nDSCD_AUGMENT_SCALE = float(_get_float_env(\"DSCD_AUGMENT_SCALE\", globals().get(\"DSCD_AUGMENT_SCALE\", 0.1)))\nDSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_env(\"DSCD_ENABLE_TRAINING_CLUSTERING\", globals().get(\"DSCD_ENABLE_TRAINING_CLUSTERING\", True))\nDSCD_ENABLE_ONLINE_CLUSTERING = _get_bool_env(\"DSCD_ENABLE_ONLINE_CLUSTERING\", globals().get(\"DSCD_ENABLE_ONLINE_CLUSTERING\", True))\nDSCD_ONLINE_CLUSTERING_FREQUENCY = max(1, _get_int_env(\"DSCD_ONLINE_CLUSTERING_FREQUENCY\", globals().get(\"DSCD_ONLINE_CLUSTERING_FREQUENCY\", 10)))\nDSCD_WARMUP_SAMPLES = max(0, _get_int_env(\"DSCD_WARMUP_SAMPLES\", globals().get(\"DSCD_WARMUP_SAMPLES\", 0)))\nDSCD_NEWSENSE_LAMBDA = float(_get_float_env(\"DSCD_NEWSENSE_LAMBDA\", globals().get(\"DSCD_NEWSENSE_LAMBDA\", 1.5)))\nDSCD_USE_COSINE_DISTANCE = _get_bool_env(\"DSCD_USE_COSINE_DISTANCE\", globals().get(\"DSCD_USE_COSINE_DISTANCE\", True))\n\nPERIODIC_DISCOVERY_FREQUENCY = max(1, _get_int_env(\"PERIODIC_DISCOVERY_FREQUENCY\", globals().get(\"PERIODIC_DISCOVERY_FREQUENCY\", 150)))\n_MAX_TOKENS_PER_DISCOVERY = max(1, _get_int_env(\"_MAX_TOKENS_PER_DISCOVERY\", globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150)))\nDSCD_MIN_LETTERS = max(1, _get_int_env(\"DSCD_MIN_LETTERS\", globals().get(\"DSCD_MIN_LETTERS\", 2)))\nDSCD_MIN_LETTER_FRACTION = float(_get_float_env(\"DSCD_MIN_LETTER_FRACTION\", globals().get(\"DSCD_MIN_LETTER_FRACTION\", 0.5)))\nDSCD_MAX_CLUSTERING_POINTS = max(1, _get_int_env(\"DSCD_MAX_CLUSTERING_POINTS\", globals().get(\"DSCD_MAX_CLUSTERING_POINTS\", 500)))\n\nENABLE_ASBN_TRAINING = _get_bool_env(\"ENABLE_ASBN_TRAINING\", globals().get(\"ENABLE_ASBN_TRAINING\", True))\nENABLE_ASBN_INFERENCE = _get_bool_env(\"ENABLE_ASBN_INFERENCE\", globals().get(\"ENABLE_ASBN_INFERENCE\", False))\nENABLE_TRG_TRAINING = _get_bool_env(\"ENABLE_TRG_TRAINING\", globals().get(\"ENABLE_TRG_TRAINING\", True))\nENABLE_TRG_INFERENCE = _get_bool_env(\"ENABLE_TRG_INFERENCE\", globals().get(\"ENABLE_TRG_INFERENCE\", True))\n\nMC_DROPOUT_PASSES = max(1, _get_int_env(\"MC_DROPOUT_PASSES\", globals().get(\"MC_DROPOUT_PASSES\", 5)))\nTRG_EVIDENCE_K = max(1, _get_int_env(\"TRG_EVIDENCE_K\", globals().get(\"TRG_EVIDENCE_K\", 3)))\nMAX_SILVER_BUFFER = max(1, _get_int_env(\"MAX_SILVER_BUFFER\", globals().get(\"MAX_SILVER_BUFFER\", 100)))\n\nCLUSTERING_TIMEOUT = max(1, _get_int_env(\"CLUSTERING_TIMEOUT\", globals().get(\"CLUSTERING_TIMEOUT\", 60)))\nMEMORY_CLEANUP_FREQUENCY = max(0, _get_int_env(\"MEMORY_CLEANUP_FREQUENCY\", globals().get(\"MEMORY_CLEANUP_FREQUENCY\", 200)))\nVALIDATION_CHECK_INTERVAL = max(1, _get_int_env(\"VALIDATION_CHECK_INTERVAL\", globals().get(\"VALIDATION_CHECK_INTERVAL\", 500)))\n\nCHECKPOINT_DIR = str(globals().get(\"CHECKPOINT_DIR\", os.environ.get(\"CHECKPOINT_DIR\", \"/kaggle/working/\")))\nCHECKPOINT_SAVE_AFTER_TRAINING = _get_bool_env(\"CHECKPOINT_SAVE_AFTER_TRAINING\", globals().get(\"CHECKPOINT_SAVE_AFTER_TRAINING\", True))\nCHECKPOINT_FILENAME = str(globals().get(\"CHECKPOINT_FILENAME\", os.environ.get(\"CHECKPOINT_FILENAME\", \"tatn_final.pt\")))\nCHECKPOINT_INTERVAL = int(_get_int_env(\"CHECKPOINT_INTERVAL\", globals().get(\"CHECKPOINT_INTERVAL\", 99999999)))\nSAVE_REPLAY_BUFFER = _get_bool_env(\"SAVE_REPLAY_BUFFER\", globals().get(\"SAVE_REPLAY_BUFFER\", False))\nLOAD_REPLAY_BUFFER = _get_bool_env(\"LOAD_REPLAY_BUFFER\", globals().get(\"LOAD_REPLAY_BUFFER\", False))\nREPLAY_BUFFER_SIZE = max(0, _get_int_env(\"REPLAY_BUFFER_SIZE\", globals().get(\"REPLAY_BUFFER_SIZE\", 25000)))\nRESUME_FROM_CHECKPOINT = _get_bool_env(\"RESUME_FROM_CHECKPOINT\", globals().get(\"RESUME_FROM_CHECKPOINT\", False))\nSAVE_DSCD_STATE = _get_bool_env(\"SAVE_DSCD_STATE\", globals().get(\"SAVE_DSCD_STATE\", True))\n\nif not os.path.exists(CHECKPOINT_DIR):\n    try:\n        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    except Exception:\n        CHECKPOINT_DIR = \"./\"\n\nTAU_LOW = float(_get_float_env(\"TAU_LOW\", globals().get(\"TAU_LOW\", 0.15)))\nTAU_HIGH = float(_get_float_env(\"TAU_HIGH\", globals().get(\"TAU_HIGH\", 0.85)))\nTAU_ACCEPT = float(_get_float_env(\"TAU_ACCEPT\", globals().get(\"TAU_ACCEPT\", 0.8)))\n\nTRG_MAX_GEN_LEN = max(1, _get_int_env(\"TRG_MAX_GEN_LEN\", globals().get(\"TRG_MAX_GEN_LEN\", 16)))\nTRG_GEN_EMBED = max(1, _get_int_env(\"TRG_GEN_EMBED\", globals().get(\"TRG_GEN_EMBED\", 64)))\nTRG_GEN_HID = max(1, _get_int_env(\"TRG_GEN_HID\", globals().get(\"TRG_GEN_HID\", 64)))\n\nTRG_SPAN_THRESHOLD = float(_get_float_env(\"TRG_SPAN_THRESHOLD\", globals().get(\"TRG_SPAN_THRESHOLD\", 0.15)))\nTRG_UNCERTAINTY_THRESHOLD = float(_get_float_env(\"TRG_UNCERTAINTY_THRESHOLD\", globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", 0.70)))\nTRG_TEMPERATURE = float(_get_float_env(\"TRG_TEMPERATURE\", globals().get(\"TRG_TEMPERATURE\", 1.0)))\n\nASBN_HIDDEN_DIM = max(1, _get_int_env(\"ASBN_HIDDEN_DIM\", globals().get(\"ASBN_HIDDEN_DIM\", 64)))\nASBN_LAMBDA = float(_get_float_env(\"ASBN_LAMBDA\", globals().get(\"ASBN_LAMBDA\", 0.1)))\nASBN_DROPOUT = float(_get_float_env(\"ASBN_DROPOUT\", globals().get(\"ASBN_DROPOUT\", 0.1)))\n\nLAMBDA_ASBN = float(_get_float_env(\"LAMBDA_ASBN\", globals().get(\"LAMBDA_ASBN\", 0.05)))\nLAMBDA_DSCD = float(_get_float_env(\"LAMBDA_DSCD\", globals().get(\"LAMBDA_DSCD\", 0.15)))\n\nTRAIN_DOMAIN = int(_get_int_env(\"TRAIN_DOMAIN\", globals().get(\"TRAIN_DOMAIN\", 0)))\nTEST_DOMAIN = int(_get_int_env(\"TEST_DOMAIN\", globals().get(\"TEST_DOMAIN\", 1)))\nUSE_DOMAIN_LABELS = _get_bool_env(\"USE_DOMAIN_LABELS\", globals().get(\"USE_DOMAIN_LABELS\", True))\n\nGRL_ALPHA_START = float(_get_float_env(\"GRL_ALPHA_START\", globals().get(\"GRL_ALPHA_START\", 0.0)))\nGRL_ALPHA_END = float(_get_float_env(\"GRL_ALPHA_END\", globals().get(\"GRL_ALPHA_END\", 1.0)))\nGRL_ALPHA_SCHEDULE = str(globals().get(\"GRL_ALPHA_SCHEDULE\", \"linear\"))\n\n_total_steps_estimate = max(1, NUM_SAMPLES // max(1, (BATCH_SIZE * ACCUMULATION_STEPS)))\nGRL_ALPHA_STEPS = max(1, _total_steps_estimate * EPOCHS)\n\nSOURCE_LANGUAGE = str(globals().get(\"SOURCE_LANGUAGE\", os.environ.get(\"SOURCE_LANGUAGE\", \"bn\")))\nTARGET_LANGUAGE = str(globals().get(\"TARGET_LANGUAGE\", os.environ.get(\"TARGET_LANGUAGE\", \"en\")))\n\nM2M100_BN_TOKEN_ID = int(globals().get(\"M2M100_BN_TOKEN_ID\", os.environ.get(\"M2M100_BN_TOKEN_ID\", 128025)))\nM2M100_EN_TOKEN_ID = int(globals().get(\"M2M100_EN_TOKEN_ID\", os.environ.get(\"M2M100_EN_TOKEN_ID\", 128022)))\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = set(globals().get(\"HOMOGRAPH_REFERENCE_LIST_BN\", [\n    \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"\n]))\nHOMOGRAPH_WATCHLIST_BN: Set[str] = set(globals().get(\"HOMOGRAPH_WATCHLIST_BN\", list(HOMOGRAPH_REFERENCE_LIST_BN)))\nHOMOGRAPH_WATCHLIST: Set[str] = set(HOMOGRAPH_WATCHLIST_BN)\nUSE_WATCHLIST_PRIORITIZATION = _get_bool_env(\"USE_WATCHLIST_PRIORITIZATION\", globals().get(\"USE_WATCHLIST_PRIORITIZATION\", False))\nWATCHLIST_ONLY_FOR_TRG = _get_bool_env(\"WATCHLIST_ONLY_FOR_TRG\", globals().get(\"WATCHLIST_ONLY_FOR_TRG\", False))\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", str(t))\n    t = t.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", str(t)).lower().strip()\n    return t\n\ndef normalize_token_key(token: str) -> str:\n    if not token:\n        return \"\"\n    token = str(token)\n    token = token.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n    for punct in \".,!?;:\\\"'()-\":\n        token = token.replace(punct, \"\")\n    return token.strip()\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\")\n            except Exception:\n                print(f\"  GPU {i}: memory stats unavailable\")\n    else:\n        print(\"[GPU MONITOR] No CUDA devices available\")\n\ndef get_checkpoint_path() -> str:\n    if not os.path.exists(CHECKPOINT_DIR):\n        try:\n            os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n        except Exception:\n            pass\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    if CHECKPOINT_INTERVAL < 99999999 and global_step >= CHECKPOINT_INTERVAL and global_step % CHECKPOINT_INTERVAL == 0:\n        return True\n    return False\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({SOURCE_LANGUAGE, TARGET_LANGUAGE})\n    return s\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    return get_tokenizer_special_tokens(tokenizer)\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(token, special_tokens: Optional[Set[str]] = None, tokenizer=None, language: str = \"bn\") -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        min_len = 2\n        if len(clean) < min_len:\n            result = False\n        else:\n            has_bengali_chars = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if '\\u0980' <= c <= '\\u09FF')\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    bengali_ratio = bengali_count / alphanum_count\n                    result = bengali_ratio >= 0.5\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\ndef fallback_is_valid_token(token, special_tokens: Optional[Set[str]] = None, language: str = \"bn\") -> bool:\n    return is_valid_token(token, special_tokens, None, language)\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    try:\n        encoded = tokenizer(text, return_offsets_mapping=True, max_length=max_length, truncation=True, add_special_tokens=False)\n        toks = tokenizer.convert_ids_to_tokens(encoded.get(\"input_ids\", []))\n        offsets = encoded.get(\"offset_mapping\", [(0, 0)] * len(toks))\n        return toks, offsets\n    except Exception:\n        return None, None\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        count = len(self.discovery_times)\n        return {\"count\": count, \"total\": total, \"avg\": total / count, \"max\": max(self.discovery_times)}\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    try:\n        torch.cuda.manual_seed_all(SEED)\n    except Exception:\n        pass\n\ntry:\n    if hasattr(torch, \"set_float32_matmul_precision\"):\n        torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nif USE_MULTI_GPU and NUM_GPUS > 1:\n    effective_batch *= NUM_GPUS\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TATN CONFIGURATION (Bengali to English)\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU and NUM_GPUS>0 else 'DISABLED'} ({NUM_GPUS} GPUs detected)\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"Dataset: {DATASET_CSV_PATH}\")\nprint(f\"Samples: {NUM_SAMPLES:,} | Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS} | AMP: {USE_AMP}\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer: {DSCD_BUFFER_SIZE} | n_min: {DSCD_N_MIN} | Max protos: {DSCD_MAX_PROTOS}\")\nprint(f\"  Dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  Use cosine distance: {DSCD_USE_COSINE_DISTANCE}\")\nprint(f\"  Online clustering: {DSCD_ENABLE_ONLINE_CLUSTERING} (freq={DSCD_ONLINE_CLUSTERING_FREQUENCY})\")\nprint(f\"  New-sense lambda: {DSCD_NEWSENSE_LAMBDA}\")\nprint(f\"  Warmup samples: {DSCD_WARMUP_SAMPLES}\")\nprint(f\"  Periodic discovery: Every {PERIODIC_DISCOVERY_FREQUENCY} optimizer updates\")\nprint(f\"  Max tokens per discovery: {_MAX_TOKENS_PER_DISCOVERY}\")\nprint(f\"  Clustering timeout: {CLUSTERING_TIMEOUT}s\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  MC Dropout passes: {MC_DROPOUT_PASSES} | TAU_LOW: {TAU_LOW}\")\nprint(f\"  TRG_SPAN_THRESHOLD: {TRG_SPAN_THRESHOLD} | TRG_UNCERTAINTY_THRESHOLD: {TRG_UNCERTAINTY_THRESHOLD}\")\nprint()\nprint(\"ASBN / Loss:\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN} | LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint(f\"  Domain labels: {USE_DOMAIN_LABELS} | GRL: {GRL_ALPHA_SCHEDULE}\")\nprint(f\"  GRL steps: {GRL_ALPHA_STEPS}\")\nprint(f\"  ASBN training: {ENABLE_ASBN_TRAINING}\")\nprint(f\"  ASBN inference: {ENABLE_ASBN_INFERENCE}\")\nprint()\nprint(\"Augmentation:\")\nprint(f\"  Apply DSCD augmentation: {globals().get('APPLY_DSCD_AUGMENTATION', False)}\")\nprint()\nprint(\"Debug Flags:\")\nprint(f\"  Discovery logging: {DEBUG_DISCOVERY}\")\nprint(f\"  Timing monitoring: {DEBUG_TIMING}\")\nprint(f\"  Verbose mode: {DEBUG_VERBOSE}\")\nprint(f\"  Verbose logging: {VERBOSE_LOGGING}\")\nprint()\nprint(\"Validation:\")\nprint(f\"  Check interval: {VALIDATION_CHECK_INTERVAL} steps\")\nprint()\nprint(\"Language Tokens (defaults):\")\nprint(f\"  Bengali (bn): {M2M100_BN_TOKEN_ID}\")\nprint(f\"  English (en): {M2M100_EN_TOKEN_ID}\")\nprint()\nprint(\"Checkpoint:\")\nprint(f\"  Path: {get_checkpoint_path()}\")\nprint(f\"  Save strategy: {'Final only' if CHECKPOINT_SAVE_AFTER_TRAINING else 'Interval'}\")\nprint(f\"  Save DSCD state: {SAVE_DSCD_STATE}\")\nprint(\"=\" * 80)\n\nif not _HAS_PANDAS:\n    print(\"[WARN] pandas not available - CSV loading will fall back to builtin dataset\")\nif not _HAS_M2M_TOKENIZER:\n    print(\"[WARN] M2M100 tokenizer class not detected (will be resolved at runtime if transformers available)\")\n\ntry:\n    test_file = os.path.join(CHECKPOINT_DIR, \".test_write\")\n    with open(test_file, \"w\") as f:\n        f.write(\"test\")\n    os.remove(test_file)\n    print(f\"[INFO] Checkpoint directory writable: {CHECKPOINT_DIR}\")\nexcept Exception as e:\n    print(f\"[WARN] Checkpoint directory not writable: {e}\")\n\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: Configuration loaded\")\nprint(\"=\" * 80)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: TOKENIZER UTILITIES (BENGALI-FOCUSED) - FIXED\n# ===========================================================================================\n\nimport threading\nimport unicodedata\nfrom typing import Tuple, List, Dict, Optional, Set, Any\nimport numpy as np\nimport torch\n\ntry:\n    SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH) if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0 else 48\nexcept Exception:\n    SAFE_OFFSET_MAX_LEN = 48\nif SAFE_OFFSET_MAX_LEN <= 0:\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept Exception:\n    _TARGET_LANG = \"en\"\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept Exception:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n\n_SPIECE_UNDERLINE = \"\\u2581\"\n\ndef _special_token_cache_key(tokenizer) -> str:\n    if tokenizer is None:\n        return \"none_tokenizer__vocab=None\"\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None) or \"unknown_tokenizer\"\n    vocab = None\n    try:\n        if hasattr(tokenizer, \"vocab_size\"):\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n            vocab = len(tokenizer.get_vocab())\n    except Exception:\n        vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    if tokenizer is None:\n        return {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\", \"__bn__\", \"__en__\"}\n\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n    special_tokens: Set[str] = set()\n    try:\n        if hasattr(tokenizer, \"all_special_tokens\"):\n            try:\n                result = getattr(tokenizer, \"all_special_tokens\")\n                if isinstance(result, (list, tuple, set)):\n                    special_tokens.update(str(x) for x in result if x)\n            except Exception:\n                pass\n        if hasattr(tokenizer, \"additional_special_tokens\"):\n            try:\n                result = getattr(tokenizer, \"additional_special_tokens\")\n                if isinstance(result, (list, tuple, set)):\n                    special_tokens.update(str(x) for x in result if x)\n            except Exception:\n                pass\n        for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\", \"cls_token\", \"sep_token\", \"mask_token\"):\n            try:\n                tok = getattr(tokenizer, attr, None)\n                if tok:\n                    special_tokens.add(str(tok))\n            except Exception:\n                pass\n        try:\n            stm = getattr(tokenizer, \"special_tokens_map\", None) or getattr(tokenizer, \"special_tokens_map_extended\", None)\n            if isinstance(stm, dict):\n                for v in stm.values():\n                    if isinstance(v, str) and v:\n                        special_tokens.add(v)\n                    elif isinstance(v, (list, tuple, set)):\n                        special_tokens.update(str(x) for x in v if x)\n        except Exception:\n            pass\n    except Exception:\n        special_tokens = set()\n\n    special_tokens.update({\n        \"__bn__\", \"__en__\", \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n        \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n    })\n\n    try:\n        vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n        preserved = {\"</s>\", \"<pad>\", \"<s>\", \"<unk__\", \"__bn__\", \"__en__\"}\n        try:\n            preserved.add(_SOURCE_LANG)\n            preserved.add(_TARGET_LANG)\n        except Exception:\n            preserved.update({\"bn\", \"en\"})\n        if isinstance(vocab, dict):\n            special_tokens = {tok for tok in special_tokens if tok in vocab or tok in preserved}\n        else:\n            special_tokens.update(preserved)\n    except Exception:\n        pass\n\n    with _SPECIAL_TOKENS_LOCK:\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n\n    return special_tokens\n\ndef _normalize_offset_mapping_for_batchencoding(enc: dict) -> dict:\n    if not isinstance(enc, dict):\n        return enc\n\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0:\n                        if isinstance(arr[0], list):\n                            enc[\"offset_mapping\"] = [(x[0], x[1]) if isinstance(x, (list, tuple)) and len(x) >= 2 else (None, None) for x in arr[0]]\n                            return enc\n                        else:\n                            # single list of offsets (no change)\n                            enc[\"offset_mapping\"] = [(x[0], x[1]) if isinstance(x, (list, tuple)) and len(x) >= 2 else (None, None) for x in arr]\n                            return enc\n                if isinstance(off, (list, tuple)) and len(off) > 0:\n                    if isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [(x[0], x[1]) if isinstance(x, (list, tuple)) and len(x) >= 2 else (None, None) for x in off[0]]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    try:\n        data = getattr(enc, \"data\", None)\n        if data and isinstance(data, dict) and \"offset_mapping\" in data and data[\"offset_mapping\"] is not None:\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [(x[0], x[1]) if isinstance(x, (list, tuple)) and len(x) >= 2 else (None, None) for x in om[0]]\n                return enc\n    except Exception:\n        pass\n\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif isinstance(input_ids, (list, tuple)) and len(input_ids) > 0 and isinstance(input_ids[0], (list, tuple)):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\ndef safe_offsets_tokenize(tokenizer, text: str, max_length: Optional[int] = None, include_special_tokens: bool = False) -> dict:\n    if tokenizer is None:\n        return {\"input_ids\": torch.tensor([[0]], dtype=torch.long), \"attention_mask\": torch.tensor([[1]], dtype=torch.long), \"offset_mapping\": []}\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = max(1, int(max_length))\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        text = \"\"\n\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit] if len(text) > char_limit else text\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    if is_fast:\n        try:\n            enc = tokenizer(sample_text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, padding=False, max_length=eff_max, add_special_tokens=include_special_tokens)\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            pass\n\n    try:\n        enc = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=False, max_length=eff_max, add_special_tokens=include_special_tokens)\n    except Exception:\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\"input_ids\": torch.tensor([[pad_id]], dtype=torch.long), \"attention_mask\": torch.tensor([[1]], dtype=torch.long)}\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                try:\n                    input_ids = enc.data[\"input_ids\"][0]\n                except Exception:\n                    input_ids = None\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\ndef reconstruct_word_spans(tokenizer, text: str, max_length: Optional[int] = None) -> Tuple[Dict[int, Optional[str]], List[str]]:\n    global _LANGUAGE_WARNING_COUNT\n\n    if tokenizer is None:\n        return {}, []\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = max(1, int(max_length))\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any('\\u0980' <= c <= '\\u09FF' for c in text)\n    has_english = any('a' <= c.lower() <= 'z' for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (sum(1 for c in text if '\\u0980' <= c <= '\\u09FF') / max(1, len(text))) * 100.0\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), English: {has_english}\")\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        encoded = safe_offsets_tokenize(tokenizer, text, max_length=eff_max, include_special_tokens=False)\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if not tokens:\n        return {}, []\n\n    if isinstance(offsets, list) and len(offsets) > 0:\n        if all(isinstance(x, tuple) for x in offsets):\n            offsets_list = offsets\n        elif isinstance(offsets[0], (list, tuple)):\n            offsets_list = [(x[0], x[1]) if isinstance(x, (list, tuple)) and len(x) >= 2 else (None, None) for x in offsets[0]]\n        else:\n            offsets_list = [(None, None)] * len(tokens)\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, Optional[str]] = {}\n    words: List[str] = []\n\n    used_any_offset = any(isinstance(o, tuple) and o[0] is not None and o[1] is not None for o in offsets_list)\n\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n        word_token_indices: List[int] = []\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            if off_start is None or off_end is None or tok in special_tokens:\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                            for tidx in word_token_indices:\n                                token_word_map[tidx] = wtext\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                word_token_indices = []\n                token_word_map[idx] = None\n                continue\n\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n                word_token_indices = [idx]\n            else:\n                if word_end is not None and off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                            for tidx in word_token_indices:\n                                token_word_map[tidx] = wtext\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                    word_token_indices = [idx]\n                else:\n                    if word_end is not None:\n                        word_end = max(word_end, off_end)\n                    else:\n                        word_end = off_end\n                    word_token_indices.append(idx)\n\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n                    for tidx in word_token_indices:\n                        token_word_map[tidx] = wtext\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    current_indices: List[int] = []\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n                    for tidx in current_indices:\n                        token_word_map[tidx] = word\n                current_parts = []\n                current_indices = []\n            token_word_map[i] = None\n            continue\n\n        clean = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            token_word_map[i] = None\n            continue\n\n        is_start = tok.startswith(\"▁\") or tok.startswith(\"Ġ\") or tok.startswith(_SPIECE_UNDERLINE)\n\n        if is_start:\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n                    for tidx in current_indices:\n                        token_word_map[tidx] = word\n            current_parts = [clean]\n            current_indices = [i]\n        else:\n            current_parts.append(clean)\n            current_indices.append(i)\n            if len(\"\".join(current_parts)) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                    for tidx in current_indices[:-1]:\n                        token_word_map[tidx] = word\n                current_parts = [clean]\n                current_indices = [i]\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n            for tidx in current_indices:\n                token_word_map[tidx] = word\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n        if tokens and word_list:\n            word_idx = 0\n            current_word = word_list[0] if word_list else None\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"▁\", \"\").replace(_SPIECE_UNDERLINE, \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    token_word_map[i] = None\n                    continue\n                if tok.startswith(\"▁\") or tok.startswith(\"Ġ\") or tok.startswith(_SPIECE_UNDERLINE):\n                    if word_idx < len(word_list) - 1:\n                        word_idx += 1\n                    current_word = word_list[word_idx] if word_idx < len(word_list) else None\n                token_word_map[i] = current_word\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\ndef is_word_token(clean_token: str, min_letters: int = 2, min_letter_fraction: float = 0.5) -> bool:\n    if not clean_token or not isinstance(clean_token, str):\n        return False\n    if len(clean_token) < min_letters:\n        return False\n    letter_count = sum(1 for c in clean_token if c.isalpha())\n    if letter_count == 0:\n        return False\n    alphanum_count = sum(1 for c in clean_token if c.isalnum())\n    if alphanum_count == 0:\n        return False\n    letter_ratio = letter_count / alphanum_count\n    return letter_ratio >= min_letter_fraction\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    sample_bn = \"কাল আমি বাজারে যাব।\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        enc_bn = safe_offsets_tokenize(tokenizer, sample_bn, max_length=32, include_special_tokens=False)\n        enc_len = int(enc_bn[\"input_ids\"].shape[-1]) if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn else \"N/A\"\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(any('\\u0980' <= c <= '\\u09FF' for c in w) for w in words_bn)\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(any('a' <= c.lower() <= 'z' for c in w) for w in words_en)\n        print(f\"  Contains English words: {has_english_words}\")\n\n        if has_bengali_words and not any('a' <= c.lower() <= 'z' for c in \"\".join(words_bn)):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\niswordtoken = is_word_token\n\nprint(\"Cell 1: Tokenizer utilities loaded\")","metadata":{"id":"WZE9PkHyH4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (BENGALI → ENGLISH TASK) - FIXED\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept Exception:\n    pd = None\n    _HAS_PANDAS = False\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ndef _get_global(name, default):\n    try:\n        return globals().get(name, default)\n    except Exception:\n        return default\n\ntry:\n    _VERBOSE_LOGGING = bool(_get_global(\"VERBOSE_LOGGING\", False))\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(_get_global(\"DEBUG_VERBOSE\", False))\nexcept Exception:\n    _DEBUG_VERBOSE = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\n\ntry:\n    _NUM_SAMPLES = int(_get_global(\"NUM_SAMPLES\", 50000))\n    if _NUM_SAMPLES <= 0:\n        _NUM_SAMPLES = 50000\nexcept Exception:\n    _NUM_SAMPLES = 50000\n\ntry:\n    _MAX_LENGTH = int(_get_global(\"MAX_LENGTH\", 48))\n    if _MAX_LENGTH <= 0:\n        _MAX_LENGTH = 48\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _SOURCE_LANG = str(_get_global(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANG = str(_get_global(\"TARGET_LANGUAGE\", \"en\"))\nexcept Exception:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n\ntry:\n    _M2M_BN_TOKEN_ID = int(_get_global(\"M2M100_BN_TOKEN_ID\", 128025))\n    _M2M_EN_TOKEN_ID = int(_get_global(\"M2M100_EN_TOKEN_ID\", 128022))\nexcept Exception:\n    _M2M_BN_TOKEN_ID = 128025\n    _M2M_EN_TOKEN_ID = 128022\n\ntry:\n    _NUM_GPUS = int(_get_global(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(_get_global(\"USE_MULTI_GPU\", _NUM_GPUS > 1))\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _NUM_WORKERS = int(_get_global(\"NUM_WORKERS\", 0))\n    if _NUM_WORKERS < 0:\n        _NUM_WORKERS = 0\nexcept Exception:\n    _NUM_WORKERS = 0\n\ntry:\n    _PIN_MEMORY = bool(_get_global(\"PIN_MEMORY\", False))\nexcept Exception:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(_get_global(\"PREFETCH_FACTOR\", 2))\n    if _PREFETCH_FACTOR <= 0:\n        _PREFETCH_FACTOR = 2\nexcept Exception:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(_get_global(\"DATASET_CSV_PATH\",\n                                        \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"))\nexcept Exception:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n\ntry:\n    _TRAIN_DOMAIN = int(_get_global(\"TRAIN_DOMAIN\", 0))\n    _TEST_DOMAIN = int(_get_global(\"TEST_DOMAIN\", 1))\n    _USE_DOMAIN_LABELS = bool(_get_global(\"USE_DOMAIN_LABELS\", False))\nexcept Exception:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = False\n\n_has_normalize = (\"normalize_bengali\" in globals()) and (\"normalize_english\" in globals())\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n\nif not _has_normalize and DEBUG_CELL2:\n    print(\"[CELL2] normalize_bengali/normalize_english not found; falling back to basic normalization\")\n\n_BENGALI_CHAR_RE = re.compile(r\"[\\u0980-\\u09FF]\")\n\n\ndef is_bengali_text(s: Optional[str]) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\n\ndef _get_safe_vocab_size(tokenizer) -> int:\n    try:\n        if tokenizer is None:\n            return 128112\n        vocab_size = getattr(tokenizer, \"vocab_size\", None)\n        if vocab_size is None:\n            try:\n                vocab_size = len(tokenizer)\n            except Exception:\n                vocab_size = 128112\n        return int(vocab_size)\n    except Exception:\n        return 128112\n\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    try:\n        tok_name = getattr(dataset, \"_tokenizer_name_or_path\", None) if dataset is not None else None\n        if tok_name:\n            try:\n                from transformers import AutoTokenizer\n                dataset.tokenizer = AutoTokenizer.from_pretrained(tok_name, use_fast=True)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n                if DEBUG_CELL2:\n                    print(f\"[CELL2-WORKER-{worker_id}] Tokenizer loaded: {tok_name}\")\n            except Exception as e:\n                cell2_dbg(\"worker_tokenizer_reload\", f\"Worker {worker_id} tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] Tokenizer rebind failed worker {worker_id}\")\n\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2 ** 31 - 1))\n        torch.manual_seed(seed % (2 ** 31 - 1))\n    except Exception:\n        pass\n\n\ndef load_and_preprocess_optimized(num_samples: Optional[int] = None, split: str = \"train\") -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from {_DATASET_CSV_PATH}\")\n\n    if not _HAS_PANDAS:\n        print(\"[CELL2] pandas not available; using fallback dataset\")\n        return _get_fallback_dataset()\n\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] CSV not found at {_DATASET_CSV_PATH}; using fallback dataset\")\n        return _get_fallback_dataset()\n\n    try:\n        df = pd.read_csv(_DATASET_CSV_PATH, dtype=str, keep_default_na=False)\n    except Exception as e:\n        print(f\"[CELL2] Failed to read CSV: {e}; using fallback dataset\")\n        return _get_fallback_dataset()\n\n    if df is None or df.shape[0] == 0:\n        print(\"[CELL2] CSV empty; using fallback dataset\")\n        return _get_fallback_dataset()\n\n    cols = [c.lower().strip() for c in df.columns.tolist()]\n    src_col = None\n    tgt_col = None\n\n    if \"src\" in cols and \"tgt\" in cols:\n        src_col = df.columns[cols.index(\"src\")]\n        tgt_col = df.columns[cols.index(\"tgt\")]\n    else:\n        for name in [\"source\", \"source_text\", \"sentence\", \"text\", \"bn\", \"bengali\"]:\n            if name in cols:\n                src_col = df.columns[cols.index(name)]\n                break\n        for name in [\"target\", \"target_text\", \"translation\", \"en\", \"english\"]:\n            if name in cols:\n                tgt_col = df.columns[cols.index(name)]\n                break\n        if src_col is None and len(df.columns) >= 1:\n            src_col = df.columns[0]\n        if tgt_col is None and len(df.columns) >= 2:\n            tgt_col = df.columns[1] if df.columns[1] != src_col else df.columns[0]\n\n    df[src_col] = df[src_col].fillna(\"\").astype(str)\n    df[tgt_col] = df[tgt_col].fillna(\"\").astype(str)\n\n    sample_src = df[src_col].iloc[0] if len(df) > 0 else \"\"\n    sample_tgt = df[tgt_col].iloc[0] if len(df) > 0 else \"\"\n\n    src_is_bengali = bool(_BENGALI_CHAR_RE.search(str(sample_src)))\n    tgt_is_bengali = bool(_BENGALI_CHAR_RE.search(str(sample_tgt)))\n    src_is_english = bool(re.search(r\"[a-zA-Z]\", str(sample_src))) and not src_is_bengali\n    tgt_is_english = bool(re.search(r\"[a-zA-Z]\", str(sample_tgt))) and not tgt_is_bengali\n\n    if src_is_english and tgt_is_bengali:\n        print(\"[CELL2] Detected src=English and tgt=Bengali. Swapping columns for bn->en task.\")\n        df = df.rename(columns={src_col: \"__temp_src__\", tgt_col: \"__temp_tgt__\"})\n        df[\"src\"] = df[\"__temp_tgt__\"]\n        df[\"tgt\"] = df[\"__temp_src__\"]\n        src_col = \"src\"\n        tgt_col = \"tgt\"\n        if len(df) > 0:\n            sample_src = df[src_col].iloc[0]\n            sample_tgt = df[tgt_col].iloc[0]\n            src_is_bengali = bool(_BENGALI_CHAR_RE.search(str(sample_src)))\n            tgt_is_english = bool(re.search(r\"[a-zA-Z]\", str(sample_tgt))) and not bool(_BENGALI_CHAR_RE.search(str(sample_tgt)))\n            if not src_is_bengali or not tgt_is_english:\n                print(\"[CELL2] WARNING: After swap columns don't clearly match bn->en. Proceeding but results may be noisy.\")\n    else:\n        if not src_is_bengali or not tgt_is_english:\n            if DEBUG_CELL2:\n                print(\"[CELL2] Warning: detected languages may not match bn->en (proceeding)\")\n\n    df = df.head(int(num_samples))\n\n    pairs: List[Tuple[str, str]] = []\n    skipped = 0\n\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading dataset\"):\n        try:\n            src_val = str(row[src_col]).strip()\n            tgt_val = str(row[tgt_col]).strip()\n            if not src_val or not tgt_val:\n                skipped += 1\n                continue\n            if not is_bengali_text(src_val):\n                skipped += 1\n                continue\n            if not re.search(r\"[a-zA-Z]\", tgt_val):\n                skipped += 1\n                continue\n            max_words = max(20, _MAX_LENGTH // 2)\n            if len(src_val.split()) > max_words or len(tgt_val.split()) > max_words:\n                skipped += 1\n                continue\n            if _has_normalize:\n                try:\n                    bn_norm = normalize_bengali(src_val)\n                    en_norm = normalize_english(tgt_val)\n                except Exception:\n                    bn_norm = src_val.strip()\n                    en_norm = tgt_val.strip().lower()\n            else:\n                bn_norm = src_val.strip()\n                en_norm = tgt_val.strip().lower()\n            if not bn_norm or not en_norm:\n                skipped += 1\n                continue\n            pairs.append((bn_norm, en_norm))\n        except Exception:\n            skipped += 1\n            continue\n\n    if DEBUG_CELL2:\n        print(f\"[CELL2] Loaded pairs: {len(pairs)}, skipped: {skipped}\")\n\n    if len(pairs) == 0:\n        print(\"[CELL2] No valid pairs loaded from CSV; using fallback dataset\")\n        return _get_fallback_dataset()\n\n    return pairs\n\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    fallback_pairs = [\n        (\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\"),\n        (\"সে আমাকে পরে কল করবে।\", \"he will call me later.\"),\n        (\"আমরা প্রতিদিন তাজা ফল খাই।\", \"we eat fresh fruits every day.\"),\n        (\"তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\", \"his hard work has brought good results.\"),\n        (\"গাছে নতুন পাতাগুলো গজিয়েছে।\", \"new leaves have sprouted on the tree.\"),\n        (\"আমি বইয়ের পাতা উল্টাচ্ছি।\", \"i am turning the pages of the book.\"),\n        (\"কাল আমি বাজারে গিয়েছিলাম।\", \"yesterday i went to the market.\"),\n        (\"কাল আমি তোমার সাথে দেখা করব।\", \"tomorrow i will meet you.\"),\n        (\"তারা আকাশে উজ্জ্বল।\", \"the stars are bright in the sky.\"),\n        (\"তারা বাড়িতে নেই।\", \"they are not at home.\"),\n        (\"ব্যাংক নদীর ধারে ভেঙে গেছে।\", \"the bank by the river has collapsed.\"),\n        (\"আমি ব্যাংকে টাকা জমা দিয়েছি।\", \"i deposited money in the bank.\"),\n        (\"বার বার চেষ্টা করতে হবে।\", \"you have to try again and again.\"),\n        (\"আমি বার খুলে ভিতরে ঢুকলাম।\", \"i opened the bar and entered.\"),\n        (\"তার মাথা ব্যথা করছে।\", \"his head is hurting.\"),\n        (\"আমি মাথা নেড়ে সম্মতি দিলাম।\", \"i nodded my head in agreement.\"),\n        (\"সে হার মেনে নিয়েছে।\", \"he accepted defeat.\"),\n        (\"আমি গলায় সোনার হার পরেছি।\", \"i am wearing a gold necklace.\"),\n        (\"পানি খুব ঠান্ডা।\", \"the water is very cold.\"),\n        (\"আমি পানি খাচ্ছি।\", \"i am drinking water.\"),\n        (\"দল খেলায় জিতেছে।\", \"the team won the game.\"),\n        (\"বাজার থেকে সবজি কিনলাম।\", \"i bought vegetables from the market.\"),\n        (\"তার নাম আহমেদ।\", \"his name is ahmed.\"),\n        (\"নাম না করে কাজ করো।\", \"work without making a name.\"),\n        (\"কথা বলা বন্ধ করো।\", \"stop talking.\"),\n        (\"বই পড়তে ভালো লাগে।\", \"i like reading books.\"),\n        (\"আমি একটি নতুন বই কিনেছি।\", \"i bought a new book.\"),\n        (\"ঘর পরিষ্কার করা হয়েছে।\", \"the house has been cleaned.\"),\n        (\"আমি ঘরে বসে আছি।\", \"i am sitting at home.\"),\n        (\"মন ভালো নেই।\", \"my mind is not good.\"),\n        (\"হাত ধুয়ে নাও।\", \"wash your hands.\"),\n        (\"দেখতে চাই বন সুন্দর।\", \"the forest is beautiful.\"),\n    ]\n    if _has_normalize:\n        try:\n            return [(normalize_bengali(bn), normalize_english(en)) for bn, en in fallback_pairs]\n        except Exception:\n            return [(bn.strip(), en.strip().lower()) for bn, en in fallback_pairs]\n    return [(bn.strip(), en.strip().lower()) for bn, en in fallback_pairs]\n\n\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, pairs: List[Tuple[str, str]], tokenizer: Any = None, max_length: Optional[int] = None, split: str = \"train\"):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = max(1, int(max_length))\n        self.tokenizer = tokenizer\n        self.split = split\n\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Too long at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init exception idx={i}\")\n        if DEBUG_CELL2:\n            print(f\"[CELL2] Dataset init: {len(self.pairs)} valid, {invalid} invalid\")\n\n        try:\n            if self.tokenizer is not None and \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            elif self.tokenizer is not None:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n        if not self.special_tokens:\n            self.special_tokens = {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                if self.tokenizer is not None:\n                    self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            vocab_size = _get_safe_vocab_size(self.tokenizer)\n\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                input_ids = enc.get(\"input_ids\")\n                attention_mask = enc.get(\"attention_mask\", None)\n                if isinstance(input_ids, torch.Tensor):\n                    input_ids = input_ids.squeeze(0)\n                elif isinstance(input_ids, list):\n                    input_ids = torch.tensor(input_ids[0] if isinstance(input_ids[0], list) else input_ids, dtype=torch.long)\n                else:\n                    input_ids = torch.tensor([int(x) for x in input_ids], dtype=torch.long)\n                if attention_mask is None:\n                    attention_mask = (input_ids != getattr(self.tokenizer, \"pad_token_id\", 1)).long()\n                elif isinstance(attention_mask, torch.Tensor) and attention_mask.dim() > 1:\n                    attention_mask = attention_mask.squeeze(0)\n                elif isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0] if isinstance(attention_mask[0], list) else attention_mask, dtype=torch.long)\n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(src_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\", add_special_tokens=False)\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            input_ids = torch.clamp(input_ids, min=0, max=vocab_size - 1)\n\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception:\n                    token_word_map = {}\n\n            if not token_word_map and tokens:\n                try:\n                    current_word_parts: List[str] = []\n                    for idx, tok in enumerate(tokens):\n                        if not isinstance(tok, str) or tok in self.special_tokens:\n                            continue\n                        clean = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                        if not clean:\n                            continue\n                        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n                            current_word_parts = [clean]\n                            token_word_map[idx] = clean\n                        else:\n                            current_word_parts.append(clean)\n                            word = \"\".join(current_word_parts)\n                            token_word_map[idx] = word\n                            for prev_idx in range(max(0, idx - len(current_word_parts) + 1), idx):\n                                token_word_map[prev_idx] = word\n                except Exception:\n                    token_word_map = {}\n\n            return input_ids, attention_mask, tokens, token_word_map\n\n        except Exception as e:\n            if DEBUG_CELL2:\n                cell2_dbg(\"encode_src_fail\", f\"Source encoding failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            vocab_size = _get_safe_vocab_size(self.tokenizer)\n\n            dec = self.tokenizer(tgt_text, max_length=self.max_length, truncation=True, padding=False, return_tensors=\"pt\", add_special_tokens=False)\n            labels = dec[\"input_ids\"].squeeze(0)\n\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1)\n\n            non_pad_mask = labels != pad_id\n            labels_clamped = torch.clamp(labels, min=0, max=vocab_size - 1)\n            labels = torch.where(non_pad_mask, labels_clamped, torch.tensor(pad_id, dtype=labels.dtype))\n\n            if labels.size(0) < self.max_length:\n                pad_length = self.max_length - labels.size(0)\n                pad_tensor = torch.full((pad_length,), -100, dtype=torch.long)\n                labels = torch.cat([labels, pad_tensor], dim=0)\n            elif labels.size(0) > self.max_length:\n                labels = labels[:self.max_length]\n\n            non_pad_mask_final = labels != -100\n            labels_clamped_final = torch.clamp(labels, min=0, max=vocab_size - 1)\n            labels = torch.where(non_pad_mask_final, labels_clamped_final, torch.tensor(-100, dtype=labels.dtype))\n\n            return labels\n\n        except Exception as e:\n            if DEBUG_CELL2:\n                cell2_dbg(\"encode_tgt_fail\", f\"Target encoding failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"আমি কল বন্ধ করেছি।\"\n            tgt = \"i turned off the tap.\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n            domain_label = max(0, min(domain_label, 255))\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label\n            }\n        except Exception:\n            pad_id = 1\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n            domain_label = max(0, min(domain_label, 255))\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                return self._make_safe_sample(\"oob\")\n\n            src, tgt = self.pairs[idx]\n\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                return self._make_safe_sample(\"bad_types\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n\n            domain_label = int(_TRAIN_DOMAIN if self.split == \"train\" else _TEST_DOMAIN)\n            domain_label = max(0, min(domain_label, 255))\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label\n            }\n\n        except Exception:\n            return self._make_safe_sample(\"unhandled\")\n\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        pass\n    return int(default_pad_id)\n\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    try:\n        t = tensor.view(-1).long()\n    except Exception:\n        t = tensor.flatten().long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    if not batch:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long)\n        }\n\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n\n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long)\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n\n    vocab_size = 128112\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            vocab_size = _get_safe_vocab_size(tk)\n    except Exception:\n        pass\n\n    inputs, masks, labs, twmaps, srcs, toks, domains = [], [], [], [], [], [], []\n\n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", _TRAIN_DOMAIN)\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = att.flatten().long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            in_ids = torch.clamp(in_ids, min=0, max=vocab_size - 1)\n\n            non_pad_mask = lab != -100\n            lab_clamped = torch.clamp(lab, min=0, max=vocab_size - 1)\n            lab = torch.where(non_pad_mask, lab_clamped, torch.tensor(-100, dtype=lab.dtype))\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(max(0, min(int(domain), 255)))\n\n        except Exception:\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long)\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n\n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.full((len(inputs),), _TRAIN_DOMAIN, dtype=torch.long)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels\n    }\n\n\ndef create_optimized_dataloader(dataset: Dataset, batch_size: Optional[int] = None, shuffle: bool = True, split: str = \"train\") -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(_get_global(\"BATCH_SIZE\", 8))\n        except Exception:\n            batch_size = 8\n\n    batch_size = max(1, int(batch_size))\n    original_batch_size = batch_size\n    adjusted = False\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        remainder = batch_size % _NUM_GPUS\n        if remainder != 0:\n            new_batch_size = batch_size - remainder\n            if new_batch_size == 0:\n                new_batch_size = _NUM_GPUS\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n\n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size from {original_batch_size} to {batch_size} for DP across {_NUM_GPUS} GPUs\")\n\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False\n    }\n\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = max(2, _PREFETCH_FACTOR)\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception:\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        per_gpu = batch_size // _NUM_GPUS\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\n\nprint(\"Cell 2: Memory-efficient data loading ready\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE - NaN/Inf FULLY HARDENED\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\nfrom concurrent.futures import ThreadPoolExecutor, Future\n\nPRINT_INTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HAS_CLUSTERING = True\nexcept Exception:\n    HAS_CLUSTERING = False\n\ntry:\n    from sklearn.cluster import KMeans\n    HAS_KMEANS = True\nexcept Exception:\n    HAS_KMEANS = False\n\ntry:\n    _DSCD_MAX_PROTOS = int(DSCD_MAX_PROTOS)\n    _DSCD_BUFFER_SIZE = int(DSCD_BUFFER_SIZE)\n    _DSCD_N_MIN = int(DSCD_N_MIN)\n    _DSCD_DISPERSION_THRESHOLD = float(DSCD_DISPERSION_THRESHOLD)\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    _DSCD_ENABLE_TRAINING_CLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\n    _DSCD_USE_COSINE_DISTANCE = bool(DSCD_USE_COSINE_DISTANCE)\n    _DSCD_ENABLE_ONLINE_CLUSTERING = bool(DSCD_ENABLE_ONLINE_CLUSTERING)\n    _DSCD_ONLINE_CLUSTERING_FREQUENCY = int(DSCD_ONLINE_CLUSTERING_FREQUENCY)\n    _APPLY_DSCD_AUGMENTATION = bool(APPLY_DSCD_AUGMENTATION)\nexcept Exception:\n    _DSCD_MAX_PROTOS = 8\n    _DSCD_BUFFER_SIZE = 50\n    _DSCD_N_MIN = 2\n    _DSCD_DISPERSION_THRESHOLD = 0.70\n    _VERBOSE_LOGGING = False\n    _DSCD_ENABLE_TRAINING_CLUSTERING = True\n    _DSCD_USE_COSINE_DISTANCE = True\n    _DSCD_ENABLE_ONLINE_CLUSTERING = True\n    _DSCD_ONLINE_CLUSTERING_FREQUENCY = 10\n    _APPLY_DSCD_AUGMENTATION = False\n\n_DSCD_MAX_PROTOS = max(1, int(_DSCD_MAX_PROTOS))\n_DSCD_BUFFER_SIZE = max(1, int(_DSCD_BUFFER_SIZE))\n_DSCD_N_MIN = max(1, int(_DSCD_N_MIN))\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _MAX_TOKENS_PER_DISCOVERY = int(globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150))\nexcept Exception:\n    _MAX_TOKENS_PER_DISCOVERY = 150\n\n_MAX_TOKENS_PER_DISCOVERY = max(1, _MAX_TOKENS_PER_DISCOVERY)\n\ntry:\n    _DSCD_NEW_SENSE_LAMBDA = float(DSCD_NEW_SENSE_LAMBDA)\n    if _DSCD_NEW_SENSE_LAMBDA <= 0:\n        _DSCD_NEW_SENSE_LAMBDA = 1.5\nexcept Exception:\n    _DSCD_NEW_SENSE_LAMBDA = 1.5\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(HOMOGRAPH_REFERENCE_LIST_BN)\nexcept Exception:\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\", \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"}\n\ntry:\n    _DSCD_MIN_LETTERS = int(DSCD_MIN_LETTERS)\n    _DSCD_MIN_LETTER_FRACTION = float(DSCD_MIN_LETTER_FRACTION)\n    _DSCD_MAX_CLUSTERING_POINTS = int(DSCD_MAX_CLUSTERING_POINTS)\nexcept Exception:\n    _DSCD_MIN_LETTERS = 2\n    _DSCD_MIN_LETTER_FRACTION = 0.5\n    _DSCD_MAX_CLUSTERING_POINTS = 500\n\n_DSCD_MIN_LETTERS = max(1, _DSCD_MIN_LETTERS)\n_DSCD_MIN_LETTER_FRACTION = min(max(0.0, _DSCD_MIN_LETTER_FRACTION), 1.0)\n_DSCD_MAX_CLUSTERING_POINTS = max(1, _DSCD_MAX_CLUSTERING_POINTS)\n\n_TRG_PUNCT_SET = set(\".,!?;:-\")\n_PUNCT_SET = _TRG_PUNCT_SET\n\n\ndef normalize_token_key(token: str) -> Optional[str]:\n    if token is None:\n        return None\n    token = unicodedata.normalize(\"NFKC\", str(token))\n    token = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"</w>\", \"\")\n    token = token.strip().lower()\n    if not token or len(token) < 2:\n        return None\n    letter_count = sum(1 for ch in token if unicodedata.category(ch).startswith(\"L\"))\n    total_chars = sum(1 for ch in token if not ch.isspace())\n    if total_chars == 0 or letter_count == 0:\n        return None\n    if letter_count < max(1, _DSCD_MIN_LETTERS):\n        return None\n    if letter_count / total_chars < _DSCD_MIN_LETTER_FRACTION:\n        return None\n    if all(c in _PUNCT_SET for c in token):\n        return None\n    return token\n\n\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    letters = sum(1 for ch in token if unicodedata.category(ch).startswith(\"L\"))\n    total = sum(1 for ch in token if not ch.isspace())\n    if total == 0 or letters < min_letters:\n        return False\n    return letters / total >= min_letter_fraction\n\n\ndef reconstruct_word_embeddings(\n    token_embeddings: torch.Tensor,\n    input_ids: Optional[torch.Tensor],\n    tokenizer,\n    device: torch.device\n) -> Tuple[torch.Tensor, List[Dict[int, str]]]:\n    batch_size = int(token_embeddings.size(0))\n    token_seq_len = int(token_embeddings.size(1))\n    embed_dim = int(token_embeddings.size(-1))\n    \n    word_embeddings_list: List[torch.Tensor] = []\n    word_maps_batch: List[Dict[int, str]] = []\n    \n    for b in range(batch_size):\n        if input_ids is not None:\n            try:\n                ids = input_ids[b].tolist()\n                tokens = tokenizer.convert_ids_to_tokens(ids)\n            except Exception:\n                tokens = [f\"tok{i}\" for i in range(token_seq_len)]\n        else:\n            tokens = [f\"tok{i}\" for i in range(token_seq_len)]\n        \n        words: List[str] = []\n        word_spans: List[Tuple[int, int]] = []\n        current_word = \"\"\n        word_start: Optional[int] = None\n        \n        for j, tok in enumerate(tokens):\n            if not isinstance(tok, str):\n                tok = str(tok)\n            \n            if tok in {\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"}:\n                if current_word and word_start is not None:\n                    words.append(current_word)\n                    word_spans.append((word_start, j))\n                current_word = \"\"\n                word_start = None\n                continue\n            \n            is_start = tok.startswith(\"▁\") or tok.startswith(\"\\u2581\") or tok.startswith(\"Ġ\")\n            clean_tok = tok.lstrip(\"▁\\u2581Ġ\")\n            \n            if is_start:\n                if current_word and word_start is not None:\n                    words.append(current_word)\n                    word_spans.append((word_start, j))\n                current_word = clean_tok\n                word_start = j\n            else:\n                current_word += clean_tok\n        \n        if current_word and word_start is not None:\n            words.append(current_word)\n            word_spans.append((word_start, len(tokens)))\n        \n        word_embs: List[torch.Tensor] = []\n        word_map: Dict[int, str] = {}\n        \n        for w_idx, (s, e) in enumerate(word_spans):\n            if e > s and 0 <= s < token_seq_len and e <= token_seq_len:\n                sub_embs = token_embeddings[b, s:e, :]\n                if sub_embs.numel() == 0:\n                    continue\n                word_emb = sub_embs.mean(dim=0)\n                word_embs.append(word_emb)\n                \n                try:\n                    word_name = words[w_idx].replace(\"▁\", \"\").strip()\n                except Exception:\n                    word_name = \"\".join(words[w_idx].split()).strip()\n                \n                word_map[w_idx] = word_name\n        \n        if not word_embs:\n            word_emb_tensor = torch.zeros(1, embed_dim, device=device, dtype=torch.float32)\n        else:\n            word_emb_tensor = torch.stack(word_embs, dim=0).to(device).to(dtype=torch.float32)\n        \n        word_embeddings_list.append(word_emb_tensor)\n        word_maps_batch.append(word_map)\n    \n    max_words = max((w.size(0) for w in word_embeddings_list), default=1)\n    padded = []\n    for w in word_embeddings_list:\n        if w.size(0) < max_words:\n            pad = torch.zeros(max_words - w.size(0), w.size(1), device=w.device, dtype=w.dtype)\n            w = torch.cat([w, pad], dim=0)\n        padded.append(w)\n    \n    word_embeddings = torch.stack(padded, dim=0)\n    return word_embeddings, word_maps_batch\n\n\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embed_dim, max_protos: Optional[int] = None):\n        if max_protos is None:\n            max_protos = _DSCD_MAX_PROTOS\n        self.embed_dim = max(1, int(embed_dim))\n        self.max_protos = max(1, int(max_protos))\n        \n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        \n        self.mu = 0.0\n        self.tau = 1e-3\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n        self.corruption_warnings = 0\n    \n    def normalize_vec(self, v: torch.Tensor) -> torch.Tensor:\n        try:\n            v = v.detach().cpu().to(dtype=torch.float32)\n            if not torch.isfinite(v).all():\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.corruption_warnings < 10:\n                    print(f\"⚠️  DSCD-STORE: Non-finite vector detected, returning zeros\")\n                self.corruption_warnings += 1\n                return torch.zeros_like(v)\n            v = torch.clamp(v, min=-100.0, max=100.0)\n            norm = v.norm()\n            if norm.item() < 1e-6:\n                return v\n            return v / (norm + 1e-9)\n        except Exception:\n            return torch.zeros(self.embed_dim, dtype=torch.float32)\n    \n    def validate_centroid(self, v: torch.Tensor) -> bool:\n        try:\n            if not isinstance(v, torch.Tensor):\n                return False\n            if v.numel() != self.embed_dim:\n                return False\n            if not torch.isfinite(v).all():\n                return False\n            if v.dtype not in [torch.float32, torch.float64, torch.float16]:\n                return False\n            norm = v.norm().item()\n            if not np.isfinite(norm) or norm > 1000.0:\n                return False\n            return True\n        except Exception:\n            return False\n    \n    def add_prototype(self, vector: torch.Tensor, current_time: Optional[float] = None, count: int = 1) -> None:\n        if current_time is None:\n            current_time = time.time()\n        \n        try:\n            v = vector.detach().cpu().clone().to(dtype=torch.float32)\n        except Exception:\n            try:\n                v = torch.tensor(np.asarray(vector), dtype=torch.float32)\n            except Exception:\n                v = torch.zeros(self.embed_dim, dtype=torch.float32)\n        \n        v = torch.clamp(v, min=-100.0, max=100.0)\n        v = self.normalize_vec(v)\n        \n        if not self.validate_centroid(v):\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.corruption_warnings < 10:\n                print(f\"⚠️  DSCD-STORE: Invalid centroid rejected in add_prototype\")\n            self.corruption_warnings += 1\n            return\n        \n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(max(1, int(count)))\n            self.creation_time.append(float(current_time))\n        else:\n            try:\n                min_idx = int(np.argmin(self.counts) if self.counts else 0)\n            except Exception:\n                min_idx = 0\n            min_idx = max(0, min(min_idx, len(self.centroids) - 1))\n            self.centroids[min_idx] = v\n            while len(self.counts) <= min_idx:\n                self.counts.append(1)\n            self.counts[min_idx] = max(1, int(count))\n            while len(self.creation_time) <= min_idx:\n                self.creation_time.append(float(current_time))\n            self.creation_time[min_idx] = float(current_time)\n    \n    def update_prototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignment_distance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.add_prototype(vector, time.time(), count=1)\n            return\n        \n        eta = max(0.0, min(0.5, float(eta)))\n        \n        try:\n            old_centroid = self.centroids[idx]\n            new_vector = vector.detach().cpu().to(dtype=torch.float32)\n            new_vector = torch.clamp(new_vector, min=-100.0, max=100.0)\n            new_vector = self.normalize_vec(new_vector)\n            \n            if not isinstance(old_centroid, torch.Tensor):\n                old_centroid = torch.tensor(np.asarray(old_centroid), dtype=torch.float32)\n            old_centroid = self.normalize_vec(old_centroid)\n            \n            updated = (1.0 - eta) * old_centroid + eta * new_vector\n            updated = torch.clamp(updated, min=-100.0, max=100.0)\n            updated = self.normalize_vec(updated)\n            \n            if not self.validate_centroid(updated):\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.corruption_warnings < 10:\n                    print(f\"⚠️  DSCD-STORE: Invalid centroid rejected in update_prototype\")\n                self.corruption_warnings += 1\n                return\n            \n            self.centroids[idx] = updated\n            while len(self.counts) <= idx:\n                self.counts.append(1)\n            self.counts[idx] = int(self.counts[idx]) + 1\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.corruption_warnings < 10:\n                print(f\"⚠️  DSCD-STORE: update_prototype failed: {type(e).__name__}\")\n            self.corruption_warnings += 1\n            return\n        \n        if assignment_distance is not None:\n            self.update_rolling_stats(float(assignment_distance))\n    \n    def update_rolling_stats(self, d: float) -> None:\n        d = max(0.0, min(10.0, float(d)))\n        \n        if not self.distances:\n            self.mu = float(d)\n            self.tau = max(1e-3, min(10.0, abs(float(d) * 0.1)))\n            self.distances = [float(d)]\n            return\n        \n        prev_mu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.mu = max(0.0, min(10.0, self.mu))\n        \n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n        self.tau = max(1e-3, min(10.0, self.tau))\n        \n        self.distances.append(float(d))\n        if len(self.distances) > 50:\n            self.distances.pop(0)\n    \n    def get_adaptive_threshold(self, lam: float = 1.0) -> float:\n        lam = max(0.0, min(5.0, float(lam)))\n        return float(max(0.0, min(10.0, self.mu + lam * self.tau)))\n    \n    def size(self) -> int:\n        return len(self.centroids)\n    \n    def ensure_consistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            if len(self.counts) > n:\n                self.counts = self.counts[:n]\n            else:\n                self.counts.extend([1] * (n - len(self.counts)))\n        \n        if len(self.creation_time) != n:\n            if len(self.creation_time) > n:\n                self.creation_time = self.creation_time[:n]\n            else:\n                self.creation_time.extend([time.time()] * (n - len(self.creation_time)))\n        \n        valid_centroids = []\n        valid_counts = []\n        valid_times = []\n        for i in range(n):\n            if self.validate_centroid(self.centroids[i]):\n                valid_centroids.append(self.centroids[i])\n                valid_counts.append(self.counts[i] if i < len(self.counts) else 1)\n                valid_times.append(self.creation_time[i] if i < len(self.creation_time) else time.time())\n        \n        if len(valid_centroids) != n:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.corruption_warnings < 10:\n                print(f\"⚠️  DSCD-STORE: Removed {n - len(valid_centroids)} corrupted centroids in ensure_consistency\")\n            self.corruption_warnings += 1\n        \n        self.centroids = valid_centroids\n        self.counts = valid_counts\n        self.creation_time = valid_times\n\n\nclass SigmaNet(nn.Module):\n    def __init__(self, embed_dim: int = 1024):\n        super().__init__()\n        embed_dim = max(1, int(embed_dim))\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1)\n        )\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=0.5)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.mlp(x)\n\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        buffer_size: Optional[int] = None,\n        max_protos: Optional[int] = None,\n        n_min: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        language: str = \"bn\",\n        enable_training_clustering: Optional[bool] = None,\n        max_clustering_points: Optional[int] = None,\n        max_candidates_per_step: int = 2,\n        dscd_min_letters: int = 2,\n        dscd_min_letter_fraction: float = 0.6,\n    ):\n        super().__init__()\n        \n        if buffer_size is None:\n            buffer_size = _DSCD_BUFFER_SIZE\n        if max_protos is None:\n            max_protos = _DSCD_MAX_PROTOS\n        if n_min is None:\n            n_min = _DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = _DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = _DSCD_MAX_CLUSTERING_POINTS\n        if enable_training_clustering is None:\n            enable_training_clustering = _DSCD_ENABLE_TRAINING_CLUSTERING\n        \n        self.embed_dim = max(1, int(embed_dim))\n        self.buffer_size = max(1, int(buffer_size))\n        self.max_protos = max(1, int(max_protos))\n        self.n_min = max(1, int(n_min))\n        self.dispersion_threshold = max(0.0, float(dispersion_threshold))\n        self.language = language\n        self.tokenizer = tokenizer\n        \n        self.dscd_min_letters = max(1, int(dscd_min_letters))\n        self.dscd_min_letter_fraction = min(max(0.0, float(dscd_min_letter_fraction)), 1.0)\n        \n        self.use_cosine_distance = bool(_DSCD_USE_COSINE_DISTANCE)\n        self.enable_online_clustering = bool(_DSCD_ENABLE_ONLINE_CLUSTERING)\n        self.online_clustering_frequency = max(1, int(_DSCD_ONLINE_CLUSTERING_FREQUENCY))\n        self.apply_augmentation = bool(_APPLY_DSCD_AUGMENTATION)\n        \n        self.sigma_net = SigmaNet(embed_dim=self.embed_dim)\n        \n        try:\n            if tokenizer is not None and \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []) if tokenizer is not None else [])\n        except Exception:\n            self.special_tokens = set()\n        \n        self.dscd_allowed_tokens: Set[str] = set()\n        self.dscd_ignored_tokens: Set[str] = set()\n        self.dscd_cache_max_size = 10000\n        \n        self.prototype_stores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.buffers: Dict[str, deque] = {}\n        self.buffers_raw: Dict[str, deque] = {}\n        self.discovered_log: List[Dict[str, Any]] = []\n        self.discovered_homographs: Set[str] = set()\n        \n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n        \n        self.dispersion_cache: Dict[str, float] = {}\n        self.dispersion_last_updated: Dict[str, float] = {}\n        self.dispersion_lock = threading.Lock()\n        \n        self.clustering_lock = threading.Lock()\n        self.buffer_lock = threading.Lock()\n        \n        self.cluster_executor = ThreadPoolExecutor(max_workers=1)\n        from collections import deque as thread_deque\n        self.active_threads = thread_deque(maxlen=100)\n        self.thread_lock = threading.Lock()\n        \n        self.last_cluster_time: Dict[str, float] = {}\n        self.cluster_cooldown_seconds = 2.0\n        \n        self.enable_training_clustering = bool(enable_training_clustering)\n        \n        self.discovery_count = 0\n        self.discovery_times: List[float] = []\n        self.clustered_tokens: Set[str] = set()\n        self.cluster_stats: Dict[str, Dict[str, Any]] = {}\n        \n        self.max_clustering_points = max(1, int(max_clustering_points))\n        self.max_candidates_per_step = max(1, int(max_candidates_per_step))\n        \n        self.token_addition_counts: Dict[str, int] = {}\n        \n        self.state_dict_errors = 0\n        self.load_state_dict_errors = 0\n        \n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"=\" * 80)\n            print(\"DSCD-INIT: MemoryEfficientDSCDOnline initialized - NaN/Inf PROTECTED\")\n            print(\"=\" * 80)\n            print(f\"  - embed_dim={self.embed_dim}, max_protos={self.max_protos}\")\n            print(f\"  - n_min={self.n_min}, dispersion_threshold={self.dispersion_threshold}\")\n            print(f\"  - use_cosine_distance={self.use_cosine_distance}\")\n            print(f\"  - Centroid norm clamp: [-100, 100]\")\n            print(f\"  - Distance clamp: [0, 10]\")\n            print(f\"  - Rolling stats bounds: [0,10], [1e-3,10]\")\n            print(\"=\" * 80)\n    \n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        state = super().state_dict(destination, prefix, keep_vars)\n        \n        plain_stores = {}\n        corrupted_count = 0\n        for token, store in self.prototype_stores.items():\n            try:\n                cent_list = []\n                for c in getattr(store, \"centroids\", []):\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            c_cpu = c.detach().cpu().to(dtype=torch.float32)\n                            if not torch.isfinite(c_cpu).all():\n                                corrupted_count += 1\n                                continue\n                            if c_cpu.numel() != self.embed_dim:\n                                corrupted_count += 1\n                                continue\n                            cent_list.append(c_cpu)\n                        else:\n                            arr = np.asarray(c, dtype=np.float32)\n                            if not np.isfinite(arr).all():\n                                corrupted_count += 1\n                                continue\n                            if arr.size != self.embed_dim:\n                                corrupted_count += 1\n                                continue\n                            cent_list.append(torch.from_numpy(arr).to(dtype=torch.float32))\n                    except Exception:\n                        corrupted_count += 1\n                        continue\n                \n                if cent_list:\n                    try:\n                        centroids_tensor = torch.stack(cent_list, dim=0)\n                    except Exception:\n                        corrupted_count += 1\n                        continue\n                else:\n                    continue\n                \n                counts = list(getattr(store, \"counts\", []))\n                if len(counts) != len(cent_list):\n                    counts = [1] * len(cent_list)\n                \n                creation_time = list(getattr(store, \"creation_time\", []))\n                if len(creation_time) != len(cent_list):\n                    creation_time = [time.time()] * len(cent_list)\n                \n                plain_stores[token] = {\n                    \"centroids\": centroids_tensor,\n                    \"counts\": [int(c) for c in counts],\n                    \"creation_time\": [float(t) for t in creation_time],\n                    \"mu\": max(0.0, min(10.0, float(getattr(store, \"mu\", 0.0)))),\n                    \"tau\": max(1e-3, min(10.0, float(getattr(store, \"tau\", 1e-3)))),\n                    \"size\": len(cent_list),\n                }\n            except Exception as e:\n                corrupted_count += 1\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.state_dict_errors < 10:\n                    print(f\"⚠️  DSCD state_dict failed for token {token}: {type(e).__name__}\")\n                self.state_dict_errors += 1\n                continue\n        \n        state[prefix + \"prototype_stores_data\"] = plain_stores\n        state[prefix + \"discovered_homographs\"] = list(self.discovered_homographs)\n        \n        if corrupted_count > 0 and (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n            print(f\"⚠️  DSCD state_dict: Skipped {corrupted_count} corrupted centroids\")\n        \n        return state\n    \n    def load_state_dict(self, state_dict, strict=True):\n        prefix = \"\"\n        plain_stores = state_dict.pop(prefix + \"prototype_stores_data\", {})\n        discovered = state_dict.pop(prefix + \"discovered_homographs\", [])\n        \n        super().load_state_dict(state_dict, strict=strict)\n        \n        if not plain_stores:\n            return\n        \n        self.prototype_stores = {}\n        self.discovered_homographs = set(discovered) if discovered else set()\n        \n        corrupted_count = 0\n        loaded_count = 0\n        for token, store_dict in plain_stores.items():\n            try:\n                store = MemoryEfficientPrototypeStore(embed_dim=self.embed_dim, max_protos=self.max_protos)\n                \n                centroids_data = store_dict.get(\"centroids\", torch.empty(0, self.embed_dim, dtype=torch.float32))\n                store.centroids = []\n                \n                try:\n                    if isinstance(centroids_data, torch.Tensor) and centroids_data.numel() > 0:\n                        if centroids_data.dim() == 2 and centroids_data.size(1) == self.embed_dim:\n                            for i in range(centroids_data.size(0)):\n                                t = centroids_data[i].detach().cpu().to(dtype=torch.float32)\n                                if torch.isfinite(t).all():\n                                    store.centroids.append(t)\n                                else:\n                                    corrupted_count += 1\n                        else:\n                            corrupted_count += 1\n                            continue\n                    elif isinstance(centroids_data, list):\n                        for c in centroids_data:\n                            if isinstance(c, torch.Tensor):\n                                c_cpu = c.detach().cpu().to(dtype=torch.float32)\n                                if c_cpu.numel() == self.embed_dim and torch.isfinite(c_cpu).all():\n                                    store.centroids.append(c_cpu)\n                                else:\n                                    corrupted_count += 1\n                            else:\n                                try:\n                                    arr = np.asarray(c, dtype=np.float32)\n                                    if arr.size == self.embed_dim and np.isfinite(arr).all():\n                                        store.centroids.append(torch.from_numpy(arr).to(dtype=torch.float32))\n                                    else:\n                                        corrupted_count += 1\n                                except Exception:\n                                    corrupted_count += 1\n                    else:\n                        corrupted_count += 1\n                        continue\n                except Exception as e:\n                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.load_state_dict_errors < 10:\n                        print(f\"⚠️  DSCD load_state_dict centroid parsing failed for {token}: {type(e).__name__}\")\n                    self.load_state_dict_errors += 1\n                    corrupted_count += 1\n                    continue\n                \n                if not store.centroids:\n                    corrupted_count += 1\n                    continue\n                \n                store.counts = store_dict.get(\"counts\", [])\n                store.creation_time = store_dict.get(\"creation_time\", [])\n                store.mu = max(0.0, min(10.0, float(store_dict.get(\"mu\", 0.0))))\n                store.tau = max(1e-3, min(10.0, float(store_dict.get(\"tau\", 1e-3))))\n                \n                try:\n                    store.ensure_consistency()\n                except Exception as e:\n                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.load_state_dict_errors < 10:\n                        print(f\"⚠️  DSCD ensure_consistency failed for {token}: {type(e).__name__}\")\n                    self.load_state_dict_errors += 1\n                    corrupted_count += 1\n                    continue\n                \n                if store.size() > 0:\n                    self.prototype_stores[token] = store\n                    loaded_count += 1\n                else:\n                    corrupted_count += 1\n                \n            except Exception as e:\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.load_state_dict_errors < 10:\n                    print(f\"⚠️  DSCD load_state_dict failed for token {token}: {type(e).__name__}\")\n                self.load_state_dict_errors += 1\n                corrupted_count += 1\n                continue\n        \n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(f\"✅ DSCD load_state_dict: Loaded {loaded_count} stores, skipped {corrupted_count} corrupted\")\n    \n    @staticmethod\n    def clean_token(token):\n        if token is None:\n            return token\n        token = unicodedata.normalize(\"NFKC\", str(token))\n        token = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\")\n        for punct in [\",\", \".\", \",\", \"!\", \"?\", \";\"]:\n            token = token.replace(punct, \"\")\n        return token.strip()\n    \n    def is_valid_multisense(self, token):\n        if token not in self.prototype_stores:\n            return False\n        store = self.prototype_stores[token]\n        total_occurrences = sum(store.counts) if getattr(store, \"counts\", None) else 0\n        min_per_proto = min(store.counts) if getattr(store, \"counts\", None) else 0\n        return store.size() >= 2 and total_occurrences >= 10 and min_per_proto >= 2\n    \n    def get_prototype_summary(self) -> Dict[str, Any]:\n        total_tokens = len(self.prototype_stores)\n        total_prototypes = sum(store.size() for store in self.prototype_stores.values())\n        num_homographs = sum(1 for store in self.prototype_stores.values() if self.is_multisense_store(store))\n        return {\n            \"total_tokens\": total_tokens,\n            \"total_prototypes\": total_prototypes,\n            \"num_homographs\": num_homographs,\n        }\n    \n    def is_multisense_store(self, store: MemoryEfficientPrototypeStore) -> bool:\n        k = store.size()\n        if k < 2:\n            return False\n        \n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(2, self.n_min // 2))\n        if strong < 2:\n            return False\n        \n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.detach().cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n            \n            if len(cents) < 2:\n                return False\n            \n            cents = np.stack(cents, axis=0)\n            if not np.isfinite(cents).all():\n                return False\n            \n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            dists = np.clip(dists, 0.0, 100.0)\n            tri_idx = np.triu_indices(len(cents), 1)\n            if tri_idx[0].size == 0:\n                return False\n            \n            tri = dists[tri_idx]\n            if tri.size == 0:\n                return False\n            \n            min_dist = float(np.clip(tri.min(), 0.0, 100.0))\n            base = max(store.tau, 1e-3)\n            return min_dist >= base * _DSCD_NEW_SENSE_LAMBDA\n        except Exception:\n            return False\n    \n    def discover_homographs_for_tokens(self, token_names: List[str], min_cluster_samples: int, dispersion_threshold: float, global_step: int) -> int:\n        discovered_in_run: List[str] = []\n        for token in token_names:\n            try:\n                success = self.cluster_buffer_to_prototypes_kmeans(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        if clean_token:\n                            self.discovered_homographs.add(clean_token)\n                            discovered_in_run.append(clean_token)\n            except Exception:\n                continue\n        \n        try:\n            self.discovered_log.append({\n                \"timestamp\": time.time(),\n                \"global_step\": global_step,\n                \"candidates_processed\": len(token_names),\n                \"discovered_count\": len(discovered_in_run),\n                \"homographs\": discovered_in_run,\n                \"total_discovered\": len(self.discovered_homographs),\n            })\n        except Exception:\n            pass\n        \n        return len(discovered_in_run)\n    \n    def discover_homographs(self, n_min: Optional[int] = None, dispersion_threshold: Optional[float] = None, min_cluster_size: int = 5, progress: bool = False, max_candidates: int = 500) -> int:\n        if n_min is None:\n            n_min = self.n_min\n        if dispersion_threshold is None:\n            dispersion_threshold = self.dispersion_threshold\n        \n        n_min = max(1, int(n_min))\n        min_cluster_size = max(1, int(min_cluster_size))\n        max_candidates = max(1, int(max_candidates))\n        \n        buffer_snapshot = {}\n        with self.buffer_lock:\n            for token, buffer in list(self.buffers_raw.items()):\n                buffer_snapshot[token] = len(buffer)\n        \n        candidates = []\n        for token, buffer_size in buffer_snapshot.items():\n            if buffer_size >= n_min:\n                dispersion = self.get_dispersion(token)\n                if dispersion >= dispersion_threshold:\n                    rank_score = dispersion * buffer_size\n                    candidates.append((token, rank_score, buffer_size, dispersion))\n        \n        if not candidates:\n            return 0\n        \n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_candidates]\n        \n        discovered = 0\n        for token, score, buf_size, disp in candidates:\n            try:\n                success = self.cluster_buffer_to_prototypes_kmeans(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        if clean_token:\n                            self.discovered_homographs.add(clean_token)\n                            discovered += 1\n            except Exception:\n                continue\n        \n        return discovered\n    \n    def periodic_discovery_check(self, global_step: int, discovery_frequency: int = 200, max_tokens_per_discovery: int = 150) -> int:\n        self.last_periodic_check = global_step\n        max_tokens_per_discovery = max(1, int(max_tokens_per_discovery))\n        \n        buffer_snapshot = {}\n        with self.buffer_lock:\n            for token, buffer in list(self.buffers_raw.items()):\n                buffer_snapshot[token] = len(buffer)\n        \n        candidates = []\n        for token, buffer_size in buffer_snapshot.items():\n            if buffer_size >= self.n_min:\n                dispersion = self.get_dispersion(token)\n                if dispersion >= self.dispersion_threshold:\n                    rank_score = dispersion * buffer_size\n                    candidates.append((token, rank_score, buffer_size, dispersion))\n        \n        if not candidates:\n            return 0\n        \n        candidates.sort(key=lambda x: x[1], reverse=True)\n        token_names = [c[0] for c in candidates[:max_tokens_per_discovery]]\n        \n        try:\n            with self.thread_lock:\n                running = any(isinstance(f, Future) and not f.done() for f in list(self.active_threads))\n            \n            if hasattr(self, \"cluster_executor\") and not running:\n                try:\n                    fut = self.cluster_executor.submit(\n                        self.discover_homographs_for_tokens,\n                        token_names,\n                        self.n_min,\n                        self.dispersion_threshold,\n                        global_step\n                    )\n                    with self.thread_lock:\n                        self.active_threads.append(fut)\n                    return 0\n                except Exception:\n                    pass\n        except Exception:\n            pass\n        \n        try:\n            discovered = self.discover_homographs_for_tokens(\n                token_names,\n                self.n_min,\n                self.dispersion_threshold,\n                global_step\n            )\n            return discovered\n        except Exception:\n            return 0\n    \n    def get_dispersion(self, token_type: str) -> float:\n        try:\n            with self.dispersion_lock:\n                last_update = self.dispersion_last_updated.get(token_type, 0.0)\n                if token_type in self.dispersion_cache and (time.time() - last_update) < 3600:\n                    return self.dispersion_cache[token_type]\n        except Exception:\n            pass\n        \n        with self.buffer_lock:\n            if token_type not in self.buffers_raw or len(self.buffers_raw[token_type]) < 2:\n                return 0.0\n            \n            try:\n                embeddings = []\n                for emb in self.buffers_raw[token_type]:\n                    try:\n                        if isinstance(emb, torch.Tensor):\n                            embeddings.append(emb.detach().cpu().numpy())\n                        else:\n                            embeddings.append(np.asarray(emb, dtype=np.float32))\n                    except Exception:\n                        continue\n                \n                if len(embeddings) < 2:\n                    return 0.0\n                \n                arr = np.stack(embeddings, axis=0)\n            except Exception:\n                return 0.0\n        \n        try:\n            if not np.isfinite(arr).all():\n                return 0.0\n            arr = np.clip(arr, -100.0, 100.0)\n            \n            centroid = arr.mean(axis=0)\n            distances = np.linalg.norm(arr - centroid[None, :], axis=1)\n            distances = np.clip(distances, 0.0, 100.0)\n            \n            if distances.size < 2:\n                return 0.0\n            \n            dispersion = float(np.std(distances))\n            dispersion = max(0.0, min(10.0, dispersion))\n            \n            with self.dispersion_lock:\n                self.dispersion_cache[token_type] = dispersion\n                self.dispersion_last_updated[token_type] = time.time()\n            \n            return dispersion\n        except Exception:\n            return 0.0\n    \n    def validate_prototypes(self, homograph_list: Optional[List[str]] = None, cluster_missing: bool = False) -> Dict[str, Any]:\n        if homograph_list is None:\n            homograph_list = list(_HOMOGRAPH_REFERENCE_LIST_BN)\n        \n        validation_results: Dict[str, Any] = {\n            \"total_tokens\": len(self.prototype_stores),\n            \"total_prototypes\": 0,\n            \"multisense_tokens\": 0,\n            \"homographs_found\": 0,\n            \"homographs_missing\": [],\n            \"avg_prototypes_per_token\": 0.0,\n            \"avg_samples_per_prototype\": 0.0,\n            \"quality_score\": 0.0,\n        }\n        \n        corrupted_stores = 0\n        total_samples = 0\n        corrupted = 0\n        for token, store in self.prototype_stores.items():\n            try:\n                num_protos = len(getattr(store, \"centroids\", []))\n                is_valid = True\n                for c in getattr(store, \"centroids\", []):\n                    if not store.validate_centroid(c):\n                        is_valid = False\n                        break\n                if not is_valid:\n                    corrupted += 1\n                    continue\n                \n                validation_results[\"total_prototypes\"] += num_protos\n                if self.is_multisense_store(store):\n                    validation_results[\"multisense_tokens\"] += 1\n                \n                try:\n                    total_samples += sum(getattr(store, \"counts\", []) or [])\n                except Exception:\n                    pass\n            except Exception:\n                corrupted += 1\n                continue\n        \n        validation_results[\"corrupted_stores\"] = corrupted\n        \n        if validation_results[\"total_tokens\"] > 0:\n            validation_results[\"avg_prototypes_per_token\"] = validation_results[\"total_prototypes\"] / validation_results[\"total_tokens\"]\n        \n        if validation_results[\"total_prototypes\"] > 0:\n            validation_results[\"avg_samples_per_prototype\"] = total_samples / validation_results[\"total_prototypes\"]\n        \n        found = 0\n        missing = []\n        for homograph in homograph_list:\n            clean_h = normalize_token_key(homograph)\n            if not clean_h:\n                continue\n            \n            found_key = None\n            if homograph in self.prototype_stores:\n                found_key = homograph\n            elif clean_h in self.prototype_stores:\n                found_key = clean_h\n            else:\n                for key in self.prototype_stores.keys():\n                    c_k = normalize_token_key(key)\n                    if c_k and (c_k == clean_h or clean_h in c_k or c_k in clean_h):\n                        found_key = key\n                        break\n            \n            if found_key:\n                store = self.prototype_stores.get(found_key)\n                found_protos = store.size() if store else 0\n                if store and self.is_multisense_store(store):\n                    found += 1\n                elif found_protos >= 1:\n                    missing.append(homograph)\n                else:\n                    missing.append(homograph)\n            else:\n                missing.append(homograph)\n        \n        validation_results[\"homographs_found\"] = found\n        validation_results[\"homographs_missing\"] = missing\n        \n        homograph_coverage = found / len(homograph_list) if homograph_list else 0.0\n        multisense_ratio = validation_results[\"multisense_tokens\"] / validation_results[\"total_tokens\"] if validation_results[\"total_tokens\"] > 0 else 0.0\n        validation_results[\"quality_score\"] = homograph_coverage * 0.6 + multisense_ratio * 0.4\n        \n        return validation_results\n    \n    def should_track_token(self, token_text: str) -> bool:\n        if not token_text or not isinstance(token_text, str):\n            return False\n        \n        if len(self.dscd_allowed_tokens) > self.dscd_cache_max_size:\n            self.dscd_allowed_tokens.clear()\n        if len(self.dscd_ignored_tokens) > self.dscd_cache_max_size:\n            self.dscd_ignored_tokens.clear()\n        \n        if token_text in self.dscd_allowed_tokens:\n            return True\n        if token_text in self.dscd_ignored_tokens:\n            return False\n        \n        if token_text in self.special_tokens:\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n        \n        clean = normalize_token_key(token_text)\n        if not clean:\n            self.dscd_ignored_tokens.add(token_text)\n            return False\n        \n        self.dscd_allowed_tokens.add(token_text)\n        return True\n    \n    def canonical_token_key(self, raw_token: str, token_word_map: Optional[Dict[int, Optional[str]]], idx: int) -> Optional[str]:\n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                return str(token_word_map[idx]).strip()\n        except Exception:\n            pass\n        return str(raw_token).strip() if raw_token is not None else None\n    \n    def cleanup_threads(self) -> None:\n        try:\n            with self.thread_lock:\n                kept = []\n                for f in list(self.active_threads):\n                    try:\n                        if isinstance(f, Future):\n                            if not f.done():\n                                kept.append(f)\n                        else:\n                            kept.append(f)\n                    except Exception:\n                        kept.append(f)\n                self.active_threads.clear()\n                self.active_threads.extend(kept)\n        except Exception:\n            pass\n    \n    def cleanup_memory(self) -> None:\n        try:\n            for token_type, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            \n            for token_type, buffer in list(self.buffers_raw.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            \n            now = time.time()\n            expired = [k for k, v in self.dispersion_last_updated.items() if now - v > 3600]\n            for k in expired:\n                self.dispersion_cache.pop(k, None)\n                self.dispersion_last_updated.pop(k, None)\n            \n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n    \n    def forward(\n        self,\n        token_embeddings: Optional[torch.Tensor],\n        token_types: Optional[List[List[str]]] = None,\n        train_mode: bool = True,\n        token_word_map: Optional[List[Dict[int, Optional[str]]]] = None,\n        h_all: Optional[torch.Tensor] = None,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        **kwargs\n    ) -> Dict[str, Any]:\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n        \n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n        \n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        \n        try:\n            word_embeddings, word_maps_from_reconstruction = reconstruct_word_embeddings(\n                token_embeddings, input_ids, self.tokenizer, device\n            )\n        except Exception:\n            word_embeddings = token_embeddings.to(device)\n            word_maps_from_reconstruction = [{} for _ in range(batch_size)]\n        \n        if input_ids is not None and token_types is None:\n            token_types = []\n            for b in range(input_ids.size(0)):\n                try:\n                    token_types.append(self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist()))\n                except Exception:\n                    token_types.append([f\"tok{i}\" for i in range(input_ids.size(1))])\n        \n        if token_types is None:\n            word_seq_len = int(word_embeddings.size(1))\n            token_types = [[f\"tok{i}\" for i in range(word_seq_len)] for _ in range(batch_size)]\n        \n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n            self.cleanup_threads()\n        \n        all_outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n        \n        word_seq_len = int(word_embeddings.size(1))\n        for b in range(batch_size):\n            word_map_b = word_maps_from_reconstruction[b] if b < len(word_maps_from_reconstruction) else {}\n            token_types_b = token_types[b] if token_types and len(token_types) > b else [f\"tok{i}\" for i in range(word_seq_len)]\n            \n            batch_outputs = self.process_sequence(\n                word_embeddings[b],\n                token_types_b,\n                device,\n                word_map=word_map_b,\n                train_mode=train_mode\n            )\n            \n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n        \n        try:\n            h_aug_list = []\n            max_seq_len = word_seq_len\n            for b in range(batch_size):\n                h_batch_list = all_outputs[\"h_augmented\"][b]\n                if isinstance(h_batch_list, list) and len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    try:\n                        h_batch = torch.stack(h_batch_list, dim=0)\n                        if h_batch.size(0) < max_seq_len:\n                            pad = max_seq_len - h_batch.size(0)\n                            h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                        elif h_batch.size(0) > max_seq_len:\n                            h_batch = h_batch[:max_seq_len]\n                    except Exception:\n                        h_batch = torch.zeros(max_seq_len, self.embed_dim, device=device)\n                else:\n                    h_batch = torch.zeros(max_seq_len, self.embed_dim, device=device)\n                h_aug_list.append(h_batch)\n            \n            all_outputs[\"h_augmented\"] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs[\"h_augmented\"] = word_embeddings\n        \n        try:\n            proto_assign_tensor = []\n            for row in all_outputs[\"proto_assignments\"]:\n                try:\n                    arr = [int(x.item()) if isinstance(x, torch.Tensor) else int(x) for x in row]\n                    proto_assign_tensor.append(torch.tensor(arr, dtype=torch.long))\n                except Exception:\n                    proto_assign_tensor.append(torch.full((word_seq_len,), -1, dtype=torch.long))\n            all_outputs[\"proto_assignments\"] = proto_assign_tensor\n        except Exception:\n            pass\n        \n        return all_outputs\n    \n    def process_sequence(\n        self,\n        token_embeddings: torch.Tensor,\n        token_types: List[Any],\n        device: torch.device,\n        word_map: Optional[Dict[int, Optional[str]]] = None,\n        train_mode: bool = True\n    ) -> Dict[str, List[Any]]:\n        seq_len = int(token_embeddings.size(0))\n        outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n        \n        w_entropy = 0.6\n        w_margin = 0.25\n        w_d1 = 0.15\n        softmax_temp = 0.7\n        eps = 1e-9\n        \n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f\"tok{j}\"\n            if not isinstance(raw_tok, str):\n                raw_tok = str(raw_tok)\n            token_key = raw_tok.replace(\"▁\", \"\").strip() or raw_tok\n            h_j = token_embeddings[j]\n            \n            if not self.should_track_token(token_key):\n                outputs[\"proto_assignments\"].append(torch.tensor(-1, dtype=torch.long))\n                outputs[\"proto_probs\"].append(torch.tensor([1.0], dtype=torch.float32))\n                outputs[\"uncertainties\"].append(0.5)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n            \n            try:\n                h_j_clamped = torch.clamp(h_j, min=-100.0, max=100.0)\n                q = F.normalize(h_j_clamped, p=2, dim=-1, eps=1e-9)\n                if not torch.isfinite(q).all():\n                    raise RuntimeError(\"non-finite query\")\n            except Exception:\n                outputs[\"proto_assignments\"].append(torch.tensor(-1, dtype=torch.long))\n                outputs[\"proto_probs\"].append(torch.tensor([1.0], dtype=torch.float32))\n                outputs[\"uncertainties\"].append(0.5)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n            \n            h_raw = h_j.detach().cpu().clone()\n            \n            with self.buffer_lock:\n                if token_key not in self.buffers:\n                    self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                    self.buffers_raw[token_key] = deque(maxlen=self.buffer_size)\n                    self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(self.embed_dim, max_protos=self.max_protos)\n                    self.token_addition_counts[token_key] = 0\n                \n                try:\n                    self.buffers[token_key].append(q.detach().cpu().clone())\n                    self.buffers_raw[token_key].append(h_raw)\n                    self.token_addition_counts[token_key] += 1\n                except Exception:\n                    pass\n            \n            if self.enable_online_clustering and train_mode and self.token_addition_counts[token_key] % self.online_clustering_frequency == 0 and len(self.buffers_raw[token_key]) >= self.n_min:\n                try:\n                    fut = self.cluster_executor.submit(self.cluster_buffer_to_prototypes_kmeans, token_key)\n                    with self.thread_lock:\n                        self.active_threads.append(fut)\n                except Exception:\n                    pass\n            \n            store = self.prototype_stores[token_key]\n            centroids_snapshot: List[torch.Tensor] = []\n            \n            with self.buffer_lock:\n                for c in getattr(store, \"centroids\", []):\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            c_n = c.detach().to(device=device, dtype=torch.float32)\n                        else:\n                            c_n = torch.tensor(np.asarray(c, dtype=np.float32), device=device, dtype=torch.float32)\n                        \n                        if c_n.numel() != self.embed_dim:\n                            continue\n                        if not torch.isfinite(c_n).all():\n                            continue\n                        \n                        c_n = torch.clamp(c_n, min=-100.0, max=100.0)\n                        c_n = F.normalize(c_n, p=2, dim=-1, eps=1e-9) if c_n.norm().item() > 1e-6 else c_n\n                        centroids_snapshot.append(c_n)\n                    except Exception:\n                        continue\n            \n            if not centroids_snapshot:\n                outputs[\"proto_assignments\"].append(torch.tensor(-1, dtype=torch.long))\n                outputs[\"proto_probs\"].append(torch.tensor([1.0], dtype=torch.float32))\n                outputs[\"uncertainties\"].append(0.5)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n            \n            try:\n                centroids_stacked = torch.stack(centroids_snapshot, dim=0)\n                \n                if not torch.isfinite(centroids_stacked).all():\n                    raise ValueError(\"centroids contain NaN/Inf\")\n                \n                if self.use_cosine_distance:\n                    q_norm = F.normalize(h_j_clamped.to(device), p=2, dim=-1, eps=1e-9).unsqueeze(0)\n                    cent_norm = F.normalize(centroids_stacked, p=2, dim=-1, eps=1e-9)\n                    sims = (q_norm @ cent_norm.t()).squeeze(0).detach().cpu().numpy().astype(np.float32)\n                    sims = np.clip(sims, -1.0, 1.0)\n                    \n                    logits = sims / max(eps, softmax_temp)\n                    logits = np.clip(logits, -50.0, 50.0)\n                    logits = logits - np.max(logits)\n                    exp_logits = np.exp(logits)\n                    probs = exp_logits / (exp_logits.sum() + eps)\n                    \n                    dists = np.clip(1.0 - sims, 0.0, 2.0)\n                else:\n                    q_vec = h_j_clamped.to(device).unsqueeze(0)\n                    dists_t = torch.norm(centroids_stacked - q_vec, dim=1).detach().cpu().numpy().astype(np.float32)\n                    dists_t = np.clip(dists_t, 0.0, 100.0)\n                    \n                    sims = -dists_t\n                    logits = sims / max(eps, softmax_temp)\n                    logits = np.clip(logits, -50.0, 50.0)\n                    logits = logits - np.max(logits)\n                    exp_logits = np.exp(logits)\n                    probs = exp_logits / (exp_logits.sum() + eps)\n                    \n                    dists = dists_t\n                \n                if probs.size == 0:\n                    raise ValueError(\"empty similarity/probabilities\")\n                \n                probs = np.clip(probs, eps, 1.0)\n                probs = probs / (probs.sum() + eps)\n                p_max = float(np.max(probs))\n                \n                if probs.size > 1:\n                    entropy_raw = -float(np.sum(probs * np.log(probs + eps)))\n                    H_norm = entropy_raw / float(max(eps, np.log(probs.size)))\n                    H_norm = float(np.clip(H_norm, 0.0, 1.0))\n                else:\n                    H_norm = 0.0\n                \n                sorted_idx = np.argsort(probs)[::-1]\n                top0 = probs[sorted_idx[0]] if probs.size > 0 else 1.0\n                top1 = probs[sorted_idx[1]] if probs.size > 1 else 0.0\n                span_raw = float(np.clip(top0 - top1, 0.0, 1.0))\n                \n                d1 = float(np.min(dists)) if dists.size > 0 else 1.0\n                d1 = max(0.0, min(10.0, d1))\n                d1_norm = d1 / (1.0 + abs(d1))\n                \n                margin = span_raw\n                uncertainty_raw = w_entropy * H_norm + w_margin * (1.0 - margin) + w_d1 * min(1.0, d1_norm)\n                uncertainty = float(np.clip(uncertainty_raw, 0.0, 1.0))\n                \n                gate_raw = p_max * (1.0 - H_norm)\n                gate = float(np.clip(gate_raw, 0.0, 1.0))\n                \n                assignment = int(sorted_idx[0]) if probs.size > 0 else -1\n                prob_tensor = torch.from_numpy(probs.astype(np.float32))\n                \n                outputs[\"proto_assignments\"].append(torch.tensor(assignment, dtype=torch.long))\n                outputs[\"proto_probs\"].append(prob_tensor)\n                outputs[\"uncertainties\"].append(float(uncertainty))\n                outputs[\"span_preds\"].append(float(span_raw))\n                outputs[\"gates\"].append(float(gate))\n                \n                h_aug = h_j\n                if self.apply_augmentation and assignment >= 0 and p_max > 0.4:\n                    try:\n                        centroid_t = centroids_snapshot[assignment]\n                        if centroid_t is not None and isinstance(centroid_t, torch.Tensor):\n                            h_aug = h_j + centroid_t.to(h_j.device).to(dtype=h_j.dtype)\n                            h_aug = torch.clamp(h_aug, min=-100.0, max=100.0)\n                    except Exception:\n                        h_aug = h_j\n                \n                outputs[\"h_augmented\"].append(h_aug)\n                \n                try:\n                    store.update_rolling_stats(d1)\n                except Exception:\n                    pass\n                \n            except Exception:\n                assignment = -1\n                outputs[\"proto_assignments\"].append(torch.tensor(assignment, dtype=torch.long))\n                outputs[\"proto_probs\"].append(torch.tensor([1.0], dtype=torch.float32))\n                outputs[\"uncertainties\"].append(0.5)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n        \n        return outputs\n    \n    def print_clusters_summary(self) -> None:\n        try:\n            items = []\n            for token, store in self.prototype_stores.items():\n                try:\n                    proto_sample_count = sum(getattr(store, \"counts\", []) or [])\n                except Exception:\n                    proto_sample_count = 0\n                \n                buffer_len = len(self.buffers_raw.get(token, [])) if token in self.buffers_raw else 0\n                protos = store.size()\n                mu = getattr(store, \"mu\", 0.0)\n                tau = getattr(store, \"tau\", 0.0)\n                \n                items.append((token, proto_sample_count, protos, mu, tau, buffer_len))\n            \n            items.sort(key=lambda x: x[1], reverse=True)\n            \n            total_samples = sum(i[1] for i in items)\n            total_protos = sum(i[2] for i in items)\n            total_buffers = sum(i[5] for i in items)\n            \n            print(f\"Total: {len(items)} clusters, {total_samples} samples, {total_protos} protos, {total_buffers} buffers\")\n        except Exception:\n            pass\n    \n    def cluster_buffer_to_prototypes_kmeans(self, token_type: str) -> bool:\n        try:\n            if not self.should_track_token(token_type):\n                return False\n            \n            with self.buffer_lock:\n                if token_type not in self.buffers_raw:\n                    return False\n                buf_snapshot = [e.clone() if isinstance(e, torch.Tensor) else torch.tensor(np.asarray(e), dtype=torch.float32)\n                               for e in self.buffers_raw[token_type]]\n            \n            if len(buf_snapshot) < self.n_min:\n                return False\n            \n            emb_list = []\n            for e in buf_snapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        arr = e.detach().cpu().numpy()\n                    else:\n                        arr = np.asarray(e, dtype=np.float32)\n                    \n                    if arr.size == self.embed_dim and np.isfinite(arr).all():\n                        arr = np.clip(arr, -100.0, 100.0)\n                        emb_list.append(arr)\n                except Exception:\n                    continue\n            \n            if len(emb_list) == 0:\n                return False\n            \n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                new_embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                new_embeddings = np.stack(emb_list, axis=0)\n            \n            if new_embeddings.shape[0] < 2:\n                return False\n            \n            norms = np.linalg.norm(new_embeddings, axis=1)\n            norms = np.clip(norms, 0.0, 100.0)\n            valid_mask = norms > 1e-6\n            if not np.any(valid_mask):\n                return False\n            \n            new_embeddings = new_embeddings[valid_mask]\n            norms = norms[valid_mask]\n            \n            if self.use_cosine_distance:\n                new_embeddings_normalized = new_embeddings / (norms[:, None] + 1e-9)\n            else:\n                new_embeddings_normalized = new_embeddings\n            \n            store = self.prototype_stores[token_type]\n            protos_added = 0\n            \n            if HAS_KMEANS and new_embeddings_normalized.shape[0] >= 2:\n                try:\n                    min_k = 1\n                    max_k = min(self.max_protos, new_embeddings_normalized.shape[0], max(1, self.n_min))\n                    k_guess = max(\n                        min_k,\n                        min(\n                            max_k,\n                            int(max(1, round(np.sqrt(max(1, new_embeddings_normalized.shape[0] / 2)))))\n                        )\n                    )\n                    \n                    if k_guess >= 2 and new_embeddings_normalized.shape[0] >= k_guess:\n                        unique_rows = np.unique(new_embeddings_normalized, axis=0)\n                        if unique_rows.shape[0] < 2:\n                            return False\n                        \n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(new_embeddings_normalized)\n                        labels = km.labels_\n                        \n                        centroids = []\n                        counts = []\n                        times = []\n                        for c_idx in range(k_guess):\n                            mask = labels == c_idx\n                            cluster_size = int(mask.sum())\n                            if cluster_size >= self.n_min:\n                                centroid = new_embeddings_normalized[mask].mean(axis=0).astype(np.float32)\n                                if not np.isfinite(centroid).all():\n                                    continue\n                                centroid = np.clip(centroid, -100.0, 100.0)\n                                \n                                c_vec = torch.from_numpy(centroid)\n                                if c_vec.numel() != self.embed_dim:\n                                    continue\n                                \n                                if self.use_cosine_distance:\n                                    c_vec = F.normalize(c_vec, p=2, dim=0, eps=1e-9)\n                                \n                                centroids.append(c_vec)\n                                counts.append(cluster_size)\n                                times.append(time.time())\n                                protos_added += 1\n                        \n                        if centroids:\n                            if len(centroids) > self.max_protos:\n                                order = np.argsort(counts)[::-1][:self.max_protos]\n                                centroids = [centroids[i] for i in order]\n                                counts = [counts[i] for i in order]\n                                times = [times[i] for i in order]\n                            \n                            store.centroids = centroids\n                            store.counts = counts\n                            store.creation_time = times\n                            \n                            try:\n                                store.labels = torch.tensor(labels)\n                            except Exception:\n                                store.labels = None\n                except Exception:\n                    protos_added = 0\n            \n            if protos_added == 0 and HAS_CLUSTERING and new_embeddings_normalized.shape[0] >= 2:\n                try:\n                    condensed = pdist(new_embeddings_normalized, metric='euclidean')\n                    condensed = np.clip(condensed, 0.0, 100.0)\n                    \n                    if condensed.size > 0:\n                        Z = linkage(condensed, method='average')\n                        max_dist = float(np.clip(np.max(condensed), 0.0, 100.0)) if condensed.size > 0 else 1.0\n                        absolute_threshold = float(self.dispersion_threshold * max_dist)\n                        \n                        clusters = fcluster(Z, t=absolute_threshold, criterion='distance') - 1\n                        \n                        if clusters.size > 0:\n                            new_centroids = []\n                            new_counts = []\n                            new_times = []\n                            for c_id in range(int(clusters.max()) + 1):\n                                mask = clusters == c_id\n                                cluster_size = int(mask.sum())\n                                if cluster_size >= self.n_min:\n                                    centroid = new_embeddings_normalized[mask].mean(axis=0).astype(np.float32)\n                                    if not np.isfinite(centroid).all():\n                                        continue\n                                    centroid = np.clip(centroid, -100.0, 100.0)\n                                    \n                                    c_vec = torch.from_numpy(centroid)\n                                    if c_vec.numel() != self.embed_dim:\n                                        continue\n                                    \n                                    if self.use_cosine_distance:\n                                        c_vec = F.normalize(c_vec, p=2, dim=0, eps=1e-9)\n                                    \n                                    new_centroids.append(c_vec)\n                                    new_counts.append(cluster_size)\n                                    new_times.append(time.time())\n                            \n                            if new_centroids:\n                                if len(new_centroids) > self.max_protos:\n                                    order = np.argsort(new_counts)[::-1][:self.max_protos]\n                                    new_centroids = [new_centroids[i] for i in order]\n                                    new_counts = [new_counts[i] for i in order]\n                                    new_times = [new_times[i] for i in order]\n                                \n                                store.centroids = new_centroids\n                                store.counts = new_counts\n                                store.creation_time = new_times\n                                \n                                try:\n                                    store.labels = torch.tensor(clusters)\n                                except Exception:\n                                    store.labels = None\n                                \n                                protos_added = len(new_centroids)\n                except Exception:\n                    protos_added = 0\n            \n            if protos_added > 0:\n                try:\n                    counts = store.counts if store.counts else [1] * len(store.centroids)\n                    total_count = int(sum(counts))\n                    mean_count = float(total_count / max(1, len(counts)))\n                    \n                    self.cluster_stats[str(token_type)] = {\n                        \"num_prototypes\": len(store.centroids),\n                        \"counts\": [int(c) for c in counts],\n                        \"total_samples\": total_count,\n                        \"mean_count\": mean_count,\n                        \"mu\": float(store.mu),\n                        \"tau\": float(store.tau),\n                    }\n                except Exception:\n                    pass\n            \n            return store.size() > 0\n        except Exception:\n            return False\n    \n    def get_explanations(self, threshold_span: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for token_type, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({\"token\": str(token_type), \"protos\": store.size()})\n        return expl\n\n\nprint(\"=\" * 80)\nprint(\"CELL3: DSCD MODULE - NaN/Inf HARDENED - COMPLETE\")\nprint(\"=\" * 80)\n","metadata":{"id":"L25pcKUPH4J2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - NaN/Inf GRADIENT HARDENED\n# ==============================================================================\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threading\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH) if int(MAX_LENGTH) > 0 else 48\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n        if _GRL_ALPHA_STEPS <= 0:\n            _GRL_ALPHA_STEPS = 10000\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.01\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = max(0.01, min(10.0, float(alpha)))\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not torch.isfinite(grad_output).all():\n            print(f\"❌ GRL: Received NaN/Inf gradient input\")\n            return torch.zeros_like(grad_output), None\n        \n        reversed_grad = -ctx.alpha * grad_output\n        reversed_grad = torch.clamp(reversed_grad, min=-10.0, max=10.0)\n        \n        if not torch.isfinite(reversed_grad).all():\n            print(f\"❌ GRL: Produced NaN/Inf gradient output\")\n            return torch.zeros_like(grad_output), None\n        \n        return reversed_grad, None\n\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    alpha = max(0.01, min(10.0, float(alpha)))\n    return GradientReversalFunction.apply(x, alpha)\n\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        input_dim = max(1, int(input_dim))\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=0.5)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not torch.isfinite(x).all():\n            print(f\"❌ LightweightDiscriminator: Input contains NaN/Inf\")\n            x = torch.where(torch.isfinite(x), x, torch.zeros_like(x))\n        \n        logits = self.classifier(x)\n        logits = torch.clamp(logits, min=-10.0, max=10.0)\n        \n        if not torch.isfinite(logits).all():\n            print(f\"❌ LightweightDiscriminator: Output contains NaN/Inf\")\n            logits = torch.where(torch.isfinite(logits), logits, torch.zeros_like(logits))\n        \n        return logits\n\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        input_dim = max(1, int(input_dim))\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 2),\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=0.5)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not torch.isfinite(x).all():\n            print(f\"❌ DomainDiscriminator: Input contains NaN/Inf\")\n            x = torch.where(torch.isfinite(x), x, torch.zeros_like(x))\n        \n        logits = self.classifier(x)\n        logits = torch.clamp(logits, min=-10.0, max=10.0)\n        \n        if not torch.isfinite(logits).all():\n            print(f\"❌ DomainDiscriminator: Output contains NaN/Inf\")\n            logits = torch.where(torch.isfinite(logits), logits, torch.zeros_like(logits))\n        \n        return logits\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 1000,\n        encoder_grl_scale: float = 0.5,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = max(1, int(embed_dim))\n\n        self.bn_source = nn.BatchNorm1d(\n            self.embed_dim, \n            eps=1e-3,\n            momentum=0.1,\n            affine=True,\n            track_running_stats=True\n        )\n        self.bn_target = nn.BatchNorm1d(\n            self.embed_dim, \n            eps=1e-3,\n            momentum=0.1,\n            affine=True,\n            track_running_stats=True\n        )\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n        self.freq_threshold = max(0.0, min(1.0, float(freq_threshold)))\n        self.uncertainty_threshold = max(0.0, min(1.0, float(uncertainty_threshold)))\n        self.gate_threshold = max(0.0, min(1.0, float(gate_threshold)))\n        self.warmup_steps = max(0, int(warmup_steps))\n        self.current_step = 0\n        self.lambda_base = {\"freq\": 0.5, \"ctx\": 0.3, \"xl\": 0.4, \"domain\": 0.5}\n        self.lambda_max = 1.0\n        self.encoder_grl_scale = max(0.1, min(1.0, float(encoder_grl_scale)))\n        self.stats_reset_interval = 10000\n\n        self.correct_domain = 0\n        self.correct_source = 0\n        self.correct_target = 0\n        self.total_samples = 0\n        self.total_source = 0\n        self.total_target = 0\n        self.domain_loss_accumulator = 0.0\n        self.asbn_loss_accumulator = 0.0\n        self._stats_lock = threading.Lock()\n        self.parse_errors = 0\n        self.last_parse_error_log = 0.0\n\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n        try:\n            if tokenizer is not None and _has_get_tokenizer_special_tokens:\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            elif tokenizer is not None:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"=\" * 80)\n            print(\"[ASBN-INIT] MemoryEfficientASBNModule initialized - NaN/Inf PROTECTED\")\n            print(\"=\" * 80)\n            print(f\"  embed_dim={self.embed_dim}, warmup_steps={self.warmup_steps}\")\n            print(f\"  encoder_grl_scale={self.encoder_grl_scale}\")\n            print(f\"  BatchNorm eps=1e-3 (hardened for float32)\")\n            print(f\"  GRL alpha range: [{_GRL_ALPHA_START:.3f}, {_GRL_ALPHA_END:.3f}]\")\n            print(f\"  stats_reset_interval={self.stats_reset_interval}\")\n            print(\"=\" * 80)\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(max(1, _GRL_ALPHA_STEPS)))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(max(1, _GRL_ALPHA_STEPS)))\n            denom = _GRL_ALPHA_START if abs(_GRL_ALPHA_START) > 1e-6 else 1e-6\n            ratio = _GRL_ALPHA_END / denom\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n        return max(0.01, min(10.0, float(alpha)))\n\n    def get_detailed_stats(self) -> Dict[str, Any]:\n        with self._stats_lock:\n            if self.total_samples == 0:\n                return {\n                    \"domain_loss\": None,\n                    \"domain_accuracy\": None,\n                    \"source_accuracy\": None,\n                    \"target_accuracy\": None,\n                    \"asbn_loss\": None,\n                    \"num_updates\": 0,\n                }\n            domain_acc = (self.correct_domain / self.total_samples) if self.total_samples > 0 else 0.0\n            source_acc = (self.correct_source / self.total_source) if self.total_source > 0 else 0.0\n            target_acc = (self.correct_target / self.total_target) if self.total_target > 0 else 0.0\n            avg_domain_loss = (self.domain_loss_accumulator / self.total_samples) if self.total_samples > 0 else 0.0\n            avg_asbn_loss = (self.asbn_loss_accumulator / self.total_samples) if self.total_samples > 0 else 0.0\n            return {\n                \"domain_loss\": float(avg_domain_loss),\n                \"domain_accuracy\": float(domain_acc),\n                \"source_accuracy\": float(source_acc),\n                \"target_accuracy\": float(target_acc),\n                \"asbn_loss\": float(avg_asbn_loss),\n                \"num_updates\": int(self.total_samples),\n            }\n\n    def get_asbn_stats(self) -> Dict[str, Any]:\n        return self.get_detailed_stats()\n\n    def reset_stats(self) -> None:\n        with self._stats_lock:\n            self.correct_domain = 0\n            self.correct_source = 0\n            self.correct_target = 0\n            self.total_samples = 0\n            self.total_source = 0\n            self.total_target = 0\n            self.domain_loss_accumulator = 0.0\n            self.asbn_loss_accumulator = 0.0\n            self.stats = {k: 0.0 for k in self.stats}\n\n    def critic_parameters(self):\n        return list(self.d_domain.parameters()) + list(self.d_freq.parameters()) + list(self.d_ctx.parameters()) + list(self.d_xl.parameters())\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (self.d_domain, self.d_freq, self.d_ctx, self.d_xl, self.bn_source, self.bn_target):\n                try:\n                    mod.to(device)\n                    if self.training:\n                        mod.train()\n                except Exception as e:\n                    if _VERBOSE_LOGGING:\n                        print(f\"❌ ASBN: Failed to move {mod.__class__.__name__} to {device}: {e}\")\n        except Exception as e:\n            print(f\"❌ ASBN: _ensure_discriminators_on_device failed: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        batch_size = max(1, int(batch_size))\n        seq_len = max(1, int(seq_len))\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n\n            if isinstance(proto_probs, torch.Tensor):\n                p = proto_probs.detach().to(device)\n                if p.dim() == 3:\n                    B, T, K = p.shape\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    vals = p[:b_max, :t_max, :].max(dim=2)[0]\n                    pmax[:b_max, :t_max] = torch.where(torch.isfinite(vals), vals, torch.full_like(vals, 0.5))\n                    return pmax\n                if p.dim() == 2:\n                    if p.size(0) == batch_size and p.size(1) == seq_len:\n                        pmax[:, :] = torch.where(torch.isfinite(p.float()), p.float(), pmax)\n                        return pmax\n                    if batch_size == 1:\n                        vals = p.max(dim=1)[0]\n                        t_max = min(seq_len, vals.size(0))\n                        pmax[0, :t_max] = torch.where(torch.isfinite(vals[:t_max]), vals[:t_max], pmax[0, :t_max])\n                        return pmax\n\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if row is None:\n                            continue\n                        if isinstance(row, torch.Tensor):\n                            r = row.detach().to(device)\n                            if r.dim() == 2:\n                                t_max = min(seq_len, r.size(0))\n                                vals = r[:t_max, :].max(dim=1)[0]\n                                pmax[b, :t_max] = torch.where(torch.isfinite(vals), vals, pmax[b, :t_max])\n                                continue\n                        if isinstance(row, (list, tuple, np.ndarray)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        arr = val.detach().cpu().numpy().astype(np.float32).ravel()\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32).ravel()\n                                    if arr.size == 0:\n                                        continue\n                                    pmax_val = float(np.nanmax(arr))\n                                    if np.isfinite(pmax_val):\n                                        pmax[b, t] = pmax_val\n                                except Exception as e:\n                                    import time\n                                    now = time.time()\n                                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and (now - self.last_parse_error_log) > 60.0:\n                                        print(f\"❌ ASBN: proto_probs parse error at b={b},t={t}: {type(e).__name__}\")\n                                        self.last_parse_error_log = now\n                                    self.parse_errors += 1\n                                    continue\n                    return pmax\n\n                if batch_size == 1 and len(proto_probs) >= 1:\n                    row = proto_probs\n                    for t in range(min(seq_len, len(row))):\n                        try:\n                            val = row[t]\n                            if isinstance(val, torch.Tensor):\n                                arr = val.detach().cpu().numpy().astype(np.float32).ravel()\n                            else:\n                                arr = np.asarray(val, dtype=np.float32).ravel()\n                            if arr.size == 0:\n                                continue\n                            pmax_val = float(np.nanmax(arr))\n                            if np.isfinite(pmax_val):\n                                pmax[0, t] = pmax_val\n                        except Exception as e:\n                            import time\n                            now = time.time()\n                            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and (now - self.last_parse_error_log) > 60.0:\n                                print(f\"❌ ASBN: proto_probs parse error at t={t}: {type(e).__name__}\")\n                                self.last_parse_error_log = now\n                            self.parse_errors += 1\n                            continue\n                    return pmax\n        except Exception as e:\n            print(f\"❌ ASBN: _parse_proto_probs_matrix exception: {type(e).__name__}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device, default: float = 0.0) -> torch.Tensor:\n        batch_size = max(1, int(batch_size))\n        seq_len = max(1, int(seq_len))\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n\n            if isinstance(mat, torch.Tensor):\n                m = mat.detach().to(device)\n                if m.dim() == 3:\n                    B, T, C = m.shape\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    try:\n                        out[:b_max, :t_max] = m[:b_max, :t_max, 0].float()\n                    except Exception:\n                        out[:b_max, :t_max] = m[:b_max, :t_max].float()\n                    return out\n                if m.dim() == 2:\n                    if m.size(0) == batch_size and m.size(1) >= 1:\n                        t_max = min(seq_len, m.size(1))\n                        out[:, :t_max] = m[:, :t_max].float()\n                        return out\n                    if batch_size == 1:\n                        t_max = min(seq_len, m.size(0))\n                        out[0, :t_max] = m[:t_max, 0].float() if m.dim() > 1 else m[:t_max].float()\n                        return out\n                if m.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, m.size(0))\n                    out[0, :t_max] = m[:t_max].float()\n                    return out\n\n            if isinstance(mat, (list, tuple, np.ndarray)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if row is None:\n                            continue\n                        if isinstance(row, torch.Tensor):\n                            r = row.detach().to(device).float()\n                            t_max = min(seq_len, r.size(0))\n                            out[b, :t_max] = r[:t_max]\n                            continue\n                        if isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    if isinstance(v, torch.Tensor):\n                                        out[b, t] = float(v.item())\n                                    else:\n                                        out[b, t] = float(v)\n                                except Exception as e:\n                                    import time\n                                    now = time.time()\n                                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and (now - self.last_parse_error_log) > 60.0:\n                                        print(f\"❌ ASBN: scalar_matrix parse error at b={b},t={t}: {type(e).__name__}\")\n                                        self.last_parse_error_log = now\n                                    self.parse_errors += 1\n                                    out[b, t] = float(default)\n                            continue\n                if batch_size == 1 and len(mat) > 0:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                out[0, t] = float(v.item())\n                            else:\n                                out[0, t] = float(v)\n                        except Exception as e:\n                            import time\n                            now = time.time()\n                            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and (now - self.last_parse_error_log) > 60.0:\n                                print(f\"❌ ASBN: scalar_matrix parse error at t={t}: {type(e).__name__}\")\n                                self.last_parse_error_log = now\n                            self.parse_errors += 1\n                            out[0, t] = float(default)\n                    return out\n        except Exception as e:\n            print(f\"❌ ASBN: _parse_scalar_matrix exception: {type(e).__name__}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor, gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        try:\n            device = pmax.device if isinstance(pmax, torch.Tensor) else (uncertainty.device if isinstance(uncertainty, torch.Tensor) else torch.device(\"cpu\"))\n            p = pmax.clone().detach().to(device) if isinstance(pmax, torch.Tensor) else torch.tensor(pmax, device=device)\n            u = uncertainty.clone().detach().to(device) if isinstance(uncertainty, torch.Tensor) else torch.tensor(uncertainty, device=device)\n            g = gate.clone().detach().to(device) if isinstance(gate, torch.Tensor) else torch.tensor(gate, device=device)\n\n            p = torch.clamp(p, 0.0, 1.0)\n            u = torch.clamp(u, 0.0, 1.0)\n            g = torch.clamp(g, 0.0, 1.0)\n\n            base = float(self.lambda_base.get(lambda_type, 0.2))\n            lam = base * (1.0 - p + 0.05) * (u + 0.05) * (g + 0.05)\n            lam = torch.clamp(lam, min=0.01, max=min(1.0, float(self.lambda_max)))\n            lam = torch.where(torch.isfinite(lam), lam, torch.full_like(lam, 0.1))\n            return lam\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"❌ ASBN: compute_lambda_scaled_tensor failed: {e}\")\n            try:\n                return torch.full_like(pmax if isinstance(pmax, torch.Tensor) else torch.tensor(pmax), 0.1)\n            except Exception:\n                return torch.tensor(0.1)\n\n    def forward(self, h: torch.Tensor, domain_labels: Optional[torch.Tensor] = None, global_step: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor, float]:\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            return h, torch.tensor(0.0, device=dev), 0.0\n\n        if global_step is not None:\n            self.current_step = max(0, int(global_step))\n\n        B, T, H = h.size()\n        device = h.device\n        domain_accuracy = 0.0\n        try:\n            self._ensure_discriminators_on_device(device)\n            h_flat = h.view(B * T, H)\n\n            if domain_labels is not None:\n                try:\n                    domain_labels = domain_labels.to(device).long()\n                except Exception:\n                    try:\n                        domain_labels = domain_labels.long()\n                        domain_labels = domain_labels.to(device)\n                    except Exception as e:\n                        print(f\"❌ ASBN: domain_labels conversion failed: {e}\")\n                        domain_labels = torch.ones((B,), dtype=torch.long, device=device)\n                if domain_labels.dim() == 0:\n                    domain_labels = domain_labels.unsqueeze(0).expand(B)\n                elif domain_labels.numel() == 1 and B > 1:\n                    domain_labels = domain_labels.view(1).expand(B).contiguous()\n                elif domain_labels.size(0) != B:\n                    domain_labels = domain_labels[:B] if domain_labels.size(0) > B else domain_labels[0].unsqueeze(0).expand(B)\n\n            domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1) if domain_labels is not None else torch.ones(B * T, dtype=torch.long, device=device)\n\n            source_mask = domain_expanded == 0\n            target_mask = domain_expanded == 1\n            h_normalized = h_flat.clone()\n\n            try:\n                source_count = int(source_mask.sum().item())\n                if source_count >= 2:\n                    src_idx = source_mask.nonzero(as_tuple=True)[0]\n                    h_src = h_flat[src_idx]\n                    if not torch.isfinite(h_src).all():\n                        print(f\"❌ ASBN: Source embeddings contain NaN/Inf before BN\")\n                        h_src = torch.where(torch.isfinite(h_src), h_src, torch.zeros_like(h_src))\n                    h_src_bn = self.bn_source(h_src)\n                    if not torch.isfinite(h_src_bn).all():\n                        print(f\"❌ ASBN: BN_source produced NaN/Inf\")\n                        h_src_bn = h_src\n                    h_normalized[src_idx] = h_src_bn\n                elif source_count == 1:\n                    src_idx = source_mask.nonzero(as_tuple=True)[0]\n                    h_normalized[src_idx] = h_flat[src_idx]\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"❌ ASBN: bn_source failed: {type(e).__name__}\")\n\n            try:\n                target_count = int(target_mask.sum().item())\n                if target_count >= 2:\n                    tgt_idx = target_mask.nonzero(as_tuple=True)[0]\n                    h_tgt = h_flat[tgt_idx]\n                    if not torch.isfinite(h_tgt).all():\n                        print(f\"❌ ASBN: Target embeddings contain NaN/Inf before BN\")\n                        h_tgt = torch.where(torch.isfinite(h_tgt), h_tgt, torch.zeros_like(h_tgt))\n                    h_tgt_bn = self.bn_target(h_tgt)\n                    if not torch.isfinite(h_tgt_bn).all():\n                        print(f\"❌ ASBN: BN_target produced NaN/Inf\")\n                        h_tgt_bn = h_tgt\n                    h_normalized[tgt_idx] = h_tgt_bn\n                elif target_count == 1:\n                    tgt_idx = target_mask.nonzero(as_tuple=True)[0]\n                    h_normalized[tgt_idx] = h_flat[tgt_idx]\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"❌ ASBN: bn_target failed: {type(e).__name__}\")\n\n            h_out = h_normalized.view(B, T, H)\n            domain_loss = torch.tensor(0.0, device=device)\n\n            if self.training and _ENABLE_ASBN_TRAINING and self.current_step >= self.warmup_steps:\n                if domain_labels is not None:\n                    try:\n                        grl_alpha = self.get_grl_alpha(self.current_step)\n                        valid_indices = torch.arange(B * T, device=device)\n                        sel_emb = h_normalized[valid_indices]\n                        sel_labels = domain_expanded[valid_indices]\n                        if sel_emb.size(0) > 0:\n                            domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                            domain_logits = self.d_domain(domain_input).to(device)\n                            if not torch.isfinite(domain_logits).all():\n                                print(f\"❌ ASBN: domain_logits contains NaN/Inf\")\n                                domain_logits = torch.where(torch.isfinite(domain_logits), domain_logits, torch.zeros_like(domain_logits))\n                            domain_loss = F.cross_entropy(domain_logits, sel_labels)\n                            domain_loss = torch.clamp(domain_loss, min=0.0, max=10.0)\n                            if not torch.isfinite(domain_loss):\n                                print(f\"❌ ASBN: domain_loss is NaN/Inf: {domain_loss}\")\n                                domain_loss = torch.tensor(0.0, device=device)\n                            else:\n                                with torch.no_grad():\n                                    domain_preds = torch.argmax(domain_logits, dim=1)\n                                    correct = int((domain_preds == sel_labels).sum().item())\n                                    domain_accuracy = float((domain_preds == sel_labels).float().mean().item())\n                                    source_mask_sel = sel_labels == 0\n                                    target_mask_sel = sel_labels == 1\n                                    with self._stats_lock:\n                                        self.correct_domain += correct\n                                        self.total_samples += int(sel_labels.size(0))\n                                        self.domain_loss_accumulator += float(domain_loss.item()) * int(sel_labels.size(0))\n                                        self.asbn_loss_accumulator += float(domain_loss.item()) * int(sel_labels.size(0))\n                                        if source_mask_sel.any():\n                                            source_correct = int((domain_preds[source_mask_sel] == sel_labels[source_mask_sel]).sum().item())\n                                            self.correct_source += source_correct\n                                            self.total_source += int(source_mask_sel.sum().item())\n                                        if target_mask_sel.any():\n                                            target_correct = int((domain_preds[target_mask_sel] == sel_labels[target_mask_sel]).sum().item())\n                                            self.correct_target += target_correct\n                                            self.total_target += int(target_mask_sel.sum().item())\n                                        if self.total_samples >= self.stats_reset_interval:\n                                            if _DEBUG_DISCOVERY:\n                                                stats = self.get_detailed_stats()\n                                                print(f\"[ASBN-STATS] Resetting after {stats['num_updates']} samples: domain_loss={stats['domain_loss']:.4f}, domain_acc={stats['domain_accuracy']:.2%}\")\n                                            self.reset_stats()\n                    except Exception as e:\n                        print(f\"❌ ASBN: Domain loss computation failed: {type(e).__name__}: {e}\")\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        domain_loss = torch.tensor(0.0, device=device)\n                else:\n                    if self.current_step % 100 == 0 and _VERBOSE_LOGGING:\n                        print(f\"⚠️  ASBN: domain_labels is None at step {self.current_step}\")\n            if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n                print(f\"[ASBN] BN applied: src={int(source_mask.sum())}, tgt={int(target_mask.sum())}\")\n            return h_out, domain_loss, domain_accuracy\n        except Exception as e:\n            print(f\"❌ ASBN: forward failed: {type(e).__name__}: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            return h, torch.tensor(0.0, device=device), 0.0\n\n    def forward_with_grl_simplified(self, h: torch.Tensor, proto_probs: Any, uncertainties: Any, gates: Any, token_word_map: Optional[List[Dict[int, str]]] = None, domain_labels: Optional[torch.Tensor] = None, global_step: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        if global_step is not None:\n            self.current_step = max(0, int(global_step))\n        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n        \n        if self.current_step < self.warmup_steps:\n            if self.current_step % 100 == 0 and (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n                print(f\"⚠️  ASBN BLOCKED BY WARMUP: step {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n            \n        if not self.training:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n            \n        if not _ENABLE_ASBN_TRAINING:\n            if self.current_step % 100 == 0 and (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n                print(f\"⚠️  ASBN TRAINING DISABLED: ENABLE_ASBN_TRAINING={_ENABLE_ASBN_TRAINING}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n            \n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            if _VERBOSE_LOGGING:\n                print(f\"❌ ASBN: Invalid input h: type={type(h)}, shape={h.shape if isinstance(h, torch.Tensor) else 'N/A'}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n\n        device = h.device\n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n        B, T, H = h.size()\n\n        if domain_labels is not None:\n            try:\n                domain_labels = domain_labels.to(device).long()\n            except Exception:\n                try:\n                    domain_labels = domain_labels.long().to(device)\n                except Exception as e:\n                    print(f\"❌ ASBN-GRL: domain_labels conversion failed: {e}\")\n                    domain_labels = torch.ones((B,), dtype=torch.long, device=device)\n            if domain_labels.dim() == 0:\n                domain_labels = domain_labels.unsqueeze(0).expand(B)\n            elif domain_labels.numel() == 1 and B > 1:\n                domain_labels = domain_labels.view(1).expand(B).contiguous()\n            elif domain_labels.size(0) != B:\n                domain_labels = domain_labels[:B] if domain_labels.size(0) > B else domain_labels[0].unsqueeze(0).expand(B)\n        else:\n            if self.current_step % 100 == 0 and (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n                print(f\"⚠️  ASBN-GRL: domain_labels is None at step {self.current_step}\")\n\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.5)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n        pmax_mat = torch.clamp(pmax_mat, 0.0, 1.0)\n        U_mat = torch.clamp(U_mat, 0.0, 1.0)\n        G_mat = torch.clamp(G_mat, 0.0, 1.0)\n\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                if (not token_str) or (isinstance(token_str, str) and len(token_str.strip()) == 0) or (token_str in self.special_tokens):\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"❌ ASBN-GRL: token_word_map filtering failed: {e}\")\n\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=True)[0]\n        if sel_idx.numel() == 0:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.current_step % 100 == 0:\n                print(\"⚠️  ASBN-GRL: No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n\n        seq_len_feature = float(T) / float(max(int(_MAX_LENGTH), 1))\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n\n        grl_alpha = self.get_grl_alpha(global_step)\n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n\n        freq_logits = self.d_freq(freq_input_grl).to(device)\n        ctx_logits = self.d_ctx(ctx_input_grl).to(device)\n        xl_logits = self.d_xl(xl_input_grl).to(device)\n\n        freq_label = (pmax_flat >= self.freq_threshold).long().to(device)\n        ctx_label = (U_flat <= self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat >= self.gate_threshold).long().to(device)\n\n        if freq_logits.size(0) == 0 or freq_label.size(0) == 0:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.current_step % 100 == 0:\n                print(\"⚠️  ASBN-GRL: Empty logits or labels\")\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n\n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n\n        loss_freq = torch.clamp(loss_freq, min=0.0, max=10.0)\n        loss_ctx = torch.clamp(loss_ctx, min=0.0, max=10.0)\n        loss_xl = torch.clamp(loss_xl, min=0.0, max=10.0)\n\n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        weighted = torch.clamp(weighted, min=0.0, max=10.0)\n        mean_weighted = torch.mean(weighted) if weighted.numel() > 0 else torch.tensor(0.0, device=device)\n\n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n\n        if domain_labels is not None:\n            try:\n                batch_indices = sel_idx // T\n                batch_indices = torch.clamp(batch_indices, 0, B - 1)\n                domain_flat = domain_labels[batch_indices].to(device).long() if domain_labels.numel() > 0 else torch.tensor([], dtype=torch.long, device=device)\n\n                if domain_flat.numel() > 0:\n                    domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                    domain_logits = self.d_domain(domain_input).to(device)\n                    domain_loss = F.cross_entropy(domain_logits, domain_flat)\n                    domain_loss = torch.clamp(domain_loss, min=0.0, max=10.0)\n                    if not torch.isfinite(domain_loss):\n                        print(f\"❌ ASBN-GRL: domain_loss is NaN/Inf: {domain_loss}\")\n                        domain_loss = torch.tensor(0.0, device=device)\n                    else:\n                        with torch.no_grad():\n                            domain_preds = torch.argmax(domain_logits, dim=1)\n                            correct = int((domain_preds == domain_flat).sum().item())\n                            domain_accuracy = torch.tensor(float((domain_preds == domain_flat).float().mean().item()), device=device) if domain_flat.numel() > 0 else torch.tensor(0.0, device=device)\n                            source_mask = domain_flat == 0\n                            target_mask = domain_flat == 1\n                            with self._stats_lock:\n                                self.correct_domain += correct\n                                self.total_samples += int(domain_flat.size(0))\n                                self.domain_loss_accumulator += float(domain_loss.item()) * int(domain_flat.size(0))\n                                if source_mask.any():\n                                    source_correct = int((domain_preds[source_mask] == domain_flat[source_mask]).sum().item())\n                                    self.correct_source += source_correct\n                                    self.total_source += int(source_mask.sum().item())\n                                if target_mask.any():\n                                    target_correct = int((domain_preds[target_mask] == domain_flat[target_mask]).sum().item())\n                                    self.correct_target += target_correct\n                                    self.total_target += int(target_mask.sum().item())\n            except Exception as e:\n                print(f\"❌ ASBN-GRL: domain classification failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n\n        encoder_loss = self.encoder_grl_scale * (mean_weighted + domain_loss)\n        encoder_loss = torch.clamp(encoder_loss, min=0.0, max=10.0)\n\n        if not torch.isfinite(encoder_loss):\n            print(f\"❌ ASBN-GRL: encoder_loss is NaN/Inf: {encoder_loss}\")\n            encoder_loss = torch.tensor(0.0, device=device)\n\n        try:\n            with self._stats_lock:\n                self.asbn_loss_accumulator += float(encoder_loss.item()) * (sel_emb.size(0) if isinstance(sel_emb, torch.Tensor) else 1)\n                if self.total_samples >= self.stats_reset_interval and _DEBUG_DISCOVERY:\n                    stats = self.get_detailed_stats()\n                    if stats['num_updates'] > 0:\n                        print(f\"[ASBN-GRL-STATS] Resetting after {stats['num_updates']} samples: domain_loss={stats['domain_loss']:.4f}, domain_acc={stats['domain_accuracy']:.2%}\")\n                    self.reset_stats()\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"❌ ASBN-GRL: Stats update failed: {e}\")\n\n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            try:\n                print(f\"[ASBN-STEP-{self.current_step}] GRL alpha={grl_alpha:.3f}, encoder_loss={float(encoder_loss.item()):.4f}, mean_weighted={float(mean_weighted.item()):.4f}, domain_loss={float(domain_loss.item()):.4f}\")\n            except Exception:\n                pass\n\n        return encoder_loss, mean_weighted, domain_loss, domain_accuracy\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        try:\n            device = next(self.parameters()).device\n        except Exception:\n            device = torch.device(\"cpu\")\n        h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n        domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n        self.train()\n        self.current_step = self.warmup_steps + 1\n        h_out, domain_loss, domain_acc = self.forward(h, domain_labels, global_step=self.current_step)\n        if not (isinstance(h_out, torch.Tensor) and h_out.shape == h.shape):\n            print(\"❌ ASBN test: forward shape mismatch\")\n            return False\n        if not (isinstance(domain_loss, torch.Tensor) and domain_loss.item() >= 0.0):\n            print(\"❌ ASBN test: domain_loss invalid\")\n            return False\n        if not (isinstance(domain_acc, float) and 0.0 <= domain_acc <= 1.0):\n            print(\"❌ ASBN test: domain_accuracy invalid\")\n            return False\n        proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n        uncertainties = torch.rand(batch_size, seq_len, device=device)\n        gates = torch.rand(batch_size, seq_len, device=device)\n        enc_loss, adv_loss, dom_loss, dom_acc = self.forward_with_grl_simplified(h, proto_probs, uncertainties, gates, domain_labels=domain_labels, global_step=self.current_step)\n        if not (isinstance(enc_loss, torch.Tensor) and enc_loss.item() >= 0.0):\n            print(\"❌ ASBN test: encoder loss invalid\")\n            return False\n        if not (isinstance(dom_acc, torch.Tensor) and 0.0 <= dom_acc.item() <= 1.0):\n            print(\"❌ ASBN test: domain accuracy from GRL invalid\")\n            return False\n        print(\"✅ ASBN test: All checks passed\")\n        return True\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN module loaded - NaN/Inf GRADIENT HARDENED\")\nprint(\"=\" * 80)\nprint(\"Features:\")\nprint(\"  - BatchNorm1d eps=1e-3 (increased from 1e-5)\")\nprint(\"  - GRL gradient magnitude bounds: [-10, +10]\")\nprint(\"  - GRL alpha minimum: 0.01 (prevents vanishing)\")\nprint(\"  - Discriminator logit clamping: [-10, +10]\")\nprint(\"  - Loss component clamping: [0, 10]\")\nprint(\"  - Xavier init gain=0.5 (conservative)\")\nprint(\"  - Lambda max reduced to 1.0 (from 2.0)\")\nprint(\"  - Dropout reduced to 0.2 (from 0.3)\")\nprint(\"=\" * 80)\n","metadata":{"id":"XrNq18UsH4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG (TRANSLATION RATIONALE GENERATION) - NaN/Inf HARDENED\n# ==============================================================================\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque, defaultdict\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport threading\nimport time\n\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\n    if _TRG_EVIDENCE_K <= 0:\n        _TRG_EVIDENCE_K = 3\nexcept Exception:\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\n    if _TRG_GEN_EMBED <= 0:\n        _TRG_GEN_EMBED = 64\nexcept Exception:\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\n    if _MAX_SILVER_BUFFER <= 0:\n        _MAX_SILVER_BUFFER = 50\nexcept Exception:\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept Exception:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _ENABLE_TRG_TRAINING = bool(globals().get(\"ENABLE_TRG_TRAINING\", True))\nexcept Exception:\n    _ENABLE_TRG_TRAINING = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TAU_HIGH = float(TAU_HIGH)\n    if _TAU_HIGH < 0 or _TAU_HIGH > 1:\n        _TAU_HIGH = 0.85\nexcept Exception:\n    _TAU_HIGH = 0.85\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\n    if _TAU_LOW < 0 or _TAU_LOW > 1:\n        _TAU_LOW = 0.25\nexcept Exception:\n    _TAU_LOW = 0.25\n\ntry:\n    _TAU_ACCEPT = float(TAU_ACCEPT)\n    if _TAU_ACCEPT < 0 or _TAU_ACCEPT > 1:\n        _TAU_ACCEPT = 0.80\nexcept Exception:\n    _TAU_ACCEPT = 0.80\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", _TAU_LOW))\n    if _TRG_UNCERTAINTY_THRESHOLD < 0 or _TRG_UNCERTAINTY_THRESHOLD > 1:\n        _TRG_UNCERTAINTY_THRESHOLD = _TAU_LOW\nexcept Exception:\n    _TRG_UNCERTAINTY_THRESHOLD = _TAU_LOW\n\ntry:\n    _TRG_SPAN_THRESHOLD = float(globals().get(\"TRG_SPAN_THRESHOLD\", globals().get(\"SPAN_THRESHOLD\", 0.05)))\n    if _TRG_SPAN_THRESHOLD < 0 or _TRG_SPAN_THRESHOLD > 1:\n        _TRG_SPAN_THRESHOLD = 0.05\nexcept Exception:\n    _TRG_SPAN_THRESHOLD = 0.05\n\ntry:\n    _TRG_TEMPERATURE = float(globals().get(\"TRG_TEMPERATURE\", 1.0))\n    if _TRG_TEMPERATURE <= 0:\n        _TRG_TEMPERATURE = 1.0\nexcept Exception:\n    _TRG_TEMPERATURE = 1.0\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = int(globals().get(\"MAX_EXPLANATIONS_PER_SENTENCE\", 10))\n    if _MAX_EXPLANATIONS_PER_SENTENCE <= 0:\n        _MAX_EXPLANATIONS_PER_SENTENCE = 10\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\n_TRG_PUNCT_SET = set(\".,;:!?\\\\\\\"\\\\'()-[]{}\\\\/\")\n\ndef _fallback_is_valid_token(token: Any, special_tokens: Set[str], tokenizer=None, language: str = \"bn\") -> bool:\n    if token is None:\n        return False\n    try:\n        token = str(token).strip()\n    except Exception:\n        return False\n    if not token or token in special_tokens:\n        return False\n    clean = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\").strip()\n    if len(clean) < 2:\n        return False\n    if not any(c.isalpha() for c in clean):\n        return False\n    if all(c in _TRG_PUNCT_SET for c in clean):\n        return False\n    if clean.isdigit():\n        return False\n    return True\n\ndef _is_word_start(raw_token: Any, token_word_map: Optional[dict], idx: int) -> bool:\n    try:\n        if not isinstance(raw_token, str):\n            raw_token = str(raw_token)\n        if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map:\n            w = token_word_map[idx]\n            if isinstance(w, str) and w.strip():\n                return True\n        if raw_token.startswith(\"▁\") or raw_token.startswith(\"Ġ\") or raw_token.startswith(\"\\u2581\"):\n            return True\n        clean = raw_token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\").strip()\n        if len(clean) < 2:\n            return False\n        if all(ch in _TRG_PUNCT_SET for ch in clean):\n            return False\n        if any(c.isalpha() for c in clean):\n            return True\n        return False\n    except Exception:\n        return False\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": \"Chose '{sense}' with high confidence ({confidence:.1%}) based on: '{evidence}'. {alternatives_text}\",\n            \"medium_confidence\": \"Selected '{sense}' with moderate confidence ({confidence:.1%}). Evidence: '{evidence}'. {alternatives_text}\",\n            \"low_confidence\": \"Uncertain; chose '{sense}' ({confidence:.1%}). Evidence: '{evidence}'. {alternatives_text} Review recommended.\",\n            \"fallback\": \"Token '{token}' analyzed. Context: '{evidence}'.\",\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n        token = str(evidence.get(\"token\", \"unknown\")).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\")\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n        confidence = max(0.0, min(1.0, confidence))\n        evidence_tokens = evidence.get(\"evidence_tokens\", []) or []\n        evidence_str = \", \".join([str(tok).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\") for tok in evidence_tokens[:_TRG_EVIDENCE_K]]) or \"limited context\"\n        alternatives = evidence.get(\"alternatives\", []) or []\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and alternatives:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), max(0.0, min(1.0, float(alt[1])))\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = \"Alternatives: \" + \", \".join(alt_parts) + \".\"\n        if confidence >= _TAU_ACCEPT:\n            key = \"high_confidence\"\n        elif confidence >= _TRG_UNCERTAINTY_THRESHOLD:\n            key = \"medium_confidence\"\n        else:\n            key = \"low_confidence\"\n        tpl = self.explanation_templates.get(key, self.explanation_templates[\"fallback\"])\n        try:\n            return tpl.format(sense=sense_name, confidence=confidence, evidence=evidence_str, alternatives_text=alternatives_text, token=token)\n        except Exception:\n            return f\"Token '{token}' -> '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n        self.extraction_failures = 0\n        self.last_failure_log = 0.0\n        try:\n            if tokenizer is not None and _has_get_tokenizer_special_tokens:\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            elif tokenizer is not None and _has_get_cached_special_tokens:\n                try:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                except Exception:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []) if tokenizer is not None else [])\n        except Exception:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(self, token_idx: int, span_start: int, span_end: int, tgt_preds: Any) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0 or span_end <= span_start:\n            return None\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n        seq_len = len(tgt_preds) if isinstance(tgt_preds, list) else int(tgt_preds.size(0))\n        if span_end > seq_len or token_idx >= seq_len:\n            return None\n        try:\n            evidence_tokens = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        try:\n                            evidence_tokens.append(str(tgt_preds[i].item()))\n                        except Exception:\n                            evidence_tokens.append(f\"token_{i}\")\n            return evidence_tokens if evidence_tokens else None\n        except Exception:\n            return None\n\n    def extract_evidence_efficiently(self, token_idx: int, tokens: List[str], dscd_outputs: Dict, token_word_map: Optional[dict] = None, decoder_attention: Optional[torch.Tensor] = None) -> Dict:\n        if not isinstance(tokens, list):\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() - self.last_failure_log > 60.0:\n                print(f\"⚠️  [TRG] extract_evidence_efficiently: tokens not a list (type={type(tokens)})\")\n                self.last_failure_log = time.time()\n            return self._create_fallback_evidence(token_idx if isinstance(token_idx, int) else 0, [])\n        if not isinstance(token_idx, int):\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() - self.last_failure_log > 60.0:\n                print(f\"⚠️  [TRG] extract_evidence_efficiently: token_idx not int (type={type(token_idx)})\")\n                self.last_failure_log = time.time()\n            return self._create_fallback_evidence(0, tokens)\n        if token_idx < 0 or token_idx >= len(tokens):\n            token_idx = max(0, min(token_idx, len(tokens) - 1)) if tokens else 0\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = _fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n        else:\n            is_valid = _fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n            uncertainty = max(0.0, min(1.0, float(uncertainty)))\n            gate = max(0.0, min(1.0, float(gate)))\n            span = max(0.0, min(1.0, float(span)))\n            evidence_tokens = None\n            if isinstance(decoder_attention, torch.Tensor):\n                try:\n                    att = decoder_attention\n                    if att.dim() == 4:\n                        att_mean = att.mean(dim=(0, 1))\n                    elif att.dim() == 3:\n                        att_mean = att.mean(dim=0)\n                    elif att.dim() == 2:\n                        att_mean = att\n                    else:\n                        att_mean = None\n                    if att_mean is not None and att_mean.dim() == 2 and token_idx < att_mean.size(0):\n                        vec = att_mean[token_idx]\n                        k = min(5, int(vec.numel()))\n                        if k > 0:\n                            top_k = torch.topk(vec, k=k).indices.cpu().numpy()\n                            evidence_tokens = [tokens[int(i)] for i in top_k if int(i) < len(tokens) and int(i) != token_idx]\n                except Exception as e:\n                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() - self.last_failure_log > 60.0:\n                        print(f\"⚠️  [TRG] Decoder attention extraction failed: {type(e).__name__}\")\n                        self.last_failure_log = time.time()\n                    evidence_tokens = None\n            if evidence_tokens is None:\n                evidence_tokens = self._extract_context_window(token_idx, tokens, token_word_map)\n            seen = set()\n            dedup = []\n            for t in (evidence_tokens or []):\n                if t not in seen:\n                    seen.add(t)\n                    dedup.append(t)\n            evidence_tokens = dedup[:_TRG_EVIDENCE_K]\n            top_senses = self._compute_sense_alternatives_fast(proto_probs, temperature=_TRG_TEMPERATURE)\n            chosen_sense = top_senses[0] if top_senses else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n            token_value = token_word_map[token_idx] if token_word_map and token_idx in token_word_map and isinstance(token_word_map[token_idx], str) and token_word_map[token_idx].strip() else raw_token\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n        except Exception as e:\n            self.extraction_failures += 1\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() - self.last_failure_log > 60.0:\n                print(f\"❌ [TRG] extract_evidence_efficiently failed: {type(e).__name__}: {e}\")\n                self.last_failure_log = time.time()\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _extract_context_window(self, token_idx: int, tokens: List[str], token_word_map: Optional[dict]) -> List[str]:\n        context_window = 3\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens = []\n        for i in range(start_idx, end_idx):\n            if i == token_idx or i >= len(tokens):\n                continue\n            rtok = tokens[i]\n            if not _is_word_start(rtok, token_word_map, i):\n                continue\n            if _has_is_valid_token:\n                try:\n                    ok = is_valid_token(rtok, self.special_tokens, self.tokenizer, language=self.language)\n                except Exception:\n                    ok = _fallback_is_valid_token(rtok, self.special_tokens, self.tokenizer, self.language)\n            else:\n                ok = _fallback_is_valid_token(rtok, self.special_tokens, self.tokenizer, self.language)\n            if ok:\n                if token_word_map and isinstance(token_word_map.get(i, \"\"), str) and token_word_map[i].strip():\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    clean = str(rtok).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\").strip()\n                    if clean:\n                        evidence_tokens.append(clean)\n        return evidence_tokens\n\n    def _safe_extract_proto_probs(self, token_idx: int, dscd_outputs: Dict) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all is None:\n                return torch.tensor([1.0], dtype=torch.float32)\n            if isinstance(pp_all, torch.Tensor):\n                p = pp_all.detach().cpu()\n                if not torch.isfinite(p).all():\n                    return torch.tensor([1.0], dtype=torch.float32)\n                if p.dim() == 3:\n                    B, T, K = p.shape\n                    row = p[0] if B > 0 else p\n                    if token_idx < row.shape[0]:\n                        vec = row[token_idx].flatten().float()\n                    else:\n                        vec = row.flatten().float()\n                elif p.dim() == 2:\n                    vec = p[token_idx].flatten().float() if token_idx < p.size(0) else p.flatten().float()\n                else:\n                    vec = p.flatten().float()\n                if vec.numel() == 0:\n                    return torch.tensor([1.0], dtype=torch.float32)\n                if not torch.isfinite(vec).all():\n                    return torch.tensor([1.0], dtype=torch.float32)\n                vec = torch.clamp(vec, min=0.0, max=1.0)\n                s = float(vec.sum().item()) if vec.numel() > 0 else 0.0\n                if s <= 1e-9:\n                    return torch.tensor([1.0], dtype=torch.float32)\n                return (vec / (s + 1e-9)).to(dtype=torch.float32)\n            if isinstance(pp_all, (list, tuple)):\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    r = row.detach().cpu().float()\n                    if not torch.isfinite(r).all():\n                        return torch.tensor([1.0], dtype=torch.float32)\n                    if r.dim() >= 1 and token_idx < r.size(0):\n                        vec = r[token_idx].flatten().float()\n                    else:\n                        vec = r.flatten().float()\n                    vec = torch.clamp(vec, min=0.0, max=1.0)\n                    s = float(vec.sum().item()) if vec.numel() > 0 else 0.0\n                    if s <= 1e-9:\n                        return torch.tensor([1.0], dtype=torch.float32)\n                    return (vec / (s + 1e-9)).to(dtype=torch.float32)\n                if isinstance(row, (list, tuple, np.ndarray)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            vec = val.detach().cpu().float().flatten()\n                        else:\n                            arr = np.asarray(val, dtype=np.float32).flatten()\n                            if arr.size == 0:\n                                return torch.tensor([1.0], dtype=torch.float32)\n                            if not np.isfinite(arr).all():\n                                return torch.tensor([1.0], dtype=torch.float32)\n                            vec = torch.from_numpy(arr).float()\n                        vec = torch.clamp(vec, min=0.0, max=1.0)\n                        s = float(vec.sum().item()) if vec.numel() > 0 else 0.0\n                        if s <= 1e-9:\n                            return torch.tensor([1.0], dtype=torch.float32)\n                        return (vec / (s + 1e-9)).to(dtype=torch.float32)\n                    maybe = row[0]\n                    if isinstance(maybe, torch.Tensor):\n                        vec = maybe.detach().cpu().float().flatten()\n                        if not torch.isfinite(vec).all():\n                            return torch.tensor([1.0], dtype=torch.float32)\n                        s = float(vec.sum().item()) if vec.numel() > 0 else 0.0\n                        if s <= 1e-9:\n                            return torch.tensor([1.0], dtype=torch.float32)\n                        return (vec / (s + 1e-9)).to(dtype=torch.float32)\n                    arr = np.asarray(maybe, dtype=np.float32).flatten()\n                    if arr.size == 0:\n                        return torch.tensor([1.0], dtype=torch.float32)\n                    if not np.isfinite(arr).all():\n                        return torch.tensor([1.0], dtype=torch.float32)\n                    vec = torch.from_numpy(arr).float()\n                    s = float(vec.sum().item())\n                    if s <= 1e-9:\n                        return torch.tensor([1.0], dtype=torch.float32)\n                    return (vec / (s + 1e-9)).to(dtype=torch.float32)\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.extraction_failures % 100 == 0:\n                print(f\"⚠️  [TRG] _safe_extract_proto_probs failed: {type(e).__name__}\")\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all is None:\n                return 0.5\n            row = U_all[0] if isinstance(U_all, (list, tuple)) and len(U_all) > 0 else U_all\n            if isinstance(row, torch.Tensor):\n                r = row.detach().cpu()\n                if r.dim() >= 1 and token_idx < r.size(0):\n                    val = float(r[token_idx].item())\n                    if not np.isfinite(val):\n                        return 0.5\n                    return max(0.0, min(1.0, float(val)))\n                if r.dim() == 0:\n                    val = float(r.item())\n                    if not np.isfinite(val):\n                        return 0.5\n                    return max(0.0, min(1.0, val))\n            if isinstance(row, (list, tuple, np.ndarray)) and token_idx < len(row):\n                val = row[token_idx]\n                if isinstance(val, torch.Tensor):\n                    fval = float(val.item())\n                else:\n                    fval = float(val)\n                if not np.isfinite(fval):\n                    return 0.5\n                return max(0.0, min(1.0, fval))\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.extraction_failures % 100 == 0:\n                print(f\"⚠️  [TRG] _safe_extract_uncertainty failed: {type(e).__name__}\")\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all is None:\n                return 0.0\n            row = G_all[0] if isinstance(G_all, (list, tuple)) and len(G_all) > 0 else G_all\n            if isinstance(row, torch.Tensor):\n                r = row.detach().cpu()\n                if r.dim() >= 1 and token_idx < r.size(0):\n                    val = float(r[token_idx].item())\n                    if not np.isfinite(val):\n                        return 0.0\n                    return max(0.0, min(1.0, val))\n                if r.dim() == 0:\n                    val = float(r.item())\n                    if not np.isfinite(val):\n                        return 0.0\n                    return max(0.0, min(1.0, val))\n            if isinstance(row, (list, tuple, np.ndarray)) and token_idx < len(row):\n                val = row[token_idx]\n                if isinstance(val, torch.Tensor):\n                    fval = float(val.item())\n                else:\n                    fval = float(val)\n                if not np.isfinite(fval):\n                    return 0.0\n                return max(0.0, min(1.0, fval))\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.extraction_failures % 100 == 0:\n                print(f\"⚠️  [TRG] _safe_extract_gate failed: {type(e).__name__}\")\n        return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all is None:\n                return 0.0\n            row = S_all[0] if isinstance(S_all, (list, tuple)) and len(S_all) > 0 else S_all\n            if isinstance(row, torch.Tensor):\n                r = row.detach().cpu()\n                if r.dim() >= 1 and token_idx < r.size(0):\n                    span_val = float(r[token_idx].item())\n                elif r.dim() == 0:\n                    span_val = float(r.item())\n                else:\n                    return 0.0\n            elif isinstance(row, (list, tuple, np.ndarray)) and token_idx < len(row):\n                val = row[token_idx]\n                if isinstance(val, torch.Tensor):\n                    span_val = float(val.item())\n                else:\n                    span_val = float(val)\n            else:\n                return 0.0\n            if not np.isfinite(span_val):\n                return 0.0\n            if span_val < 0.0:\n                now = time.time()\n                if self.span_clamp_warnings < 10 or (now - self.last_warning_time) > 60.0:\n                    if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                        print(f\"⚠️  [TRG] Negative span {span_val:.3f} clamped to 0.0\")\n                    self.span_clamp_warnings += 1\n                    self.last_warning_time = now\n                return 0.0\n            if span_val > 1.0:\n                now = time.time()\n                if self.span_clamp_warnings < 10 or (now - self.last_warning_time) > 60.0:\n                    if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                        print(f\"⚠️  [TRG] Span {span_val:.3f} clamped to 1.0\")\n                    self.span_clamp_warnings += 1\n                    self.last_warning_time = now\n                return 1.0\n            return float(span_val)\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.extraction_failures % 100 == 0:\n                print(f\"⚠️  [TRG] _safe_extract_span failed: {type(e).__name__}\")\n        return 0.0\n\n    def compute_span(self, sense_probs: Any) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n            if isinstance(probs, torch.Tensor):\n                probs = probs.detach().cpu().flatten().numpy().tolist()\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n            if len(probs) < 2:\n                return 0.0\n            sorted_probs = sorted([float(p) for p in probs if np.isfinite(float(p))], reverse=True)\n            if len(sorted_probs) < 2:\n                return 0.0\n            span = sorted_probs[0] - sorted_probs[1]\n            return float(max(0.0, min(1.0, span)))\n        except Exception:\n            return 0.0\n\n    def _compute_sense_alternatives_fast(self, proto_probs: Any, temperature: float = 1.0) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n            probs = proto_probs.flatten().float()\n            if not torch.isfinite(probs).all():\n                return [(\"unknown\", 0.5)]\n            probs = torch.clamp(probs, min=1e-9, max=1.0)\n            temperature = max(0.1, min(10.0, float(temperature)))\n            if temperature != 1.0 and probs.numel() > 1:\n                probs = probs / (probs.sum() + 1e-9)\n                log_probs = torch.log(probs + 1e-9)\n                scaled_log_probs = log_probs / temperature\n                probs = F.softmax(scaled_log_probs, dim=0)\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [(f\"sense_{int(indices[i].item())}\", max(0.0, min(1.0, float(probs_sorted[i].item())))) for i in range(top_k)]\n            else:\n                return [(\"sense_0\", max(0.0, min(1.0, float(probs[0].item()))))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(self, token_idx: int, tokens: List[str]) -> Dict:\n        token = tokens[token_idx] if isinstance(tokens, list) and 0 <= token_idx < len(tokens) else \"UNK\"\n        return {\"token\": token, \"token_idx\": token_idx, \"evidence_tokens\": [], \"chosen_sense\": (\"unknown\", 0.5), \"alternatives\": [], \"uncertainty\": 0.5, \"gate\": 0.0, \"span\": 0.0}\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"discovered_homographs\"):\n                    homograph_tokens = set(self.dscd_module.discovered_homographs)\n                elif hasattr(self.dscd_module, \"prototype_stores\"):\n                    for token, store in self.dscd_module.prototype_stores.items():\n                        try:\n                            size = store.size() if hasattr(store, \"size\") else (len(getattr(store, \"centroids\", [])) if hasattr(store, \"centroids\") else 0)\n                            if size >= 2:\n                                clean = str(token).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\").strip()\n                                if clean:\n                                    homograph_tokens.add(clean)\n                        except Exception:\n                            continue\n            else:\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() % 300 < 1.0:\n                    print(\"⚠️  [TRG] dscd_module is None, cannot get homographs\")\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"❌ [TRG] get_homograph_tokens_from_dscd failed: {type(e).__name__}: {e}\")\n        return homograph_tokens\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(self, embed_dim: Optional[int] = None, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        super().__init__()\n        self.embed_dim = max(1, int(embed_dim) if embed_dim is not None else int(_TRG_GEN_EMBED))\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        try:\n            if tokenizer is not None and _has_get_tokenizer_special_tokens:\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            elif tokenizer is not None and _has_get_cached_special_tokens:\n                try:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                except Exception:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []) if tokenizer is not None else [])\n        except Exception:\n            self.special_tokens = set()\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(tokenizer, language=language, dscd_module=dscd_module)\n        self.silver_buffer = deque(maxlen=max(1, int(_MAX_SILVER_BUFFER)))\n        self._silver_lock = threading.Lock()\n        self.stats_reset_interval = 1000\n        self.explanations_generated = 0\n        self.explanations_generated_training = 0\n        self.explanations_generated_inference = 0\n        self.high_confidence_explanations = 0\n        self.low_confidence_explanations = 0\n        self.empty_evidence_count = 0\n        self.total_evidence_tokens = 0\n        self.tokens_filtered_word_start = 0\n        self.tokens_filtered_validity = 0\n        self.tokens_filtered_ambiguity = 0\n        self.dscd_homographs_explained = 0\n        self._stats_lock = threading.Lock()\n        self._last_stats_log = 0.0\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(\"=\" * 80)\n            print(\"[TRG-INIT] CompleteTRGWithExplanations - NaN/Inf HARDENED\")\n            print(\"=\" * 80)\n            print(f\"  uncertainty={_TRG_UNCERTAINTY_THRESHOLD:.2f} span={_TRG_SPAN_THRESHOLD:.2f}\")\n            print(f\"  Training: {'ENABLED' if _ENABLE_TRG_TRAINING else 'DISABLED'}\")\n            print(f\"  Inference: {'ENABLED' if _ENABLE_TRG_INFERENCE else 'DISABLED'}\")\n            print(f\"  All extracted values clamped to [0, 1]\")\n            print(f\"  NaN/Inf protection on proto_probs, uncertainty, gate, span\")\n            print(\"=\" * 80)\n\n    def _update_stats(self, evidence: Dict, is_dscd_homograph: bool = False, is_training: bool = False) -> None:\n        with self._stats_lock:\n            self.explanations_generated += 1\n            if is_training:\n                self.explanations_generated_training += 1\n            else:\n                self.explanations_generated_inference += 1\n            if is_dscd_homograph:\n                self.dscd_homographs_explained += 1\n            if not evidence.get(\"evidence_tokens\"):\n                self.empty_evidence_count += 1\n            else:\n                self.total_evidence_tokens += len(evidence[\"evidence_tokens\"])\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = max(0.0, min(1.0, float(chosen[1])))\n                except Exception:\n                    confidence = 0.5\n            if confidence >= _TAU_ACCEPT:\n                self.high_confidence_explanations += 1\n            elif confidence < _TRG_UNCERTAINTY_THRESHOLD:\n                self.low_confidence_explanations += 1\n            if self.explanations_generated >= self.stats_reset_interval:\n                if _DEBUG_DISCOVERY and time.time() - self._last_stats_log > 60.0:\n                    stats = self.get_statistics()\n                    print(f\"[TRG-STATS] {stats}\")\n                    self._last_stats_log = time.time()\n                self.reset_statistics()\n\n    def _add_to_silver_buffer(self, evidence: Dict, explanation: str, tokens: List[str]) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = max(0.0, min(1.0, float(chosen[1])))\n            entry = {\"token\": str(evidence.get(\"token\", \"UNK\"))[:20], \"explanation\": str(explanation)[:150], \"confidence\": conf}\n            with self._silver_lock:\n                self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(self, token_idx: int, tokens: List[str], dscd_outputs: Dict, token_word_map: Optional[dict] = None, decoder_attention: Optional[torch.Tensor] = None, is_dscd_homograph: bool = False) -> Tuple[str, Dict]:\n        if not _ENABLE_TRG_INFERENCE and not self.training:\n            return \"\", {}\n        if not _ENABLE_TRG_TRAINING and self.training:\n            return \"\", {}\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = _fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n        else:\n            is_valid = _fallback_is_valid_token(raw_token, self.special_tokens, self.tokenizer, self.language)\n        if not is_valid:\n            return \"\", {}\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(token_idx, tokens, dscd_outputs, token_word_map=token_word_map, decoder_attention=decoder_attention)\n            if (not evidence.get(\"evidence_tokens\")) and (not is_dscd_homograph):\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.explanations_generated % 100 == 0:\n                    print(f\"⚠️  [TRG] Token {token_idx} ({raw_token}): No evidence & not DSCD homograph, skipping\")\n                return \"\", {}\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence, is_dscd_homograph=is_dscd_homograph, is_training=self.training)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception as e:\n            if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and self.explanations_generated % 100 == 0:\n                print(f\"❌ [TRG] generate_explanation_for_token failed: {type(e).__name__}: {e}\")\n            return \"\", {}\n\n    @staticmethod\n    def _to_list_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n        try:\n            if isinstance(x, torch.Tensor):\n                arr = x.detach().cpu().numpy().flatten()\n                result = []\n                for v in arr.tolist():\n                    fv = float(v)\n                    if not np.isfinite(fv):\n                        result.append(0.0)\n                    else:\n                        result.append(max(0.0, min(1.0, fv)))\n                return result\n            if isinstance(x, (list, tuple, np.ndarray)):\n                out = []\n                for v in x:\n                    try:\n                        if isinstance(v, torch.Tensor):\n                            fval = float(v.flatten()[0].item())\n                        else:\n                            fval = float(v)\n                        if not np.isfinite(fval):\n                            out.append(0.0)\n                        else:\n                            out.append(max(0.0, min(1.0, fval)))\n                    except Exception:\n                        out.append(0.0)\n                return out\n            fval = float(x)\n            if not np.isfinite(fval):\n                return [0.0]\n            return [max(0.0, min(1.0, fval))]\n        except Exception:\n            return []\n\n    def process_sentence_for_explanations(self, tokens: List[str], dscd_outputs: Dict, token_word_map: Optional[dict] = None, uncertainty_threshold: Optional[float] = None, span_threshold: Optional[float] = None, decoder_attention: Optional[torch.Tensor] = None, max_explanations: int = _MAX_EXPLANATIONS_PER_SENTENCE) -> List[Dict]:\n        if not _ENABLE_TRG_INFERENCE and not self.training:\n            return []\n        if not _ENABLE_TRG_TRAINING and self.training:\n            return []\n        uncertainty_threshold = float(uncertainty_threshold) if uncertainty_threshold is not None else float(_TRG_UNCERTAINTY_THRESHOLD)\n        span_threshold = float(span_threshold) if span_threshold is not None else float(_TRG_SPAN_THRESHOLD)\n        max_explanations = max(1, int(max_explanations))\n        explanations: List[Dict] = []\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() % 300 < 1.0:\n                    print(\"⚠️  [TRG] dscd_outputs is empty or not dict\")\n                return explanations\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n            if not U_all or not U_all[0]:\n                if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and time.time() % 300 < 1.0:\n                    print(\"⚠️  [TRG] uncertainties not found in dscd_outputs\")\n                return explanations\n            U = self._to_list_helper(U_all[0])\n            S = self._to_list_helper(S_all[0]) if S_all and S_all[0] else [0.0] * len(U)\n            if len(S) < len(U):\n                S.extend([0.0] * (len(U) - len(S)))\n            if not U:\n                return explanations\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n            for idx in range(min(len(tokens), len(U))):\n                tok = tokens[idx]\n                clean_tok = str(tok).replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\").strip()\n                if not _is_word_start(tok, token_word_map, idx):\n                    with self._stats_lock:\n                        self.tokens_filtered_word_start += 1\n                    continue\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(tok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        valid = _fallback_is_valid_token(tok, self.special_tokens, self.tokenizer, self.language)\n                else:\n                    valid = _fallback_is_valid_token(tok, self.special_tokens, self.tokenizer, self.language)\n                if not valid:\n                    with self._stats_lock:\n                        self.tokens_filtered_validity += 1\n                    continue\n                u = float(U[idx]) if idx < len(U) else 0.5\n                s = float(S[idx]) if idx < len(S) else 0.0\n                u = max(0.0, min(1.0, u))\n                s = max(0.0, min(1.0, s))\n                in_dscd = clean_tok in dscd_homographs\n                if in_dscd:\n                    priority = 1\n                elif (u >= uncertainty_threshold) and (s >= span_threshold):\n                    priority = 2\n                elif u >= uncertainty_threshold:\n                    priority = 3\n                elif s >= span_threshold:\n                    priority = 4\n                else:\n                    with self._stats_lock:\n                        self.tokens_filtered_ambiguity += 1\n                    continue\n                candidates.append((idx, u, s, clean_tok, priority, idx))\n            if not candidates:\n                return explanations\n            candidates.sort(key=lambda t: (t[4], -t[1], -t[2], t[5]))\n            count = 0\n            for (token_idx, u, s, clean_tok, priority, _) in candidates:\n                if count >= max_explanations:\n                    break\n                try:\n                    proto_probs = None\n                    try:\n                        proto_probs = self.evidence_extractor._safe_extract_proto_probs(token_idx, dscd_outputs)\n                    except Exception:\n                        proto_probs = None\n                    single_proto = (isinstance(proto_probs, torch.Tensor) and proto_probs.numel() == 1) or (isinstance(proto_probs, (list, tuple, np.ndarray)) and len(proto_probs) == 1)\n                    if single_proto and (s <= 1e-4) and (u <= (0.5 * uncertainty_threshold)) and (not (clean_tok in dscd_homographs)):\n                        with self._stats_lock:\n                            self.tokens_filtered_ambiguity += 1\n                        continue\n                    explanation_text, evidence = self.generate_explanation_for_token(token_idx, tokens, dscd_outputs, token_word_map=token_word_map, decoder_attention=decoder_attention, is_dscd_homograph=(priority == 1))\n                    if explanation_text and evidence:\n                        out_token = token_word_map[token_idx] if token_word_map and token_idx in token_word_map else tokens[token_idx].replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\"@@\", \"\").replace(\"</w>\", \"\")\n                        explanations.append({\"token_idx\": token_idx, \"token\": out_token, \"explanation\": explanation_text, \"uncertainty\": max(0.0, min(1.0, u)), \"span\": max(0.0, min(1.0, s)), \"dscd_discovered\": (priority == 1), \"priority\": priority})\n                        count += 1\n                except Exception as e:\n                    if (_VERBOSE_LOGGING or _DEBUG_DISCOVERY) and count % 50 == 0:\n                        print(f\"❌ [TRG] Explanation generation failed for token {token_idx}: {type(e).__name__}\")\n                    continue\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"❌ [TRG] process_sentence_for_explanations failed: {type(e).__name__}: {e}\")\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self._stats_lock:\n            total = max(self.explanations_generated, 1)\n            avg_evidence_tokens = (self.total_evidence_tokens / total) if self.explanations_generated > 0 else 0.0\n            return {\n                \"explanations_generated\": self.explanations_generated,\n                \"explanations_generated_training\": self.explanations_generated_training,\n                \"explanations_generated_inference\": self.explanations_generated_inference,\n                \"high_confidence_explanations\": self.high_confidence_explanations,\n                \"low_confidence_explanations\": self.low_confidence_explanations,\n                \"empty_evidence_count\": self.empty_evidence_count,\n                \"total_evidence_tokens\": self.total_evidence_tokens,\n                \"tokens_filtered_word_start\": self.tokens_filtered_word_start,\n                \"tokens_filtered_validity\": self.tokens_filtered_validity,\n                \"tokens_filtered_ambiguity\": self.tokens_filtered_ambiguity,\n                \"dscd_homographs_explained\": self.dscd_homographs_explained,\n                \"high_confidence_rate\": self.high_confidence_explanations / total,\n                \"low_confidence_rate\": self.low_confidence_explanations / total,\n                \"empty_evidence_rate\": self.empty_evidence_count / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.dscd_homographs_explained / total\n            }\n\n    def reset_statistics(self) -> None:\n        with self._stats_lock:\n            self.explanations_generated = 0\n            self.explanations_generated_training = 0\n            self.explanations_generated_inference = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.empty_evidence_count = 0\n            self.total_evidence_tokens = 0\n            self.tokens_filtered_word_start = 0\n            self.tokens_filtered_validity = 0\n            self.tokens_filtered_ambiguity = 0\n            self.dscd_homographs_explained = 0\n\n    def clear_silver_buffer(self) -> None:\n        with self._silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        try:\n            tokens = [\"▁আমি\", \"▁কল\", \"▁বন্ধ\", \"▁করেছি\", \"।\"]\n            dscd_outputs = {\"proto_probs\": [[torch.tensor([0.6, 0.4]) for _ in tokens]], \"uncertainties\": [[0.1, 0.5, 0.2, 0.1, 0.0]], \"span_preds\": [[0.05, 0.3, 0.1, 0.05, 0.0]], \"gates\": [[0.2, 0.8, 0.3, 0.2, 0.0]]}\n            token_word_map = {0: \"আমি\", 1: \"কল\", 2: \"বন্ধ\", 3: \"করেছি\", 4: \"।\"}\n            self.eval()\n            explanations = self.process_sentence_for_explanations(tokens=tokens, dscd_outputs=dscd_outputs, token_word_map=token_word_map, max_explanations=3)\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG-TEST] Generated {len(explanations)} explanations\")\n            self.reset_statistics()\n            return True\n        except Exception:\n            traceback.print_exc()\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Ready (DATA-DRIVEN) - NaN/Inf HARDENED\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\nprint(f\"  - Span threshold: {_TRG_SPAN_THRESHOLD:.2f}\")\nprint(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\nprint(f\"  - TAU_HIGH: {_TAU_HIGH:.2f}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW:.2f}\")\nprint(f\"  - TAU_ACCEPT: {_TAU_ACCEPT:.2f}\")\nprint(f\"  - Max explanations: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(f\"  - Evidence K: {_TRG_EVIDENCE_K}\")\nprint(f\"  - Training mode: {'ENABLED' if _ENABLE_TRG_TRAINING else 'DISABLED'}\")\nprint(f\"  - Inference mode: {'ENABLED' if _ENABLE_TRG_INFERENCE else 'DISABLED'}\")\nprint(\"NaN/Inf Protections:\")\nprint(\"  ✅ All extracted values clamped to [0, 1]\")\nprint(\"  ✅ NaN check on proto_probs before normalization\")\nprint(\"  ✅ Division by zero protection (epsilon=1e-9)\")\nprint(\"  ✅ Temperature bounds: [0.1, 10.0]\")\nprint(\"  ✅ Confidence bounds enforced in explanations\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"svk-wKO7H4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TATN MODEL - NaN/Inf GRADIENT FULLY PROTECTED - STEP 124 FIX\n# ==============================================================================\n\nfrom typing import List, Dict, Optional, Any, Tuple\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is None:\n            return default\n        v = int(val)\n        return v if v > 0 else default\n    except Exception:\n        return default\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is None:\n            return default\n        return float(val)\n    except Exception:\n        return default\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is None:\n            return default\n        return bool(val)\n    except Exception:\n        return default\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\n_DSCD_BUFFER_SIZE         = max(1, _get_int_global(\"DSCD_BUFFER_SIZE\", 50))\n_DSCD_MAX_PROTOS          = max(1, _get_int_global(\"DSCD_MAX_PROTOS\", 8))\n_DSCD_N_MIN               = max(1, _get_int_global(\"DSCD_N_MIN\", 2))\n_DSCD_DISPERSION_THRESHOLD= float(_get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.70))\n\n_ENABLE_ASBN_TRAINING     = _get_bool_global(\"ENABLE_ASBN_TRAINING\", True)\n_ENABLE_TRG_INFERENCE     = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\n_MEMORY_CLEANUP_FREQUENCY = max(0,  _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 200))\n\n_NUM_GPUS                 = max(1, _get_int_global(\"NUM_GPUS\",\n                                   torch.cuda.device_count() if torch.cuda.is_available() else 1))\n_USE_GC                   = _get_bool_global(\"GRADIENT_CHECKPOINTING\", False)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\"DSCD_ENABLE_TRAINING_CLUSTERING\", True)\n\n_LAMBDA_ASBN              = max(0.0, float(_get_float_global(\"LAMBDA_ASBN\", 0.05)))\n_LAMBDA_DSCD              = max(0.0, float(_get_float_global(\"LAMBDA_DSCD\", 0.15)))\n_VERBOSE_LOGGING          = _get_bool_global(\"VERBOSE_LOGGING\", False)\n_DEBUG_DISCOVERY          = _get_bool_global(\"DEBUG_DISCOVERY\", False)\n_DEBUG_TIMING             = _get_bool_global(\"DEBUG_TIMING\", False)\n\n_PERIODIC_DISCOVERY_FREQUENCY = max(1, _get_int_global(\"PERIODIC_DISCOVERY_FREQUENCY\", 150))\n_VALIDATION_CHECK_INTERVAL    = max(1, _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 500))\n\n_SPAN_THRESHOLD           = max(0.0, min(1.0, float(_get_float_global(\"SPAN_THRESHOLD\", 0.15))))\n_UNCERTAINTY_THRESHOLD    = max(0.0, min(1.0, float(_get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.25))))\n_TRG_UNCERTAINTY_THRESHOLD= max(0.0, min(1.0, float(_get_float_global(\"TRG_UNCERTAINTY_THRESHOLD\",\n                                                                       _UNCERTAINTY_THRESHOLD))))\n_TAU_LOW                  = max(0.0, min(1.0, float(_get_float_global(\"TAU_LOW\", 0.25))))\n\n_TRAIN_DOMAIN             = _get_int_global(\"TRAIN_DOMAIN\", 0)\n_TEST_DOMAIN              = _get_int_global(\"TEST_DOMAIN\", 1)\n_USE_DOMAIN_LABELS        = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(globals().get(\"M2M100_EN_TOKEN_ID\", 128022))\nexcept Exception:\n    _M2M100_EN_TOKEN_ID = 128022\ntry:\n    _M2M100_BN_TOKEN_ID = int(globals().get(\"M2M100_BN_TOKEN_ID\", 128025))\nexcept Exception:\n    _M2M100_BN_TOKEN_ID = 128025\n\n_LABEL_SMOOTHING = max(0.0, min(1.0, float(_get_float_global(\"LABEL_SMOOTHING\", 0.1))))\n_DECODER_DROPOUT = max(0.0, min(1.0, float(_get_float_global(\"DECODER_DROPOUT\", 0.1))))\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and enc_output and isinstance(enc_output[0], torch.Tensor):\n        return enc_output[0]\n    return None\n\n\ndef build_token_word_map_sentencepiece(input_ids: torch.Tensor, tokenizer) -> List[Dict[int, str]]:\n    batch_word_maps = []\n    for b in range(input_ids.size(0)):\n        try:\n            tokens = tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n        except Exception:\n            tokens = [str(x) for x in input_ids[b].tolist()]\n        word_map: Dict[int, Optional[str]] = {}\n        current_word = \"\"\n        word_start = 0\n        for i, token in enumerate(tokens):\n            if not token:\n                word_map[i] = None\n                continue\n            if token in {\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"}:\n                word_map[i] = None\n                continue\n            if token.startswith(\"▁\") or token.startswith(\"Ġ\") or token.startswith(\"\\u2581\"):\n                if current_word:\n                    clean = current_word.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"\\u2581\", \"\").strip()\n                    if clean:\n                        for j in range(word_start, i):\n                            word_map[j] = clean\n                current_word = token\n                word_start = i\n            else:\n                current_word += token\n        if current_word:\n            clean = current_word.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"\\u2581\", \"\").strip()\n            if clean:\n                for j in range(word_start, len(tokens)):\n                    word_map[j] = clean\n        batch_word_maps.append(word_map)\n    return batch_word_maps\n\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int\n) -> Dict[str, Any]:\n    if not isinstance(device, torch.device):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    defaults = {\n        \"h_augmented\": torch.zeros(batch_size, seq_len, embed_dim,\n                                   device=device, dtype=torch.float32),\n        \"proto_probs\": [[torch.tensor([1.0], device=device, dtype=torch.float32)\n                         for _ in range(seq_len)] for _ in range(batch_size)],\n        \"uncertainties\": [[torch.tensor(0.5, device=device, dtype=torch.float32)\n                           for _ in range(seq_len)] for _ in range(batch_size)],\n        \"gates\": [[torch.tensor(0.0, device=device, dtype=torch.float32)\n                   for _ in range(seq_len)] for _ in range(batch_size)],\n        \"span_preds\": [[torch.tensor(0.0, device=device, dtype=torch.float32)\n                        for _ in range(seq_len)] for _ in range(batch_size)],\n        \"proto_assignments\": [torch.full((seq_len,), -1, dtype=torch.long, device=device)\n                              for _ in range(batch_size)],\n    }\n    if not isinstance(raw, dict):\n        return defaults\n\n    out = defaults.copy()\n\n    try:\n        h = raw.get(\"h_augmented\", None)\n        if isinstance(h, torch.Tensor):\n            if not torch.isfinite(h).all():\n                if _VERBOSE_LOGGING:\n                    print(\"⚠️  TATN: h_augmented contains NaN/Inf, zeroing out\")\n                h = torch.zeros_like(h)\n            h = torch.clamp(h, min=-100.0, max=100.0)\n            if h.shape == (batch_size, seq_len, embed_dim):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = h.to(device).reshape(batch_size, seq_len, embed_dim)\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    def _norm_scalar_matrix(key: str, default_scalar: float) -> List[List[torch.Tensor]]:\n        mat = raw.get(key, None)\n        res = [[torch.tensor(default_scalar, device=device, dtype=torch.float32)\n                for _ in range(seq_len)] for _ in range(batch_size)]\n\n        if mat is None:\n            return res\n\n        if isinstance(mat, (list, tuple)) and len(mat) == batch_size:\n            for b in range(batch_size):\n                row = mat[b]\n                if isinstance(row, torch.Tensor):\n                    r = row.detach().to(device)\n                    if r.dim() == 1:\n                        for t in range(min(seq_len, r.size(0))):\n                            val = r[t].float()\n                            if not torch.isfinite(val):\n                                val = torch.tensor(default_scalar, device=device)\n                            val = torch.clamp(val, min=0.0, max=1.0)\n                            res[b][t] = val\n                    elif r.dim() == 2:\n                        for t in range(min(seq_len, r.size(0))):\n                            val = r[t, 0].float()\n                            if not torch.isfinite(val):\n                                val = torch.tensor(default_scalar, device=device)\n                            val = torch.clamp(val, min=0.0, max=1.0)\n                            res[b][t] = val\n                elif isinstance(row, (list, tuple, np.ndarray)):\n                    for t in range(min(seq_len, len(row))):\n                        try:\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                fv = v.to(device).float()\n                                if not torch.isfinite(fv):\n                                    fv = torch.tensor(default_scalar, device=device)\n                                fv = torch.clamp(fv, min=0.0, max=1.0)\n                                res[b][t] = fv\n                            else:\n                                fv = float(v)\n                                if not np.isfinite(fv):\n                                    fv = default_scalar\n                                fv = max(0.0, min(1.0, fv))\n                                res[b][t] = torch.tensor(fv, device=device)\n                        except Exception:\n                            res[b][t] = torch.tensor(default_scalar, device=device)\n            return res\n\n        if isinstance(mat, torch.Tensor):\n            m = mat.detach().to(device)\n            if m.dim() == 3 and m.size(0) >= batch_size:\n                for b in range(batch_size):\n                    for t in range(min(seq_len, m.size(1))):\n                        v = m[b, t]\n                        if not torch.isfinite(v).all():\n                            v = torch.tensor(default_scalar, device=device)\n                        else:\n                            v = v if v.numel() == 1 else v.flatten()[0]\n                            v = torch.clamp(v, min=0.0, max=1.0)\n                        res[b][t] = v\n                return res\n            if m.dim() == 2:\n                if m.size(0) >= batch_size:\n                    for b in range(batch_size):\n                        for t in range(min(seq_len, m.size(1))):\n                            v = m[b, t].float()\n                            if not torch.isfinite(v):\n                                v = torch.tensor(default_scalar, device=device)\n                            v = torch.clamp(v, min=0.0, max=1.0)\n                            res[b][t] = v\n                    return res\n                else:\n                    for t in range(min(seq_len, m.size(0))):\n                        v = m[t, 0].float()\n                        if not torch.isfinite(v):\n                            v = torch.tensor(default_scalar, device=device)\n                        v = torch.clamp(v, min=0.0, max=1.0)\n                        res[0][t] = v\n                    return res\n\n        if isinstance(mat, (list, tuple, np.ndarray)) and batch_size == 1:\n            for t in range(min(seq_len, len(mat))):\n                try:\n                    v = mat[t]\n                    if isinstance(v, torch.Tensor):\n                        fv = v.to(device).float()\n                        if not torch.isfinite(fv):\n                            fv = torch.tensor(default_scalar, device=device)\n                        fv = torch.clamp(fv, min=0.0, max=1.0)\n                        res[0][t] = fv\n                    else:\n                        fv = float(v)\n                        if not np.isfinite(fv):\n                            fv = default_scalar\n                        fv = max(0.0, min(1.0, fv))\n                        res[0][t] = torch.tensor(fv, device=device)\n                except Exception:\n                    res[0][t] = torch.tensor(default_scalar, device=device)\n            return res\n\n        return res\n\n    def _norm_proto_probs() -> List[List[torch.Tensor]]:\n        mat = raw.get(\"proto_probs\", None)\n        res = defaults[\"proto_probs\"]\n        if mat is None:\n            return res\n\n        def _to_vec(x) -> torch.Tensor:\n            if isinstance(x, torch.Tensor):\n                v = x.detach().to(device).float().flatten()\n            else:\n                arr = np.asarray(x, dtype=np.float32).flatten()\n                v = torch.from_numpy(arr).to(device).float()\n            if v.numel() == 0:\n                v = torch.tensor([1.0], device=device)\n            if not torch.isfinite(v).all():\n                return torch.tensor([1.0], device=device)\n            v = torch.clamp(v, min=1e-9, max=1.0)\n            s = v.sum()\n            if not torch.isfinite(s) or s <= 1e-9:\n                return torch.tensor([1.0], device=device)\n            v = v / (s + 1e-9)\n            v = torch.clamp(v, min=1e-9, max=1.0)\n            return v\n\n        if isinstance(mat, list) and len(mat) == batch_size:\n            out_pp = []\n            for b in range(batch_size):\n                row = mat[b]\n                row_vecs = []\n                if isinstance(row, (list, tuple)):\n                    for t in range(min(seq_len, len(row))):\n                        row_vecs.append(_to_vec(row[t]))\n                    while len(row_vecs) < seq_len:\n                        row_vecs.append(torch.tensor([1.0], device=device))\n                elif isinstance(row, torch.Tensor):\n                    r = row.detach().to(device)\n                    if r.dim() == 2:\n                        for t in range(min(seq_len, r.size(0))):\n                            row_vecs.append(_to_vec(r[t]))\n                    elif r.dim() == 1:\n                        row_vecs.append(_to_vec(r))\n                        while len(row_vecs) < seq_len:\n                            row_vecs.append(_to_vec(r))\n                if not row_vecs:\n                    row_vecs = [torch.tensor([1.0], device=device) for _ in range(seq_len)]\n                out_pp.append(row_vecs[:seq_len])\n            return out_pp\n\n        if isinstance(mat, torch.Tensor) and mat.dim() == 3:\n            B, T, K = mat.shape\n            out_pp = []\n            for b in range(min(batch_size, B)):\n                row_vecs = []\n                for t in range(min(seq_len, T)):\n                    row_vecs.append(_to_vec(mat[b, t]))\n                while len(row_vecs) < seq_len:\n                    row_vecs.append(torch.tensor([1.0], device=device))\n                out_pp.append(row_vecs[:seq_len])\n            while len(out_pp) < batch_size:\n                out_pp.append([torch.tensor([1.0], device=device) for _ in range(seq_len)])\n            return out_pp\n\n        if isinstance(mat, torch.Tensor) and mat.dim() == 2:\n            B, T = mat.shape\n            out_pp = []\n            for b in range(min(batch_size, B)):\n                row_vecs = []\n                for t in range(min(seq_len, T)):\n                    p = float(mat[b, t].item())\n                    if not np.isfinite(p):\n                        p = 0.5\n                    p = max(0.0, min(1.0, p))\n                    row_vecs.append(_to_vec([p, 1.0-p]))\n                while len(row_vecs) < seq_len:\n                    row_vecs.append(torch.tensor([1.0], device=device))\n                out_pp.append(row_vecs[:seq_len])\n            while len(out_pp) < batch_size:\n                out_pp.append([torch.tensor([1.0], device=device) for _ in range(seq_len)])\n            return out_pp\n\n        if isinstance(mat, (list, tuple)) and batch_size == 1:\n            row = mat\n            row_vecs = []\n            for t in range(min(seq_len, len(row))):\n                row_vecs.append(_to_vec(row[t]))\n            while len(row_vecs) < seq_len:\n                row_vecs.append(torch.tensor([1.0], device=device))\n            return [row_vecs[:seq_len]]\n\n        return res\n\n    out[\"proto_probs\"]   = _norm_proto_probs()\n    out[\"uncertainties\"] = _norm_scalar_matrix(\"uncertainties\", default_scalar=0.5)\n    out[\"gates\"]         = _norm_scalar_matrix(\"gates\",         default_scalar=0.0)\n    out[\"span_preds\"]    = _norm_scalar_matrix(\"span_preds\",    default_scalar=0.0)\n\n    pa = raw.get(\"proto_assignments\", None)\n    try:\n        if isinstance(pa, list) and len(pa) == batch_size:\n            safe_pa = []\n            for brow in pa:\n                if isinstance(brow, torch.Tensor):\n                    t = brow.to(device).long().view(-1)\n                    if t.size(0) < seq_len:\n                        t = torch.cat([t, torch.full((seq_len - t.size(0),),\n                                                     -1, dtype=torch.long, device=device)], dim=0)\n                    elif t.size(0) > seq_len:\n                        t = t[:seq_len]\n                    safe_pa.append(t)\n                elif isinstance(brow, (list, tuple, np.ndarray)):\n                    arr = list(brow)[:seq_len]\n                    arr += [-1] * max(0, seq_len - len(arr))\n                    safe_pa.append(torch.tensor(arr, dtype=torch.long, device=device))\n                else:\n                    safe_pa.append(torch.full((seq_len,), -1, dtype=torch.long, device=device))\n            out[\"proto_assignments\"] = safe_pa\n        elif isinstance(pa, torch.Tensor):\n            pa = pa.to(device).long()\n            if pa.dim() == 2:\n                B, T = pa.shape\n                safe_pa = []\n                for b in range(min(batch_size, B)):\n                    row = pa[b]\n                    if row.size(0) < seq_len:\n                        row = torch.cat(\n                            [row, torch.full((seq_len - row.size(0),), -1, dtype=torch.long, device=device)],\n                            dim=0\n                        )\n                    elif row.size(0) > seq_len:\n                        row = row[:seq_len]\n                    safe_pa.append(row)\n                while len(safe_pa) < batch_size:\n                    safe_pa.append(torch.full((seq_len,), -1, dtype=torch.long, device=device))\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n\n    return out\n\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self._config_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n        self.asbn_forward_errors = 0\n        self.dscd_forward_errors = 0\n\n        try:\n            self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n                \"facebook/m2m100_418M\",\n                torch_dtype=torch.float32,\n                use_cache=False\n            )\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load m2m100 model: {e}\")\n\n        try:\n            self.mbart.config.use_cache = False\n            self.mbart.config.label_smoothing_factor = _LABEL_SMOOTHING\n            self.mbart.config.dropout = _DECODER_DROPOUT\n            if hasattr(self.mbart.config, \"attention_dropout\"):\n                self.mbart.config.attention_dropout = _DECODER_DROPOUT\n            if hasattr(self.mbart.config, \"activation_dropout\"):\n                self.mbart.config.activation_dropout = _DECODER_DROPOUT\n            if hasattr(self.mbart.config, \"decoder_dropout\"):\n                self.mbart.config.decoder_dropout = _DECODER_DROPOUT\n        except Exception:\n            pass\n\n        try:\n            emb = self.mbart.get_input_embeddings()\n            model_emb_count = getattr(emb, \"num_embeddings\", None)\n            \n            tok_len = getattr(tokenizer, \"vocab_size\", None)\n            if tok_len is None and hasattr(tokenizer, \"__len__\"):\n                try:\n                    tok_len = len(tokenizer)\n                except Exception:\n                    tok_len = None\n            \n            if isinstance(model_emb_count, int) and isinstance(tok_len, int):\n                if model_emb_count != tok_len:\n                    print(f\"⚠️  Vocab size mismatch detected: model={model_emb_count}, tokenizer={tok_len}\")\n                    print(f\"🔧 Auto-resizing model embeddings to match tokenizer...\")\n                    \n                    try:\n                        self.mbart.resize_token_embeddings(tok_len)\n                        \n                        new_emb = self.mbart.get_input_embeddings()\n                        new_count = getattr(new_emb, \"num_embeddings\", None)\n                        \n                        if new_count == tok_len:\n                            print(f\"✅ Successfully resized embeddings: {model_emb_count} → {new_count}\")\n                        else:\n                            print(f\"❌ Resize failed: expected {tok_len}, got {new_count}\")\n                            raise RuntimeError(f\"Embedding resize verification failed: expected {tok_len}, got {new_count}\")\n                        \n                    except Exception as e:\n                        print(f\"❌ FATAL: Cannot resize embeddings: {type(e).__name__}: {e}\")\n                        print(f\"   This will cause CUDA device-side assert errors!\")\n                        raise RuntimeError(f\"Embedding resize failed: {e}\")\n                else:\n                    print(f\"✅ Vocab sizes match: model={model_emb_count}, tokenizer={tok_len}\")\n            else:\n                if model_emb_count is None:\n                    print(f\"⚠️  WARNING: Cannot verify vocab sizes (model embedding count is None)\")\n                if tok_len is None:\n                    print(f\"⚠️  WARNING: Cannot verify vocab sizes (tokenizer length is None)\")\n            \n            self.vocab_size = tok_len if tok_len is not None else 128104\n            \n        except Exception as e:\n            print(f\"❌ Vocab size check failed: {type(e).__name__}: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            raise\n\n        try:\n            en_token_id = None\n            bn_token_id = None\n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                try:\n                    en_token_id = self.tokenizer.get_lang_id(_TARGET_LANGUAGE)\n                except Exception:\n                    en_token_id = None\n                try:\n                    bn_token_id = self.tokenizer.get_lang_id(_SOURCE_LANGUAGE)\n                except Exception:\n                    bn_token_id = None\n            if en_token_id is None:\n                en_token_id = _M2M100_EN_TOKEN_ID\n            if bn_token_id is None:\n                bn_token_id = _M2M100_BN_TOKEN_ID\n            \n            en_token_id = int(en_token_id)\n            bn_token_id = int(bn_token_id)\n            \n            if en_token_id >= self.vocab_size:\n                print(f\"⚠️  WARNING: EN token {en_token_id} >= vocab {self.vocab_size}, clamping to {self.vocab_size - 1}\")\n                en_token_id = self.vocab_size - 1\n            if bn_token_id >= self.vocab_size:\n                print(f\"⚠️  WARNING: BN token {bn_token_id} >= vocab {self.vocab_size}, clamping to {self.vocab_size - 1}\")\n                bn_token_id = self.vocab_size - 1\n            \n            en_token_id = max(0, min(en_token_id, self.vocab_size - 1))\n            bn_token_id = max(0, min(bn_token_id, self.vocab_size - 1))\n            \n            with self._config_lock:\n                if hasattr(self.mbart.config, \"forced_bos_token_id\"):\n                    self.mbart.config.forced_bos_token_id = en_token_id\n                if hasattr(self.mbart.config, \"decoder_start_token_id\"):\n                    self.mbart.config.decoder_start_token_id = en_token_id\n            \n            self.en_token_id = en_token_id\n            self.bn_token_id = bn_token_id\n            \n            print(f\"✅ Language tokens validated: EN={self.en_token_id}, BN={self.bn_token_id}, vocab_size={self.vocab_size}\")\n            \n        except Exception as e:\n            print(f\"❌ Language token validation failed: {type(e).__name__}: {e}\")\n            self.en_token_id = min(_M2M100_EN_TOKEN_ID, self.vocab_size - 1)\n            self.bn_token_id = min(_M2M100_BN_TOKEN_ID, self.vocab_size - 1)\n\n        try:\n            if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = max(1, int(getattr(self.mbart.config, \"d_model\", 1024)))\n\n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            self.dscd = dscd_cls(\n                embed_dim=embed_dim,\n                tokenizer=tokenizer,\n                buffer_size=_DSCD_BUFFER_SIZE,\n                max_protos=_DSCD_MAX_PROTOS,\n                n_min=_DSCD_N_MIN,\n                language=_SOURCE_LANGUAGE,\n                dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                max_clustering_points=500,\n                max_candidates_per_step=1,\n            )\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found\")\n\n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(embed_dim, tokenizer, language=_SOURCE_LANGUAGE)\n            except Exception:\n                self.asbn = self._build_stub_asbn()\n        else:\n            self.asbn = self._build_stub_asbn()\n\n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg_system = trg_cls(embed_dim, tokenizer,\n                                          language=_SOURCE_LANGUAGE,\n                                          dscd_module=self.dscd)\n            except Exception:\n                self.trg_system = self._build_stub_trg()\n        else:\n            self.trg_system = self._build_stub_trg()\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"\\n\" + \"=\" * 80)\n            print(\"TATN Initialized - MemoryOptimizedTATNWithExplanations - STEP 124 HARDENED\")\n            print(\"=\" * 80)\n            print(f\"  - Embed dim: {embed_dim}\")\n            print(f\"  - Vocab size: {self.vocab_size}\")\n            print(f\"  - EN token: {self.en_token_id}, BN token: {self.bn_token_id}\")\n            print(f\"  - Label smoothing: {_LABEL_SMOOTHING}\")\n            print(f\"  - Decoder dropout: {_DECODER_DROPOUT}\")\n            print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n            print(f\"  - Validation interval: {_VALIDATION_CHECK_INTERVAL}\")\n            print(f\"  - Lambda ASBN: {_LAMBDA_ASBN}, Lambda DSCD: {_LAMBDA_DSCD}\")\n            print(\"  - All losses clamped: translation [0, 100], asbn [0, 10], dscd [0, 5]\")\n            print(\"  - All scalar matrices clamped to [0, 1]\")\n            print(\"  - Proto probs NaN/Inf protected with element clamp after division\")\n            print(\"  - ASBN h_aug NaN revert protection enabled\")\n            print(\"=\" * 80 + \"\\n\")\n\n    def _build_stub_asbn(self):\n        class _StubASBN(nn.Module):\n            def forward(self, h, domain_labels=None, global_step=None):\n                dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                return h, torch.tensor(0.0, device=dev), 0.0\n            def forward_with_grl_simplified(self, h, *args, **kwargs):\n                dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n                return torch.tensor(0.0, device=dev), torch.tensor(0.0, device=dev), \\\n                       torch.tensor(0.0, device=dev), torch.tensor(0.0, device=dev)\n            def critic_parameters(self):\n                return []\n            def reset_stats(self):\n                pass\n            def get_detailed_stats(self):\n                return {\"domain_loss\": 0.0, \"domain_accuracy\": 0.0,\n                        \"source_accuracy\": 0.0, \"target_accuracy\": 0.0,\n                        \"asbn_loss\": 0.0, \"num_updates\": 0}\n            def get_asbn_stats(self):\n                return self.get_detailed_stats()\n        return _StubASBN()\n\n    def _build_stub_trg(self):\n        class _StubTRG:\n            def process_sentence_for_explanations(self, *args, **kwargs):\n                return []\n            def get_statistics(self):\n                return {\"explanations_generated\": 0}\n            def reset_statistics(self):\n                pass\n        return _StubTRG()\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(proto_probs_list, gates_list=None, min_gate: float = 0.0) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n        if dev is None:\n            dev = torch.device(\"cpu\")\n\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    try:\n                        if float(gl[j]) < min_gate:\n                            continue\n                    except Exception:\n                        pass\n                try:\n                    p = torch.clamp(probs.to(dev).float(), min=1e-9, max=1.0)\n                    if not torch.isfinite(p).all():\n                        continue\n                    s = p.sum()\n                    if not torch.isfinite(s) or s <= 1e-9:\n                        continue\n                    p = p / (s + 1e-9)\n                    H = -torch.sum(p * torch.log(p + 1e-9))\n                    H = torch.clamp(H, min=0.0, max=10.0)\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / max(1, count)\n\n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None\n    ) -> List[dict]:\n        if token_word_map is not None and len(token_word_map) == batch_size:\n            if all(isinstance(m, dict) for m in token_word_map):\n                return token_word_map\n        if not _has_reconstruct_word_spans:\n            return build_token_word_map_sentencepiece(input_ids, self.tokenizer)\n        word_maps_batch: List[dict] = []\n        for b in range(batch_size):\n            try:\n                if src_texts and b < len(src_texts) and isinstance(src_texts[b], str):\n                    text = src_texts[b]\n                else:\n                    text = self.tokenizer.decode(input_ids[b], skip_special_tokens=True)\n                if not text.strip():\n                    word_maps_batch.append({})\n                    continue\n                wm, _ = reconstruct_word_spans(self.tokenizer, text, max_length=seq_len)\n                cleaned = {}\n                for idx, word in wm.items():\n                    if isinstance(idx, int) and 0 <= idx < seq_len and isinstance(word, str):\n                        cleaned[idx] = word.replace(\"▁\", \"\").strip()\n                word_maps_batch.append(cleaned)\n            except Exception:\n                word_maps_batch.append({})\n        return word_maps_batch\n\n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None\n    ) -> Optional[torch.Tensor]:\n        if not _USE_DOMAIN_LABELS:\n            if _VERBOSE_LOGGING:\n                print(\"⚠️  Domain labels disabled (USE_DOMAIN_LABELS=False)\")\n            return None\n        try:\n            import random\n            seed_val = self.global_step + 42\n            random.seed(seed_val)\n            ids = list(range(batch_size))\n            random.shuffle(ids)\n            n_train = max(1, batch_size // 2)\n            train_idx = set(ids[:n_train])\n            labels = [ _TRAIN_DOMAIN if i in train_idx else _TEST_DOMAIN for i in range(batch_size) ]\n            result = torch.tensor(labels, dtype=torch.long, device=device)\n            if result.size(0) != batch_size:\n                print(f\"❌ TATN: Domain labels size mismatch: {result.size(0)} != {batch_size}\")\n                return None\n            return result\n        except Exception as e:\n            print(f\"❌ TATN: Domain label extraction failed: {type(e).__name__}: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            return None\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = True\n    ):\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask required\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(\"input_ids/attention_mask must be 2D\")\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        if torch.cuda.is_available() and _MEMORY_CLEANUP_FREQUENCY > 0 and \\\n           current_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n            try:\n                torch.cuda.empty_cache()\n            except Exception:\n                pass\n            if gc.isenabled():\n                gc.collect()\n\n        if self.training and _DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if current_step - self.last_discovery_step >= _PERIODIC_DISCOVERY_FREQUENCY:\n                try:\n                    max_tokens_per = globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150)\n                    self.dscd.periodic_discovery_check(\n                        current_step,\n                        _PERIODIC_DISCOVERY_FREQUENCY,\n                        max_tokens_per\n                    )\n                except Exception as e:\n                    if _VERBOSE_LOGGING:\n                        print(f\"❌ TATN: Periodic discovery failed: {e}\")\n                self.last_discovery_step = current_step\n\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n        except Exception as e:\n            print(f\"❌ TATN: Encoder forward failed: {type(e).__name__}: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            try:\n                enc_outputs = self.mbart.get_encoder()(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask\n                )\n            except Exception as e2:\n                print(f\"❌ TATN: Fallback encoder also failed: {e2}\")\n                raise\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None or not isinstance(h, torch.Tensor) or h.dim() != 3:\n            print(f\"❌ TATN: Invalid encoder output h: type={type(h)}, shape={h.shape if isinstance(h, torch.Tensor) else 'N/A'}\")\n            try:\n                h = self.mbart.get_input_embeddings()(input_ids).to(device)\n            except Exception:\n                h = torch.zeros(batch_size, seq_len,\n                                int(getattr(self.mbart.config, \"d_model\", 1024)),\n                                device=device)\n        \n        if not torch.isfinite(h).all():\n            print(\"❌ TATN: Encoder hidden states contain NaN/Inf, zeroing out\")\n            h = torch.zeros_like(h)\n        h = torch.clamp(h, min=-100.0, max=100.0)\n        \n        embed_dim = int(h.size(-1))\n\n        training_mode = (labels is not None) and self.training\n\n        token_word_map = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n        domain_labels = self._extract_domain_labels(batch_size, device, src_texts)\n\n        raw_dscd = None\n        if use_dscd:\n            try:\n                raw_dscd = self.dscd.forward(\n                    h,\n                    token_types=None,\n                    train_mode=self.training,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_word_map=token_word_map\n                )\n                if not isinstance(raw_dscd, dict) or \"h_augmented\" not in raw_dscd:\n                    print(\"⚠️  TATN: DSCD returned invalid output (not dict or missing h_augmented)\")\n                    raw_dscd = None\n            except Exception as e:\n                print(f\"❌ TATN: DSCD forward failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                self.dscd_forward_errors += 1\n                raw_dscd = None\n\n        if raw_dscd is None:\n            if _VERBOSE_LOGGING and current_step % 100 == 0:\n                print(f\"⚠️  TATN: Using DSCD fallback (uncertainty=0.5) at step {current_step}\")\n            raw_dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [[torch.tensor([1.0], dtype=torch.float32, device=device)\n                                 for _ in range(seq_len)] for _ in range(batch_size)],\n                \"uncertainties\": [[torch.tensor(0.5, dtype=torch.float32, device=device)\n                                   for _ in range(seq_len)] for _ in range(batch_size)],\n                \"gates\": [[torch.tensor(0.0, dtype=torch.float32, device=device)\n                           for _ in range(seq_len)] for _ in range(batch_size)],\n                \"span_preds\": [[torch.tensor(0.0, dtype=torch.float32, device=device)\n                                for _ in range(seq_len)] for _ in range(batch_size)],\n                \"proto_assignments\": [torch.full((seq_len,), -1, dtype=torch.long, device=device)\n                                      for _ in range(batch_size)],\n            }\n\n        dscd = _normalize_dscd_outputs(raw_dscd, batch_size, seq_len, device, embed_dim)\n\n        h_aug = dscd.get(\"h_augmented\", h)\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            if _VERBOSE_LOGGING:\n                print(f\"⚠️  TATN: h_augmented invalid, using original h\")\n            h_aug = h\n        \n        if not torch.isfinite(h_aug).all():\n            print(\"❌ TATN: h_augmented contains NaN/Inf after DSCD, using original h\")\n            h_aug = h\n        \n        h_aug_pre_asbn = h_aug.detach().clone()\n\n        asbn_bn_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = 0.0\n        if use_asbn and domain_labels is not None:\n            try:\n                asbn_result = self.asbn.forward(h_aug, domain_labels=domain_labels, global_step=current_step)\n                if isinstance(asbn_result, tuple) and len(asbn_result) == 3:\n                    h_aug, asbn_bn_loss, domain_accuracy = asbn_result\n                elif isinstance(asbn_result, tuple) and len(asbn_result) == 2:\n                    h_aug, asbn_bn_loss = asbn_result\n                    domain_accuracy = 0.0\n                    if self.asbn_forward_errors < 10:\n                        print(f\"⚠️  TATN: ASBN returned 2 values (expected 3), domain_accuracy set to 0.0\")\n                        self.asbn_forward_errors += 1\n                else:\n                    print(f\"❌ TATN: ASBN returned unexpected type: {type(asbn_result)}\")\n                    asbn_bn_loss = torch.tensor(0.0, device=device)\n                    domain_accuracy = 0.0\n                    self.asbn_forward_errors += 1\n                if not torch.isfinite(asbn_bn_loss):\n                    print(f\"❌ TATN: ASBN BN loss is NaN/Inf: {asbn_bn_loss}\")\n                    asbn_bn_loss = torch.tensor(0.0, device=device)\n                else:\n                    asbn_bn_loss = torch.clamp(asbn_bn_loss, 0.0, 10.0)\n                \n                if not torch.isfinite(h_aug).all():\n                    print(\"❌ TATN: ASBN produced NaN/Inf in h_aug, reverting to pre-ASBN state\")\n                    h_aug = h_aug_pre_asbn\n                \n            except Exception as e:\n                print(f\"❌ TATN: ASBN forward failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                h_aug = h_aug_pre_asbn\n                asbn_bn_loss = torch.tensor(0.0, device=device)\n                domain_accuracy = 0.0\n                self.asbn_forward_errors += 1\n        elif use_asbn and domain_labels is None:\n            if current_step % 100 == 0 and _VERBOSE_LOGGING:\n                print(f\"⚠️  TATN: ASBN skipped (domain_labels=None) at step {current_step}\")\n\n        try:\n            enc_for_decoder = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                attentions=getattr(enc_outputs, \"attentions\", None),\n            )\n        except Exception:\n            enc_for_decoder = (h_aug,)\n\n        if training_mode:\n            try:\n                pad_id = getattr(self.tokenizer, \"pad_token_id\", 1)\n            except Exception:\n                pad_id = 1\n\n            try:\n                bos = int(getattr(self.mbart.config, \"decoder_start_token_id\", self.en_token_id))\n                \n                if bos < 0 or bos >= self.vocab_size:\n                    print(f\"⚠️  WARNING: Invalid BOS token {bos}, clamping to vocab range [0, {self.vocab_size-1}]\")\n                    bos = max(0, min(bos, self.vocab_size - 1))\n                \n                bos = max(0, min(bos, self.vocab_size - 1))\n                \n                bos_col = torch.full((batch_size, 1), bos, dtype=torch.long, device=device)\n                \n                pad_mask = (labels == pad_id)\n                labels_clamped = torch.clamp(labels, min=0, max=self.vocab_size - 1)\n                labels_clamped[pad_mask] = -100\n                \n                labels_shifted = labels_clamped[:, :-1]\n                \n                decoder_input_ids = torch.cat([bos_col, labels_shifted], dim=1)\n                \n                decoder_input_ids = torch.clamp(decoder_input_ids, min=0, max=self.vocab_size - 1)\n                \n                decoder_attention_mask = (decoder_input_ids != pad_id).long()\n                \n            except Exception as e:\n                print(f\"❌ TATN: Decoder input construction failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                decoder_input_ids = None\n                decoder_attention_mask = None\n\n            try:\n                seq_outputs = self.mbart(\n                    input_ids=None,\n                    attention_mask=attention_mask,\n                    encoder_outputs=enc_for_decoder,\n                    decoder_input_ids=decoder_input_ids,\n                    decoder_attention_mask=decoder_attention_mask,\n                    labels=labels_clamped,\n                    use_cache=False,\n                    return_dict=True,\n                )\n                translation_loss = seq_outputs.loss\n                if translation_loss is None or not torch.isfinite(translation_loss):\n                    print(f\"❌ TATN: Translation loss is None or NaN/Inf: {translation_loss}\")\n                    translation_loss = torch.tensor(10.0, device=device)\n                else:\n                    translation_loss = torch.clamp(translation_loss, 0.0, 100.0)\n            except Exception as e:\n                print(f\"❌ TATN: MBART forward failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                translation_loss = torch.tensor(10.0, device=device)\n\n            asbn_loss = torch.tensor(0.0, device=device)\n            if use_asbn and _ENABLE_ASBN_TRAINING:\n                try:\n                    asbn_grl_result = self.asbn.forward_with_grl_simplified(\n                        h_aug,\n                        dscd.get(\"proto_probs\", None),\n                        dscd.get(\"uncertainties\", None),\n                        dscd.get(\"gates\", None),\n                        token_word_map=token_word_map,\n                        domain_labels=domain_labels,\n                        global_step=current_step,\n                    )\n                    if isinstance(asbn_grl_result, tuple) and len(asbn_grl_result) >= 1:\n                        asbn_loss = asbn_grl_result[0]\n                    else:\n                        print(f\"❌ TATN: ASBN GRL returned unexpected type: {type(asbn_grl_result)}\")\n                        asbn_loss = torch.tensor(0.0, device=device)\n                    if not isinstance(asbn_loss, torch.Tensor):\n                        print(f\"❌ TATN: ASBN GRL returned non-tensor: {type(asbn_loss)}\")\n                        asbn_loss = torch.tensor(float(asbn_loss), device=device)\n                    if not torch.isfinite(asbn_loss):\n                        print(f\"❌ TATN: ASBN GRL loss is NaN/Inf: {asbn_loss}\")\n                        asbn_loss = torch.tensor(0.0, device=device)\n                    else:\n                        asbn_loss = torch.clamp(asbn_loss, 0.0, 10.0)\n                except Exception as e:\n                    print(f\"❌ TATN: ASBN GRL forward failed: {type(e).__name__}: {e}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    asbn_loss = torch.tensor(0.0, device=device)\n            elif use_asbn and not _ENABLE_ASBN_TRAINING:\n                if current_step % 100 == 0 and _VERBOSE_LOGGING:\n                    print(f\"⚠️  TATN: ASBN GRL disabled (ENABLE_ASBN_TRAINING=False)\")\n\n            dscd_reg = torch.tensor(0.0, device=device)\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(\n                    dscd.get(\"proto_probs\", []),\n                    gates_list=dscd.get(\"gates\", []),\n                    min_gate=0.0,\n                )\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(float(dscd_reg), device=device)\n                if not torch.isfinite(dscd_reg):\n                    print(f\"❌ TATN: DSCD reg is NaN/Inf: {dscd_reg}\")\n                    dscd_reg = torch.tensor(0.0, device=device)\n                else:\n                    dscd_reg = torch.clamp(dscd_reg.to(device), 0.0, 5.0)\n            except Exception as e:\n                print(f\"❌ TATN: DSCD entropy regularizer failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            if not torch.isfinite(translation_loss):\n                print(f\"❌ TATN: Translation loss NaN/Inf: {translation_loss}, resetting to 10.0\")\n                translation_loss = torch.tensor(10.0, device=device)\n            if not torch.isfinite(asbn_loss):\n                print(f\"❌ TATN: ASBN loss NaN/Inf: {asbn_loss}, resetting to 0.0\")\n                asbn_loss = torch.tensor(0.0, device=device)\n            if not torch.isfinite(asbn_bn_loss):\n                print(f\"❌ TATN: ASBN BN loss NaN/Inf: {asbn_bn_loss}, resetting to 0.0\")\n                asbn_bn_loss = torch.tensor(0.0, device=device)\n            if not torch.isfinite(dscd_reg):\n                print(f\"❌ TATN: DSCD reg NaN/Inf: {dscd_reg}, resetting to 0.0\")\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            translation_loss = torch.clamp(translation_loss, 0.0, 100.0)\n            asbn_loss = torch.clamp(asbn_loss, 0.0, 10.0)\n            asbn_bn_loss = torch.clamp(asbn_bn_loss, 0.0, 10.0)\n            dscd_reg = torch.clamp(dscd_reg, 0.0, 5.0)\n\n            total_asbn_loss = asbn_loss + asbn_bn_loss\n            total_loss = translation_loss + \\\n                         _LAMBDA_ASBN * total_asbn_loss + \\\n                         _LAMBDA_DSCD * dscd_reg\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n            if not torch.isfinite(total_loss):\n                print(f\"❌ TATN: Total loss is NaN/Inf: {total_loss}, using translation_loss only\")\n                total_loss = translation_loss\n\n            try:\n                del enc_outputs, h, raw_dscd, h_aug_pre_asbn\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return {\n                \"loss\": total_loss,\n                \"translation_loss\": translation_loss,\n                \"asbn_loss\": total_asbn_loss,\n                \"dscd_loss\": dscd_reg,\n                \"model_output\": {\"loss\": translation_loss},\n                \"domain_accuracy\": torch.tensor(domain_accuracy, device=device, dtype=torch.float32),\n            }\n\n        explanations_list: List[List[Dict[str, Any]]] = [[] for _ in range(batch_size)]\n\n        if (not self.training) and _ENABLE_TRG_INFERENCE:\n            token_word_map_len = len(token_word_map) if token_word_map else 0\n            if token_word_map_len != batch_size:\n                if _VERBOSE_LOGGING:\n                    print(f\"⚠️  TATN: token_word_map length mismatch: {token_word_map_len} != {batch_size}\")\n            \n            for b in range(batch_size):\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n                except Exception:\n                    tokens = [str(x) for x in input_ids[b].tolist()]\n\n                dscd_for_trg = {\n                    \"proto_probs\": [dscd[\"proto_probs\"][b]],\n                    \"uncertainties\": [dscd[\"uncertainties\"][b]],\n                    \"gates\": [dscd[\"gates\"][b]],\n                    \"span_preds\": [dscd[\"span_preds\"][b]],\n                }\n                tok_map_b = token_word_map[b] if token_word_map and b < token_word_map_len else None\n\n                try:\n                    exps = self.trg_system.process_sentence_for_explanations(\n                        tokens=tokens,\n                        dscd_outputs=dscd_for_trg,\n                        token_word_map=tok_map_b,\n                        uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                        span_threshold=_SPAN_THRESHOLD,\n                        decoder_attention=None,\n                        max_explanations=globals().get(\"MAX_EXPLANATIONS_PER_SENTENCE\", 10),\n                    )\n                    if isinstance(exps, list):\n                        explanations_list[b] = exps\n                except Exception as e:\n                    if _VERBOSE_LOGGING:\n                        print(f\"❌ TATN: TRG explanation generation failed for batch {b}: {e}\")\n                    explanations_list[b] = []\n\n        outputs = {\n            \"encoder_outputs\": enc_outputs,\n            \"dscd_outputs\": dscd,\n            \"sense_augmented_embeddings\": h_aug,\n            \"explanations\": explanations_list,\n            \"asbn_loss\": asbn_bn_loss,\n            \"domain_accuracy\": domain_accuracy,\n            \"ambiguity_signals\": {\n                \"span\": dscd.get(\"span_preds\", []),\n                \"uncertainty\": dscd.get(\"uncertainties\", []),\n                \"confidence\": [\n                    [\n                        max(0.0, min(1.0, 1.0 - (float(u.item()) if isinstance(u, torch.Tensor) else float(u))))\n                        for u in row\n                    ]\n                    for row in dscd.get(\"uncertainties\", [])\n                ],\n                \"proto_probs\": dscd.get(\"proto_probs\", []),\n            },\n        }\n\n        try:\n            del h, raw_dscd\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return outputs\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        **kwargs\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n            **kwargs,\n        )\n\n    def generate(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        max_length: int = 128,\n        num_beams: int = 5,\n        early_stopping: bool = True,\n        **kwargs\n    ) -> torch.Tensor:\n        device = input_ids.device\n        batch_size = input_ids.size(0)\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n            enc_wrapped = BaseModelOutput(\n                last_hidden_state=_safe_get_last_hidden_state(enc_outputs),\n                hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                attentions=getattr(enc_outputs, \"attentions\", None),\n            )\n            \n            forced_bos_id = getattr(self.mbart.config, \"forced_bos_token_id\", self.en_token_id)\n            forced_bos_id = int(forced_bos_id)\n            forced_bos_id = max(0, min(forced_bos_id, self.vocab_size - 1))\n            \n            eos_token_id = getattr(self.tokenizer, \"eos_token_id\", 2)\n            eos_token_id = int(eos_token_id)\n            eos_token_id = max(0, min(eos_token_id, self.vocab_size - 1))\n            \n            pad_token_id = getattr(self.tokenizer, \"pad_token_id\", 1)\n            pad_token_id = int(pad_token_id)\n            pad_token_id = max(0, min(pad_token_id, self.vocab_size - 1))\n\n            gen_kwargs = dict(\n                input_ids=None,\n                attention_mask=attention_mask,\n                encoder_outputs=enc_wrapped,\n                max_length=min(max_length, 100),\n                min_length=1,\n                num_beams=min(num_beams, 4),\n                early_stopping=True,\n                no_repeat_ngram_size=3,\n                repetition_penalty=3.0,\n                length_penalty=1.2,\n                do_sample=False,\n                forced_bos_token_id=forced_bos_id,\n                eos_token_id=eos_token_id,\n                pad_token_id=pad_token_id,\n                num_return_sequences=1,\n                output_scores=False,\n                return_dict_in_generate=False,\n            )\n            gen_kwargs.update(kwargs)\n            outputs = self.mbart.generate(**gen_kwargs)\n            if outputs.size(1) > max_length:\n                outputs = outputs[:, :max_length]\n            if outputs.size(1) < 2:\n                print(f\"⚠️  TATN: Generate returned short output: {outputs.shape}\")\n            return outputs\n        except Exception as e:\n            print(f\"❌ TATN: Generate failed: {type(e).__name__}: {e}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            fallback_bos = getattr(self.tokenizer, \"bos_token_id\", self.en_token_id)\n            fallback_bos = max(0, min(int(fallback_bos), self.vocab_size - 1))\n            fallback_eos = getattr(self.tokenizer, \"eos_token_id\", 2)\n            fallback_eos = max(0, min(int(fallback_eos), self.vocab_size - 1))\n            return torch.tensor([[fallback_bos, fallback_eos]], dtype=torch.long, device=device).expand(batch_size, 2)\n\n    def get_component_stats(self) -> Dict[str, Any]:\n        stats: Dict[str, Any] = {\n            \"global_step\": self.global_step,\n            \"last_discovery_step\": self.last_discovery_step,\n            \"last_validation_step\": self.last_validation_step,\n            \"asbn_forward_errors\": self.asbn_forward_errors,\n            \"dscd_forward_errors\": self.dscd_forward_errors,\n        }\n        try:\n            stats[\"dscd\"] = {\n                \"total_tokens\": len(self.dscd.prototype_stores),\n                \"total_prototypes\": sum(store.size() for store in self.dscd.prototype_stores.values()),\n                \"num_homographs\": len(self.dscd.discovered_homographs),\n            }\n        except Exception:\n            stats[\"dscd\"] = {\"total_tokens\": 0, \"total_prototypes\": 0, \"num_homographs\": 0}\n        try:\n            stats[\"asbn\"] = (\n                self.asbn.get_detailed_stats()\n                if hasattr(self.asbn, \"get_detailed_stats\") else {}\n            )\n        except Exception:\n            stats[\"asbn\"] = {}\n        try:\n            stats[\"trg\"] = (\n                self.trg_system.get_statistics()\n                if hasattr(self.trg_system, \"get_statistics\") else {}\n            )\n        except Exception:\n            stats[\"trg\"] = {}\n        return stats\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 6: TATN Ready - STEP 124 NaN/Inf FIX APPLIED\")\nprint(\"=\" * 80)\nprint(\"Config:\")\nprint(f\"  - Source: {_SOURCE_LANGUAGE}, Target: {_TARGET_LANGUAGE}\")\nprint(f\"  - Label smoothing: {_LABEL_SMOOTHING}\")\nprint(f\"  - Decoder dropout: {_DECODER_DROPOUT}\")\nprint(f\"  - DSCD clustering enabled: {_DSCD_ENABLE_TRAINING_CLUSTERING}\")\nprint(f\"  - ASBN training enabled: {_ENABLE_ASBN_TRAINING}\")\nprint(f\"  - TRG inference enabled: {_ENABLE_TRG_INFERENCE}\")\nprint(f\"  - Discovery freq: {_PERIODIC_DISCOVERY_FREQUENCY}\")\nprint(f\"  - λ_ASBN: {_LAMBDA_ASBN}, λ_DSCD: {_LAMBDA_DSCD}\")\nprint(\"NaN/Inf Protections Applied:\")\nprint(\"  ✅ Encoder h NaN check + clamp [-100, 100]\")\nprint(\"  ✅ h_augmented NaN zeroing after DSCD\")\nprint(\"  ✅ h_aug_pre_asbn saved before ASBN forward\")\nprint(\"  ✅ ASBN h_aug NaN → REVERT to h_aug_pre_asbn (CRITICAL FIX)\")\nprint(\"  ✅ Proto probs element clamp AFTER division normalization\")\nprint(\"  ✅ All scalar matrices clamped [0, 1] with NaN check\")\nprint(\"  ✅ All losses clamped: translation [0, 100], asbn [0, 10], dscd [0, 5]\")\nprint(\"  ✅ Decoder input IDs clamped to [0, vocab_size-1]\")\nprint(\"  ✅ BOS/EOS/PAD tokens validated and bounded\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP - NaN/Inf GRADIENT FULLY HARDENED + LABEL VALIDATION\n# ==============================================================================\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\n_CELL7_WORDMAP_BUILT_COUNT = 0\n_CELL7_WORDMAP_PROVIDED_COUNT = 0\n_CELL7_FORWARD_CALL_COUNT = 0\n_CELL7_BACKWARD_SUCCESS_COUNT = 0\n\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\n\ntry:\n    _DEVICE = DEVICE\n    if not isinstance(_DEVICE, torch.device):\n        _DEVICE = torch.device(str(_DEVICE))\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept Exception:\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept Exception:\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept Exception:\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept Exception:\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept Exception:\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept Exception:\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept Exception:\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept Exception:\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept Exception:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept Exception:\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\",\n        \"তারা\", \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast(enabled=True)\n    except Exception:\n        return nullcontext()\n\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\n\ndef _build_token_to_word_map(tokenizer, input_ids):\n    batch_word_maps = []\n    for batch_idx in range(input_ids.size(0)):\n        try:\n            tokens = tokenizer.convert_ids_to_tokens(input_ids[batch_idx].tolist())\n        except Exception:\n            tokens = [str(x) for x in input_ids[batch_idx].tolist()]\n\n        word_map = {}\n        current_word = \"\"\n        word_start_idx = 0\n\n        for i, token in enumerate(tokens):\n            if not token or token in [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"]:\n                word_map[i] = None\n                continue\n\n            if token.startswith(\"▁\") or token.startswith(\"\\u2581\") or token.startswith(\"Ġ\"):\n                if current_word:\n                    clean_word = current_word.replace(\"▁\", \"\").replace(\"\\u2581\", \"\").replace(\"Ġ\", \"\").strip()\n                    if clean_word:\n                        for j in range(word_start_idx, i):\n                            word_map[j] = clean_word\n                current_word = token\n                word_start_idx = i\n            else:\n                current_word += token\n\n        if current_word:\n            clean_word = current_word.replace(\"▁\", \"\").replace(\"\\u2581\", \"\").replace(\"Ġ\", \"\").strip()\n            if clean_word:\n                for j in range(word_start_idx, len(tokens)):\n                    word_map[j] = clean_word\n\n        batch_word_maps.append(word_map)\n    return batch_word_maps\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        word_prototype_counts = defaultdict(int)\n        for token_key, store in stores.items():\n            try:\n                num_protos = 0\n                if hasattr(store, \"size\") and callable(getattr(store, \"size\")):\n                    try:\n                        num_protos = int(store.size())\n                    except Exception:\n                        num_protos = 0\n                else:\n                    cent = getattr(store, \"centroids\", None)\n                    try:\n                        num_protos = len(cent) if cent is not None else 0\n                    except Exception:\n                        num_protos = 0\n\n                clean_token = (\n                    str(token_key)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean_token:\n                    word_prototype_counts[clean_token] = max(word_prototype_counts[clean_token], num_protos)\n            except Exception:\n                continue\n\n        return {w for w, c in word_prototype_counts.items() if c >= 2}\n    except Exception:\n        return set()\n\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n        stores = getattr(dscd, \"prototype_stores\", None) or {}\n        return len(stores)\n    except Exception:\n        return 0\n\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        return getattr(core, \"dscd\", None)\n    except Exception:\n        return None\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        dscd = _get_dscd_safe(model)\n        if dscd is None:\n            return\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        items = []\n        for token, store in stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                n_protos = int(store.size()) if hasattr(store, \"size\") and callable(getattr(store, \"size\")) else (len(getattr(store, \"centroids\", [])) if getattr(store, \"centroids\", None) is not None else 0)\n                items.append((token, total_count, n_protos))\n            except Exception:\n                continue\n        items.sort(key=lambda x: x[1], reverse=True)\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot) in enumerate(items[:top_n], 1):\n                tok_str = str(tok)[:20]\n                print(f\"  {i:2d}. {tok_str:20s} samples={cnt:4d} protos={prot}\")\n    except Exception:\n        pass\n\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n        if hasattr(dscd, \"discovered_log\") and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[DISCOVERY-STATUS] Step {global_step}: {total_discovered} discovery events\")\n    except Exception:\n        pass\n\n\ndef _check_gradients(model: torch.nn.Module, global_step: int):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        asbn = getattr(core, \"asbn\", None)\n        dscd_grad_count = 0\n        dscd_total_params = 0\n        if dscd is not None:\n            for p in dscd.parameters():\n                dscd_total_params += 1\n                if p.grad is not None and torch.isfinite(p.grad).all():\n                    dscd_grad_count += 1\n        asbn_grad_count = 0\n        asbn_total_params = 0\n        if asbn is not None:\n            for p in asbn.parameters():\n                asbn_total_params += 1\n                if p.grad is not None and torch.isfinite(p.grad).all():\n                    asbn_grad_count += 1\n        dscd_status = \"✓\" if dscd_grad_count > 0 else \"✗ NO GRADS\"\n        asbn_status = \"✓\" if asbn_grad_count > 0 else \"✗ NO GRADS\"\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(f\"[GRAD-CHECK] Step {global_step}: DSCD {dscd_status} ({dscd_grad_count}/{dscd_total_params}) | ASBN {asbn_status} ({asbn_grad_count}/{asbn_total_params})\")\n    except Exception:\n        pass\n\n\ndef _extract_loss_components(forward_out, device) -> Dict[str, float]:\n    components = {\n        \"total_loss\": 0.0,\n        \"translation_loss\": 0.0,\n        \"asbn_loss\": 0.0,\n        \"dscd_loss\": 0.0,\n        \"domain_accuracy\": 0.0,\n    }\n    \n    if forward_out is None or not isinstance(forward_out, dict):\n        return components\n    \n    try:\n        if \"translation_loss\" in forward_out:\n            tl = forward_out[\"translation_loss\"]\n            if isinstance(tl, torch.Tensor):\n                if not torch.isfinite(tl).all():\n                    print(f\"⚠️  [EXTRACT] translation_loss contains NaN/Inf, setting to 0.0\")\n                    tl = torch.tensor(0.0, device=device)\n                tl = torch.clamp(tl, min=0.0, max=100.0)\n                components[\"translation_loss\"] = float(tl.mean().item()) if tl.numel() > 1 else float(tl.item())\n            elif tl is not None:\n                try:\n                    val = float(tl)\n                    if not np.isfinite(val):\n                        val = 0.0\n                    components[\"translation_loss\"] = max(0.0, min(100.0, val))\n                except Exception:\n                    pass\n    except Exception:\n        pass\n    \n    try:\n        if \"asbn_loss\" in forward_out:\n            al = forward_out[\"asbn_loss\"]\n            if isinstance(al, torch.Tensor):\n                if not torch.isfinite(al).all():\n                    print(f\"⚠️  [EXTRACT] asbn_loss contains NaN/Inf, setting to 0.0\")\n                    al = torch.tensor(0.0, device=device)\n                al = torch.clamp(al, min=0.0, max=10.0)\n                components[\"asbn_loss\"] = float(al.mean().item()) if al.numel() > 1 else float(al.item())\n            elif al is not None:\n                try:\n                    val = float(al)\n                    if not np.isfinite(val):\n                        val = 0.0\n                    components[\"asbn_loss\"] = max(0.0, min(10.0, val))\n                except Exception:\n                    pass\n    except Exception:\n        pass\n    \n    try:\n        if \"dscd_loss\" in forward_out:\n            dl = forward_out[\"dscd_loss\"]\n            if isinstance(dl, torch.Tensor):\n                if not torch.isfinite(dl).all():\n                    print(f\"⚠️  [EXTRACT] dscd_loss contains NaN/Inf, setting to 0.0\")\n                    dl = torch.tensor(0.0, device=device)\n                dl = torch.clamp(dl, min=0.0, max=5.0)\n                components[\"dscd_loss\"] = float(dl.mean().item()) if dl.numel() > 1 else float(dl.item())\n            elif dl is not None:\n                try:\n                    val = float(dl)\n                    if not np.isfinite(val):\n                        val = 0.0\n                    components[\"dscd_loss\"] = max(0.0, min(5.0, val))\n                except Exception:\n                    pass\n    except Exception:\n        pass\n    \n    try:\n        if \"domain_accuracy\" in forward_out:\n            da = forward_out[\"domain_accuracy\"]\n            if isinstance(da, torch.Tensor):\n                if not torch.isfinite(da).all():\n                    da = torch.tensor(0.0, device=device)\n                da = torch.clamp(da, min=0.0, max=1.0)\n                components[\"domain_accuracy\"] = float(da.mean().item()) if da.numel() > 1 else float(da.item())\n            elif isinstance(da, (int, float)):\n                val = float(da)\n                if not np.isfinite(val):\n                    val = 0.0\n                components[\"domain_accuracy\"] = max(0.0, min(1.0, val))\n    except Exception:\n        pass\n    \n    components[\"total_loss\"] = (\n        components[\"translation_loss\"] + \n        components[\"asbn_loss\"] + \n        components[\"dscd_loss\"]\n    )\n    \n    if components[\"total_loss\"] == 0.0 and \"loss\" in forward_out:\n        try:\n            main_loss = forward_out[\"loss\"]\n            if isinstance(main_loss, torch.Tensor):\n                if not torch.isfinite(main_loss).all():\n                    print(f\"⚠️  [EXTRACT] main loss contains NaN/Inf, setting to 0.0\")\n                    main_loss = torch.tensor(0.0, device=device)\n                main_loss = torch.clamp(main_loss, min=0.0, max=100.0)\n                components[\"total_loss\"] = float(main_loss.mean().item()) if main_loss.numel() > 1 else float(main_loss.item())\n                if components[\"translation_loss\"] == 0.0:\n                    components[\"translation_loss\"] = components[\"total_loss\"]\n        except Exception:\n            pass\n    \n    return components\n\n\ndef _clear_all_gradients_aggressively(model, optimizer, phi_optimizer):\n    try:\n        optimizer.zero_grad(set_to_none=True)\n        if phi_optimizer is not None:\n            phi_optimizer.zero_grad(set_to_none=True)\n    except Exception:\n        pass\n    \n    try:\n        for param in model.parameters():\n            param.grad = None\n    except Exception:\n        pass\n    \n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    except Exception:\n        pass\n\n\ndef _sanitize_labels(labels: torch.Tensor, vocab_size: int, pad_token_id: int, device: torch.device) -> torch.Tensor:\n    labels = labels.clone()\n    \n    labels[labels >= vocab_size] = -100\n    labels[labels < -100] = -100\n    \n    if pad_token_id is not None and pad_token_id >= 0:\n        labels[labels == pad_token_id] = -100\n    \n    return labels\n\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True,\n    enable_asbn_training: bool = True,\n) -> torch.nn.Module:\n    global _CELL7_WORDMAP_BUILT_COUNT, _CELL7_WORDMAP_PROVIDED_COUNT\n    global _CELL7_FORWARD_CALL_COUNT, _CELL7_BACKWARD_SUCCESS_COUNT\n\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n\n    try:\n        vocab_size = len(tokenizer)\n    except Exception:\n        try:\n            vocab_size = tokenizer.vocab_size\n        except Exception:\n            vocab_size = 128104\n            print(f\"⚠️  [TRAIN] Cannot determine vocab_size, using default: {vocab_size}\")\n    \n    try:\n        pad_token_id = tokenizer.pad_token_id\n        if pad_token_id is None:\n            pad_token_id = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else 1\n    except Exception:\n        pad_token_id = 1\n\n    print(f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, accum_steps={accumulation_steps}\")\n    print(f\"[TRAIN] Vocab size: {vocab_size}, Pad token ID: {pad_token_id}\")\n    print(f\"[TRAIN] Validation: {'enabled' if enable_validation and validate_every > 0 else 'disabled'}\")\n    print(f\"[TRAIN] ASBN Training: {'ENABLED' if enable_asbn_training and phi_optimizer is not None else 'DISABLED'}\")\n    print(f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\")\n    print(f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(f\"[TRAIN] Gradient clip norm: {_GRAD_CLIP_NORM}\")\n\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=_USE_AMP and torch.cuda.is_available())\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"translation_losses\": [],\n        \"asbn_losses\": [],\n        \"dscd_losses\": [],\n        \"domain_accuracies\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"asbn_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"trg_explanation_history\": [],\n        \"gradient_checks\": [],\n        \"loss_component_breakdown\": [],\n        \"nan_gradient_events\": 0,\n        \"extreme_gradient_events\": 0,\n        \"label_corruption_fixes\": 0,\n    }\n\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n    last_asbn_loss = 0.0\n    last_translation_loss = 0.0\n    last_dscd_loss = 0.0\n    last_domain_accuracy = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(\"=\" * 80)\n\n        model.train()\n\n        try:\n            core = model.module if hasattr(model, \"module\") else model\n            trg = getattr(core, \"trg_system\", None)\n            if trg and hasattr(trg, \"reset_statistics\"):\n                try:\n                    trg.reset_statistics()\n                except Exception:\n                    pass\n            asbn = getattr(core, \"asbn\", None)\n            if asbn and hasattr(asbn, \"reset_stats\"):\n                try:\n                    asbn.reset_stats()\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n        try:\n            optimizer.zero_grad(set_to_none=True)\n            if phi_optimizer is not None:\n                phi_optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n\n        progress = None\n        try:\n            progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", dynamic_ncols=True)\n            for batch_idx, batch in enumerate(progress):\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n\n                if _DEBUG_DISCOVERY and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    _check_discovery_status(model, global_step)\n\n                if global_step % 50 == 0 and (_DEBUG_DISCOVERY or _VERBOSE_LOGGING):\n                    try:\n                        core_model = model.module if hasattr(model, \"module\") else model\n                        dscd = getattr(core_model, \"dscd\", None)\n                        if dscd and hasattr(dscd, \"buffers\"):\n                            lock = getattr(dscd, \"buffer_lock\", None)\n                            if lock:\n                                with lock:\n                                    buffer_sizes = [len(dscd.buffers[t]) for t in dscd.buffers]\n                            else:\n                                buffer_sizes = [len(dscd.buffers[t]) for t in dscd.buffers]\n                            avg_size = sum(buffer_sizes) / len(buffer_sizes) if buffer_sizes else 0\n                            ready_for_discovery = sum(1 for s in buffer_sizes if s >= max(2, getattr(dscd, \"n_min\", 2)))\n                            print(f\"[BUFFER] Step {global_step}: types={len(buffer_sizes)} avg_size={avg_size:.1f} ready={ready_for_discovery}\")\n                    except Exception:\n                        pass\n\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    if global_step % 100 == 0:\n                        print(f\"⚠️  [TRAIN] Step {global_step}: Batch is None\")\n                    continue\n\n                try:\n                    input_ids = batch.get(\"input_ids\", None) if isinstance(batch, dict) else None\n                    attention_mask = batch.get(\"attention_mask\", None) if isinstance(batch, dict) else None\n                    labels = batch.get(\"labels\", None) if isinstance(batch, dict) else None\n\n                    if input_ids is None or attention_mask is None or labels is None:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"missing_fields\"] += 1\n                        if global_step % 100 == 0:\n                            missing = []\n                            if input_ids is None:\n                                missing.append(\"input_ids\")\n                            if attention_mask is None:\n                                missing.append(\"attention_mask\")\n                            if labels is None:\n                                missing.append(\"labels\")\n                            print(f\"⚠️  [TRAIN] Step {global_step}: Missing fields: {missing}\")\n                        continue\n\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        bsz = int(input_ids.size(0))\n                        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            if global_step % 100 == 0:\n                                print(f\"⚠️  [TRAIN] Step {global_step}: Batch size {bsz} too small for {_NUM_GPUS} GPUs\")\n                            continue\n                        if keep != bsz:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n\n                    input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                    attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                    labels = labels.to(_DEVICE, non_blocking=True)\n\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        if global_step % 100 == 0:\n                            print(f\"⚠️  [TRAIN] Step {global_step}: Empty batch after processing\")\n                        continue\n\n                    labels = _sanitize_labels(labels, vocab_size, pad_token_id, _DEVICE)\n                    training_stats[\"label_corruption_fixes\"] += 1\n                    \n                    if (labels == -100).all():\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"all_labels_ignored\"] += 1\n                        if global_step % 100 == 0:\n                            print(f\"⚠️  [TRAIN] Step {global_step}: All labels are -100 (padding only)\")\n                        continue\n                    \n                    valid_label_count = (labels != -100).sum().item()\n                    if valid_label_count == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"no_valid_labels\"] += 1\n                        if global_step % 100 == 0:\n                            print(f\"⚠️  [TRAIN] Step {global_step}: No valid labels in batch\")\n                        continue\n\n                    token_word_map_for_batch = batch.get(\"token_word_map\", None) if isinstance(batch, dict) else None\n                    if token_word_map_for_batch is None or not isinstance(token_word_map_for_batch, list):\n                        try:\n                            token_word_map_for_batch = _build_token_to_word_map(tokenizer, input_ids)\n                            _CELL7_WORDMAP_BUILT_COUNT += 1\n                        except Exception:\n                            token_word_map_for_batch = [{} for _ in range(input_ids.size(0))]\n                    else:\n                        _CELL7_WORDMAP_PROVIDED_COUNT += 1\n\n                    if not token_word_map_for_batch or len(token_word_map_for_batch) != input_ids.size(0):\n                        token_word_map_for_batch = [{} for _ in range(input_ids.size(0))]\n\n                    _CELL7_FORWARD_CALL_COUNT += 1\n\n                    forward_kwargs = {\n                        \"input_ids\": input_ids,\n                        \"attention_mask\": attention_mask,\n                        \"labels\": labels,\n                        \"src_texts\": batch.get(\"src_text\", None) if isinstance(batch, dict) else None,\n                        \"token_word_map\": token_word_map_for_batch,\n                    }\n\n                    with get_amp_ctx():\n                        forward_out = model(**forward_kwargs)\n\n                    loss_tensor = None\n                    if isinstance(forward_out, torch.Tensor):\n                        loss_tensor = forward_out\n                    elif isinstance(forward_out, dict):\n                        loss_candidates = (\"loss\", \"total_loss\", \"translation_loss\")\n                        for k in loss_candidates:\n                            if k in forward_out:\n                                loss_tensor = forward_out.get(k)\n                                break\n                        if loss_tensor is None and \"model_output\" in forward_out and isinstance(forward_out[\"model_output\"], dict) and \"loss\" in forward_out[\"model_output\"]:\n                            loss_tensor = forward_out[\"model_output\"][\"loss\"]\n                    elif isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                        loss_tensor = forward_out[0]\n\n                    if loss_tensor is None:\n                        print(f\"❌ [TRAIN] Step {global_step}: Forward returned None loss! forward_out type={type(forward_out)}, keys={forward_out.keys() if isinstance(forward_out, dict) else 'N/A'}\")\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"none_loss\"] += 1\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        accumulated_steps = 0\n                        continue\n\n                    loss_components = _extract_loss_components(forward_out, _DEVICE)\n                    last_translation_loss = loss_components[\"translation_loss\"]\n                    last_asbn_loss = loss_components[\"asbn_loss\"]\n                    last_dscd_loss = loss_components[\"dscd_loss\"]\n                    last_domain_accuracy = loss_components[\"domain_accuracy\"]\n                    \n                    training_stats[\"translation_losses\"].append(last_translation_loss)\n                    training_stats[\"asbn_losses\"].append(last_asbn_loss)\n                    training_stats[\"dscd_losses\"].append(last_dscd_loss)\n                    training_stats[\"domain_accuracies\"].append(last_domain_accuracy)\n                    training_stats[\"loss_component_breakdown\"].append(loss_components)\n\n                    if not isinstance(loss_tensor, torch.Tensor):\n                        try:\n                            loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE, dtype=torch.float32)\n                        except Exception:\n                            print(f\"❌ [TRAIN] Step {global_step}: Cannot convert loss to tensor: {type(loss_tensor)}\")\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"loss_convert_fail\"] += 1\n                            _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                            accumulated_steps = 0\n                            continue\n                    else:\n                        loss_tensor = loss_tensor.to(_DEVICE)\n\n                    if loss_tensor.numel() > 1:\n                        loss_tensor = loss_tensor.mean()\n\n                    if not torch.isfinite(loss_tensor):\n                        print(f\"❌ [TRAIN] Step {global_step}: NaN/Inf loss BEFORE backward! trans={last_translation_loss:.4f} asbn={last_asbn_loss:.4f} dscd={last_dscd_loss:.4f}\")\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"nan_loss_pre_backward\"] += 1\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        accumulated_steps = 0\n                        continue\n\n                    loss_tensor = torch.clamp(loss_tensor, min=0.0, max=100.0)\n                    loss_val = float(loss_tensor.item())\n\n                    last_forward_loss = loss_val\n                    epoch_losses.append(loss_val)\n                    training_stats[\"total_loss\"].append(loss_val)\n\n                    accum_steps_safe = max(1, accumulation_steps)\n                    loss_scaled = loss_tensor / (accum_steps_safe + 1e-9)\n                    \n                    if not torch.isfinite(loss_scaled):\n                        print(f\"❌ [TRAIN] Step {global_step}: loss_scaled is NaN/Inf after division\")\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"nan_loss_scaled\"] += 1\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        accumulated_steps = 0\n                        continue\n                    \n                    loss_scaled = torch.clamp(loss_scaled, min=0.0, max=50.0)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n\n                    try:\n                        if scaler.is_enabled():\n                            scaler.scale(loss_scaled).backward()\n                        else:\n                            loss_scaled.backward()\n                        _CELL7_BACKWARD_SUCCESS_COUNT += 1\n                        \n                        has_grads = False\n                        has_nan_grads = False\n                        has_extreme_grads = False\n                        max_grad_norm_before_clip = 0.0\n                        \n                        for p in model.parameters():\n                            if p.grad is not None:\n                                has_grads = True\n                                if not torch.isfinite(p.grad).all():\n                                    has_nan_grads = True\n                                    break\n                                grad_norm = p.grad.data.norm(2).item()\n                                max_grad_norm_before_clip = max(max_grad_norm_before_clip, grad_norm)\n                                if grad_norm > 100.0:\n                                    has_extreme_grads = True\n                        \n                        if not has_grads:\n                            print(f\"⚠️  [TRAIN] Step {global_step}: Backward completed but NO gradients created\")\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"no_grads\"] += 1\n                            _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                            accumulated_steps = 0\n                            continue\n                        \n                        if has_nan_grads:\n                            print(f\"❌ [TRAIN] Step {global_step}: Backward created NaN/Inf gradients\")\n                            training_stats[\"skipped_batches\"] += 1\n                            training_stats[\"nan_gradient_events\"] += 1\n                            skip_reasons[\"nan_grads_post_backward\"] += 1\n                            _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                            accumulated_steps = 0\n                            continue\n                        \n                        if has_extreme_grads:\n                            if _VERBOSE_LOGGING:\n                                print(f\"⚠️  [TRAIN] Step {global_step}: Extreme gradient detected (max_norm={max_grad_norm_before_clip:.2f}), will clip\")\n                            training_stats[\"extreme_gradient_events\"] += 1\n                            \n                    except Exception as e:\n                        print(f\"❌ [TRAIN] Step {global_step}: Backward failed: {type(e).__name__}: {e}\")\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"backward_failed\"] += 1\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        accumulated_steps = 0\n                        continue\n\n                    accumulated_steps += 1\n\n                    if accumulated_steps >= accumulation_steps:\n                        update_success = True\n                        try:\n                            if scaler.is_enabled():\n                                scaler.unscale_(optimizer)\n                                \n                                try:\n                                    grad_norm = torch.nn.utils.clip_grad_norm_(\n                                        model.parameters(), \n                                        _GRAD_CLIP_NORM,\n                                        error_if_nonfinite=False\n                                    )\n                                except RuntimeError as e:\n                                    if \"non-finite\" in str(e).lower():\n                                        print(f\"❌ [TRAIN] Step {global_step}: Gradient clipping detected non-finite gradients\")\n                                        update_success = False\n                                        training_stats[\"nan_gradient_events\"] += 1\n                                        grad_norm = torch.tensor(float('inf'))\n                                    else:\n                                        raise\n                                \n                                if not torch.isfinite(grad_norm):\n                                    print(f\"❌ [TRAIN] Step {global_step}: Gradient norm is NaN/Inf: {grad_norm}\")\n                                    update_success = False\n                                    training_stats[\"nan_gradient_events\"] += 1\n                                else:\n                                    scaler.step(optimizer)\n                                    training_stats[\"optimizer_updates\"] += 1\n                                \n                                if phi_optimizer is not None and enable_asbn_training:\n                                    phi_params = []\n                                    for g in phi_optimizer.param_groups:\n                                        phi_params.extend([p for p in g.get(\"params\", [])])\n                                    \n                                    if phi_params:\n                                        try:\n                                            scaler.unscale_(phi_optimizer)\n                                        except RuntimeError as e:\n                                            if \"unscale_() has already been called\" not in str(e):\n                                                if _VERBOSE_LOGGING:\n                                                    print(f\"⚠️  [TRAIN] Step {global_step}: Phi unscale failed: {e}\")\n                                        \n                                        try:\n                                            phi_grad_norm = torch.nn.utils.clip_grad_norm_(\n                                                phi_params, \n                                                _GRAD_CLIP_NORM,\n                                                error_if_nonfinite=False\n                                            )\n                                        except RuntimeError as e:\n                                            if \"non-finite\" in str(e).lower():\n                                                if _VERBOSE_LOGGING:\n                                                    print(f\"❌ [TRAIN] Step {global_step}: Phi gradient clipping detected non-finite gradients\")\n                                                phi_grad_norm = torch.tensor(float('inf'))\n                                            else:\n                                                raise\n                                        \n                                        if not torch.isfinite(phi_grad_norm):\n                                            if _VERBOSE_LOGGING:\n                                                print(f\"❌ [TRAIN] Step {global_step}: Phi gradient norm is NaN/Inf: {phi_grad_norm}\")\n                                        else:\n                                            try:\n                                                scaler.step(phi_optimizer)\n                                                training_stats[\"asbn_updates\"] += 1\n                                            except Exception as e:\n                                                print(f\"❌ [TRAIN] Step {global_step}: Phi optimizer step failed: {type(e).__name__}: {e}\")\n                                    else:\n                                        if global_step % 100 == 0 and _VERBOSE_LOGGING:\n                                            print(f\"⚠️  [TRAIN] Step {global_step}: Phi optimizer has no parameters\")\n                                \n                                scaler.update()\n                            else:\n                                try:\n                                    grad_norm = torch.nn.utils.clip_grad_norm_(\n                                        model.parameters(), \n                                        _GRAD_CLIP_NORM,\n                                        error_if_nonfinite=False\n                                    )\n                                except RuntimeError as e:\n                                    if \"non-finite\" in str(e).lower():\n                                        print(f\"❌ [TRAIN] Step {global_step}: Gradient clipping detected non-finite gradients\")\n                                        update_success = False\n                                        training_stats[\"nan_gradient_events\"] += 1\n                                        grad_norm = torch.tensor(float('inf'))\n                                    else:\n                                        raise\n                                \n                                if not torch.isfinite(grad_norm):\n                                    print(f\"❌ [TRAIN] Step {global_step}: Gradient norm is NaN/Inf: {grad_norm}\")\n                                    update_success = False\n                                    training_stats[\"nan_gradient_events\"] += 1\n                                else:\n                                    optimizer.step()\n                                    training_stats[\"optimizer_updates\"] += 1\n                                \n                                if phi_optimizer is not None and enable_asbn_training:\n                                    phi_params = []\n                                    for g in phi_optimizer.param_groups:\n                                        phi_params.extend([p for p in g.get(\"params\", [])])\n                                    \n                                    if phi_params:\n                                        try:\n                                            phi_grad_norm = torch.nn.utils.clip_grad_norm_(\n                                                phi_params, \n                                                _GRAD_CLIP_NORM,\n                                                error_if_nonfinite=False\n                                            )\n                                        except RuntimeError as e:\n                                            if \"non-finite\" in str(e).lower():\n                                                if _VERBOSE_LOGGING:\n                                                    print(f\"❌ [TRAIN] Step {global_step}: Phi gradient clipping detected non-finite gradients\")\n                                                phi_grad_norm = torch.tensor(float('inf'))\n                                            else:\n                                                raise\n                                        \n                                        if not torch.isfinite(phi_grad_norm):\n                                            if _VERBOSE_LOGGING:\n                                                print(f\"❌ [TRAIN] Step {global_step}: Phi gradient norm is NaN/Inf: {phi_grad_norm}\")\n                                        else:\n                                            try:\n                                                phi_optimizer.step()\n                                                training_stats[\"asbn_updates\"] += 1\n                                            except Exception as e:\n                                                print(f\"❌ [TRAIN] Step {global_step}: Phi optimizer step failed: {type(e).__name__}: {e}\")\n                                    else:\n                                        if global_step % 100 == 0 and _VERBOSE_LOGGING:\n                                            print(f\"⚠️  [TRAIN] Step {global_step}: Phi optimizer has no parameters\")\n\n                            optimizer.zero_grad(set_to_none=True)\n                            if phi_optimizer is not None and enable_asbn_training:\n                                phi_optimizer.zero_grad(set_to_none=True)\n\n                            if not update_success:\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"nan_grad_during_update\"] += 1\n                                _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"❌ [OOM] OOM at step {global_step}\")\n                                _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                                for p in model.parameters():\n                                    p.grad = None\n                                clear_all_gpu_caches()\n                                accumulated_steps = 0\n                                continue\n                            else:\n                                print(f\"❌ [TRAIN] Step {global_step}: Optimizer step RuntimeError: {e}\")\n                                if _VERBOSE_LOGGING:\n                                    traceback.print_exc()\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n                        except Exception as e:\n                            print(f\"❌ [TRAIN] Step {global_step}: Optimizer step failed: {type(e).__name__}: {e}\")\n                            if _VERBOSE_LOGGING:\n                                traceback.print_exc()\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n                        finally:\n                            accumulated_steps = 0\n\n                    if global_step % 100 == 0 and (_DEBUG_DISCOVERY or _VERBOSE_LOGGING):\n                        _check_gradients(model, global_step)\n                        training_stats[\"gradient_checks\"].append({\n                            \"step\": global_step,\n                            \"timestamp\": time.time(),\n                        })\n\n                    if enable_validation and validate_every and validate_every > 0 and global_step % validate_every == 0:\n                        if accumulated_steps > 0:\n                            print(f\"⚠️  [TRAIN] Step {global_step}: Validation triggered mid-accumulation, flushing {accumulated_steps} gradients\")\n                            _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                            accumulated_steps = 0\n                        try:\n                            val_fn = globals().get(\"comprehensive_epoch_validation\", None)\n                            if callable(val_fn):\n                                print(f\"🔍 [VALIDATION] Running validation at step {global_step}...\")\n                                validation_results = val_fn(\n                                    model,\n                                    tokenizer,\n                                    epoch,\n                                    global_step,\n                                    _SOURCE_LANGUAGE,\n                                    _TARGET_LANGUAGE,\n                                    _MAX_LENGTH,\n                                    _DEVICE,\n                                )\n                                if validation_results and validation_results.get(\"validation_completed\", False):\n                                    training_stats[\"epoch_validations\"].append(validation_results)\n                                    print(f\"✅ [VALIDATION] Completed at step {global_step}\")\n                                    pending_validation = False\n                                else:\n                                    print(f\"⚠️  [VALIDATION] Failed at step {global_step}\")\n                            else:\n                                print(f\"⚠️  [VALIDATION] comprehensive_epoch_validation not found\")\n                        except Exception as e:\n                            print(f\"❌ [VALIDATION] Failed at step {global_step}: {type(e).__name__}: {e}\")\n                            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                                traceback.print_exc()\n                        model.train()\n\n                    if global_step % DEBUG_PRINT_INTERVAL == 0 and (_DEBUG_DISCOVERY or _VERBOSE_LOGGING):\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cluster_count = _get_cluster_count(model)\n                        print(f\"[TRAIN-DEBUG] step={global_step} loss={last_forward_loss:.4f} trans={last_translation_loss:.4f} asbn={last_asbn_loss:.4f} dscd={last_dscd_loss:.4f} clusters={cluster_count}\")\n                        _print_top_clusters(model, top_n=5)\n\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"❌ [OOM] Caught OOM at step {global_step}\")\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        for p in model.parameters():\n                            p.grad = None\n                        clear_all_gpu_caches()\n                        accumulated_steps = 0\n                        continue\n                    else:\n                        print(f\"❌ [TRAIN] Step {global_step}: RuntimeError: {type(e).__name__}: {e}\")\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                        accumulated_steps = 0\n                        continue\n\n                except Exception as e:\n                    print(f\"❌ [TRAIN] Step {global_step}: Exception: {type(e).__name__}: {e}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    _clear_all_gradients_aggressively(model, optimizer, phi_optimizer)\n                    accumulated_steps = 0\n                    continue\n\n                processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n                expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n                success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n                cluster_count = _get_cluster_count(model)\n                next_disc_str = \"NA\"\n                try:\n                    if _PERIODIC_DISCOVERY_FREQUENCY and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                        steps_to_next = _PERIODIC_DISCOVERY_FREQUENCY - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                        if steps_to_next >= _PERIODIC_DISCOVERY_FREQUENCY:\n                            steps_to_next = 0\n                        next_disc_str = f\"next_disc_in={steps_to_next}\"\n                except Exception:\n                    next_disc_str = \"next_disc=err\"\n                \n                safe_last_forward_loss = max(0.0, min(last_forward_loss, 1000.0))\n                safe_last_translation_loss = max(0.0, min(last_translation_loss, 1000.0))\n                safe_last_asbn_loss = max(0.0, min(last_asbn_loss, 100.0))\n                safe_last_dscd_loss = max(0.0, min(last_dscd_loss, 100.0))\n                \n                progress.set_postfix_str(f\"loss={safe_last_forward_loss:.4f} trans={safe_last_translation_loss:.4f} asbn={safe_last_asbn_loss:.4f} dscd={safe_last_dscd_loss:.4f} dom_acc={last_domain_accuracy:.2f} rate={success_rate:.1f}% clusters={cluster_count} {next_disc_str}\")\n\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n\n        if accumulated_steps > 0:\n            print(f\"⚠️  [TRAIN] End of epoch {epoch}: Flushing {accumulated_steps} accumulated gradients\")\n            try:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    try:\n                        grad_norm = torch.nn.utils.clip_grad_norm_(\n                            model.parameters(), \n                            _GRAD_CLIP_NORM,\n                            error_if_nonfinite=False\n                        )\n                    except RuntimeError as e:\n                        if \"non-finite\" in str(e).lower():\n                            print(f\"❌ [TRAIN] End of epoch {epoch}: Gradient clipping detected non-finite gradients\")\n                            grad_norm = torch.tensor(float('inf'))\n                        else:\n                            raise\n                    \n                    if torch.isfinite(grad_norm):\n                        scaler.step(optimizer)\n                        training_stats[\"optimizer_updates\"] += 1\n                        print(f\"✅ [TRAIN] End of epoch {epoch}: Flushed main optimizer (grad_norm={grad_norm:.4f})\")\n                    else:\n                        print(f\"❌ [TRAIN] End of epoch {epoch}: Gradient norm is NaN/Inf: {grad_norm}\")\n                    \n                    if phi_optimizer is not None and enable_asbn_training:\n                        try:\n                            scaler.unscale_(phi_optimizer)\n                        except RuntimeError as e:\n                            if \"unscale_() has already been called\" not in str(e):\n                                print(f\"⚠️  [TRAIN] End of epoch {epoch}: Phi unscale failed: {e}\")\n                        \n                        phi_params = []\n                        for g in phi_optimizer.param_groups:\n                            phi_params.extend([p for p in g.get(\"params\", [])])\n                        \n                        if phi_params:\n                            try:\n                                phi_grad_norm = torch.nn.utils.clip_grad_norm_(\n                                    phi_params, \n                                    _GRAD_CLIP_NORM,\n                                    error_if_nonfinite=False\n                                )\n                            except RuntimeError as e:\n                                if \"non-finite\" in str(e).lower():\n                                    print(f\"❌ [TRAIN] End of epoch {epoch}: Phi gradient clipping detected non-finite gradients\")\n                                    phi_grad_norm = torch.tensor(float('inf'))\n                                else:\n                                    raise\n                            \n                            if torch.isfinite(phi_grad_norm):\n                                try:\n                                    scaler.step(phi_optimizer)\n                                    training_stats[\"asbn_updates\"] += 1\n                                    print(f\"✅ [TRAIN] End of epoch {epoch}: Flushed phi optimizer (grad_norm={phi_grad_norm:.4f})\")\n                                except Exception as e:\n                                    print(f\"❌ [TRAIN] End of epoch {epoch}: Phi step failed: {e}\")\n                            else:\n                                print(f\"❌ [TRAIN] End of epoch {epoch}: Phi gradient norm is NaN/Inf: {phi_grad_norm}\")\n                    \n                    scaler.update()\n                else:\n                    try:\n                        grad_norm = torch.nn.utils.clip_grad_norm_(\n                            model.parameters(), \n                            _GRAD_CLIP_NORM,\n                            error_if_nonfinite=False\n                        )\n                    except RuntimeError as e:\n                        if \"non-finite\" in str(e).lower():\n                            print(f\"❌ [TRAIN] End of epoch {epoch}: Gradient clipping detected non-finite gradients\")\n                            grad_norm = torch.tensor(float('inf'))\n                        else:\n                            raise\n                    \n                    if torch.isfinite(grad_norm):\n                        optimizer.step()\n                        training_stats[\"optimizer_updates\"] += 1\n                        print(f\"✅ [TRAIN] End of epoch {epoch}: Flushed main optimizer (grad_norm={grad_norm:.4f})\")\n                    else:\n                        print(f\"❌ [TRAIN] End of epoch {epoch}: Gradient norm is NaN/Inf: {grad_norm}\")\n                    \n                    if phi_optimizer is not None and enable_asbn_training:\n                        phi_params = []\n                        for g in phi_optimizer.param_groups:\n                            phi_params.extend([p for p in g.get(\"params\", [])])\n                        \n                        if phi_params:\n                            try:\n                                phi_grad_norm = torch.nn.utils.clip_grad_norm_(\n                                    phi_params, \n                                    _GRAD_CLIP_NORM,\n                                    error_if_nonfinite=False\n                                )\n                            except RuntimeError as e:\n                                if \"non-finite\" in str(e).lower():\n                                    print(f\"❌ [TRAIN] End of epoch {epoch}: Phi gradient clipping detected non-finite gradients\")\n                                    phi_grad_norm = torch.tensor(float('inf'))\n                                else:\n                                    raise\n                            \n                            if torch.isfinite(phi_grad_norm):\n                                try:\n                                    phi_optimizer.step()\n                                    training_stats[\"asbn_updates\"] += 1\n                                    print(f\"✅ [TRAIN] End of epoch {epoch}: Flushed phi optimizer (grad_norm={phi_grad_norm:.4f})\")\n                                except Exception as e:\n                                    print(f\"❌ [TRAIN] End of epoch {epoch}: Phi step failed: {e}\")\n                            else:\n                                print(f\"❌ [TRAIN] End of epoch {epoch}: Phi gradient norm is NaN/Inf: {phi_grad_norm}\")\n\n                optimizer.zero_grad(set_to_none=True)\n                if phi_optimizer is not None:\n                    phi_optimizer.zero_grad(set_to_none=True)\n            except Exception as e:\n                print(f\"❌ [TRAIN] End of epoch {epoch}: Gradient flush failed: {type(e).__name__}: {e}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n        expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n        success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n        cluster_count = _get_cluster_count(model)\n\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        epoch_start_idx = max(0, len(training_stats[\"loss_component_breakdown\"]) - len(epoch_losses))\n        epoch_components = training_stats[\"loss_component_breakdown\"][epoch_start_idx:]\n        avg_translation_loss = float(np.mean([c[\"translation_loss\"] for c in epoch_components])) if epoch_components else 0.0\n        avg_asbn_loss = float(np.mean([c[\"asbn_loss\"] for c in epoch_components])) if epoch_components else 0.0\n        avg_dscd_loss = float(np.mean([c[\"dscd_loss\"] for c in epoch_components])) if epoch_components else 0.0\n        avg_domain_accuracy = float(np.mean([c[\"domain_accuracy\"] for c in epoch_components])) if epoch_components else 0.0\n        \n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} SUMMARY\")\n        print(\"=\" * 80)\n        print(f\" Duration (min): {epoch_duration_min:.2f}\")\n        print(f\" Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\" ASBN updates: {training_stats['asbn_updates']}\")\n        print(f\" Batches: processed={processed_batches}, skipped={training_stats['skipped_batches']}\")\n        print(f\" Label corruption fixes: {training_stats['label_corruption_fixes']}\")\n        print(f\" Success rate: {success_rate:.1f}%\")\n        print(f\" Clustered Token Types: {cluster_count}\")\n        print(f\" Avg Epoch Loss: {avg_epoch_loss:.4f}\")\n        print(f\"   - Translation: {avg_translation_loss:.4f}\")\n        print(f\"   - ASBN: {avg_asbn_loss:.4f}\")\n        print(f\"   - DSCD: {avg_dscd_loss:.4f}\")\n        print(f\"   - Domain Accuracy: {avg_domain_accuracy:.2%}\")\n        print(f\" NaN gradient events: {training_stats['nan_gradient_events']}\")\n        print(f\" Extreme gradient events: {training_stats['extreme_gradient_events']}\")\n\n        if skip_reasons:\n            print(f\" Skip reasons: {dict(skip_reasons)}\")\n\n        try:\n            val_fn = globals().get(\"comprehensive_epoch_validation\", None)\n            if callable(val_fn):\n                print(f\"🔍 [VALIDATION] Running end-of-epoch validation for epoch {epoch}...\")\n                validation_results = val_fn(\n                    model=model,\n                    tokenizer=tokenizer,\n                    epoch=epoch,\n                    global_step=global_step,\n                    source_lang=_SOURCE_LANGUAGE,\n                    target_lang=_TARGET_LANGUAGE,\n                    max_length=_MAX_LENGTH,\n                    device=_DEVICE,\n                )\n                if validation_results is not None and validation_results.get(\"validation_completed\", False):\n                    training_stats[\"epoch_validations\"].append(validation_results)\n                    training_stats[\"dscd_quality_history\"].append(validation_results.get(\"dscd_quality_score\", 0.0))\n                    training_stats[\"asbn_domain_accuracy_history\"].append(validation_results.get(\"asbn_domain_accuracy\", 0.0))\n                    training_stats[\"trg_explanation_history\"].append(validation_results.get(\"trg_total_explanations\", 0))\n                    print(f\"✅ [VALIDATION] End-of-epoch validation completed for epoch {epoch}\")\n                else:\n                    print(f\"⚠️  [VALIDATION] End-of-epoch validation failed for epoch {epoch}\")\n        except Exception as e:\n            print(f\"❌ [VALIDATION] End-of-epoch validation failed for epoch {epoch}: {type(e).__name__}: {e}\")\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n        print(\"-\" * 80)\n        if skip_reasons:\n            for k, v in skip_reasons.items():\n                print(f\"  {k}: {v}\")\n        print(\"=\" * 80)\n\n    try:\n        checkpoint_dir = Path(\"/kaggle/working\")\n        if not checkpoint_dir.exists():\n            checkpoint_dir = Path(\".\")\n            print(f\"[CHECKPOINT] /kaggle/working not found, using current directory: {checkpoint_dir.absolute()}\")\n\n        checkpoint_path = checkpoint_dir / \"tatn_final.pt\"\n        core_model = model.module if hasattr(model, \"module\") else model\n\n        dscd_state = {}\n        try:\n            if hasattr(core_model, \"dscd\"):\n                dscd = core_model.dscd\n                lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                \n                prototype_stores_data = {}\n                corrupted_count = 0\n                valid_count = 0\n                \n                if lock:\n                    with lock:\n                        stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n                else:\n                    stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n                for token, store in stores.items():\n                    try:\n                        centroids = getattr(store, \"centroids\", None)\n                        counts = getattr(store, \"counts\", None)\n                        \n                        if not isinstance(centroids, list) or not isinstance(counts, list):\n                            corrupted_count += 1\n                            continue\n                        \n                        if len(centroids) == 0 or len(centroids) != len(counts):\n                            corrupted_count += 1\n                            continue\n                        \n                        if any(c <= 0 for c in counts):\n                            corrupted_count += 1\n                            continue\n                        \n                        cent_list = []\n                        valid_entry = True\n                        for c in centroids:\n                            try:\n                                if isinstance(c, torch.Tensor):\n                                    if not torch.isfinite(c).all():\n                                        corrupted_count += 1\n                                        valid_entry = False\n                                        break\n                                    cent_list.append(c.detach().cpu().tolist())\n                                else:\n                                    arr = np.asarray(c)\n                                    if not np.isfinite(arr).all():\n                                        corrupted_count += 1\n                                        valid_entry = False\n                                        break\n                                    cent_list.append(arr.tolist())\n                            except Exception:\n                                corrupted_count += 1\n                                valid_entry = False\n                                break\n                        \n                        if valid_entry and cent_list and len(cent_list) == len(counts):\n                            store_data = {\"centroids\": cent_list, \"counts\": [int(c) for c in counts]}\n                            prototype_stores_data[str(token)] = store_data\n                            valid_count += 1\n                    except Exception:\n                        corrupted_count += 1\n                        continue\n\n                dscd_state = {\n                    \"prototype_stores_data\": prototype_stores_data,\n                    \"valid_stores\": valid_count,\n                    \"corrupted_stores\": corrupted_count,\n                }\n                \n                if corrupted_count > 0:\n                    print(f\"⚠️  [CHECKPOINT] DSCD: {corrupted_count} corrupted stores skipped, {valid_count} valid stores saved\")\n        except Exception as e:\n            print(f\"❌ [CHECKPOINT] DSCD state extraction failed: {type(e).__name__}: {e}\")\n            dscd_state = {}\n\n        checkpoint_data = {\n            \"epochs_trained\": epochs,\n            \"global_steps\": global_step,\n            \"final_train_loss\": training_stats[\"epoch_losses\"][-1] if training_stats[\"epoch_losses\"] else 0.0,\n            \"final_translation_loss\": avg_translation_loss,\n            \"final_asbn_loss\": avg_asbn_loss,\n            \"final_dscd_loss\": avg_dscd_loss,\n            \"final_domain_accuracy\": avg_domain_accuracy,\n            \"model_state_dict\": core_model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"phi_optimizer_state_dict\": phi_optimizer.state_dict() if phi_optimizer is not None else None,\n            \"scaler_state_dict\": scaler.state_dict() if scaler is not None else None,\n            \"training_stats\": training_stats,\n            \"dscd_state\": dscd_state,\n            \"config\": {\n                \"SPAN_THRESHOLD\": globals().get(\"SPAN_THRESHOLD\", 0.15),\n                \"TAU_LOW\": globals().get(\"TAU_LOW\", 0.25),\n                \"UNCERTAINTY_THRESHOLD\": globals().get(\"UNCERTAINTY_THRESHOLD\", 0.25),\n                \"TRG_UNCERTAINTY_THRESHOLD\": globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", 0.25),\n                \"LAMBDA_ASBN\": globals().get(\"LAMBDA_ASBN\", 0.05),\n                \"LAMBDA_DSCD\": globals().get(\"LAMBDA_DSCD\", 0.15),\n                \"TRG_TEMPERATURE\": globals().get(\"TRG_TEMPERATURE\", 1.0),\n                \"PERIODIC_DISCOVERY_FREQUENCY\": _PERIODIC_DISCOVERY_FREQUENCY,\n                \"NUM_EPOCHS\": epochs,\n                \"BATCH_SIZE\": _BATCH_SIZE,\n                \"LEARNING_RATE\": optimizer.param_groups[0][\"lr\"] if optimizer.param_groups else 0.0,\n            },\n        }\n\n        torch.save(checkpoint_data, checkpoint_path)\n        print(f\"[CHECKPOINT] Saved to {checkpoint_path}\")\n        try:\n            print(f\"[CHECKPOINT] Size: {checkpoint_path.stat().st_size / (1024**2):.2f} MB\")\n        except Exception:\n            pass\n\n    except Exception as e:\n        print(f\"❌ [CHECKPOINT] Failed to save: {type(e).__name__}: {e}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    print(\"=\" * 80)\n    print(\"FINAL TRAINING STATISTICS\")\n    print(\"=\" * 80)\n\n    processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n    expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n    success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n\n    print(f\"[TRAIN] Success Rate: {success_rate:.1f}%\")\n    print(f\"[TRAIN] Total Steps: {global_step}\")\n    print(f\"[TRAIN] Optimizer Updates: {training_stats['optimizer_updates']}\")\n    print(f\"[TRAIN] ASBN Updates: {training_stats['asbn_updates']}\")\n    print(f\"[TRAIN] Label Corruption Fixes: {training_stats['label_corruption_fixes']}\")\n    print(f\"[TRAIN] Gradient Checks: {len(training_stats['gradient_checks'])}\")\n    print(f\"[TRAIN] NaN Gradient Events: {training_stats['nan_gradient_events']}\")\n    print(f\"[TRAIN] Extreme Gradient Events: {training_stats['extreme_gradient_events']}\")\n    print(f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\")\n\n    if training_stats[\"dscd_quality_history\"]:\n        print(\"[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats[\"dscd_quality_history\"], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n\n    if training_stats[\"asbn_domain_accuracy_history\"]:\n        print(\"[TRAIN] ASBN Domain Accuracy Trend:\")\n        for i, acc in enumerate(training_stats[\"asbn_domain_accuracy_history\"], 1):\n            print(f\"  Epoch {i}: {acc:.1%}\")\n\n    if training_stats[\"trg_explanation_history\"]:\n        print(\"[TRAIN] TRG Explanation Count Trend:\")\n        for i, count in enumerate(training_stats[\"trg_explanation_history\"], 1):\n            print(f\"  Epoch {i}: {count} explanations\")\n\n    print(\"=\" * 80)\n    return model\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training loop ready - NaN/Inf GRADIENT + LABEL VALIDATION FULLY HARDENED\")\nprint(\"=\" * 80)\nprint(\"Features:\")\nprint(\"  - Label sanitization BEFORE forward (vocab bounds + pad=-100)\")\nprint(\"  - Empty label batch detection (all -100 check)\")\nprint(\"  - Pre-backward NaN loss detection\")\nprint(\"  - Post-backward gradient validation with magnitude check\")\nprint(\"  - Epsilon-protected loss scaling (div by accum_steps + 1e-9)\")\nprint(\"  - RuntimeError catch for clip_grad_norm_ non-finite detection\")\nprint(\"  - Loss component extraction with NaN/Inf validation\")\nprint(\"  - Domain accuracy clamped to [0, 1]\")\nprint(\"  - Aggressive gradient clearing on failure\")\nprint(\"  - NaN/extreme gradient event tracking\")\nprint(\"  - Label corruption fix counter\")\nprint(\"=\" * 80)\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: INFERENCE PIPELINE WITH TOKEN VALIDATION - DATAPARALLEL COMPATIBLE\n# ==============================================================================\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nfrom typing import List, Dict, Any, Optional\nfrom collections import defaultdict\nimport threading\nimport gc\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\n    if not isinstance(_DEVICE, torch.device):\n        _DEVICE = torch.device(str(_DEVICE))\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept Exception:\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept Exception:\n    _SPAN_THRESHOLD = 0.15\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\nexcept Exception:\n    _TAU_LOW = 0.25\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(UNCERTAINTY_THRESHOLD)\nexcept Exception:\n    _UNCERTAINTY_THRESHOLD = _TAU_LOW\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TRG_UNCERTAINTY_THRESHOLD)\nexcept Exception:\n    _TRG_UNCERTAINTY_THRESHOLD = _UNCERTAINTY_THRESHOLD\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = int(MAX_EXPLANATIONS_PER_SENTENCE)\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept Exception:\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\",\n        \"তারা\", \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept Exception:\n    _M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    _VOCAB_SIZE = int(VOCAB_SIZE)\nexcept Exception:\n    _VOCAB_SIZE = 128112\n\n_SUBWORD_PUNCT_SET = {\".\", \",\", \"!\", \"?\", \"-\"}\n\n\ndef _get_store_size(store) -> int:\n    try:\n        if hasattr(store, \"size\") and callable(getattr(store, \"size\")):\n            return int(store.size())\n        centroids = getattr(store, \"centroids\", None)\n        if centroids is not None:\n            if isinstance(centroids, torch.Tensor):\n                return int(centroids.size(0))\n            elif isinstance(centroids, list):\n                return len(centroids)\n        return 0\n    except Exception:\n        return 0\n\n\ndef get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}))\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}))\n\n        word_prototype_counts = defaultdict(int)\n        for token_key, store in stores.items():\n            try:\n                num_protos = _get_store_size(store)\n                clean_token = (\n                    str(token_key)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean_token:\n                    word_prototype_counts[clean_token] = max(word_prototype_counts[clean_token], num_protos)\n            except Exception:\n                continue\n\n        homographs = {w for w, c in word_prototype_counts.items() if c >= 2}\n        return homographs\n    except Exception:\n        return set()\n\n\ndef build_token_to_word_map(tokenizer, input_ids: torch.Tensor) -> List[Dict[int, str]]:\n    batch_word_maps = []\n    for b in range(input_ids.size(0)):\n        try:\n            token_ids_list = input_ids[b].tolist()\n            token_ids_list = [max(0, min(int(tid), _VOCAB_SIZE - 1)) for tid in token_ids_list]\n            \n            try:\n                tokens = tokenizer.convert_ids_to_tokens(token_ids_list)\n            except Exception:\n                toks = []\n                for idv in token_ids_list:\n                    try:\n                        toks.append(tokenizer.decode([idv], skip_special_tokens=True))\n                    except Exception:\n                        toks.append(str(idv))\n                tokens = toks\n        except Exception:\n            tokens = [\"<unk>\"] * input_ids.size(1)\n\n        word_map: Dict[int, Optional[str]] = {}\n        current_word = \"\"\n        word_start_idx = 0\n        for i, token in enumerate(tokens):\n            if not token or token in [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"\"]:\n                word_map[i] = None\n                continue\n            if token.startswith(\"▁\"):\n                if current_word:\n                    clean_word = current_word.replace(\"▁\", \"\").strip()\n                    if clean_word:\n                        for j in range(word_start_idx, i):\n                            word_map[j] = clean_word\n                current_word = token\n                word_start_idx = i\n            else:\n                current_word += token\n        if current_word:\n            clean_word = current_word.replace(\"▁\", \"\").strip()\n            if clean_word:\n                for j in range(word_start_idx, len(tokens)):\n                    word_map[j] = clean_word\n        cleaned_map = {int(k): str(v) for k, v in word_map.items() if v}\n        batch_word_maps.append(cleaned_map)\n    return batch_word_maps\n\n\nclass InferenceStatistics:\n    def __init__(self):\n        self.lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self.lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.sum_span = 0.0\n            self.sum_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        if not isinstance(result, dict):\n            return\n        \n        with self.lock:\n            self.total_inferences += 1\n            translation = result.get(\"translation\", \"\")\n            if translation and translation != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get(\"explanations\", [])\n            if not isinstance(explanations, list):\n                explanations = []\n            \n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                if not isinstance(exp, dict):\n                    continue\n                try:\n                    conf = float(exp.get(\"confidence\", 0.5))\n                    if not math.isfinite(conf):\n                        conf = 0.5\n                    \n                    self.total_confidence += conf\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf < 0.4:\n                        self.low_confidence_explanations += 1\n\n                    span_val = float(exp.get(\"span\", 0.0))\n                    unc_val = float(exp.get(\"uncertainty\", 0.0))\n                    \n                    if math.isfinite(span_val):\n                        self.sum_span += span_val\n                    if math.isfinite(unc_val):\n                        self.sum_uncertainty += unc_val\n\n                    word = str(exp.get(\"ambiguous_word\", exp.get(\"token\", \"\"))).strip()\n                    clean_word = (\n                        word.replace(\"▁\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .replace(\"##\", \"\")\n                        .replace(\"@@\", \"\")\n                        .replace(\"</w>\", \"\")\n                        .lower()\n                    )\n                    if clean_word:\n                        self.token_counts[clean_word] += 1\n                        self.token_confidences[clean_word].append(conf)\n                        if dscd_homographs and clean_word in dscd_homographs:\n                            self.dscd_homographs_explained.add(clean_word)\n                        if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                            self.reference_homographs_explained.add(clean_word)\n                except Exception:\n                    continue\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self.lock:\n            total_exp = max(self.total_explanations, 1)\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n            avg_conf = (self.total_confidence / total_exp) if total_exp > 0 else 0.0\n            avg_span = (self.sum_span / total_exp) if total_exp > 0 else 0.0\n            avg_unc = (self.sum_uncertainty / total_exp) if total_exp > 0 else 0.0\n            return {\n                \"total_inferences\": self.total_inferences,\n                \"successful_translations\": self.successful_translations,\n                \"failed_translations\": self.failed_translations,\n                \"success_rate\": self.successful_translations / max(self.total_inferences, 1),\n                \"total_explanations\": self.total_explanations,\n                \"explanations_per_inference\": self.total_explanations / max(self.total_inferences, 1),\n                \"high_confidence_rate\": self.high_confidence_explanations / total_exp,\n                \"low_confidence_rate\": self.low_confidence_explanations / total_exp,\n                \"avg_confidence\": avg_conf,\n                \"avg_span\": avg_span,\n                \"avg_uncertainty\": avg_unc,\n                \"dscd_homographs_explained\": list(self.dscd_homographs_explained),\n                \"reference_homographs_explained\": list(self.reference_homographs_explained),\n                \"dscd_empty_warnings\": self.dscd_empty_warnings,\n                \"unique_tokens_explained\": unique_tokens,\n                \"diversity_ratio\": diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2f}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n        if summary[\"dscd_homographs_explained\"]:\n            print(f\"DSCD homographs explained: {len(summary['dscd_homographs_explained'])}\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'])}\")\n        if summary[\"reference_homographs_explained\"]:\n            print(f\"Reference homographs explained: {len(summary['reference_homographs_explained'])}\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'])}\")\n        if summary[\"dscd_empty_warnings\"] > 0:\n            print(f\"DSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80)\n\n\nINFERENCE_STATS = InferenceStatistics()\n\n\ndef to_device(batch_enc: Any, device: torch.device):\n    try:\n        if hasattr(batch_enc, \"to\"):\n            return batch_enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(batch_enc, dict):\n        out = {}\n        for k, v in batch_enc.items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                elif isinstance(v, dict):\n                    out[k] = to_device(v, device)\n                elif isinstance(v, (list, tuple)):\n                    out[k] = [t.to(device) if isinstance(t, torch.Tensor) else t for t in v]\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n    return batch_enc\n\n\ndef extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    if raw_out is None:\n        return {}\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"explanations\" in raw_out or \"proto_probs\" in raw_out or \"h_augmented\" in raw_out:\n            return raw_out\n        for key in [\"dscd_outputs\", \"dscd\", \"dscd_out\"]:\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n        return raw_out\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return extract_dscd_outputs(item)\n    return {}\n\n\ndef get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    if not dscd:\n        return []\n    expl = dscd.get(\"explanations\", None)\n    if expl is None:\n        for alt in (\"explanations_per_sentence\", \"trg_explanations\", \"exps\"):\n            if alt in dscd:\n                expl = dscd[alt]\n                break\n    if expl is None:\n        return []\n    if isinstance(expl, list):\n        if len(expl) > 0 and isinstance(expl[0], dict):\n            return [expl]\n        if len(expl) > 0 and isinstance(expl[0], list):\n            return expl\n    return []\n\n\ndef is_subword_token(token: str) -> bool:\n    if not token or len(token.strip()) == 0:\n        return True\n    token = token.strip()\n    if token.startswith(\"▁\") or token.startswith(\"Ġ\") or token.startswith(\"##\") or token.startswith(\"@@\"):\n        return False\n    if (len(token) == 1 and token in _SUBWORD_PUNCT_SET) or token.isdigit():\n        return True\n    return False\n\n\ndef should_filter_explanation(expl: Dict[str, Any], span_th: float, unc_th: float) -> bool:\n    try:\n        token = expl.get(\"ambiguous_word\", expl.get(\"token\", \"\"))\n        if is_subword_token(str(token)):\n            return True\n        span = float(expl.get(\"span\", 0.0))\n        uncertainty = float(expl.get(\"uncertainty\", 0.0))\n        if not math.isfinite(span) or not math.isfinite(uncertainty):\n            return True\n        ambiguous = (span < span_th) and (uncertainty >= unc_th)\n        return not ambiguous\n    except Exception:\n        return True\n\n\ndef force_english_bos(tokenizer, mbart_model) -> Optional[int]:\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in (_TARGET_LANGUAGE, \"en_XX\", \"en\", \"eng\"):\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = int(lid)\n                        break\n                except Exception:\n                    continue\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            forced_id = tokenizer.lang_code_to_id.get(_TARGET_LANGUAGE, None)\n            if forced_id is not None:\n                forced_id = int(forced_id)\n    except Exception:\n        forced_id = None\n\n    if forced_id is None:\n        forced_id = _M2M100_EN_TOKEN_ID\n\n    try:\n        if forced_id is not None and hasattr(mbart_model, \"config\"):\n            mbart_model.config.forced_bos_token_id = int(forced_id)\n            mbart_model.config.decoder_start_token_id = int(forced_id)\n    except Exception:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[INF] Could not set forced BOS on mbart config\")\n    return forced_id\n\n\ndef safe_generate(mbart, input_ids=None, encoder_outputs=None, attention_mask=None, max_length=64, num_beams=4, **kwargs):\n    try:\n        if encoder_outputs is not None:\n            generated = mbart.generate(\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n        else:\n            generated = mbart.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n        \n        if generated is not None and isinstance(generated, torch.Tensor):\n            generated = torch.clamp(generated, min=0, max=_VOCAB_SIZE - 1)\n        \n        return generated\n        \n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            if _DEBUG_DISCOVERY:\n                print(\"[INF] OOM during generation, retrying with smaller beams...\")\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            if encoder_outputs is not None:\n                generated = mbart.generate(\n                    encoder_outputs=encoder_outputs,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n            else:\n                generated = mbart.generate(\n                    input_ids,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n            \n            if generated is not None and isinstance(generated, torch.Tensor):\n                generated = torch.clamp(generated, min=0, max=_VOCAB_SIZE - 1)\n            \n            return generated\n        raise\n\n\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n    if device is None:\n        device = _DEVICE\n    elif not isinstance(device, torch.device):\n        try:\n            device = torch.device(str(device))\n        except Exception:\n            device = _DEVICE\n    \n    span_th = _SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    unc_th = _UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[INF] Starting inference - input: {input_sentence[:120]}\")\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    dscd_homographs = set()\n    try:\n        dscd_homographs = get_dscd_homographs(model)\n    except Exception:\n        dscd_homographs = set()\n\n    encoder_hidden = None\n    encoder_hidden_adjusted = None\n\n    try:\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=_MAX_LENGTH,\n        )\n        enc = to_device(enc, device)\n        \n        if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n            enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], min=0, max=_VOCAB_SIZE - 1)\n\n        model.eval()\n        core = model.module if hasattr(model, \"module\") else model\n        src_texts = [input_sentence]\n        dscd_validated = False\n\n        try:\n            dscd = getattr(core, \"dscd\", None)\n            if dscd is not None:\n                lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", {})\n                        num_stores = len(stores)\n                        multi_sense = sum(1 for s in stores.values() if _get_store_size(s) >= 2)\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", {})\n                    num_stores = len(stores)\n                    multi_sense = sum(1 for s in stores.values() if _get_store_size(s) >= 2)\n                \n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD: types={num_stores}, multi_sense={multi_sense}, discovered={len(dscd_homographs)}\")\n                if num_stores == 0:\n                    if track_stats:\n                        INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n        except Exception:\n            pass\n\n        with torch.inference_mode():\n            raw_dscd_out = {}\n            mbart = getattr(core, \"mbart\", None)\n            if mbart is None:\n                raise RuntimeError(\"Model backend missing .mbart\")\n\n            encoder_outputs_raw = mbart.model.encoder(\n                input_ids=enc.get(\"input_ids\"),\n                attention_mask=enc.get(\"attention_mask\"),\n            )\n            \n            if hasattr(encoder_outputs_raw, \"last_hidden_state\"):\n                encoder_hidden = encoder_outputs_raw.last_hidden_state\n            elif isinstance(encoder_outputs_raw, tuple) and len(encoder_outputs_raw) > 0 and isinstance(\n                encoder_outputs_raw[0], torch.Tensor\n            ):\n                encoder_hidden = encoder_outputs_raw[0]\n            else:\n                encoder_hidden = encoder_outputs_raw\n\n            if not isinstance(encoder_hidden, torch.Tensor):\n                raise RuntimeError(f\"Invalid encoder hidden state type: {type(encoder_hidden)}\")\n            \n            if encoder_hidden.dim() != 3:\n                raise RuntimeError(f\"Invalid encoder hidden state shape: {encoder_hidden.shape}\")\n            \n            if not torch.isfinite(encoder_hidden).all():\n                if _VERBOSE_LOGGING:\n                    print(f\"[INF] WARNING: Encoder hidden state contains NaN/Inf values\")\n                encoder_hidden = torch.nan_to_num(encoder_hidden, nan=0.0, posinf=1.0, neginf=-1.0)\n\n            try:\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                            use_dscd=True,\n                            use_asbn=False,\n                        )\n                    except TypeError:\n                        raw_dscd_out = core.forward_with_explanations(\n                            enc.get(\"input_ids\"),\n                            enc.get(\"attention_mask\"),\n                            src_texts,\n                        )\n                else:\n                    out = core.forward(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=src_texts,\n                        labels=None,\n                        use_dscd=True,\n                        use_asbn=False,\n                    )\n                    raw_dscd_out = extract_dscd_outputs(out) if isinstance(out, dict) else {}\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[INF] forward_with_explanations failed: {e}\")\n                raw_dscd_out = {}\n\n            dscd_out = extract_dscd_outputs(raw_dscd_out)\n\n            if isinstance(raw_dscd_out, dict) and \"sense_augmented_embeddings\" in raw_dscd_out:\n                encoder_hidden_adjusted = raw_dscd_out[\"sense_augmented_embeddings\"]\n            elif \"h_augmented\" in dscd_out:\n                encoder_hidden_adjusted = dscd_out[\"h_augmented\"]\n            else:\n                encoder_hidden_adjusted = encoder_hidden\n\n            if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                if encoder_hidden_adjusted.dim() != 3:\n                    if _VERBOSE_LOGGING:\n                        print(f\"[INF] Adjusted embeddings wrong dims: {encoder_hidden_adjusted.shape}\")\n                    encoder_hidden_adjusted = encoder_hidden\n                elif encoder_hidden_adjusted.shape != encoder_hidden.shape:\n                    if _VERBOSE_LOGGING:\n                        print(f\"[INF] Adjusted embeddings shape mismatch: {encoder_hidden_adjusted.shape} vs {encoder_hidden.shape}\")\n                    encoder_hidden_adjusted = encoder_hidden\n                elif not torch.isfinite(encoder_hidden_adjusted).all():\n                    if _VERBOSE_LOGGING:\n                        print(f\"[INF] Adjusted embeddings contain NaN/Inf\")\n                    encoder_hidden_adjusted = torch.nan_to_num(encoder_hidden_adjusted, nan=0.0, posinf=1.0, neginf=-1.0)\n            else:\n                encoder_hidden_adjusted = encoder_hidden\n\n            forced_id = force_english_bos(tokenizer, mbart)\n            orig_use_cache = getattr(mbart.config, \"use_cache\", None) if hasattr(mbart, \"config\") else None\n            try:\n                if hasattr(mbart, \"config\"):\n                    mbart.config.use_cache = True\n            except Exception:\n                pass\n\n            try:\n                if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    encoder_hidden_adjusted = encoder_hidden_adjusted.to(device)\n                    from transformers.modeling_outputs import BaseModelOutput\n\n                    encoder_outputs_for_decoder = BaseModelOutput(last_hidden_state=encoder_hidden_adjusted)\n                    generated = safe_generate(\n                        mbart,\n                        encoder_outputs=encoder_outputs_for_decoder,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=4,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                        repetition_penalty=2.5,\n                        no_repeat_ngram_size=2,\n                        length_penalty=1.0,\n                        do_sample=False,\n                    )\n                else:\n                    generated = safe_generate(\n                        mbart,\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=4,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                        repetition_penalty=2.5,\n                        no_repeat_ngram_size=2,\n                        length_penalty=1.0,\n                        do_sample=False,\n                    )\n                \n                if generated is not None and isinstance(generated, torch.Tensor):\n                    generated = torch.clamp(generated, min=0, max=_VOCAB_SIZE - 1)\n                \n                translation = (\n                    tokenizer.decode(generated[0], skip_special_tokens=True) if generated is not None and len(generated) > 0 else \"\"\n                )\n            finally:\n                try:\n                    if hasattr(mbart, \"config\") and orig_use_cache is not None:\n                        mbart.config.use_cache = orig_use_cache\n                except Exception:\n                    pass\n\n            sentence_explanations = []\n            try:\n                trg = getattr(core, \"trg_system\", None)\n                if trg and hasattr(trg, \"process_sentence_for_explanations\"):\n                    token_word_map = build_token_to_word_map(tokenizer, enc.get(\"input_ids\"))\n                    token_word_map_single = token_word_map[0] if token_word_map else {}\n                    try:\n                        input_ids_clamped = torch.clamp(enc.get(\"input_ids\"), min=0, max=_VOCAB_SIZE - 1)\n                        tokens_batch = tokenizer.convert_ids_to_tokens(input_ids_clamped[0].tolist())\n                    except Exception:\n                        tokens_batch = [\n                            tokenizer.decode([i], skip_special_tokens=True)\n                            for i in input_ids_clamped[0].tolist()\n                        ]\n\n                    dscd_for_trg = {}\n                    for k in (\"uncertainties\", \"span_preds\", \"gates\", \"proto_probs\"):\n                        if k in dscd_out:\n                            dscd_for_trg[k] = dscd_out[k]\n\n                    trg_result = trg.process_sentence_for_explanations(\n                        tokens=tokens_batch,\n                        dscd_outputs=dscd_for_trg,\n                        token_word_map=token_word_map_single,\n                        uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                        span_threshold=_SPAN_THRESHOLD,\n                        decoder_attention=None,\n                        max_explanations=_MAX_EXPLANATIONS_PER_SENTENCE,\n                    )\n                    sentence_explanations = trg_result if isinstance(trg_result, list) else []\n                else:\n                    explanations_list = get_explanations_list(dscd_out)\n                    sentence_explanations = explanations_list[0] if explanations_list else []\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[INF] TRG explanation generation failed: {e}\")\n                sentence_explanations = []\n\n            def is_real_ambiguity(e: Dict[str, Any]) -> bool:\n                try:\n                    s = float(e.get(\"span\", 0.0))\n                    u = float(e.get(\"uncertainty\", 0.0))\n                    if not math.isfinite(s) or not math.isfinite(u):\n                        return False\n                    return (s < span_th) and (u >= unc_th)\n                except Exception:\n                    return False\n\n            real_amb_count = 0\n            out_explanations: List[Dict[str, Any]] = []\n            confidences: List[float] = []\n            spans: List[float] = []\n            uncertainties: List[float] = []\n\n            if isinstance(sentence_explanations, list):\n                for ex in sentence_explanations:\n                    try:\n                        word = ex.get(\"ambiguous_word\", ex.get(\"token\", \"\"))\n                        if not isinstance(word, str):\n                            word = \"\"\n                        clean_word = (\n                            word.replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .replace(\"##\", \"\")\n                            .replace(\"@@\", \"\")\n                            .replace(\"</w>\", \"\")\n                            .strip()\n                        )\n                        if clean_word:\n                            ex[\"ambiguous_word\"] = clean_word\n\n                        if should_filter_explanation(ex, span_th, unc_th):\n                            continue\n\n                        is_real = is_real_ambiguity(ex)\n                        if is_real:\n                            real_amb_count += 1\n\n                        s = float(ex.get(\"span\", 0.0))\n                        u = float(ex.get(\"uncertainty\", 0.0))\n                        \n                        if not math.isfinite(s):\n                            s = 0.0\n                        if not math.isfinite(u):\n                            u = 0.5\n                        \n                        confidence = ex.get(\"confidence\", None)\n                        if confidence is None:\n                            confidence = max(0.0, min(1.0, (s * (1.0 - u))))\n                        confidence = float(confidence)\n                        \n                        if not math.isfinite(confidence):\n                            confidence = 0.5\n\n                        confidences.append(confidence)\n                        spans.append(s)\n                        uncertainties.append(u)\n\n                        out_explanations.append(\n                            {\n                                \"ambiguous_word\": ex.get(\"ambiguous_word\", ex.get(\"token\", \"N/A\")),\n                                \"position\": ex.get(\"position\", ex.get(\"token_idx\", \"N/A\")),\n                                \"explanation\": ex.get(\"explanation\", ex.get(\"explain\", \"\")),\n                                \"uncertainty\": float(u),\n                                \"span\": float(s),\n                                \"confidence\": confidence,\n                                \"is_real_amb\": bool(is_real),\n                            }\n                        )\n                    except Exception:\n                        continue\n\n            quality_metrics = {\n                \"total_raw_explanations\": len(sentence_explanations) if isinstance(sentence_explanations, list) else 0,\n                \"filtered_explanations\": (len(sentence_explanations) - len(out_explanations))\n                if isinstance(sentence_explanations, list)\n                else 0,\n                \"high_confidence_count\": sum(1 for c in confidences if c >= 0.65),\n                \"low_confidence_count\": sum(1 for c in confidences if c < 0.4),\n                \"avg_confidence\": (sum(confidences) / len(confidences)) if confidences else 0.0,\n                \"avg_span\": (sum(spans) / len(spans)) if spans else 0.0,\n                \"avg_uncertainty\": (sum(uncertainties) / len(uncertainties)) if uncertainties else 0.0,\n            }\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n\n            if track_stats:\n                INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n            return result\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": str(e)[:200],\n        }\n        if track_stats:\n            INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"আমি কল বন্ধ করেছি।\",\n            \"কাল আমি বই কিনব।\",\n            \"পাতা ঝরে পড়েছে।\",\n            \"তিনি ব্যাংক গেছেন।\",\n            \"আমি ভালো আছি।\",\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations\")\n    print(\"=\" * 80)\n\n    INFERENCE_STATS.reset()\n    for s in sentences:\n        print(f\"\\n{s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n        print(f\"Translation: {res.get('translation', 'N/A')}\")\n        print(f\"Ambiguous words detected: {res.get('ambiguous_words_detected', 0)}\")\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(\n                f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, high={quality.get('high_confidence_count', 0)}, low={quality.get('low_confidence_count', 0)}\"\n            )\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                conf = ex.get(\"confidence\", 0.0)\n                print(f\"  {idx}. {ex['ambiguous_word']} (pos={ex.get('position', 'N/A')}, conf={conf:.3f})\")\n                print(f\"     {ex.get('explanation', 'N/A')[:200]}\")\n        else:\n            print(\"  No explanations\")\n    print(\"=\" * 80)\n    INFERENCE_STATS.print_summary()\n\n\ndef dscd_discovery_warmup(model, tokenizer, num_sents: int = 8000, batch_size: int = 64, max_len: Optional[int] = None):\n    if max_len is None:\n        max_len = _MAX_LENGTH\n    core = model.module if hasattr(model, \"module\") else model\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component\")\n            return\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", None)\n        orig_nmin = getattr(dscd, \"nmin\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"nmin\"):\n                dscd.nmin = max(3, int(getattr(dscd, \"nmin\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for bn, _ in pairs[:num_sents]]\n                if len(texts) < num_sents // 2:\n                    print(f\"[WARMUP] WARNING: Only loaded {len(texts)} texts, expected {num_sents}\")\n            else:\n                base = [\n                    \"আমি কল বন্ধ করেছি।\",\n                    \"কাল আমি বই কিনব।\",\n                    \"পাতা ঝরে পড়েছে।\",\n                    \"তিনি ব্যাংক গেছেন।\",\n                    \"আমি ভালো আছি।\",\n                ]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n                print(f\"[WARMUP] WARNING: Using fallback data ({len(texts)} sentences)\")\n        except Exception:\n            texts = []\n\n        if not texts:\n            print(\"[WARMUP] ERROR: No texts loaded\")\n            return\n\n        processed = 0\n        core.eval()\n        print(f\"[WARMUP] Processing {len(texts)} sentences in batches of {batch_size}...\")\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                try:\n                    enc = tokenizer(\n                        batch,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_len,\n                    )\n                    enc = to_device(enc, _DEVICE)\n                    \n                    if \"input_ids\" in enc and isinstance(enc[\"input_ids\"], torch.Tensor):\n                        enc[\"input_ids\"] = torch.clamp(enc[\"input_ids\"], min=0, max=_VOCAB_SIZE - 1)\n                    \n                    if hasattr(core, \"forward_with_explanations\"):\n                        core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch,\n                            use_dscd=True,\n                            use_asbn=False,\n                        )\n                    else:\n                        core.mbart.model.encoder(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                        )\n                    processed += len(batch)\n                    current_time = time.time()\n                    if i % (batch_size * 10) == 0 or current_time - last_print > 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n                        print(\n                            f\"[WARMUP] {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%) rate={rate:.1f} sents/s ETA {eta:.0f}s\"\n                        )\n                        last_print = current_time\n                    del enc\n                except Exception:\n                    continue\n\n        total_time = time.time() - start_time\n        print(f\"[WARMUP] Completed in {total_time:.1f}s ({processed/total_time:.1f} sents/s)\")\n\n        try:\n            lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n            num_types = len(stores)\n            total_protos = sum(_get_store_size(store) for store in stores.values())\n            multi = sum(1 for store in stores.values() if _get_store_size(store) >= 2)\n            print(\"[WARMUP] Summary:\")\n            print(f\"  - Token types: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n            dscd_homographs = get_dscd_homographs(model)\n            print(f\"[WARMUP] Discovered Homographs: {len(dscd_homographs)}\")\n            reference_found = dscd_homographs.intersection(_HOMOGRAPH_REFERENCE_LIST)\n            print(f\"[WARMUP] Reference found: {len(reference_found)} / {len(_HOMOGRAPH_REFERENCE_LIST)}\")\n        except Exception:\n            pass\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\") and orig_enable is not None:\n                dscd.enable_training_clustering = orig_enable\n            if hasattr(dscd, \"nmin\") and orig_nmin is not None:\n                dscd.nmin = orig_nmin\n            if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n    except Exception:\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n\nprint(\"=\" * 80)\nprint(\"Cell 8: Inference pipeline ready - DATAPARALLEL COMPATIBLE\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Source language: {_SOURCE_LANGUAGE}\")\nprint(f\"  - Target language: {_TARGET_LANGUAGE}\")\nprint(f\"  - Max length: {_MAX_LENGTH}\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - TRG uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Max explanations per sentence: {_MAX_EXPLANATIONS_PER_SENTENCE}\")\nprint(f\"  - Vocab size: {_VOCAB_SIZE}\")\nprint(\"=\" * 80)\n","metadata":{"id":"7Dxg7ck0H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION WITH TOKEN VALIDATION - FIXED\n# ==============================================================================\n\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nimport math\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept Exception:\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(globals().get(\"TRG_SPAN_THRESHOLD\", SPAN_THRESHOLD))\nexcept Exception:\n    try:\n        _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\n    except Exception:\n        _SPAN_THRESHOLD = 0.15\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\nexcept Exception:\n    _TAU_LOW = 0.25\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(globals().get(\"TRG_UNCERTAINTY_THRESHOLD\", _TAU_LOW))\nexcept Exception:\n    _UNCERTAINTY_THRESHOLD = _TAU_LOW\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\n    if not isinstance(_DEVICE, torch.device):\n        _DEVICE = torch.device(str(_DEVICE))\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VOCAB_SIZE = int(VOCAB_SIZE)\nexcept Exception:\n    _VOCAB_SIZE = 128112\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept Exception:\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\n\ndef _get_store_size(store) -> int:\n    try:\n        if hasattr(store, \"size\") and callable(getattr(store, \"size\")):\n            return int(store.size())\n        centroids = getattr(store, \"centroids\", None)\n        if centroids is not None:\n            if isinstance(centroids, torch.Tensor):\n                return int(centroids.size(0))\n            elif isinstance(centroids, list):\n                return len(centroids)\n        return 0\n    except Exception:\n        return 0\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                return len(stores)\n        else:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            return len(stores)\n    except Exception:\n        return 0\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        word_prototype_counts = defaultdict(int)\n\n        if lock:\n            with lock:\n                prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                items = list(prototype_stores.items())\n        else:\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            items = list(prototype_stores.items())\n\n        for token_key, store in items:\n            try:\n                num_protos = _get_store_size(store)\n                clean_token = (\n                    str(token_key)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean_token:\n                    word_prototype_counts[clean_token] = max(\n                        word_prototype_counts[clean_token], num_protos\n                    )\n            except Exception:\n                continue\n\n        homographs = {w for w, c in word_prototype_counts.items() if c >= 2}\n        return homographs\n    except Exception:\n        return set()\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                try:\n                    total_count = sum(getattr(store, \"counts\", []) or [])\n                except Exception:\n                    total_count = 0\n\n                n_protos = _get_store_size(store)\n\n                is_valid = True\n                try:\n                    if not getattr(store, \"centroids\", None):\n                        is_valid = False\n                    if (\n                        not getattr(store, \"counts\", None)\n                        or sum(getattr(store, \"counts\", [])) <= 0\n                    ):\n                        is_valid = False\n                except Exception:\n                    is_valid = False\n\n                if is_valid:\n                    cluster_info.append(\n                        {\n                            \"token\": token,\n                            \"count\": total_count,\n                            \"protos\": n_protos,\n                            \"mu\": getattr(store, \"mu\", 0.0),\n                            \"tau\": getattr(store, \"tau\", 0.0),\n                        }\n                    )\n            except Exception:\n                continue\n\n        if not cluster_info:\n            print(\"[CLUSTER] No valid clusters to display\")\n            return\n\n        cluster_info.sort(key=lambda x: x[\"count\"], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info[\"token\"])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}\"\n                f\"{info['protos']:<10}{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n        print(\"-\" * 90)\n    except Exception:\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef _safe_float(x: Any, default: float = 0.0) -> float:\n    try:\n        if x is None:\n            return float(default)\n        if isinstance(x, torch.Tensor):\n            val = float(x.item())\n        else:\n            val = float(x)\n        if not math.isfinite(val):\n            return float(default)\n        return val\n    except Exception:\n        return float(default)\n\n\ndef _is_valid_explanation_item(expl: Dict[str, Any]) -> bool:\n    if not isinstance(expl, dict):\n        return False\n    span = _safe_float(expl.get(\"span\", 0.0), 0.0)\n    unc = _safe_float(expl.get(\"uncertainty\", 0.0), 0.0)\n    conf = expl.get(\"confidence\", None)\n    if conf is None:\n        conf = max(span, 1.0 - unc)\n    conf = _safe_float(conf, 0.0)\n    return (span > 1e-3) or (unc > 1e-3) or (conf > 0.4)\n\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None,\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Pure Data-Driven)\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\", [\"কল\"]),\n        (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\", [\"কাল\"]),\n        (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\", [\"পাতা\"]),\n        (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\", [\"ব্যাংক\"]),\n        (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল = fruit/result\", [\"ফল\"]),\n        (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা = head/top\", [\"মাথা\"]),\n        (\"কল থেকে কল এসেছে।\", \"A call came from the tap\", \"Multiple কল\", [\"কল\"]),\n        (\"কালকে কাল মেঘ দেখা গেছে।\", \"Yesterday black clouds were seen\", \"Multiple কাল\", [\"কাল\"]),\n        (\"আজ ভাল আবহাওয়া।\", \"Weather is good today\", \"Simple\", []),\n        (\"আমি ভালো আছি।\", \"I am fine\", \"Simple\", []),\n        (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Simple\", []),\n        (\"এটা আমার বই।\", \"This is my book\", \"Simple\", []),\n        (\n            \"তিনি ব্যাংকে কাজ করেন এবং ব্যাংকে বসে থাকেন।\",\n            \"He works at the bank and sits on the embankment\",\n            \"Long with multiple\",\n            [\"ব্যাংক\"],\n        ),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    quality_metrics = {\n        \"total_confidence\": 0.0,\n        \"confidence_samples\": 0,\n        \"high_confidence_count\": 0,\n        \"medium_confidence_count\": 0,\n        \"low_confidence_count\": 0,\n        \"confidences\": [],\n        \"spans\": [],\n        \"uncertainties\": [],\n    }\n\n    homograph_tracking = {\n        \"test_expected_homographs\": set(),\n        \"dscd_discovered_homographs\": set(),\n        \"explained_homographs\": set(),\n        \"homograph_explanations\": defaultdict(list),\n    }\n\n    error_tracking = {\n        \"translation_failures\": 0,\n        \"dscd_failures\": 0,\n        \"trg_failures\": 0,\n        \"timeout_errors\": 0,\n        \"oom_errors\": 0,\n        \"other_errors\": 0,\n        \"error_details\": [],\n        \"per_test_status\": [],\n    }\n\n    timing_metrics = {\n        \"total_time\": 0.0,\n        \"per_test_times\": [],\n        \"avg_test_time\": 0.0,\n    }\n\n    discovery_validated = False\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd and hasattr(dscd, \"discovered_log\") and dscd.discovered_log:\n            discovery_validated = True\n            if _DEBUG_DISCOVERY:\n                last_discovery = dscd.discovered_log[-1]\n                discovered = last_discovery.get(\"discovered\", 0)\n                candidates = last_discovery.get(\"candidates\", 0)\n                print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(\"[EVAL] No discovery log found\")\n    except Exception:\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn and hasattr(asbn, \"get_detailed_stats\"):\n            result = asbn.get_detailed_stats()\n            if isinstance(result, dict):\n                asbn_stats = result\n        elif asbn and hasattr(asbn, \"get_asbn_stats\"):\n            result = asbn.get_asbn_stats()\n            if isinstance(result, dict):\n                asbn_stats = result\n        \n        if asbn_stats:\n            domain_acc = _safe_float(asbn_stats.get(\"domain_accuracy\", 0.0), 0.0)\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] ASBN: domain_acc={domain_acc:.2%}\")\n    except Exception:\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg_system\", None)\n        if trg and hasattr(trg, \"get_statistics\"):\n            result = trg.get_statistics()\n            if isinstance(result, dict):\n                trg_stats = result\n            if _DEBUG_DISCOVERY and trg_stats:\n                exp_gen = int(trg_stats.get(\"explanations_generated\", 0))\n                print(f\"[EVAL] TRG: {exp_gen} total\")\n    except Exception:\n        if _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    homograph_tracking[\"dscd_discovered_homographs\"] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking[\"dscd_discovered_homographs\"] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", None)\n                        store_count = len(stores) if stores else 0\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", None)\n                    store_count = len(stores) if stores else 0\n                if store_count == 0 and \"dscd_discovery_warmup\" in globals():\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(\n                            model,\n                            tokenizer,\n                            num_sents=4000,\n                            batch_size=64,\n                            max_len=_MAX_LENGTH,\n                        )\n                        homograph_tracking[\n                            \"dscd_discovered_homographs\"\n                        ] = _get_dscd_homographs(core_model)\n                    except Exception:\n                        print(\"[EVAL] Warmup failed\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            span = _safe_float(expl.get(\"span\", 0.0), 0.0)\n            u = _safe_float(expl.get(\"uncertainty\", 0.0), 0.0)\n            conf = expl.get(\"confidence\", None)\n            if conf is None:\n                conf = max(span, 1.0 - u)\n            conf = _safe_float(conf, 0.0)\n            return (span > _SPAN_THRESHOLD) or (u < _UNCERTAINTY_THRESHOLD and conf >= 0.6)\n        except Exception:\n            return False\n\n    def _compute_similarity(pred: str, expected: str) -> float:\n        try:\n            if not isinstance(pred, str) or not isinstance(expected, str):\n                return 0.0\n            pred_words = set(pred.lower().strip().split())\n            exp_words = set(expected.lower().strip().split())\n            if not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            return overlap / len(exp_words)\n        except Exception:\n            return 0.0\n\n    for _, _, _, expected_homos in test_sentences:\n        for h in expected_homos:\n            clean_h = h.strip().lower()\n            if clean_h:\n                homograph_tracking[\"test_expected_homographs\"].add(clean_h)\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(\n        test_sentences, 1\n    ):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            \"test_id\": idx,\n            \"success\": False,\n            \"translation_ok\": False,\n            \"explanations_count\": 0,\n            \"error\": None,\n        }\n\n        try:\n            if \"translate_with_explanations\" not in globals():\n                print(\"[EVAL] translate_with_explanations not available\")\n                error_tracking[\"other_errors\"] += 1\n                test_status[\"error\"] = \"function_not_available\"\n                error_tracking[\"per_test_status\"].append(test_status)\n                continue\n\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                device=_DEVICE,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD,\n                track_stats=False,\n            )\n\n            if not isinstance(result, dict):\n                print(\"[EVAL] Invalid result type\")\n                error_tracking[\"other_errors\"] += 1\n                test_status[\"error\"] = \"invalid_result\"\n                error_tracking[\"per_test_status\"].append(test_status)\n                continue\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            raw_explanations = result.get(\"explanations\", []) or []\n\n            filtered_explanations: List[Dict[str, Any]] = []\n            seen_tokens_positions = set()\n            for ex in raw_explanations:\n                try:\n                    span_val = _safe_float(ex.get(\"span\", 0.0), 0.0)\n                    unc_val = _safe_float(ex.get(\"uncertainty\", 0.0), 0.0)\n                    conf_val = ex.get(\"confidence\", None)\n                    if conf_val is None:\n                        conf_val = max(span_val, 1.0 - unc_val)\n                    conf_val = _safe_float(conf_val, 0.0)\n\n                    token_word = ex.get(\"ambiguous_word\", ex.get(\"token\", \"\"))\n                    if not isinstance(token_word, str):\n                        token_word = str(token_word)\n                    token_word_clean = (\n                        token_word.replace(\"▁\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .replace(\"##\", \"\")\n                        .replace(\"@@\", \"\")\n                        .replace(\"</w>\", \"\")\n                        .strip()\n                    )\n                    \n                    if not token_word_clean:\n                        continue\n                    \n                    pos = ex.get(\"position\", ex.get(\"token_idx\", None))\n                    try:\n                        pos_i = int(pos) if pos is not None else None\n                    except Exception:\n                        pos_i = None\n\n                    dedupe_key = (token_word_clean.lower(), pos_i)\n                    if dedupe_key in seen_tokens_positions:\n                        continue\n                    seen_tokens_positions.add(dedupe_key)\n\n                    if _is_valid_explanation_item(\n                        {\"span\": span_val, \"uncertainty\": unc_val, \"confidence\": conf_val}\n                    ):\n                        cleaned = {\n                            \"ambiguous_word\": token_word_clean,\n                            \"position\": pos_i,\n                            \"explanation\": str(\n                                ex.get(\"explanation\", ex.get(\"explain\", \"\"))\n                            )[:512],\n                            \"uncertainty\": float(unc_val),\n                            \"span\": float(span_val),\n                            \"confidence\": float(conf_val),\n                            \"is_raw\": ex,\n                        }\n                        filtered_explanations.append(cleaned)\n                except Exception:\n                    continue\n\n            amb_count = len(filtered_explanations)\n            explanations = filtered_explanations\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n                for j, expl in enumerate(explanations, 1):\n                    try:\n                        span_val = float(expl.get(\"span\", 0.0))\n                        u_val = float(expl.get(\"uncertainty\", 0.0))\n                        conf_val = float(expl.get(\"confidence\", 0.0))\n                    except Exception:\n                        span_val = 0.0\n                        u_val = 0.0\n                        conf_val = 0.0\n\n                    marker = (\n                        f\"[S>{_SPAN_THRESHOLD:.2f}]\" if span_val > _SPAN_THRESHOLD else \"          \"\n                    )\n                    word = expl.get(\"ambiguous_word\", \"N/A\")\n                    pos = expl.get(\"position\", \"N/A\")\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics[\"confidences\"].append(conf_val)\n                    quality_metrics[\"spans\"].append(span_val)\n                    quality_metrics[\"uncertainties\"].append(u_val)\n                    quality_metrics[\"total_confidence\"] += conf_val\n                    quality_metrics[\"confidence_samples\"] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics[\"high_confidence_count\"] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics[\"medium_confidence_count\"] += 1\n                    else:\n                        quality_metrics[\"low_confidence_count\"] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = (\n                        str(word)\n                        .replace(\"▁\", \"\")\n                        .replace(\"Ġ\", \"\")\n                        .replace(\"##\", \"\")\n                        .replace(\"@@\", \"\")\n                        .replace(\"</w>\", \"\")\n                        .strip()\n                        .lower()\n                    )\n                    if clean_word:\n                        homograph_tracking[\"explained_homographs\"].add(clean_word)\n                        homograph_tracking[\"homograph_explanations\"][clean_word].append(\n                            {\n                                \"sentence\": src_text,\n                                \"confidence\": conf_val,\n                                \"span\": span_val,\n                                \"uncertainty\": u_val,\n                            }\n                        )\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status[\"explanations_count\"] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            sim_threshold = 0.25\n            is_non_error = translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            )\n            if (\n                translation\n                and translation.strip()\n                and is_non_error\n                and similarity >= sim_threshold\n            ):\n                successful_translations += 1\n                test_status[\"translation_ok\"] = True\n                test_status[\"success\"] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking[\"translation_failures\"] += 1\n                test_status[\"error\"] = \"translation_failed\"\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking[\"oom_errors\"] += 1\n                test_status[\"error\"] = \"oom\"\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking[\"timeout_errors\"] += 1\n                test_status[\"error\"] = \"timeout\"\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking[\"other_errors\"] += 1\n                test_status[\"error\"] = \"runtime\"\n            error_tracking[\"error_details\"].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking[\"other_errors\"] += 1\n            test_status[\"error\"] = type(e).__name__\n            error_tracking[\"error_details\"].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking[\"per_test_status\"].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics[\"per_test_times\"].append(test_time)\n\n        print(\"-\" * 60)\n\n    timing_metrics[\"total_time\"] = time.time() - eval_start\n    timing_metrics[\"avg_test_time\"] = (\n        sum(timing_metrics[\"per_test_times\"]) / len(timing_metrics[\"per_test_times\"])\n        if timing_metrics[\"per_test_times\"]\n        else 0.0\n    )\n\n    if quality_metrics[\"confidence_samples\"] > 0:\n        quality_metrics[\"avg_confidence\"] = (\n            quality_metrics[\"total_confidence\"] / quality_metrics[\"confidence_samples\"]\n        )\n        quality_metrics[\"avg_span\"] = (\n            sum(quality_metrics[\"spans\"]) / len(quality_metrics[\"spans\"])\n            if quality_metrics[\"spans\"]\n            else 0.0\n        )\n        quality_metrics[\"avg_uncertainty\"] = (\n            sum(quality_metrics[\"uncertainties\"])\n            / len(quality_metrics[\"uncertainties\"])\n            if quality_metrics[\"uncertainties\"]\n            else 0.0\n        )\n        if quality_metrics[\"confidences\"]:\n            sorted_conf = sorted(quality_metrics[\"confidences\"])\n            n = len(sorted_conf)\n            quality_metrics[\"confidence_p25\"] = sorted_conf[max(0, n // 4)]\n            quality_metrics[\"confidence_p50\"] = sorted_conf[max(0, n // 2)]\n            quality_metrics[\"confidence_p75\"] = sorted_conf[min(n - 1, 3 * n // 4)]\n    else:\n        quality_metrics[\"avg_confidence\"] = 0.0\n        quality_metrics[\"avg_span\"] = 0.0\n        quality_metrics[\"avg_uncertainty\"] = 0.0\n\n    explained_from_dscd = set()\n    if homograph_tracking.get(\"explained_homographs\") and homograph_tracking.get(\n        \"dscd_discovered_homographs\"\n    ):\n        explained_from_dscd = homograph_tracking[\"explained_homographs\"].intersection(\n            homograph_tracking[\"dscd_discovered_homographs\"]\n        )\n\n    test_expected_discovered = set()\n    if homograph_tracking.get(\"test_expected_homographs\") and homograph_tracking.get(\n        \"dscd_discovered_homographs\"\n    ):\n        test_expected_discovered = homograph_tracking[\n            \"test_expected_homographs\"\n        ].intersection(homograph_tracking[\"dscd_discovered_homographs\"])\n\n    reference_discovered = set()\n    if _HOMOGRAPH_REFERENCE_LIST and homograph_tracking.get(\n        \"dscd_discovered_homographs\"\n    ):\n        reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n            homograph_tracking[\"dscd_discovered_homographs\"]\n        )\n\n    homograph_tracking[\"explained_from_dscd_rate\"] = (\n        len(explained_from_dscd) / len(homograph_tracking[\"dscd_discovered_homographs\"])\n        if homograph_tracking.get(\"dscd_discovered_homographs\")\n        else 0.0\n    )\n    homograph_tracking[\"test_expected_discovery_rate\"] = (\n        len(test_expected_discovered)\n        / len(homograph_tracking[\"test_expected_homographs\"])\n        if homograph_tracking.get(\"test_expected_homographs\")\n        else 0.0\n    )\n    homograph_tracking[\"reference_discovery_rate\"] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\n            \"total_words\": 0,\n            \"multi_sense_words\": 0,\n            \"total_prototypes\": 0,\n            \"corrupted_stores\": 0,\n        }\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototype_stores\") or {})\n            else:\n                stores = dict(getattr(dscd, \"prototype_stores\") or {})\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            corrupted = 0\n\n            for _, store in stores.items():\n                sz = _get_store_size(store)\n                \n                is_valid = True\n                try:\n                    if (\n                        not getattr(store, \"centroids\", None)\n                        or len(getattr(store, \"centroids\", [])) == 0\n                    ):\n                        is_valid = False\n                    if (\n                        not getattr(store, \"counts\", None)\n                        or sum(getattr(store, \"counts\", [])) <= 0\n                    ):\n                        is_valid = False\n                    if hasattr(store, \"mu\"):\n                        mu_val = getattr(store, \"mu\")\n                        if isinstance(mu_val, (int, float)):\n                            if not math.isfinite(mu_val) or mu_val < 0 or mu_val > 10:\n                                is_valid = False\n                except Exception:\n                    is_valid = False\n\n                if not is_valid:\n                    corrupted += 1\n                    continue\n\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n\n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n                \"corrupted_stores\": corrupted,\n            }\n    except Exception:\n        dscd_stats = {\n            \"total_words\": 0,\n            \"multi_sense_words\": 0,\n            \"total_prototypes\": 0,\n            \"corrupted_stores\": 0,\n        }\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(\n        f\"  Success rate: {(successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0:.1f}%\"\n    )\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics.get('avg_confidence', 0.0):.3f}\")\n    print(f\"  Avg span: {quality_metrics.get('avg_span', 0.0):.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics.get('avg_uncertainty', 0.0):.3f}\")\n\n    if \"confidence_p50\" in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: {quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking[\"explained_homographs\"]:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking[\"explained_homographs\"])[:10]:\n            exps = homograph_tracking[\"homograph_explanations\"].get(homo, [])\n            count = len(exps)\n            avg_conf = (\n                sum(e[\"confidence\"] for e in exps) / len(exps) if exps else 0.0\n            )\n            in_dscd = (\n                \"[D]\" if homo in homograph_tracking[\"dscd_discovered_homographs\"] else \"   \"\n            )\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(\n        f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\"\n    )\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats.get(\"corrupted_stores\", 0) > 0:\n        print(f\"  Corrupted stores: {dscd_stats['corrupted_stores']}\")\n    if dscd_stats[\"total_words\"] > 0:\n        print(\n            f\"  Multi-sense ratio: {dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        domain_acc = _safe_float(asbn_stats.get(\"domain_accuracy\", 0.0), 0.0)\n        print(f\"  Domain accuracy: {domain_acc:.2%}\")\n        if \"source_accuracy\" in asbn_stats:\n            src_acc = _safe_float(asbn_stats.get(\"source_accuracy\", 0.0), 0.0)\n            tgt_acc = _safe_float(asbn_stats.get(\"target_accuracy\", 0.0), 0.0)\n            print(f\"  Source accuracy: {src_acc:.2%}\")\n            print(f\"  Target accuracy: {tgt_acc:.2%}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        exp_gen = int(trg_stats.get(\"explanations_generated\", 0))\n        hc_rate = _safe_float(trg_stats.get(\"high_confidence_rate\", 0.0), 0.0)\n        print(f\"  Total explanations: {exp_gen}\")\n        print(f\"  High confidence: {hc_rate:.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum(\n        [\n            error_tracking[\"translation_failures\"],\n            error_tracking[\"dscd_failures\"],\n            error_tracking[\"trg_failures\"],\n            error_tracking[\"timeout_errors\"],\n            error_tracking[\"oom_errors\"],\n            error_tracking[\"other_errors\"],\n        ]\n    )\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = _safe_float(baseline_metrics.get(\"success_rate_pct\", 0.0), 0.0)\n            current_success = (\n                successful_translations / total_tests * 100.0 if total_tests > 0 else 0.0\n            )\n            success_delta = current_success - baseline_success\n            \n            baseline_expl = int(baseline_metrics.get(\"total_explanations\", 0))\n            expl_delta = total_explanations - baseline_expl\n            \n            baseline_qm = baseline_metrics.get(\"quality_metrics\", {})\n            if isinstance(baseline_qm, dict):\n                baseline_quality = _safe_float(baseline_qm.get(\"avg_confidence\", 0.0), 0.0)\n            else:\n                baseline_quality = 0.0\n            quality_delta = quality_metrics.get(\"avg_confidence\", 0.0) - baseline_quality\n            \n            print(\n                f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\"\n            )\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics.get('avg_confidence', 0.0):.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n            \n            baseline_ht = baseline_metrics.get(\"homograph_tracking\", {})\n            if isinstance(baseline_ht, dict):\n                baseline_homo_rate = _safe_float(baseline_ht.get(\"explained_from_dscd_rate\", 0.0), 0.0)\n                homo_delta = (\n                    homograph_tracking[\"explained_from_dscd_rate\"]\n                    - baseline_homo_rate\n                )\n                print(\n                    f\"  Explanation rate: \"\n                    f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                    f\"({homo_delta:+.1%})\"\n                )\n        except Exception as e:\n            print(f\"  Comparison failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"No explanations generated\")\n    if dscd_stats[\"total_words\"] < 100:\n        warnings.append(\"Very few prototypes (<100)\")\n    \n    corrupted_threshold = float(dscd_stats[\"total_words\"]) * 0.1\n    if dscd_stats[\"total_words\"] > 0 and dscd_stats.get(\"corrupted_stores\", 0) > corrupted_threshold:\n        warnings.append(f\"High corruption rate ({dscd_stats.get('corrupted_stores', 0)} stores)\")\n    \n    if quality_metrics[\"low_confidence_count\"] > quality_metrics[\"high_confidence_count\"]:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking[\"explained_from_dscd_rate\"] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    \n    if asbn_stats:\n        asbn_domain_acc = _safe_float(asbn_stats.get(\"domain_accuracy\", 0.0), 0.0)\n        if asbn_domain_acc < 0.5:\n            warnings.append(\"ASBN domain accuracy <50%\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] All systems nominal\")\n\n    print(\"=\" * 80)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (\n            successful_translations / total_tests * 100.0 if total_tests > 0 else 0.0\n        ),\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n    }\n\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\" * 60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model, tokenizer, run_warmup=False, compare_baseline=False\n        )\n        assert \"total_tests\" in result\n        assert \"quality_metrics\" in result\n        assert \"homograph_tracking\" in result\n        print(\"Evaluation pipeline test passed\")\n        print(\"=\" * 60 + \"\\n\")\n        return True\n    except Exception as e:\n        print(f\"Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\" * 60 + \"\\n\")\n        return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Testing & evaluation ready - TOKEN VALIDATION FIXED\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\"  ✓ Added _DEVICE type validation (torch.device)\")\nprint(\"  ✓ Added _VOCAB_SIZE constant for token validation\")\nprint(\"  ✓ Added empty token validation in explanation filtering\")\nprint(\"  ✓ Added type validation in _compute_similarity()\")\nprint(\"  ✓ Added math.isfinite() check for mu values in DSCD stores\")\nprint(\"  ✓ Added OOM recovery with torch.cuda.empty_cache()\")\nprint(\"  ✓ Enhanced baseline comparison with dict type checks\")\nprint(\"  ✓ All existing fixes from previous version preserved\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"8uL574F8H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE (FINAL INTEGRATION) - FIXED\n# ==============================================================================\nimport os\nimport time\nimport traceback\nimport math\nfrom typing import Tuple, Optional, Dict, Any, List\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef _g(name, default):\n    try:\n        return globals().get(name, default)\n    except Exception:\n        return default\n\ndef _safe_float(x: Any, default: float = 0.0) -> float:\n    try:\n        if x is None:\n            return float(default)\n        if isinstance(x, torch.Tensor):\n            val = float(x.item())\n        else:\n            val = float(x)\n        if not math.isfinite(val):\n            return float(default)\n        return val\n    except Exception:\n        return float(default)\n\ndef _safe_int(x: Any, default: int = 0) -> int:\n    try:\n        if x is None:\n            return default\n        return int(x)\n    except Exception:\n        return default\n\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n\n    raw_device = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        try:\n            _DEVICE = torch.device(str(raw_device))\n        except Exception:\n            _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 48))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 1e-5))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", True))\n    _ENABLE_TRG_INFERENCE = bool(_g(\"ENABLE_TRG_INFERENCE\", True))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 500))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 150))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 4000))\n\n    raw_homo = _g(\"HOMOGRAPH_REFERENCE_LIST_BN\", [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_homo)\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n\n    _FREEZE_ENCODER = bool(_g(\"FREEZE_ENCODER\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\n    _VERBOSE_LOGGING = bool(_g(\"VERBOSE_LOGGING\", False))\n    _DEBUG_DISCOVERY = bool(_g(\"DEBUG_DISCOVERY\", False))\n\n    _M2M100_EN_TOKEN_ID = int(_g(\"M2M100_EN_TOKEN_ID\", 128022))\n    _M2M100_BN_TOKEN_ID = int(_g(\"M2M100_BN_TOKEN_ID\", 128025))\n\n    _SPAN_THRESHOLD = float(_g(\"SPAN_THRESHOLD\", 0.15))\n    _TAU_LOW = float(_g(\"TAU_LOW\", 0.25))\n    _UNCERTAINTY_THRESHOLD = float(_g(\"UNCERTAINTY_THRESHOLD\", _TAU_LOW))\n    _TRG_UNCERTAINTY_THRESHOLD = float(_g(\"TRG_UNCERTAINTY_THRESHOLD\", _UNCERTAINTY_THRESHOLD))\n\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 48\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 1e-5\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _VALIDATION_CHECK_INTERVAL = 500\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n    _DSCD_WARMUP_SAMPLES = 4000\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\"}\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = False\n    _DEBUG_TIMING = False\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _M2M100_EN_TOKEN_ID = 128022\n    _M2M100_BN_TOKEN_ID = 128025\n    _SPAN_THRESHOLD = 0.15\n    _TAU_LOW = 0.25\n    _UNCERTAINTY_THRESHOLD = 0.25\n    _TRG_UNCERTAINTY_THRESHOLD = 0.25\n\n_CHECKPOINT_DIR = _g(\"CHECKPOINT_DIR\", \"/kaggle/working\")\n_CHECKPOINT_PATH = os.path.join(_CHECKPOINT_DIR, _g(\"CHECKPOINT_FILENAME\", \"tatn_final.pt\"))\n\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals() and callable(globals()[\"clear_all_gpu_caches\"]):\n            globals()[\"clear_all_gpu_caches\"]()\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    v = d\n    for k in keys:\n        if not isinstance(v, dict):\n            return default\n        v = v.get(k, None)\n        if v is None:\n            return default\n    return v\n\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import M2M100TokenizerFast as FastTok\n        return FastTok.from_pretrained(model_name, local_files_only=local_files_only)\n    except Exception:\n        pass\n    try:\n        from transformers import M2M100Tokenizer\n        return M2M100Tokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load tokenizer for {model_name}: {e}\")\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        _safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    return True\n\ndef _resolve_thresholds():\n    span_candidates = [\n        _g(\"TRG_SPAN_THRESHOLD\", None),\n        _g(\"SPAN_THRESHOLD\", None),\n        _g(\"TRG_SPAN\", None),\n    ]\n    tau_candidates = [\n        _g(\"TRG_UNCERTAINTY_THRESHOLD\", None),\n        _g(\"UNCERTAINTY_THRESHOLD\", None),\n        _g(\"TAU_LOW\", None),\n    ]\n    span = next((v for v in span_candidates if v is not None), None)\n    tau = next((v for v in tau_candidates if v is not None), None)\n    try:\n        span = float(span) if span is not None else 0.15\n    except Exception:\n        span = 0.15\n    try:\n        tau = float(tau) if tau is not None else 0.25\n    except Exception:\n        tau = 0.25\n    return span, tau\n\ndef main_pipeline() -> Tuple[Any, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN MAIN PIPELINE - COMPLETE INTEGRATION\")\n    print(\"=\" * 80)\n\n    span_thresh, tau_low = _resolve_thresholds()\n    unc_thresh = float(_g(\"UNCERTAINTY_THRESHOLD\", tau_low))\n    trg_unc_thresh = float(_g(\"TRG_UNCERTAINTY_THRESHOLD\", unc_thresh))\n\n    print(\"Configuration:\")\n    print(f\"  - Span threshold: {span_thresh}\")\n    print(f\"  - TAU_LOW: {tau_low}\")\n    print(f\"  - Uncertainty threshold: {unc_thresh}\")\n    print(f\"  - TRG uncertainty threshold: {trg_unc_thresh}\")\n    print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - ASBN training: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\n    print(f\"  - Epochs: {_EPOCHS}\")\n    print(f\"  - Batch size: {_BATCH_SIZE}\")\n\n    print(\"=\" * 80)\n    pipeline_start = time.time()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    initialize_environment()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 1] Loading tokenizer...\")\n    try:\n        tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    except Exception as e:\n        print(f\"[PHASE 1] Tokenizer load failed: {e}\")\n        raise\n\n    try:\n        if hasattr(tokenizer, \"src_lang\"):\n            tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    try:\n        vocab_size = getattr(tokenizer, \"vocab_size\", None)\n        if vocab_size is None:\n            try:\n                vocab_size = len(tokenizer)\n            except Exception:\n                vocab_size = None\n    except Exception:\n        vocab_size = None\n\n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size if vocab_size else 'unknown'})\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(f\"\\n[PHASE 2] Loading data ({_NUM_SAMPLES} samples)...\")\n    pairs = None\n    if \"load_and_preprocess_optimized\" in globals() and callable(globals()[\"load_and_preprocess_optimized\"]):\n        try:\n            pairs = globals()[\"load_and_preprocess_optimized\"](_NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals() or not callable(globals().get(\"MemoryEfficientDataset\")):\n        raise RuntimeError(\"MemoryEfficientDataset not found - please run Cell 2 or provide dataset implementation\")\n\n    DatasetCls = globals()[\"MemoryEfficientDataset\"]\n    try:\n        dataset = DatasetCls(pairs, tokenizer, max_length=_MAX_LENGTH)\n    except Exception:\n        class _SimpleDataset:\n            def __init__(self, pairs, tokenizer, max_length=48):\n                self.pairs = pairs\n                self.tokenizer = tokenizer\n                self.max_length = max_length\n            def __len__(self):\n                return len(self.pairs)\n            def __getitem__(self, idx):\n                s, t = self.pairs[idx]\n                enc_s = self.tokenizer(\n                    s,\n                    truncation=True,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    return_tensors=\"pt\",\n                )\n                enc_t = self.tokenizer(\n                    t,\n                    truncation=True,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    return_tensors=\"pt\",\n                )\n                labels = enc_t[\"input_ids\"].squeeze(0)\n                pad_token_id = getattr(self.tokenizer, \"pad_token_id\", 1)\n                labels = labels.masked_fill(labels == pad_token_id, -100)\n                return {\n                    \"input_ids\": enc_s[\"input_ids\"].squeeze(0),\n                    \"attention_mask\": enc_s[\"attention_mask\"].squeeze(0),\n                    \"labels\": labels,\n                    \"src_text\": s,\n                    \"tokens\": [],\n                    \"token_word_map\": {},\n                    \"domain_label\": 0,\n                }\n        dataset = _SimpleDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n\n    collate_fn = globals().get(\"safe_collate\", None)\n    if \"create_optimized_dataloader\" in globals() and callable(globals()[\"create_optimized_dataloader\"]):\n        try:\n            train_loader = globals()[\"create_optimized_dataloader\"](\n                dataset, batch_size=_BATCH_SIZE, shuffle=True\n            )\n        except Exception:\n            dl_kwargs = {\n                \"batch_size\": _BATCH_SIZE,\n                \"shuffle\": True,\n                \"num_workers\": 0,\n                \"pin_memory\": torch.cuda.is_available(),\n            }\n            if collate_fn is not None:\n                dl_kwargs[\"collate_fn\"] = collate_fn\n            train_loader = DataLoader(dataset, **dl_kwargs)\n    else:\n        dl_kwargs = {\n            \"batch_size\": _BATCH_SIZE,\n            \"shuffle\": True,\n            \"num_workers\": 0,\n            \"pin_memory\": torch.cuda.is_available(),\n        }\n        if collate_fn is not None:\n            dl_kwargs[\"collate_fn\"] = collate_fn\n        train_loader = DataLoader(dataset, **dl_kwargs)\n\n    try:\n        actual_batches = len(train_loader)\n        if actual_batches == 0:\n            raise RuntimeError(\"Dataloader is empty - no batches to train on\")\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {actual_batches} batches\")\n    except TypeError as e:\n        print(f\"[PHASE 2] Warning: Cannot determine batch count: {e}\")\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples\")\n    except Exception as e:\n        print(f\"[PHASE 2] Warning: Dataset validation error: {e}\")\n        print(\"[PHASE 2] Dataset loaded\")\n    \n    del pairs\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 3] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals() or not callable(globals().get(\"MemoryOptimizedTATNWithExplanations\")):\n        raise RuntimeError(\"Model class MemoryOptimizedTATNWithExplanations not found - run Cell 6\")\n\n    ModelCls = globals()[\"MemoryOptimizedTATNWithExplanations\"]\n    try:\n        model_core = ModelCls(tokenizer)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to instantiate model: {e}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        device_ids = list(range(_NUM_GPUS))\n        print(f\"[PHASE 3] Using DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        model = model_core\n\n    try:\n        model = model.to(_DEVICE)\n    except Exception as e:\n        print(f\"[PHASE 3] Failed to move model to {_DEVICE}: {e}\")\n        raise\n\n    core_model = model.module if hasattr(model, \"module\") else model\n\n    try:\n        dscd_present = hasattr(core_model, \"dscd\") and core_model.dscd is not None\n        asbn_present = hasattr(core_model, \"asbn\") and core_model.asbn is not None\n        trg_present = hasattr(core_model, \"trg_system\") and core_model.trg_system is not None\n        \n        print(\"[PHASE 3] Component check:\")\n        print(f\"  - DSCD: {'OK' if dscd_present else 'MISSING'}\")\n        print(f\"  - ASBN: {'OK' if asbn_present else 'MISSING'}\")\n        print(f\"  - TRG: {'OK' if trg_present else 'MISSING'}\")\n        \n        if not dscd_present:\n            print(\"[PHASE 3] WARNING: DSCD module not found!\")\n        if not asbn_present:\n            print(\"[PHASE 3] WARNING: ASBN module not found!\")\n        if not trg_present:\n            print(\"[PHASE 3] WARNING: TRG module not found!\")\n            \n        if not (dscd_present and asbn_present):\n            raise RuntimeError(\"Critical components missing - ensure Cell 3, 4, 5 ran successfully\")\n    except Exception as e:\n        print(f\"[PHASE 3] Component validation failed: {e}\")\n        raise\n\n    try:\n        mbart = getattr(core_model, \"mbart\", None)\n        if mbart is not None:\n            try:\n                emb = mbart.get_input_embeddings()\n                model_emb_count = getattr(emb, \"num_embeddings\", None)\n            except Exception:\n                model_emb_count = None\n            \n            tok_len = vocab_size\n            \n            if isinstance(model_emb_count, int) and isinstance(tok_len, int):\n                if model_emb_count != tok_len:\n                    print(f\"[PHASE 3] ❌ FATAL: Vocab mismatch: model={model_emb_count}, tokenizer={tok_len}\")\n                    print(f\"[PHASE 3] Cell 6 should have fixed this - check Cell 6 output\")\n                    raise RuntimeError(f\"Vocab size mismatch will cause CUDA errors: {model_emb_count} != {tok_len}\")\n                else:\n                    print(f\"[PHASE 3] ✅ Vocab sizes verified: model={model_emb_count}, tokenizer={tok_len}\")\n            else:\n                print(f\"[PHASE 3] ⚠️  Cannot verify vocab sizes: model={model_emb_count}, tokenizer={tok_len}\")\n            \n            forced_bos = None\n            try:\n                if hasattr(tokenizer, \"get_lang_id\"):\n                    try:\n                        forced_bos = int(tokenizer.get_lang_id(_TARGET_LANGUAGE))\n                    except Exception:\n                        pass\n                \n                if forced_bos is None:\n                    forced_bos = _M2M100_EN_TOKEN_ID\n                \n                if hasattr(mbart, \"config\"):\n                    mbart.config.forced_bos_token_id = int(forced_bos)\n                    mbart.config.decoder_start_token_id = int(forced_bos)\n                    print(f\"[PHASE 3] Set forced_bos_token_id to {forced_bos} for {_TARGET_LANGUAGE}\")\n            except Exception as e:\n                print(f\"[PHASE 3] Warning: Failed to set forced_bos_token_id: {e}\")\n    except Exception as e:\n        print(f\"[PHASE 3] Model config update failed: {e}\")\n\n    if _FREEZE_ENCODER:\n        try:\n            enc_obj = getattr(core_model, \"mbart\", None)\n            if enc_obj is not None and hasattr(enc_obj, \"model\") and hasattr(enc_obj.model, \"encoder\"):\n                for p in enc_obj.model.encoder.parameters():\n                    p.requires_grad = False\n            elif hasattr(core_model, \"encoder\"):\n                for p in core_model.encoder.parameters():\n                    p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n\n    print(\"[PHASE 3] Model initialized\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 4] Setting up optimizers...\")\n    critic_params: List[torch.nn.Parameter] = []\n    try:\n        if hasattr(core_model, \"asbn\") and core_model.asbn is not None:\n            if hasattr(core_model.asbn, \"critic_parameters\") and callable(getattr(core_model.asbn, \"critic_parameters\")):\n                try:\n                    critic_params = list(core_model.asbn.critic_parameters())\n                except Exception:\n                    print(\"[PHASE 4] WARNING: critic_parameters() failed, using all ASBN parameters\")\n                    critic_params = [p for p in core_model.asbn.parameters()]\n            else:\n                print(\"[PHASE 4] WARNING: No critic_parameters() method, using all ASBN parameters\")\n                critic_params = [p for p in core_model.asbn.parameters()]\n    except Exception:\n        critic_params = []\n\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n\n    if not base_params:\n        print(\"[PHASE 4] ERROR: No trainable base parameters found!\")\n        raise RuntimeError(\"No parameters to optimize - model might be frozen\")\n\n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n\n    phi_optimizer = None\n    if _ENABLE_ASBN_TRAINING:\n        if critic_params:\n            phi_params = [p for p in critic_params if p.requires_grad]\n            if phi_params:\n                phi_optimizer = torch.optim.AdamW(phi_params, lr=_LR_PHI)\n                print(f\"[PHASE 4] ASBN optimizer created ({len(phi_params)} params)\")\n            else:\n                print(\"[PHASE 4] WARNING: ASBN critic parameters found but none require grad\")\n        else:\n            print(\"[PHASE 4] WARNING: ASBN training enabled but no critic parameters found\")\n    else:\n        print(\"[PHASE 4] ASBN training disabled\")\n\n    print(f\"[PHASE 4] Base optimizer: {len(base_params)} parameters\")\n    print(\"[PHASE 4] Optimizers ready\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Optimizers: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 5] Pre-training validation...\")\n    try:\n        print(\"[PHASE 5] Checking model vocab alignment...\")\n        core_check = model.module if hasattr(model, \"module\") else model\n        mbart_check = getattr(core_check, \"mbart\", None)\n        \n        if mbart_check:\n            emb_check = mbart_check.get_input_embeddings()\n            model_vocab = _safe_int(emb_check.num_embeddings, 0)\n            tok_vocab = _safe_int(vocab_size, 0) if isinstance(vocab_size, int) else 0\n            \n            if tok_vocab == 0:\n                try:\n                    tok_vocab = len(tokenizer)\n                except Exception:\n                    tok_vocab = 0\n            \n            print(f\"[PHASE 5] Model vocab size: {model_vocab}\")\n            print(f\"[PHASE 5] Tokenizer vocab size: {tok_vocab}\")\n            \n            if model_vocab == 0 or tok_vocab == 0:\n                print(f\"[PHASE 5] WARNING: Cannot verify vocab sizes\")\n            elif model_vocab != tok_vocab:\n                print(f\"[PHASE 5] ❌ FATAL: Vocab mismatch! Training will crash!\")\n                raise RuntimeError(f\"Vocab size mismatch: model={model_vocab}, tokenizer={tok_vocab}\")\n            else:\n                print(f\"[PHASE 5] ✅ Vocab sizes match - safe to train\")\n        \n        print(\"[PHASE 5] Validating first batch...\")\n        first_batch = next(iter(train_loader))\n        \n        try:\n            for k, v in first_batch.items():\n                if isinstance(v, torch.Tensor):\n                    first_batch[k] = v.to(_DEVICE)\n        except Exception as e:\n            print(f\"[PHASE 5] Warning: Could not move batch to device: {e}\")\n        \n        input_ids = first_batch[\"input_ids\"]\n        labels = first_batch[\"labels\"]\n        \n        max_input = _safe_int(input_ids.max().item(), 0)\n        valid_labels = labels[labels != -100]\n        max_label = _safe_int(valid_labels.max().item(), 0) if valid_labels.numel() > 0 else 0\n        \n        print(f\"[PHASE 5] Max input_id: {max_input} (must be < {model_vocab})\")\n        print(f\"[PHASE 5] Max label: {max_label} (must be < {model_vocab})\")\n        \n        if model_vocab > 0 and (max_input >= model_vocab or max_label >= model_vocab):\n            print(f\"[PHASE 5] ❌ FATAL: Batch contains out-of-range tokens!\")\n            raise RuntimeError(\"Dataset contains invalid token IDs\")\n        else:\n            print(f\"[PHASE 5] ✅ Batch tokens within valid range\")\n            \n    except StopIteration:\n        print(\"[PHASE 5] ERROR: Dataloader is empty!\")\n        raise RuntimeError(\"No batches in train_loader\")\n    except Exception as e:\n        print(f\"[PHASE 5] Pre-training validation failed: {e}\")\n        raise\n\n    print(\"\\n[PHASE 5B] Baseline evaluation...\")\n    baseline_metrics: Optional[Dict[str, Any]] = None\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        has_prototypes = False\n        if dscd is not None:\n            proto_stores = getattr(dscd, \"prototype_stores\", None)\n            if proto_stores:\n                lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                if lock:\n                    with lock:\n                        has_prototypes = len(proto_stores) > 0\n                else:\n                    has_prototypes = len(proto_stores) > 0\n        if has_prototypes:\n            print(\"[PHASE 5B] Prototypes exist - skipping baseline\")\n        elif \"comprehensive_post_training_testing\" in globals() and callable(globals()[\"comprehensive_post_training_testing\"]):\n            result = globals()[\"comprehensive_post_training_testing\"](\n                model, tokenizer, run_warmup=False, compare_baseline=False\n            )\n            if isinstance(result, dict):\n                baseline_metrics = result\n                print(\"[PHASE 5B] Baseline complete\")\n            else:\n                print(\"[PHASE 5B] Baseline returned invalid type\")\n        else:\n            print(\"[PHASE 5B] Skipping baseline (function not found)\")\n    except Exception as e:\n        print(f\"[PHASE 5B] Baseline failed: {e}\")\n        baseline_metrics = None\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Pre-training validation: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    _safe_clear_gpu_caches()\n\n    print(\"\\n[PHASE 6] Training...\")\n    trained_model = model\n    training_stats = None\n    if \"train_memory_efficient_tatn\" in globals() and callable(globals()[\"train_memory_efficient_tatn\"]):\n        try:\n            res = globals()[\"train_memory_efficient_tatn\"](\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=(_VALIDATION_CHECK_INTERVAL > 0),\n                enable_asbn_training=_ENABLE_ASBN_TRAINING,\n            )\n            if isinstance(res, tuple) and len(res) == 2:\n                trained_model, training_stats = res\n            else:\n                trained_model = res\n                training_stats = None\n            print(\"[PHASE 6] Training complete\")\n        except Exception as e:\n            print(f\"[PHASE 6] Training failed: {e}\")\n            if _VERBOSE_LOGGING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n            trained_model = model\n            training_stats = None\n    else:\n        print(\"[PHASE 6] Skipping training (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    _safe_clear_gpu_caches()\n\n    print(\"\\n[PHASE 7] Post-training validation...\")\n    try:\n        core_for_validation = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        dscd = getattr(core_for_validation, \"dscd\", None)\n        if dscd is None:\n            print(\"[PHASE 7] No DSCD module\")\n        else:\n            proto_stores = getattr(dscd, \"prototype_stores\", None) or {}\n            lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n            if lock:\n                with lock:\n                    stores = dict(proto_stores)\n            else:\n                stores = dict(proto_stores)\n\n            def _store_size(s):\n                try:\n                    if hasattr(s, \"size\") and callable(getattr(s, \"size\")):\n                        return int(s.size())\n                    cents = getattr(s, \"centroids\", None)\n                    if isinstance(cents, torch.Tensor):\n                        return int(cents.size(0))\n                    elif isinstance(cents, list):\n                        return len(cents)\n                    return 0\n                except Exception:\n                    return 0\n\n            total_protos = sum(_store_size(store) for store in stores.values())\n            multi_sense = sum(1 for store in stores.values() if _store_size(store) >= 2)\n            print(\"[PHASE 7] DSCD status:\")\n            print(f\"  - Tokens: {len(stores)}\")\n            print(f\"  - Prototypes: {total_protos}\")\n            print(f\"  - Multi-sense: {multi_sense}\")\n            if len(stores) == 0 or total_protos == 0:\n                print(\"[PHASE 7] WARNING: No prototypes created during training\")\n    except Exception as e:\n        print(f\"[PHASE 7] Validation failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Validation: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    _safe_clear_gpu_caches()\n\n    print(\"\\n[PHASE 8] Post-training evaluation...\")\n    eval_results: Optional[Dict[str, Any]] = None\n    if \"comprehensive_post_training_testing\" in globals() and callable(globals()[\"comprehensive_post_training_testing\"]):\n        try:\n            core_for_eval = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            trg = getattr(core_for_eval, \"trg_system\", None)\n            if trg and hasattr(trg, \"reset_statistics\"):\n                try:\n                    trg.reset_statistics()\n                except Exception:\n                    pass\n            result = globals()[\"comprehensive_post_training_testing\"](\n                trained_model,\n                tokenizer,\n                run_warmup=False,\n                compare_baseline=(isinstance(baseline_metrics, dict) and len(baseline_metrics) > 0),\n                baseline_metrics=baseline_metrics,\n            )\n            if isinstance(result, dict):\n                eval_results = result\n                print(\"[PHASE 8] Evaluation complete\")\n            else:\n                print(\"[PHASE 8] Evaluation returned invalid type\")\n        except Exception as e:\n            print(f\"[PHASE 8] Evaluation failed: {e}\")\n            eval_results = None\n    else:\n        print(\"[PHASE 8] Skipping evaluation (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n    _safe_clear_gpu_caches()\n\n    print(\"\\n[PHASE 9] Saving checkpoint...\")\n    try:\n        os.makedirs(_CHECKPOINT_DIR, exist_ok=True)\n        core_for_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        was_training = getattr(core_for_save, \"training\", False)\n        core_for_save.eval()\n        try:\n            model_state = {}\n            for k, v in core_for_save.state_dict().items():\n                try:\n                    if isinstance(v, torch.Tensor):\n                        if v.numel() > 0 and torch.isfinite(v).all():\n                            model_state[k] = v.cpu().detach().clone()\n                        else:\n                            print(f\"[PHASE 9] WARNING: Skipping corrupted tensor: {k}\")\n                    else:\n                        model_state[k] = v\n                except Exception as e:\n                    print(f\"[PHASE 9] WARNING: Failed to save parameter {k}: {e}\")\n\n            dscd_state = {}\n            if hasattr(core_for_save, \"dscd\") and core_for_save.dscd is not None:\n                dscd_obj = core_for_save.dscd\n                if hasattr(dscd_obj, \"state_dict\") and callable(getattr(dscd_obj, \"state_dict\")):\n                    try:\n                        raw_state = dscd_obj.state_dict()\n                        if isinstance(raw_state, dict) and \"prototype_stores_data\" in raw_state:\n                            stores = raw_state.get(\"prototype_stores_data\", {}) or {}\n                            valid_stores = {}\n                            corrupted_count = 0\n                            for token, store_dict in stores.items():\n                                if not isinstance(store_dict, dict):\n                                    corrupted_count += 1\n                                    continue\n                                centroids = store_dict.get(\"centroids\", None)\n                                counts = store_dict.get(\"counts\", None)\n                                is_valid = False\n                                try:\n                                    if isinstance(centroids, torch.Tensor):\n                                        if centroids.numel() > 0 and centroids.dim() == 2 and torch.isfinite(centroids).all():\n                                            if isinstance(counts, list) and len(counts) == centroids.size(0) and sum(counts) > 0:\n                                                is_valid = True\n                                    elif isinstance(centroids, list) and len(centroids) > 0:\n                                        if isinstance(counts, list) and len(counts) == len(centroids) and sum(counts) > 0:\n                                            is_valid = True\n                                except Exception:\n                                    is_valid = False\n                                if is_valid:\n                                    valid_stores[token] = store_dict\n                                else:\n                                    corrupted_count += 1\n                            dscd_state = {\"prototype_stores_data\": valid_stores}\n                            if corrupted_count > 0:\n                                print(f\"[PHASE 9] Filtered {corrupted_count} corrupted prototype stores\")\n                    except Exception as e:\n                        print(f\"[PHASE 9] DSCD state extraction failed: {e}\")\n                        dscd_state = {}\n\n            optimizer_state = None\n            try:\n                optimizer_state = optimizer.state_dict()\n            except Exception:\n                optimizer_state = None\n\n            phi_optimizer_state = None\n            try:\n                if phi_optimizer is not None:\n                    phi_optimizer_state = phi_optimizer.state_dict()\n            except Exception:\n                phi_optimizer_state = None\n\n            checkpoint = {\n                \"model_state_dict\": model_state,\n                \"dscd_state\": dscd_state,\n                \"optimizer_state_dict\": optimizer_state,\n                \"phi_optimizer_state_dict\": phi_optimizer_state,\n                \"baseline_metrics\": baseline_metrics,\n                \"eval_results\": eval_results,\n                \"training_stats\": training_stats,\n                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"config\": {\n                    \"epochs\": _EPOCHS,\n                    \"batch_size\": _BATCH_SIZE,\n                    \"span_threshold\": span_thresh,\n                    \"tau_low\": tau_low,\n                    \"uncertainty_threshold\": unc_thresh,\n                    \"trg_uncertainty_threshold\": trg_unc_thresh,\n                    \"discovery_frequency\": _PERIODIC_DISCOVERY_FREQUENCY,\n                    \"enable_asbn_training\": _ENABLE_ASBN_TRAINING,\n                    \"enable_trg_inference\": _ENABLE_TRG_INFERENCE,\n                },\n            }\n            torch.save(checkpoint, _CHECKPOINT_PATH)\n            try:\n                verify = torch.load(_CHECKPOINT_PATH, map_location=\"cpu\")\n                has_model = isinstance(verify.get(\"model_state_dict\", None), dict) and len(verify.get(\"model_state_dict\", {})) > 0\n                has_dscd = isinstance(verify.get(\"dscd_state\", None), dict) and len(verify.get(\"dscd_state\", {})) > 0\n                has_phi = verify.get(\"phi_optimizer_state_dict\", None) is not None\n                \n                print(f\"[PHASE 9] Checkpoint saved: {_CHECKPOINT_PATH}\")\n                print(f\"  - Model: {'OK' if has_model else 'MISSING'}\")\n                print(f\"  - DSCD: {'OK' if has_dscd else 'MISSING'}\")\n                print(f\"  - Phi optimizer: {'OK' if has_phi else 'N/A'}\")\n                \n                if has_model:\n                    try:\n                        test_load = {}\n                        corrupted_tensors = 0\n                        for k, v in verify[\"model_state_dict\"].items():\n                            if isinstance(v, torch.Tensor):\n                                if torch.isfinite(v).all():\n                                    test_load[k] = v\n                                else:\n                                    corrupted_tensors += 1\n                        if corrupted_tensors > 0:\n                            print(f\"  - WARNING: {corrupted_tensors} corrupted tensors in checkpoint\")\n                        else:\n                            print(f\"  - Model state loadable: OK ({len(test_load)} tensors)\")\n                    except Exception as e:\n                        print(f\"  - Model state loadable: FAILED ({e})\")\n                \n                if has_dscd:\n                    d_state = verify.get(\"dscd_state\", {})\n                    stores_data = d_state.get(\"prototype_stores_data\", {}) if isinstance(d_state, dict) else {}\n                    num_tokens = len(stores_data) if isinstance(stores_data, dict) else 0\n                    multi_sense_count = 0\n                    if isinstance(stores_data, dict):\n                        for sd in stores_data.values():\n                            try:\n                                cent = sd.get(\"centroids\", None)\n                                if isinstance(cent, torch.Tensor) and cent.size(0) >= 2:\n                                    multi_sense_count += 1\n                                elif isinstance(cent, list) and len(cent) >= 2:\n                                    multi_sense_count += 1\n                            except Exception:\n                                continue\n                    print(f\"  - DSCD tokens: {num_tokens}\")\n                    print(f\"  - Multi-sense: {multi_sense_count}\")\n            except Exception as e:\n                print(f\"[PHASE 9] Checkpoint written but verification failed: {e}\")\n        finally:\n            if was_training:\n                try:\n                    core_for_save.train()\n                except Exception:\n                    pass\n    except Exception as e:\n        print(f\"[PHASE 9] Checkpoint failed: {e}\")\n        if _VERBOSE_LOGGING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Checkpoint: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 10] Final component validation...\")\n    try:\n        core_final = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        dscd_ok = False\n        if hasattr(core_final, \"dscd\"):\n            proto_stores = getattr(core_final.dscd, \"prototype_stores\", None)\n            if proto_stores:\n                lock = getattr(core_final.dscd, \"buffer_lock\", None) or getattr(core_final.dscd, \"clustering_lock\", None)\n                if lock:\n                    with lock:\n                        dscd_ok = len(proto_stores) > 0\n                else:\n                    dscd_ok = len(proto_stores) > 0\n        asbn_ok = hasattr(core_final, \"asbn\") and callable(getattr(core_final.asbn, \"forward\", None))\n        trg_ok = hasattr(core_final, \"trg_system\") and callable(getattr(core_final.trg_system, \"process_sentence_for_explanations\", None))\n        print(\"[PHASE 10] Component validation:\")\n        print(f\"  - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n        print(f\"  - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n        print(f\"  - TRG: {'OK' if trg_ok else 'MISSING'}\")\n    except Exception as e:\n        print(f\"[PHASE 10] Validation failed: {e}\")\n\n    pipeline_time = time.time() - pipeline_start\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"[TIMING] Total time: {pipeline_time:.2f}s ({pipeline_time/60:.2f} min)\")\n    print(f\"[TRAINING] Completed {_EPOCHS} epoch(s)\")\n\n    if isinstance(baseline_metrics, dict) and isinstance(eval_results, dict):\n        baseline_success = _safe_float(baseline_metrics.get(\"success_rate_pct\", 0.0), 0.0)\n        final_success = _safe_float(eval_results.get(\"success_rate_pct\", 0.0), 0.0)\n        improvement = final_success - baseline_success\n        print(\n            f\"[EVALUATION] Baseline -> Final: {baseline_success:.1f}% -> \"\n            f\"{final_success:.1f}%, Improvement: {improvement:+.1f}%\"\n        )\n    elif isinstance(eval_results, dict):\n        final_success = _safe_float(eval_results.get(\"success_rate_pct\", 0.0), 0.0)\n        print(f\"[EVALUATION] Success rate: {final_success:.1f}%\")\n    else:\n        print(\"[EVALUATION] No results\")\n\n    print(f\"\\n[CHECKPOINT] {_CHECKPOINT_PATH if os.path.exists(_CHECKPOINT_PATH) else 'Not saved'}\")\n    if os.path.exists(_CHECKPOINT_PATH):\n        try:\n            size_mb = os.path.getsize(_CHECKPOINT_PATH) / 1024**2\n            print(f\"  - Size: {size_mb:.2f} MB\")\n        except Exception:\n            pass\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Usage: trained_model, tokenizer = main_pipeline()\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n    return trained_model, tokenizer\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 10: Main pipeline ready - FIXED\")\nprint(\"=\" * 80)\nprint(\"KEY FIXES APPLIED:\")\nprint(\"  ✓ REMOVED tokenizer modification code (lines 230-245)\")\nprint(\"  ✓ Cell 6 now handles all vocab resizing automatically\")\nprint(\"  ✓ Simplified vocab size handling (no string 'unknown')\")\nprint(\"  ✓ Enhanced error messages with emoji indicators\")\nprint(\"  ✓ All vocab checks now fail-fast with RuntimeError\")\nprint(\"  ✓ Removed 'tokenizer_modified' from checkpoint config\")\n\n","metadata":{"id":"kEux2BVXH4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER (FINAL) - FIXED\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\nfrom collections import defaultdict\nfrom typing import Any, Dict, Set\n\n# Safe configuration load with defaults (robust retrieval from globals)\ndef _g(name, default):\n    try:\n        return globals().get(name, default)\n    except Exception:\n        return default\n\ntry:\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _EPOCHS = int(_g(\"EPOCHS\", 2))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 4))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 16))\n\n    raw_device = _g(\"DEVICE\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        try:\n            _DEVICE = torch.device(str(raw_device))\n        except Exception:\n            _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", True))\n    _ENABLE_TRG_INFERENCE = bool(_g(\"ENABLE_TRG_INFERENCE\", True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 150))\n    _VERBOSE_LOGGING = bool(_g(\"VERBOSE_LOGGING\", False))\n    _DEBUG_DISCOVERY = bool(_g(\"DEBUG_DISCOVERY\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", _NUM_GPUS > 1))\n\n    # TRG/ambiguity thresholds - prefer TRG-specific values if available\n    _SPAN_THRESHOLD = float(_g(\"TRG_SPAN_THRESHOLD\", _g(\"SPAN_THRESHOLD\", 0.15)))\n    _TAU_LOW = float(_g(\"TAU_LOW\", 0.25))\n    _UNCERTAINTY_THRESHOLD = float(_g(\"UNCERTAINTY_THRESHOLD\", _TAU_LOW))\n    _TRG_UNCERTAINTY_THRESHOLD = float(_g(\"TRG_UNCERTAINTY_THRESHOLD\", _UNCERTAINTY_THRESHOLD))\n\n    _M2M100_EN_TOKEN_ID = int(_g(\"M2M100_EN_TOKEN_ID\", 128022))\n    _M2M100_BN_TOKEN_ID = int(_g(\"M2M100_BN_TOKEN_ID\", 128025))\n\n    raw_list = _g(\"HOMOGRAPH_REFERENCE_LIST_BN\", [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w).strip().lower() for w in raw_list if w is not None)\n\n    cell0_loaded = \"NUM_SAMPLES\" in globals()\nexcept Exception as e:\n    # Fallbacks if global read fails\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 150\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.15\n    _TAU_LOW = 0.25\n    _UNCERTAINTY_THRESHOLD = 0.25\n    _TRG_UNCERTAINTY_THRESHOLD = 0.25\n    _M2M100_EN_TOKEN_ID = 128022\n    _M2M100_BN_TOKEN_ID = 128025\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}\n    cell0_loaded = False\n\n_CHECKPOINT_PATH = _g(\"CHECKPOINT_PATH\", _g(\"CHECKPOINT_DIR\", \"/kaggle/working\") + \"/tatn_final.pt\")\n\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        a_i = int(a)\n        b_i = int(b)\n        if b_i <= 0:\n            return 0\n        return math.ceil(a_i / b_i)\n    except Exception:\n        return 0\n\n\ndef _format_duration(seconds: float) -> str:\n    try:\n        seconds = float(seconds)\n        if seconds < 60:\n            return f\"{seconds:.1f}s\"\n        if seconds < 3600:\n            return f\"{seconds/60:.1f}min\"\n        return f\"{seconds/3600:.2f}hr\"\n    except Exception:\n        return \"N/A\"\n\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, default)\n        if result is default:\n            return default\n    return result\n\n\ndef _get_dscd_homographs(model) -> Set[str]:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if not dscd or not hasattr(dscd, \"prototype_stores\"):\n            return set()\n\n        lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n        if lock:\n            with lock:\n                stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        word_prototype_counts = defaultdict(int)\n        for token, store in stores.items():\n            try:\n                num_protos = 0\n                if hasattr(store, \"size\") and callable(getattr(store, \"size\")):\n                    try:\n                        num_protos = int(store.size())\n                    except Exception:\n                        num_protos = 0\n                else:\n                    centroids = getattr(store, \"centroids\", None)\n                    try:\n                        num_protos = len(centroids) if centroids is not None else 0\n                    except Exception:\n                        num_protos = 0\n\n                clean = (\n                    str(token)\n                    .replace(\"▁\", \"\")\n                    .replace(\"Ġ\", \"\")\n                    .replace(\"##\", \"\")\n                    .replace(\"@@\", \"\")\n                    .replace(\"</w>\", \"\")\n                    .strip()\n                    .lower()\n                )\n                if clean:\n                    word_prototype_counts[clean] = max(word_prototype_counts[clean], num_protos)\n            except Exception:\n                continue\n\n        homographs = {w for w, c in word_prototype_counts.items() if c >= 2}\n        return homographs\n    except Exception:\n        return set()\n\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\n\n# entrypoint\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN - COMPLETE EXECUTION\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  TRG uncertainty threshold: {_TRG_UNCERTAINTY_THRESHOLD}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    # Ensure main pipeline exists\n    if \"main_pipeline\" not in globals() or not callable(globals().get(\"main_pipeline\")):\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed or main_pipeline not defined\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n            if _DEBUG_TIMING:\n                print(\"   Expected runtime: depends on config and hardware\")\n\n            pipeline_start = time.time()\n            # main_pipeline returns (model, tokenizer) in this notebook\n            res = globals()[\"main_pipeline\"]()\n            # support both returning (model, tokenizer) or model only\n            if isinstance(res, tuple) and len(res) >= 2:\n                trained_model, tokenizer = res[0], res[1]\n            elif res is None:\n                trained_model, tokenizer = None, None\n            else:\n                trained_model = res\n                tokenizer = globals().get(\"tokenizer\", None)\n\n            pipeline_duration = time.time() - pipeline_start\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            if trained_model is None or tokenizer is None:\n                pipeline_success = False\n                failure_category = \"PIPELINE_NO_OUTPUT\"\n                failure_details = \"main_pipeline did not return (model, tokenizer)\"\n            else:\n                pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n            if \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n                print(\"\\nFixes:\")\n                print(\"   1. Reduce BATCH_SIZE (try 2-4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10k-20k)\")\n                print(\"   3. Increase ACCUMULATION_STEPS (32-64)\")\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    # If pipeline succeeded, proceed to verification and light evaluation\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        # THREAD CLEANUP: wait for clustering threads/futures to finish (safe, bounded wait)\n        print(\"\\n[THREAD CLEANUP]\")\n        print(\"Waiting for clustering threads to complete (bounded)...\")\n        try:\n            core = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            dscd = getattr(core, \"dscd\", None)\n            if dscd and hasattr(dscd, \"active_threads\"):\n                try:\n                    lock = getattr(dscd, \"thread_lock\", None) or getattr(dscd, \"buffer_lock\", None)\n                    if lock:\n                        with lock:\n                            threads = list(dscd.active_threads) if hasattr(dscd.active_threads, \"__iter__\") else []\n                    else:\n                        threads = list(dscd.active_threads) if hasattr(dscd.active_threads, \"__iter__\") else []\n\n                    if threads:\n                        print(f\"   Found {len(threads)} active threads/futures\")\n                        completed, timed_out = 0, 0\n                        for i, worker in enumerate(threads):\n                            try:\n                                if hasattr(worker, \"result\") and callable(getattr(worker, \"result\")):\n                                    try:\n                                        worker.result(timeout=10)\n                                        completed += 1\n                                    except Exception:\n                                        timed_out += 1\n                                elif hasattr(worker, \"is_alive\") and hasattr(worker, \"join\"):\n                                    if worker.is_alive():\n                                        try:\n                                            worker.join(timeout=10)\n                                        except Exception:\n                                            pass\n                                        if worker.is_alive():\n                                            timed_out += 1\n                                        else:\n                                            completed += 1\n                                    else:\n                                        completed += 1\n                                else:\n                                    completed += 1\n                            except Exception:\n                                timed_out += 1\n                                continue\n                        try:\n                            if lock:\n                                with lock:\n                                    try:\n                                        dscd.active_threads.clear()\n                                    except Exception:\n                                        pass\n                            else:\n                                try:\n                                    dscd.active_threads.clear()\n                                except Exception:\n                                    pass\n                        except Exception:\n                            pass\n                        print(f\"   Cleanup complete: {completed} completed, {timed_out} timed out\")\n                        if timed_out > 0 and _DEBUG_DISCOVERY:\n                            print(f\"   Warning: {timed_out} workers abandoned\")\n                    else:\n                        print(\"   No active threads/futures\")\n                except Exception as e:\n                    print(f\"   Thread cleanup error: {type(e).__name__}: {str(e)[:100]}\")\n            else:\n                print(\"   No thread tracking available\")\n        except Exception as e:\n            print(f\"   Thread cleanup failed: {type(e).__name__}: {str(e)[:100]}\")\n\n        time.sleep(0.5)\n        print(\"   Ready for evaluation\")\n\n        # PROTOTYPE EXTRACTION & VERIFICATION (safe extraction)\n        print(\"\\n[PROTOTYPE EXTRACTION & VERIFICATION]\")\n        print(\"Extracting and validating DSCD prototypes...\")\n        prototype_extraction_success = False\n        total_prototypes_extracted = 0\n        multi_sense_tokens_extracted = 0\n\n        try:\n            core = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            dscd = getattr(core, \"dscd\", None)\n            if dscd and hasattr(dscd, \"prototype_stores\"):\n                lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                if lock:\n                    with lock:\n                        stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n                else:\n                    stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n                prototype_data = {}\n                for token, store in stores.items():\n                    try:\n                        centroids = getattr(store, \"centroids\", None)\n                        counts = getattr(store, \"counts\", None)\n                        cent_count = 0\n                        if isinstance(centroids, torch.Tensor):\n                            cent_count = centroids.size(0)\n                        elif isinstance(centroids, list):\n                            cent_count = len(centroids)\n                        if cent_count > 0 and isinstance(counts, list) and len(counts) == cent_count and sum(counts) > 0:\n                            if isinstance(centroids, torch.Tensor):\n                                centroids_out = centroids.detach().cpu().tolist()\n                            else:\n                                centroids_out = []\n                                for c in centroids:\n                                    if isinstance(c, torch.Tensor):\n                                        centroids_out.append(c.detach().cpu().tolist())\n                                    else:\n                                        try:\n                                            centroids_out.append([float(x) for x in c])\n                                        except Exception:\n                                            centroids_out.append(c)\n                            prototype_data[str(token)] = {\"centroids\": centroids_out, \"counts\": [int(c) for c in counts]}\n                            total_prototypes_extracted += len(counts)\n                            if len(counts) >= 2:\n                                multi_sense_tokens_extracted += 1\n                    except Exception:\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n                        continue\n\n                print(\n                    f\"  Extracted prototypes: tokens={len(prototype_data)} total_protos={total_prototypes_extracted} multi_sense={multi_sense_tokens_extracted}\"\n                )\n                if len(prototype_data) == 0:\n                    print(\"  WARNING: No prototypes extracted!\")\n                elif total_prototypes_extracted == 0:\n                    print(\"  WARNING: Prototypes extracted but all empty!\")\n                else:\n                    prototype_extraction_success = True\n                    if os.path.exists(_CHECKPOINT_PATH):\n                        try:\n                            ckpt = torch.load(_CHECKPOINT_PATH, map_location=\"cpu\")\n                            dscd_state = ckpt.get(\"dscd_state\", {}) or {}\n                            stored = dscd_state.get(\"prototype_stores_data\", {}) if isinstance(dscd_state, dict) else {}\n                            if not isinstance(stored, dict) or len(stored) != len(prototype_data):\n                                dscd_state[\"prototype_stores_data\"] = prototype_data\n                                ckpt[\"dscd_state\"] = dscd_state\n                                ckpt[\"prototype_stats\"] = {\n                                    \"total_tokens\": len(prototype_data),\n                                    \"total_prototypes\": total_prototypes_extracted,\n                                    \"multi_sense_tokens\": multi_sense_tokens_extracted,\n                                }\n                                torch.save(ckpt, _CHECKPOINT_PATH)\n                                print(\"  Checkpoint updated with verified prototypes\")\n                            else:\n                                print(\"  Checkpoint prototypes match extracted data\")\n                        except Exception as e:\n                            print(f\"  Failed to update checkpoint: {type(e).__name__}: {str(e)[:100]}\")\n            else:\n                print(\"  DSCD module not found or has no prototype_stores\")\n        except Exception as e:\n            print(f\"  Prototype extraction failed: {type(e).__name__}: {str(e)[:150]}\")\n\n        # CHECKPOINT BASIC VALIDATION\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024 ** 2)\n                print(f\"  File: {_CHECKPOINT_PATH} ({size_mb:.1f} MB)\")\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location=\"cpu\")\n                model_state = ckpt.get(\"model_state_dict\", {})\n                has_model = isinstance(model_state, dict) and len(model_state) > 0\n                dscd_state = ckpt.get(\"dscd_state\", {}) or {}\n                has_dscd = (\n                    isinstance(dscd_state, dict) and len(dscd_state.get(\"prototype_stores_data\", {})) > 0\n                    if isinstance(dscd_state, dict)\n                    else False\n                )\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n                if has_dscd:\n                    stores_data = dscd_state.get(\"prototype_stores_data\", {}) if isinstance(dscd_state, dict) else {}\n                    num_tokens = len(stores_data) if isinstance(stores_data, dict) else 0\n                    corrupted = 0\n                    if isinstance(stores_data, dict):\n                        for token, sd in stores_data.items():\n                            if not isinstance(sd, dict):\n                                corrupted += 1\n                                continue\n                            cent = sd.get(\"centroids\", [])\n                            counts = sd.get(\"counts\", [])\n                            if not cent or not counts or len(cent) != len(counts) or sum(counts) <= 0:\n                                corrupted += 1\n                    print(f\"  Tokens: {num_tokens}\")\n                    if corrupted > 0:\n                        print(f\"  WARNING: {corrupted} corrupted stores\")\n                    if num_tokens > 0 and corrupted < num_tokens * 0.1:\n                        checkpoint_valid = True\n                        print(\"  Status: VALID\")\n                    elif num_tokens > 0:\n                        print(\"  Status: CORRUPTED\")\n                    else:\n                        print(\"  Status: EMPTY DSCD\")\n                else:\n                    print(\"  Status: MISSING DSCD\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n        except Exception as e:\n            print(f\"  Validation failed: {type(e).__name__}: {str(e)[:100]}\")\n\n        # COMPONENT SUMMARY\n        print(\"\\n[COMPONENTS]\")\n        try:\n            core = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            dscd = getattr(core, \"dscd\", None)\n            if dscd and hasattr(dscd, \"get_prototype_summary\"):\n                try:\n                    lock = getattr(dscd, \"buffer_lock\", None) or getattr(dscd, \"clustering_lock\", None)\n                    if lock:\n                        with lock:\n                            dscd_stats = dscd.get_prototype_summary()\n                    else:\n                        dscd_stats = dscd.get_prototype_summary()\n                    if isinstance(dscd_stats, dict):\n                        print(\"  DSCD:\")\n                        print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                        print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                        print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                except Exception:\n                    pass\n            asbn = getattr(core, \"asbn\", None)\n            if asbn and hasattr(asbn, \"get_detailed_stats\"):\n                try:\n                    result = asbn.get_detailed_stats()\n                    if isinstance(result, dict):\n                        print(\"  ASBN:\")\n                        print(f\"    - Domain accuracy: {result.get('domain_accuracy', 0):.2%}\")\n                except Exception:\n                    pass\n            trg = getattr(core, \"trg_system\", None)\n            if trg and hasattr(trg, \"get_statistics\"):\n                try:\n                    result = trg.get_statistics()\n                    if isinstance(result, dict):\n                        print(\"  TRG:\")\n                        print(f\"    - Explanations: {result.get('explanations_generated', 0)}\")\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n        # INFERENCE VALIDATION (light)\n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing disambiguation on short set of sentences...\")\n        print(\"-\" * 80)\n        _safe_cleanup()\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(sorted(dscd_homographs))[:10]}\")\n\n        test_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"কল (tap/call)\"),\n            (\"কাল আমি বই কিনব।\", \"কাল (tomorrow/yesterday)\"),\n            (\"পাতা ঝরে পড়েছে।\", \"পাতা (leaf/page)\"),\n        ]\n\n        inference_times = []\n        try:\n            if \"translate_with_explanations\" not in globals() or not callable(globals().get(\"translate_with_explanations\")):\n                print(\"translate_with_explanations not available - run Cell 8\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}. {desc}\")\n                        print(f\"   Input: {sentence}\")\n                        inf_start = time.time()\n                        # Use TRG-specific uncertainty threshold; span threshold from config\n                        res = globals()[\"translate_with_explanations\"](\n                            trained_model,\n                            tokenizer,\n                            sentence,\n                            device=_DEVICE,\n                            span_threshold=_SPAN_THRESHOLD,\n                            uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                            track_stats=False,\n                        )\n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n                        if isinstance(res, dict):\n                            translation = res.get(\"translation\", \"N/A\")\n                            amb_count = int(res.get(\"ambiguous_words_detected\", 0) or 0)\n                            exs = res.get(\"explanations\", []) or []\n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs and isinstance(exs, list):\n                                for exp in exs:\n                                    if isinstance(exp, dict):\n                                        word = exp.get(\"ambiguous_word\", exp.get(\"token\", \"N/A\"))\n                                        clean = (\n                                            str(word)\n                                            .replace(\"▁\", \"\")\n                                            .replace(\"Ġ\", \"\")\n                                            .replace(\"##\", \"\")\n                                            .replace(\"@@\", \"\")\n                                            .replace(\"</w>\", \"\")\n                                            .strip()\n                                            .lower()\n                                        )\n                                        if clean in dscd_homographs:\n                                            dscd_homographs_detected.add(clean)\n                                        try:\n                                            conf = float(exp.get(\"confidence\", 0.5))\n                                            span = float(exp.get(\"span\", 0.0))\n                                            u = float(exp.get(\"uncertainty\", 0.0))\n                                            print(f\"   -> '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                        except Exception:\n                                            print(f\"   -> '{word}': (no metrics)\")\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n                        _safe_cleanup()\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__}: {str(e)[:200]}\")\n                        if _DEBUG_DISCOVERY:\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n                        inference_failed += 1\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n                if inference_times:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg per sentence\")\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                else:\n                    print(\"No DSCD homographs detected in test sentences\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries\")\n                    else:\n                        print(f\"   -> DSCD has {len(dscd_homographs)} homographs but none in test sentences\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        # SYSTEM TEST: quick component presence checks\n        print(\"\\n[SYSTEM TEST]\")\n        try:\n            core = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n            dscd_ok = hasattr(core, \"dscd\") and callable(getattr(core.dscd, \"forward\", None))\n            asbn_ok = hasattr(core, \"asbn\") and callable(getattr(core.asbn, \"forward\", None))\n            trg_ok = hasattr(core, \"trg_system\") and callable(getattr(core.trg_system, \"process_sentence_for_explanations\", None))\n            mbart_ok = False\n            if hasattr(core, \"mbart\"):\n                mbart = core.mbart\n                mbart_ok = callable(getattr(mbart, \"generate\", None))\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK' if dscd_ok else 'MISSING'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n            print(f\"    - TRG: {'OK' if trg_ok else 'MISSING'}\")\n            print(f\"    - M2M100: {'OK' if mbart_ok else 'MISSING'}\")\n            if dscd_ok and asbn_ok and trg_ok and mbart_ok:\n                print(\"  All components operational\")\n            else:\n                print(\"  Some components missing\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n        print(\"\\n1. Single translation:\")\n        print(\"   result = translate_with_explanations(trained_model, tokenizer, 'আমি কল বন্ধ করেছি।')\")\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(\"       res = translate_with_explanations(trained_model, tokenizer, sent)\")\n        print(\"\\n3. Load checkpoint:\")\n        print(\"   ckpt = torch.load('/kaggle/working/tatn_final.pt')\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if prototype_extraction_success:\n            print(f\"\\nPrototypes saved: {total_prototypes_extracted} prototypes from {multi_sense_tokens_extracted} multi-sense tokens\")\n        else:\n            print(\"\\nPrototype extraction or checkpoint verification incomplete\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        # Pipeline failed - diagnostics\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n        print(\"\\n[DIAGNOSTICS]\")\n        components = {\n            \"Cell 0\": \"NUM_SAMPLES\" in globals(),\n            \"Cell 1\": \"reconstruct_word_spans\" in globals(),\n            \"Cell 2\": \"MemoryEfficientDataset\" in globals(),\n            \"Cell 3\": \"MemoryEfficientDSCDOnline\" in globals(),\n            \"Cell 4\": \"MemoryEfficientASBNModule\" in globals(),\n            \"Cell 5\": \"CompleteTRGWithExplanations\" in globals(),\n            \"Cell 6\": \"MemoryOptimizedTATNWithExplanations\" in globals(),\n            \"Cell 7\": \"train_memory_efficient_tatn\" in globals(),\n            \"Cell 8\": \"translate_with_explanations\" in globals(),\n            \"Cell 9\": \"comprehensive_post_training_testing\" in globals(),\n            \"Cell 10\": \"main_pipeline\" in globals(),\n        }\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status} {comp}\")\n\n        print(\"\\n[RECOVERY]\")\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"  ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"  Then RESTART kernel and re-run Cells 0-11\")\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory in Cell 0:\")\n            print(\"  BATCH_SIZE = 2\")\n            print(\"  NUM_SAMPLES = 15000\")\n            print(\"  ACCUMULATION_STEPS = 32\")\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0 and re-run Cell 11 for details\")\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check checkpoint exists: os.path.exists('%s')\" % _CHECKPOINT_PATH)\n        else:\n            print(\"\\n-> General steps: enable DEBUG, re-run Cells 0-11, verify data and GPU availability\")\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS\")\n        if \"checkpoint_valid\" in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID\")\n        else:\n            print(\"Checkpoint: CHECK NEEDED\")\n        if \"prototype_extraction_success\" in locals() and prototype_extraction_success:\n            print(f\"Prototypes: SAVED ({total_prototypes_extracted} total)\")\n        else:\n            print(\"Prototypes: EXTRACTION FAILED\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n\n    print(\"=\" * 80)\n    _safe_cleanup()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution wrapper ready - FIXED\")\nprint(\"=\" * 80)","metadata":{"id":"9n4Hrn1wH4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}