{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13835048,"sourceType":"datasetVersion","datasetId":8811181}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers sentence-transformers\n!pip install transformers==4.30.2 --no-deps --force-reinstall\n!pip install sentencepiece tokenizers sacremoses\n!pip install scipy scikit-learn\n!pip install --upgrade \"protobuf==3.20.3\"\n# optional:\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"id":"W8IIWAEHH4Jy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: ‚ö° OPTIMIZED ULTRA-FAST TATN CONFIGURATION (FIXED FOR OOM & DSCD)\n# ==============================================================================\n# ‚úÖ FIXED: OOM at step 114 (reduced batch size, accumulation, buffer size)\n# ‚úÖ FIXED: DSCD dispersion threshold (0.05 ‚Üí 0.25 to reduce noise)\n# ‚úÖ FIXED: Added CSV dataset path configuration for local file loading\n# - Consistent, safe defaults for DSCD / TRG / ASBN across notebook\n# - Prefer fast tokenizer when available (no heavy model downloads)\n# - Aligned TAU/thresholds with DSCDv2 (TAU_LOW=0.4)\n# - Validation disabled for speed by default (VALIDATION_CHECK_INTERVAL = 0)\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\n# Add pandas for CSV reading\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n    print(\"[WARN] pandas not available; CSV loading will fail\")\n\n# Try to import fast tokenizer variant when available (no model download here)\ntry:\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer\n    except Exception:\n        M2M100Tokenizer = None\n\n# datasets import is used in data cells; keep import but avoid heavy ops here\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\n# Reduce noisy warnings; keep tokenizer workers single-threaded for stability\nwarnings.filterwarnings('ignore')\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n\n# ==============================================================================\n# MULTI-GPU CONFIGURATION\n# ==============================================================================\nNUM_GPUS = torch.cuda.device_count()\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    mode = \"Single GPU Mode\" if torch.cuda.is_available() else \"CPU Mode\"\n    print(f\"[Cell 0] {mode}\")\n\nprint(f\"[Cell 0] Device: {DEVICE} (visible GPUs: {NUM_GPUS})\")\n\n# ==============================================================================\n# DATASET CONFIGURATION (LOCAL CSV FILE)\n# ==============================================================================\n# ‚ö†Ô∏è UPDATE THIS PATH to match your Kaggle input dataset location\n# Format: /kaggle/input/<dataset-slug>/<filename>.csv\n# Example: /kaggle/input/bengali-homograph-dataset/homograph_data.csv\n\nDATASET_CSV_PATH = \"/kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\"  # ‚Üê CHANGE THIS\n\n# Validate dataset path exists (early warning)\nif not os.path.exists(DATASET_CSV_PATH):\n    print(f\"[WARN] Dataset CSV not found at: {DATASET_CSV_PATH}\")\n    print(\"[WARN] Training will use fallback dataset if file is not accessible\")\nelse:\n    print(f\"[INFO] Dataset CSV found: {DATASET_CSV_PATH}\")\n    # Quick validation of CSV structure\n    try:\n        if _HAS_PANDAS:\n            _test_df = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n            if 'src' not in _test_df.columns or 'tgt' not in _test_df.columns:\n                print(f\"[ERROR] CSV missing required columns 'src' and/or 'tgt'\")\n                print(f\"[ERROR] Found columns: {list(_test_df.columns)}\")\n            else:\n                print(f\"[INFO] CSV validation passed (columns: {list(_test_df.columns)})\")\n            del _test_df\n    except Exception as e:\n        print(f\"[WARN] Could not validate CSV structure: {e}\")\n\n# ==============================================================================\n# ULTRA-FAST CONFIGURATION (user-tunable)\n# ==============================================================================\n\nBATCH_SIZE = 100              # ‚Üê FIXED: Changed from 128 (saves ~0.5 GB)\nNUM_SAMPLES = 50000          # Maximum samples to load from CSV\nMAX_LENGTH = 48               # Maximum sequence length for tokenization\nLR_NMT = 2e-5                 # Learning rate for main NMT model\nLR_TRG = 1e-5                 # Learning rate for TRG component\nLR_PHI = 1e-5                 # Learning rate for sense disambiguation\nEPOCHS = 2                    # Number of training epochs\nGRAD_CLIP_NORM = 1.0          # Gradient clipping threshold\nUSE_AMP = True                # Automatic Mixed Precision (saves memory)\nPRINT_INTERVAL = 500          # Print training stats every N steps\nSEED = 42                     # Random seed for reproducibility\n\n# ==============================================================================\n# MEMORY / PERFORMANCE SETTINGS\n# ==============================================================================\n\nACCUMULATION_STEPS = 16       # ‚Üê FIXED: Gradient accumulation steps (saves 8 GB!)\nMC_DROPOUT_PASSES = 0         # Monte Carlo dropout passes (0 = disabled for speed)\nTRG_EVIDENCE_K = 3            # Top-K evidence for TRG\nMAX_SILVER_BUFFER = 50        # Maximum silver label buffer size\n\nNUM_WORKERS = 2               # DataLoader workers (2 is safe for most systems)\nPIN_MEMORY = True             # Pin memory for faster GPU transfer\nPREFETCH_FACTOR = 2           # Number of batches to prefetch per worker\n\n# ==============================================================================\n# DSCD PARAMETERS (balanced defaults; change if you know resource limits)\n# ==============================================================================\n\nDSCD_BUFFER_SIZE = 20         # ‚Üê FIXED: Changed from 300 (saves 2.6 GB!)\nDSCD_MAX_PROTOS = 8           # Maximum prototypes per sense cluster\nDSCD_N_MIN = 5                # Minimum samples before creating new cluster\nDSCD_DISPERSION_THRESHOLD = 0.25  # ‚Üê Already correct (was 0.05 in old versions)\nDSCD_EMBED_DIM = 1024         # DSCD embedding dimension\nDSCD_TEMPERATURE = 0.7        # Temperature for contrastive loss\nDSCD_DROPOUT = 0.1            # Dropout rate for DSCD\nDSCD_AUGMENT_SCALE = 0.1      # Data augmentation noise scale\nDSCD_ENABLE_TRAINING_CLUSTERING = True  # Enable clustering during training\nDSCD_WARMUP_SAMPLES = 8000    # Warmup period before enabling clustering\n\n# ==============================================================================\n# CONTROL FLAGS\n# ==============================================================================\n\nENABLE_ASBN_TRAINING = True   # Train Ambiguity-Sensitive Batch Normalization\nENABLE_ASBN_INFERENCE = True  # Use ASBN during inference\nENABLE_TRG_TRAINING = False   # Train Target-side Gradient Reversal (disabled for speed)\nENABLE_TRG_INFERENCE = True   # Use TRG during inference\n\nCLUSTERING_TIMEOUT = 5        # Timeout (seconds) for clustering operations\nMEMORY_CLEANUP_FREQUENCY = 100  # Clean memory every N steps\nPERIODIC_DISCOVERY_FREQUENCY = 999999  # Periodic sense discovery (effectively disabled)\n\n# Validation: set to 0 to disable periodic validation checks for speed\nVALIDATION_CHECK_INTERVAL = 0  # ‚Üê Set to 0 for maximum training speed\n\nVERBOSE_LOGGING = False       # Disable verbose logging for speed\n\n# ==============================================================================\n# CHECKPOINT SETTINGS\n# ==============================================================================\n\nCHECKPOINT_DIR = \"./checkpoints\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nCHECKPOINT_INTERVAL = 20000   # Save checkpoint every N steps\nSAVE_REPLAY_BUFFER = False    # Save replay buffer in checkpoints (saves disk space when False)\nLOAD_REPLAY_BUFFER = False    # Load replay buffer from checkpoint\nREPLAY_BUFFER_SIZE = 25000    # Maximum replay buffer size\nRESUME_FROM_CHECKPOINT = False  # Resume training from checkpoint\nCHECKPOINT_PATH = \"\"          # Path to checkpoint file (if resuming)\n\n# ==============================================================================\n# TRG / UNCERTAINTY HYPERPARAMETERS (aligned to DSCDv2.tex)\n# ==============================================================================\n\nTAU_HIGH = 0.85               # High confidence threshold\nTAU_LOW = 0.4                 # Low confidence threshold (DSCDv2 aligned)\nTAU_ACCEPT = 0.8              # Acceptance threshold for pseudo-labels\nTRG_MAX_GEN_LEN = 16          # Maximum generation length for TRG\nTRG_GEN_EMBED = 64            # TRG generator embedding dimension\nTRG_GEN_HID = 64              # TRG generator hidden dimension\nSPAN_THRESHOLD = 0.3          # Span detection threshold\n\n# ==============================================================================\n# ASBN PARAMETERS\n# ==============================================================================\n\nASBN_HIDDEN_DIM = 64          # ASBN hidden dimension\nASBN_LAMBDA = 0.1             # ASBN regularization weight\nASBN_DROPOUT = 0.1            # ASBN dropout rate\n\nLAMBDA_ASBN = 0.10            # Loss weight for ASBN component\nLAMBDA_DSCD = 0.05            # Loss weight for DSCD component\n\n# ==============================================================================\n# LANGUAGE SETTINGS\n# ==============================================================================\n\nBN_LANG = \"bn\"                # Bengali language code\nEN_LANG = \"en\"                # English language code\nSOURCE_LANGUAGE = 'bn'        # Source language for translation\n\n# Bengali homograph watchlist for targeted disambiguation\nHOMOGRAPH_WATCHLIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\nWATCHLIST_ONLY_FOR_TRG = False  # Apply watchlist only to TRG (False = apply everywhere)\n\n# ==============================================================================\n# MEMORY OPTIMIZATION FLAGS\n# ==============================================================================\n\nGRADIENT_CHECKPOINTING = True  # Enable gradient checkpointing to save memory\n\n# ==============================================================================\n# UTILITY FUNCTIONS\n# ==============================================================================\n\ndef normalize_bengali(t: str) -> str:\n    \"\"\"Normalize Bengali text using NFKC Unicode normalization.\"\"\"\n    if not t:\n        return \"\"\n    return unicodedata.normalize(\"NFKC\", t).strip()\n\ndef normalize_english(t: str) -> str:\n    \"\"\"Normalize English text: NFKC + lowercase + strip.\"\"\"\n    if not t:\n        return \"\"\n    return unicodedata.normalize(\"NFKC\", t).lower().strip()\n\ndef empty_cuda_cache():\n    \"\"\"Safely empty CUDA cache and run garbage collection.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize():\n    \"\"\"Safely synchronize CUDA operations.\"\"\"\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage():\n    \"\"\"Print GPU memory usage for all visible GPUs.\"\"\"\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n                print(f\"[GPU] {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\")\n            except Exception:\n                print(f\"[GPU] {i}: memory stats unavailable\")\n\n# ==============================================================================\n# TIMEOUT DECORATOR\n# ==============================================================================\n\nclass FunctionTimeoutError(Exception):\n    \"\"\"Custom exception for function timeout.\"\"\"\n    pass\n\ndef with_timeout(seconds):\n    \"\"\"\n    Decorator to enforce timeout on functions.\n    Returns None if function exceeds timeout.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None  # Timeout occurred\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\n# ==============================================================================\n# SPECIAL TOKENS & VALIDATION HELPERS\n# ==============================================================================\n\ndef get_special_tokens(tokenizer) -> set:\n    \"\"\"Extract special tokens from tokenizer.\"\"\"\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({BN_LANG, EN_LANG})\n    return s\n\n# Lightweight token validity with thread-safe caching\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(token, special_tokens: Optional[set] = None,\n                   tokenizer=None, language: str = 'bn') -> bool:\n    \"\"\"\n    Check if token is valid for homograph disambiguation.\n    Uses thread-safe caching for performance.\n    \"\"\"\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n    \n    # Check cache first\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    # Clean token (remove subword markers)\n    clean = token.replace('‚ñÅ', '').replace('##', '').strip()\n    \n    # Bengali homograph watchlist check (always valid)\n    try:\n        if language == 'bn' and clean in HOMOGRAPH_WATCHLIST_BN:\n            result = True\n            with _cache_lock:\n                if len(_token_validation_cache) < _cache_max_size:\n                    _token_validation_cache[cache_key] = result\n            return result\n    except Exception:\n        pass\n\n    # Special token check\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        # Length check (Bengali needs 2+ chars, English needs 3+)\n        min_len = 2 if language == 'bn' else 3\n        if len(clean) < min_len:\n            result = False\n        elif not any(c.isalpha() for c in clean):\n            # Must contain at least one alphabetic character\n            result = False\n        else:\n            # Must be at least 60% alphabetic\n            alpha_count = sum(c.isalpha() for c in clean)\n            if alpha_count / max(1, len(clean)) < 0.6:\n                result = False\n            else:\n                result = True\n\n    # Cache result\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    \"\"\"\n    Safely tokenize text with offset mapping.\n    Returns (tokens, offsets) or (None, None) on failure.\n    \"\"\"\n    try:\n        encoded = tokenizer(\n            text,\n            return_offsets_mapping=True,\n            max_length=max_length,\n            truncation=True,\n            add_special_tokens=False\n        )\n        toks = tokenizer.convert_ids_to_tokens(encoded.get('input_ids', []))\n        offsets = encoded.get('offset_mapping', [(0, 0)] * len(toks))\n        return toks, offsets\n    except Exception:\n        return None, None\n\n# ==============================================================================\n# RANDOM SEEDS & BACKEND TWEAKS\n# ==============================================================================\n\n# Set all random seeds for reproducibility\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# PyTorch performance optimizations\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\n# cuDNN optimizations (benchmark mode for consistent input sizes)\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed\n\n# ==============================================================================\n# CONFIGURATION SUMMARY\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚ö° OPTIMIZED ULTRA-FAST TATN CONFIGURATION (Cell 0 - FIXED FOR OOM)\")\nprint(\"=\"*80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs visible)\")\nprint(f\"Dataset source: LOCAL CSV (Custom Bengali-English homograph dataset)\")\nprint(f\"Dataset path: {DATASET_CSV_PATH}\")\nprint(f\"Dataset samples: {NUM_SAMPLES:,} (maximum to load)\")\nprint(f\"Batch Size: {BATCH_SIZE} x {ACCUMULATION_STEPS} grad-accum steps\")\nprint(f\"Effective batch size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\nprint(f\"Max Length: {MAX_LENGTH} tokens\")\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Workers: {NUM_WORKERS}, Prefetch: {PREFETCH_FACTOR}, Pin memory: {PIN_MEMORY}\")\nprint(f\"AMP: {'ENABLED' if USE_AMP else 'DISABLED'}\")\nprint(f\"Validation interval: {VALIDATION_CHECK_INTERVAL} ({'DISABLED' if VALIDATION_CHECK_INTERVAL == 0 else 'ENABLED'})\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer size: {DSCD_BUFFER_SIZE}\")\nprint(f\"  Max prototypes: {DSCD_MAX_PROTOS}\")\nprint(f\"  n_min: {DSCD_N_MIN}\")\nprint(f\"  dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  embedding dim: {DSCD_EMBED_DIM}\")\nprint(f\"  temperature: {DSCD_TEMPERATURE}\")\nprint(f\"  training clustering: {'ENABLED' if DSCD_ENABLE_TRAINING_CLUSTERING else 'DISABLED (warmup only)'}\")\nprint(f\"  warmup samples: {DSCD_WARMUP_SAMPLES}\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  TAU_LOW: {TAU_LOW}, TAU_HIGH: {TAU_HIGH}, TAU_ACCEPT: {TAU_ACCEPT}\")\nprint(f\"  span threshold: {SPAN_THRESHOLD}\")\nprint(f\"  TRG training: {'ENABLED' if ENABLE_TRG_TRAINING else 'DISABLED'}\")\nprint(f\"  TRG inference: {'ENABLED' if ENABLE_TRG_INFERENCE else 'DISABLED'}\")\nprint()\nprint(\"ASBN / Loss weights:\")\nprint(f\"  ASBN training: {'ENABLED' if ENABLE_ASBN_TRAINING else 'DISABLED'}\")\nprint(f\"  ASBN inference: {'ENABLED' if ENABLE_ASBN_INFERENCE else 'DISABLED'}\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN}\")\nprint(f\"  LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint()\nprint(\"Learning Rates:\")\nprint(f\"  NMT: {LR_NMT}, TRG: {LR_TRG}, PHI: {LR_PHI}\")\nprint(\"=\"*80)\nprint(\"üîß MEMORY OPTIMIZATIONS APPLIED:\")\nprint(f\"  ‚Ä¢ Batch size reduced: 128 ‚Üí {BATCH_SIZE}\")\nprint(f\"  ‚Ä¢ Accumulation reduced: 16 ‚Üí {ACCUMULATION_STEPS} (saves ~8 GB)\")\nprint(f\"  ‚Ä¢ DSCD buffer reduced: 300 ‚Üí {DSCD_BUFFER_SIZE} (saves ~2.6 GB)\")\nprint(f\"  ‚Ä¢ Gradient checkpointing: {'ENABLED' if GRADIENT_CHECKPOINTING else 'DISABLED'}\")\nprint(f\"  ‚Ä¢ Expected memory: ~6.5 GB per GPU (safe on 14.7 GB)\")\nprint(\"=\"*80)\n\n# Final sanity checks and warnings\nif not (0.0 <= TAU_LOW <= 1.0):\n    print(\"[WARN] TAU_LOW out of range [0, 1]; resetting to 0.4\")\n    TAU_LOW = 0.4\n\nif not (0.0 <= TAU_HIGH <= 1.0):\n    print(\"[WARN] TAU_HIGH out of range [0, 1]; resetting to 0.85\")\n    TAU_HIGH = 0.85\n\nif TAU_LOW >= TAU_HIGH:\n    print(\"[WARN] TAU_LOW >= TAU_HIGH; swapping values\")\n    TAU_LOW, TAU_HIGH = 0.4, 0.85\n\nif VALIDATION_CHECK_INTERVAL != 0:\n    print(f\"[INFO] Validation enabled every {VALIDATION_CHECK_INTERVAL} steps\")\n    print(\"[INFO] For maximum training speed, set VALIDATION_CHECK_INTERVAL = 0\")\n\nif not _HAS_PANDAS:\n    print(\"[ERROR] pandas is required for CSV loading but not available!\")\n    print(\"[ERROR] Install with: !pip install pandas\")\n\nprint(\"‚úÖ Cell 0: Configuration loaded (FIXED: OOM prevention + DSCD optimization + CSV support).\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1 - SAFE TOKENIZER UTILITIES (HARDENED)\n# - Robust special-token caching\n# - Deterministic offset normalization (encoded[\"offset_mapping\"] always present)\n# - Fast / slow tokenizer handling improved\n# - Word-span reconstruction fallback order: offsets -> SPM markers -> whitespace\n# ===========================================================================================\n\nimport threading\nfrom typing import Tuple, List, Dict, Optional\nimport numpy as np\nimport torch\n\n# Local defaults to avoid hard dependency on other cells\ntry:\n    SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\nexcept NameError:\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"  # default to Bengali if not specified\n\n# Thread-safe cache for special tokens\n_SPECIAL_TOKENS_CACHE: Dict[str, set] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n\n\ndef _special_token_cache_key(tokenizer) -> str:\n    \"\"\"Build a stable key for caching special token sets for a tokenizer.\"\"\"\n    # tokenizer.name_or_path is preferred; fallback to repr\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None) or repr(tokenizer)\n    # determine vocab size safely\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    # final key:\n    return f\"{name}__vocab={vocab}\"\n\n\ndef get_tokenizer_special_tokens(tokenizer) -> set:\n    \"\"\"\n    Return a cached set of special tokens for `tokenizer`.\n    The result is conservative (includes common placeholders) and avoids\n    repeated expensive introspection.\n    \"\"\"\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens = set()\n        try:\n            # Try common tokenizer attributes in order of availability\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    special_tokens.update(x for x in getattr(tokenizer, \"all_special_tokens\") or [] if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    special_tokens.update(x for x in getattr(tokenizer, \"additional_special_tokens\") or [] if x)\n                except Exception:\n                    pass\n            # single-token attributes\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\", \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            # special_tokens_map or extended map\n            try:\n                stm = getattr(tokenizer, \"special_tokens_map\", None) or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n\n        except Exception:\n            # fallback to safe conservative set\n            special_tokens = set()\n\n        # Add conservative language / placeholder tokens likely useful for m2m100 & friends\n        special_tokens.update({\n            \"bn_IN\", \"en_XX\",\n            \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"\n        })\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    \"\"\"\n    Normalize a BatchEncoding (from HF tokenizer) so that enc['offset_mapping']\n    is set and in Python list-of-(start,end) tuples for the first example in the batch.\n    This function mutates enc in-place and returns it.\n    \"\"\"\n    # prefer the direct key if present (works for fast tokenizers)\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            # Case: tensor (pt) or list-of-lists\n            try:\n                # If pt tensor\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    # arr is typically [[ [s,e], [s,e], ... ]]\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, list) and len(x) == 2 else (None, None) for x in arr[0]]\n                        return enc\n                # If already list-like\n                if isinstance(off, (list, tuple)):\n                    # ensure first-element list -> normalize its elements to tuples\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in off[0]]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # Last resort: if BatchEncoding exposes .data with offset_mapping, try that\n    try:\n        data = getattr(enc, \"data\", None)\n        if data and isinstance(data, dict) and \"offset_mapping\" in data and data[\"offset_mapping\"] is not None:\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in om[0]]\n                return enc\n    except Exception:\n        pass\n\n    # If we reach here, ensure enc[\"offset_mapping\"] exists and is a list for the first example (sequence length placeholder)\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            # input_ids may be tensor or list\n            if hasattr(input_ids, \"shape\"):\n                seq_len = int(input_ids.shape[-1])\n            elif isinstance(input_ids, (list, tuple)) and len(input_ids) > 0 and isinstance(input_ids[0], (list, tuple)):\n                seq_len = len(input_ids[0])\n        # create placeholder offsets\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\n\ndef safe_offsets_tokenize(tokenizer, text: str, max_length: Optional[int] = None,\n                          include_special_tokens: bool = False) -> dict:\n    \"\"\"\n    Tokenize `text` with tokenizer and *guarantee* that the return value has:\n      - 'input_ids' and optionally 'attention_mask' (as returned by HF tokenizer)\n      - 'offset_mapping' key present and normalized to a list of (start,end) tuples\n        for the first example in the batch (or an empty list if unavailable).\n\n    Parameters:\n      tokenizer: HF tokenizer instance (fast or slow)\n      text: input string\n      max_length: token truncation max (defaults to SAFE_OFFSET_MAX_LEN)\n      include_special_tokens: whether to include special tokens in tokenization\n    \"\"\"\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str):\n        text = \"\" if text is None else str(text)\n\n    # Limit characters to avoid pathological inputs\n    char_limit = min(eff_max * 20, 2000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    # Prefer the fast path; ensure we ask for offsets and tensor outputs for convenience\n    if is_fast:\n        try:\n            enc = tokenizer(\n                sample_text,\n                return_offsets_mapping=True,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=False,\n                max_length=eff_max,\n                add_special_tokens=include_special_tokens,\n            )\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            # fallthrough to slow path\n            pass\n\n    # Slow tokenizer path: ask for ids, then build best-effort offsets\n    try:\n        enc = tokenizer(\n            sample_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=eff_max,\n            add_special_tokens=include_special_tokens,\n        )\n    except Exception:\n        # If the tokenizer call fails completely, produce a minimal encoding\n        # that downstream code can still handle.\n        enc = {\"input_ids\": torch.tensor([[tokenizer.pad_token_id if hasattr(tokenizer, \"pad_token_id\") else 0]]),\n               \"attention_mask\": torch.tensor([[1]])}\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    # Try to compute a fallback offset map by aligning decoded token text to source\n    try:\n        # get sequence of token ids (first example)\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            # try alternative access\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n        tokens = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n        # Build offsets by searching token text in source progressively\n        offsets_list = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            # clean subword markers commonly used by SPM/BPE/fast tokenizers\n            token_text = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            # naive search from current position\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n        # normalize to same format expected by _normalize_offset_mapping_for_batchencoding\n        enc[\"offset_mapping\"] = offsets_list\n        # ensure normalized (wrap as first-example list)\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        # fallback: ensure offset_mapping exists\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n\ndef reconstruct_word_spans(tokenizer, text: str, max_length: Optional[int] = None) -> Tuple[Dict[int, str], List[str]]:\n    \"\"\"\n    Return:\n      - token_word_map: mapping token_index -> reconstructed word string (best-effort)\n      - words: list[str] of words discovered in order\n\n    Strategy:\n      1) Use tokenizer offsets when available -> group contiguous character spans into words.\n      2) If offsets unavailable or unhelpful, use SPM-style '‚ñÅ' or 'ƒ†' markers to assemble subwords.\n      3) Finally fallback to whitespace-splitting.\n    \"\"\"\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    char_limit = min(eff_max * 20, 2000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    # Get normalized encoding (guarantees offset_mapping exists)\n    try:\n        encoded = safe_offsets_tokenize(tokenizer, text, max_length=eff_max, include_special_tokens=False)\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    # ensure input_ids and tokens exist\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    # Ensure offsets is a list with len(tokens) (if possible)\n    if isinstance(offsets, list) and len(offsets) > 0 and all(isinstance(x, tuple) for x in offsets):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(offsets[0], (list, tuple)):\n        offsets_list = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in offsets[0]]\n    else:\n        # not usable\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, str] = {}\n    words: List[str] = []\n\n    # 1) Use offsets to group contiguous spans into words\n    used_any_offset = any((isinstance(o, tuple) and o[0] is not None and o[1] is not None) for o in offsets_list)\n    if used_any_offset:\n        word_start = None\n        word_end = None\n        word_accum = \"\"\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start, off_end = (int(off[0]) if off[0] is not None else None, int(off[1]) if off[1] is not None else None)\n            except Exception:\n                off_start, off_end = None, None\n            if off_start is None or off_end is None:\n                # token with no offsets: close existing word and skip\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                word_accum = \"\"\n                token_word_map[idx] = \"UNK\"\n                continue\n\n            # optionally skip special tokens\n            if tok in special_tokens:\n                token_word_map[idx] = \"\"\n                continue\n\n            # Start new word if needed\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                # If this token begins after the previous end -> new word\n                if off_start > word_end:\n                    # flush previous\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            # map token to the current word slice (best-effort)\n            try:\n                current_word = text[word_start:word_end].strip()\n                token_word_map[idx] = current_word if current_word else \"UNK\"\n            except Exception:\n                token_word_map[idx] = \"UNK\"\n\n        # flush last\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    # 2) Fallback to SPM/BPE marker assembly (tokens marked with '‚ñÅ' or 'ƒ†')\n    token_word_map = {}\n    assembled = []\n    current = \"\"\n    running_word = \"\"\n    for i, tok in enumerate(tokens):\n        # skip special tokens\n        if tok in special_tokens:\n            token_word_map[i] = \"\"\n            continue\n        # normalize token text\n        clean = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").strip()\n        if not clean:\n            token_word_map[i] = \"\"\n            continue\n        if (tok.startswith(\"‚ñÅ\") or tok.startswith(\"ƒ†\")):\n            # new word\n            if current:\n                assembled.append(current)\n            current = clean\n            running_word = current\n        else:\n            # continuation subword\n            current = current + clean\n            running_word = current\n        token_word_map[i] = running_word if running_word else \"UNK\"\n    if current:\n        assembled.append(current)\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    # 3) Final fallback: whitespace-split the original text and assign tokens approximately\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n        if tokens and word_list:\n            widx = 0\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").strip()\n                if not clean:\n                    token_word_map[i] = \"\"\n                    continue\n                token_word_map[i] = word_list[min(widx, len(word_list) - 1)]\n                # Heuristic: if token looks long or contains punctuation advance\n                if len(clean) > len(token_word_map[i]) or clean.endswith((\".\", \",\", \";\", \"‡•§\", \"?\" , \"!\" )):\n                    widx = min(widx + 1, len(word_list) - 1)\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\n\n# ===========================================================================================\n# LIGHTWEIGHT SELF-TEST\n# ===========================================================================================\ndef test_tokenizer_utilities_quick(tokenizer=None):\n    \"\"\"\n    If tokenizer is None, this will only sanity-check Python-level logic.\n    If tokenizer is provided (HF tokenizer), it will run a quick encode + reconstruct.\n    \"\"\"\n    sample = \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶Ø‡¶æ‡¶¨‡•§\"  # Bengali: \"Tomorrow I will go to the market.\"\n    print(\"Running tokenizer-utils quick test...\")\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: basic logic OK.\")\n            return True\n        enc = safe_offsets_tokenize(tokenizer, sample, max_length=32, include_special_tokens=False)\n        print(\"  Encoded input_ids len:\", int(enc[\"input_ids\"].shape[-1]) if \"input_ids\" in enc else \"N/A\")\n        print(\"  Offset mapping (first 10):\", (enc.get(\"offset_mapping\") or [])[:10])\n        token_map, words = reconstruct_word_spans(tokenizer, sample, max_length=32)\n        print(\"  Reconstructed words:\", words)\n        print(\"  Token->word examples:\", {k: token_map[k] for k in list(token_map.keys())[:6]})\n        return True\n    except Exception as e:\n        print(\"Tokenizer utilities quick test failed:\", repr(e))\n        return False\n\n\n# This print is a gentle confirmation that the utilities loaded.\nprint(\"‚úÖ Cell 1 (tokenizer utilities) loaded and hardened.\")","metadata":{"id":"WZE9PkHyH4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (FIXED & HARDENED + CSV SUPPORT)\n# ==============================================================================\n# ‚úÖ FIXED: Replaced Samanantar with local CSV loading\n# ‚úÖ FIXED: Added pandas-based CSV reader with proper column mapping\n# ‚úÖ FIXED: Enhanced error handling and validation\n# - Robust fallbacks when datasets/tokenizer utilities are missing\n# - Safer DP-divisible batching logic (floor to nearest multiple by default)\n# - Worker init rebinds tokenizer safely for multiprocessing workers\n# - Deterministic per-worker seeding\n# - Safe collate that always returns stackable tensors and preserves token_word_map\n# - Defensive behaviors and verbose debug prints controlled by VERBOSE_LOGGING\n# ==============================================================================\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\n# Pandas import for CSV reading (required for local dataset)\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\n# Optional import - datasets library (not needed for CSV mode)\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\n# -------------------------\n# Debug control\n# -------------------------\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT):\n    \"\"\"Debug print with rate limiting.\"\"\"\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\n\n# -------------------------\n# Local fallbacks for globals (explicit, safe)\n# -------------------------\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _BN_LANG = BN_LANG\n    _EN_LANG = EN_LANG\nexcept NameError:\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n    print(\"[CELL2] WARNING: BN_LANG/EN_LANG not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\n# Check availability of utility functions from Cell 0\n_has_normalize = ('normalize_bengali' in globals()) and ('normalize_english' in globals())\n_has_reconstruct_word_spans = 'reconstruct_word_spans' in globals()\n_has_safe_offsets_tokenize = 'safe_offsets_tokenize' in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n# -------------------------\n# Utility: detect Bengali text heuristically\n# -------------------------\n_BENGALI_CHAR_RE = re.compile(r'[\\u0980-\\u09FF]')\n\ndef is_bengali_text(s: str) -> bool:\n    \"\"\"Check if text contains Bengali Unicode characters.\"\"\"\n    if not isinstance(s, str) or not s:\n        return False\n    # if any Bengali char present, treat as Bengali\n    return bool(_BENGALI_CHAR_RE.search(s))\n\n\n# -------------------------\n# Worker init: reattach tokenizer and set per-worker seed\n# -------------------------\ndef _dataloader_worker_init_fn(worker_id: int):\n    \"\"\"Initialize DataLoader worker with tokenizer and deterministic seed.\"\"\"\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    \n    # Try to rebind tokenizer from the main process globals into the worker dataset\n    try:\n        if dataset is not None:\n            tk = globals().get('tokenizer', None)\n            if tk is not None:\n                try:\n                    # attach tokenizer reference only (avoid copying heavy state)\n                    dataset.tokenizer = tk\n                    dataset.is_fast = getattr(tk, \"is_fast\", False)\n                except Exception:\n                    dataset.tokenizer = tk\n                    dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] tokenizer rebind failed in worker {worker_id}\")\n            traceback.print_exc()\n    \n    # Set a deterministic-ish per-worker seed to avoid RNG issues\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        # incorporate worker id and time low bits to change per-worker seed\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\n\n# -------------------------\n# Data loading and preprocessing (CSV-BASED)\n# -------------------------\ndef load_and_preprocess_optimized(num_samples: Optional[int] = None) -> List[Tuple[str, str]]:\n    \"\"\"\n    Load parallel bn-en pairs from local CSV file.\n    CSV format: idx,src,tgt (where src=English, tgt=Bengali)\n    Returns list of (bn, en) pairs.\n    Falls back to a small hard-coded set if CSV load fails.\n    \"\"\"\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n    \n    # Validate pandas availability\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Install with: !pip install pandas\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    # Validate CSV file exists\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    try:\n        # Read CSV file\n        print(f\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        \n        # Validate required columns\n        if 'src' not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing 'src' column. Found columns: {list(df.columns)}\")\n            return _get_fallback_dataset()\n        \n        if 'tgt' not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing 'tgt' column. Found columns: {list(df.columns)}\")\n            return _get_fallback_dataset()\n        \n        # Limit to num_samples\n        df = df.head(num_samples)\n        \n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n        \n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n        \n        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading dataset\"):\n            try:\n                # src = English, tgt = Bengali\n                en = str(row['src']).strip()\n                bn = str(row['tgt']).strip()\n                \n                # Basic validation\n                if not en or not bn:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", f\"Empty src/tgt at idx={idx}\")\n                    continue\n                \n                # Check for \"nan\" string from pandas\n                if en.lower() == 'nan' or bn.lower() == 'nan':\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", f\"NaN value at idx={idx}\")\n                    continue\n                \n                # Length check (avoid extremely long sentences)\n                max_words = max(40, _MAX_LENGTH)\n                if len(en.split()) > max_words or len(bn.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", f\"Too long at idx={idx}: en={len(en.split())} bn={len(bn.split())} words\")\n                    continue\n                \n                # Normalize if available\n                if _has_normalize:\n                    bn_norm = normalize_bengali(bn)\n                    en_norm = normalize_english(en)\n                else:\n                    bn_norm = bn\n                    en_norm = en.lower()\n                \n                # Ensure normalization didn't create empty strings\n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", f\"Empty after normalization at idx={idx}\")\n                    continue\n                \n                # Store as (Bengali, English) pair - IMPORTANT ORDER!\n                pairs.append((bn_norm, en_norm))\n                \n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception idx={idx}: {type(e).__name__}: {str(e)[:100]}\")\n                continue\n        \n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        \n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            return _get_fallback_dataset()\n        \n        return pairs\n        \n    except FileNotFoundError:\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    \n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        print(f\"[CELL2] Traceback: {traceback.format_exc().splitlines()[-3:]}\")\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    \"\"\"Return a small fallback dataset for debugging/testing.\"\"\"\n    print(\"[CELL2] Using fallback small dataset (5 samples)\")\n    fallback_pairs = [\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"i turned off the tap.\"),\n        (\"‡¶∏‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶™‡¶∞‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶¨‡ßá‡•§\", \"he will call me later.\"),\n        (\"‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¶‡¶ø‡¶® ‡¶§‡¶æ‡¶ú‡¶æ ‡¶´‡¶≤ ‡¶ñ‡¶æ‡¶á‡•§\", \"we eat fresh fruits every day.\"),\n        (\"‡¶§‡¶æ‡¶∞ ‡¶ï‡¶†‡ßã‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∂‡ßç‡¶∞‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶´‡¶≤ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§\", \"his hard work has brought good results.\"),\n        (\"‡¶ó‡¶æ‡¶õ‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡¶æ‡¶§‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ó‡¶ú‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§\", \"new leaves have sprouted on the tree.\")\n    ]\n    if _has_normalize:\n        return [(normalize_bengali(bn), normalize_english(en)) for bn, en in fallback_pairs]\n    else:\n        return [(bn.strip(), en.lower().strip()) for bn, en in fallback_pairs]\n\n\n# -------------------------\n# Dataset Class\n# -------------------------\nclass MemoryEfficientDataset(Dataset):\n    \"\"\"\n    Memory-efficient dataset that returns dicts with:\n      - input_ids, attention_mask: torch.LongTensor [L]\n      - labels: torch.LongTensor [L] with pad->-100\n      - token_word_map: dict token_idx->word\n      - src_text: original source string\n      - tokens: list of token strings\n    The tokenizer attribute is excluded from pickled state so DataLoader workers don't crash.\n    \"\"\"\n\n    def __init__(self, pairs: List[Tuple[str, str]], tokenizer: Any = None, max_length: Optional[int] = None):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n        \n        # Validate and filter pairs\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                \n                src, tgt = p\n                \n                # Type validation\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                \n                # Empty check\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                \n                # Length sanity check (character level)\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                \n                self.pairs.append((src, tgt))\n                \n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n        \n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid pairs filtered\")\n\n        # Get special tokens for filtering\n        try:\n            if 'get_special_tokens' in globals():\n                self.special_tokens = get_special_tokens(self.tokenizer)\n            elif 'get_tokenizer_special_tokens' in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {_BN_LANG, _EN_LANG, \"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n            cell2_dbg(\"special_tokens_fallback\", \"Used explicit fallback special tokens\")\n\n    def __getstate__(self):\n        \"\"\"Prepare state for pickling (exclude tokenizer).\"\"\"\n        state = self.__dict__.copy()\n        # avoid serializing tokenizer into worker processes\n        state['tokenizer'] = None\n        state['_tokenizer_name_or_path'] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore state after unpickling (rebind tokenizer).\"\"\"\n        self.__dict__.update(state)\n        try:\n            # rebind tokenizer from global if available (set by worker_init_fn)\n            self.tokenizer = globals().get('tokenizer', None)\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n        except Exception:\n            self.tokenizer = None\n            self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        \"\"\"Encode source (Bengali) text.\"\"\"\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        \n        try:\n            # Ensure tokenizer is available\n            if self.tokenizer is None:\n                try:\n                    self.tokenizer = globals().get('tokenizer', None)\n                    self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n                except Exception:\n                    self.tokenizer = None\n                    self.is_fast = False\n\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            # Set source language hints if tokenizer supports it\n            try:\n                if hasattr(self.tokenizer, \"src_lang\"):\n                    self.tokenizer.src_lang = _BN_LANG\n            except Exception:\n                pass\n\n            # Prefer safe_offsets_tokenize if available\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                try:\n                    input_ids = enc[\"input_ids\"].squeeze(0) if isinstance(enc[\"input_ids\"], torch.Tensor) else torch.tensor(enc[\"input_ids\"][0])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0])\n                \n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids))\n                if isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                \n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                # Standard tokenization\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=False\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            # Build token-word mapping\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict):\n                        token_word_map = wm\n                except Exception:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {traceback.format_exc().splitlines()[-1]}\")\n                    token_word_map = {}\n            else:\n                # Fallback: mark tokens starting with SPM markers as word starts\n                try:\n                    for idx, tok in enumerate(tokens):\n                        if isinstance(tok, str) and (tok.startswith(\"‚ñÅ\") or tok.startswith(\"ƒ†\")):\n                            token_word_map[idx] = tok.replace(\"‚ñÅ\", \"\").replace(\"ƒ†\", \"\").strip()\n                except Exception:\n                    token_word_map = {}\n\n            return input_ids, attention_mask, tokens, token_word_map\n            \n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}: {str(e)[:60]}\")\n            # Return safe placeholder\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        \"\"\"Encode target (English) text.\"\"\"\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        \n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get('tokenizer', None)\n            \n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n            \n            # Set target language hints where supported\n            try:\n                if hasattr(self.tokenizer, \"tgt_lang\"):\n                    self.tokenizer.tgt_lang = _EN_LANG\n            except Exception:\n                pass\n            \n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            \n            # Replace pad tokens with -100 (ignore index for loss)\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer else 1\n            labels[labels == int(pad_id)] = -100\n            \n            return labels\n            \n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}: {str(e)[:60]}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\"):\n        \"\"\"Create a safe fallback sample.\"\"\"\n        try:\n            src = \"‡¶Ü‡¶Æ‡¶ø\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens\n            }\n        except Exception:\n            pad_id = 1\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": []\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"Get a single sample by index.\"\"\"\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx} len={len(self.pairs)}\")\n                return self._make_safe_sample(\"oob\")\n            \n            src, tgt = self.pairs[idx]\n            \n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\n\n# ---------------------------\n# Collation and DataLoader helpers\n# ---------------------------\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    \"\"\"Infer pad token id from tokenizer.\"\"\"\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    \"\"\"Pad or truncate tensor to exact length.\"\"\"\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    \n    t = tensor.view(-1).long()\n    L = t.size(0)\n    \n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Robust collate: ensures stackable tensors and safe structure.\n    Pads/truncates all sequences to _MAX_LENGTH deterministically.\n    \"\"\"\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    \n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]]\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n\n    inputs, masks, labs, twmaps, srcs, toks = [], [], [], [], [], []\n    \n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]]\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n\n    # DP-divisible adjustment: trim downward to nearest multiple to avoid OOM\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        bsz = input_ids.size(0)\n        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n        if keep > 0 and keep < bsz:\n            cell2_dbg(\"dp_trunc\", f\"DP truncate from {bsz} to {keep}\")\n            input_ids = input_ids[:keep]\n            attention_mask = attention_mask[:keep]\n            labels = labels[:keep]\n            twmaps = twmaps[:keep]\n            srcs = srcs[:keep]\n            toks = toks[:keep]\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks\n    }\n\n\ndef create_optimized_dataloader(dataset: Dataset, batch_size: Optional[int] = None, shuffle: bool = True) -> DataLoader:\n    \"\"\"\n    Create a DataLoader with safe defaults and deterministic worker init.\n    By default, if _USE_MULTI_GPU the batch_size will be floored to nearest multiple of _NUM_GPUS\n    to avoid oversubscribing GPU memory.\n    \"\"\"\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n    batch_size = int(batch_size)\n\n    # Floor to nearest multiple for multi-GPU\n    adjust_upwards = False  # change to True if you prefer increasing to next multiple\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        if adjust_upwards:\n            adjusted = ((batch_size + _NUM_GPUS - 1) // _NUM_GPUS) * _NUM_GPUS\n            print(f\"[CELL2] Adjusting batch size {batch_size} ‚Üí {adjusted} to be DP-divisible (GPUs={_NUM_GPUS})\")\n            batch_size = adjusted\n        else:\n            adjusted = (batch_size // _NUM_GPUS) * _NUM_GPUS\n            if adjusted == 0:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original batch_size.\")\n            else:\n                print(f\"[CELL2] Adjusting batch size {batch_size} ‚Üí {adjusted} (floor to multiple of {_NUM_GPUS}) to avoid OOM.\")\n                batch_size = adjusted\n\n    # Validate num_workers\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n    \n    # Only set worker_init_fn if using workers\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}: {str(e)[:200]}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\n\nprint(\"‚úÖ Cell 2: Memory-efficient data loading ready (FIXED: CSV support + hardened error handling)\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE - COMPLETELY FIXED WITH SAVE/LOAD/VALIDATION\n# ==============================================================================\n# ‚úÖ FIXED: Added state_dict() for prototype serialization (ERROR #1 FIX)\n# ‚úÖ FIXED: Added load_state_dict() for prototype restoration (ERROR #2 FIX)\n# ‚úÖ FIXED: Added validate_prototypes() for quality checking (ERROR #3 FIX)\n# ‚úÖ FIXED: Enhanced should_track_token() to check existing stores (ERROR #4 FIX)\n# ‚úÖ ADDED: Comprehensive debug logging for all operations (ERROR #5 FIX)\n# ‚úÖ ADDED: Quality scoring system (homograph coverage + multi-sense ratio)\n# \n# Original features preserved:\n# ‚úÖ Integrated homograph watchlist from Cell 0 for priority tracking\n# ‚úÖ Enhanced token filtering to prioritize research homographs\n# ‚úÖ Atomic centroid snapshot under clustering_lock (race condition fix)\n# ‚úÖ Vectorized distance computation using centroid snapshot\n# ‚úÖ Sampling of large buffers before clustering (OOM prevention)\n# ‚úÖ Hierarchical clustering (scipy) with sklearn KMeans fallback\n# ‚úÖ CPU-only prototype storage and clustering\n# ‚úÖ Unicode-aware token filtering (Bengali/Latin aware)\n# ==============================================================================\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any\n\nPRINT_INTERVAL = 200  # debug print cadence\n\n# Optional SciPy import for hierarchical clustering\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HAS_CLUSTERING = True\nexcept Exception:\n    HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available - hierarchical clustering disabled\")\n\n# Optional sklearn KMeans fallback\ntry:\n    from sklearn.cluster import KMeans\n    HAS_KMEANS = True\nexcept Exception:\n    HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available - KMeans fallback disabled\")\n\n# Fallback config values (will be overridden by globals if present)\ntry:\n    DSCD_MAX_PROTOS = DSCD_MAX_PROTOS\n    DSCD_BUFFER_SIZE = DSCD_BUFFER_SIZE\n    DSCD_N_MIN = DSCD_N_MIN\n    DSCD_DISPERSION_THRESHOLD = DSCD_DISPERSION_THRESHOLD\n    VERBOSE_LOGGING = VERBOSE_LOGGING\nexcept Exception:\n    DSCD_MAX_PROTOS = 8\n    DSCD_BUFFER_SIZE = 20\n    DSCD_N_MIN = 5\n    DSCD_DISPERSION_THRESHOLD = 0.25\n    VERBOSE_LOGGING = True\n    print(\"[CELL3] WARNING: Using default DSCD config values\")\n\n# Import homograph watchlist from Cell 0 (if available)\ntry:\n    HOMOGRAPH_WATCHLIST_BN = HOMOGRAPH_WATCHLIST_BN\n    print(f\"[CELL3] ‚úÖ Loaded homograph watchlist from Cell 0: {HOMOGRAPH_WATCHLIST_BN}\")\nexcept Exception:\n    HOMOGRAPH_WATCHLIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n    print(f\"[CELL3] ‚ö†Ô∏è Using default homograph watchlist: {HOMOGRAPH_WATCHLIST_BN}\")\n\n# Max points to use in expensive clustering (avoid OOM)\ntry:\n    DSCD_MAX_CLUSTERING_POINTS = int(DSCD_MAX_CLUSTERING_POINTS)\nexcept Exception:\n    DSCD_MAX_CLUSTERING_POINTS = 2000\n\n# Helper flags for utility function availability\nHAS_IS_VALID_TOKEN = 'is_valid_token' in globals()\nHAS_GET_SPECIAL_TOKENS = ('get_tokenizer_special_tokens' in globals()) or ('get_cached_special_tokens' in globals())\n\nif VERBOSE_LOGGING:\n    print(\"\\n\" + \"=\"*80)\n    print(\"[CELL3-CONFIG] DSCD Configuration (Enhanced with Homograph Support):\")\n    print(\"=\"*80)\n    print(f\"  DSCD_BUFFER_SIZE: {DSCD_BUFFER_SIZE}\")\n    print(f\"  DSCD_MAX_PROTOS: {DSCD_MAX_PROTOS}\")\n    print(f\"  DSCD_N_MIN: {DSCD_N_MIN}\")\n    print(f\"  DSCD_DISPERSION_THRESHOLD: {DSCD_DISPERSION_THRESHOLD}\")\n    print(f\"  DSCD_MAX_CLUSTERING_POINTS: {DSCD_MAX_CLUSTERING_POINTS}\")\n    print(f\"  HAS_CLUSTERING (scipy): {HAS_CLUSTERING}\")\n    print(f\"  HAS_KMEANS (sklearn): {HAS_KMEANS}\")\n    print(f\"  Homograph watchlist size: {len(HOMOGRAPH_WATCHLIST_BN)}\")\n    print(\"=\"*80 + \"\\n\")\n\n\n# ==============================================================================\n# Token helper: Unicode-aware check whether token is a \"word\" worth clustering\n# ==============================================================================\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    \"\"\"\n    Return True if token should be treated as a word (eligible for clustering).\n    - min_letters: minimum number of Unicode letters required in token (default 2).\n    - min_letter_fraction: fraction of non-space characters that must be Unicode letters (default 0.6).\n    This is language-agnostic (counts Unicode letters) and will allow Bengali, Latin, etc.\n    \"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if token == \"\":\n        return False\n\n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith(\"L\"):   # Unicode letter\n            letters += 1\n        if not ch.isspace():\n            total += 1\n\n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if (letters / total) < min_letter_fraction:\n        return False\n    return True\n\n\n# ==============================================================================\n# PROTOTYPE STORE CLASS\n# ==============================================================================\nclass MemoryEfficientPrototypeStore:\n    \"\"\"Store prototypes (centroids) for a single token type, with counts per proto.\"\"\"\n    def __init__(self, embed_dim, max_protos=None):\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        self.embed_dim = embed_dim\n        self.max_protos = int(max_protos)\n        self.centroids = []      # cpu tensors\n        self.counts = []         # integer cluster sizes\n        self.creation_time = []\n        self.distances = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n\n    def add_prototype(self, vector, current_time=None, count=1):\n        \"\"\"Add or replace a prototype centroid. vector is a torch tensor (any device).\"\"\"\n        if current_time is None:\n            current_time = time.time()\n        # Always keep prototypes on CPU to avoid GPU memory churn\n        try:\n            v = vector.detach().cpu().clone()\n        except Exception:\n            # accept numpy arrays too\n            try:\n                v = torch.from_numpy(np.asarray(vector, dtype=np.float32)).cpu()\n            except Exception:\n                return\n        \n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creation_time.append(current_time)\n        else:\n            # replace the least-supported prototype\n            try:\n                min_idx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            except Exception:\n                min_idx = 0\n            # ensure lists align\n            if min_idx < len(self.centroids):\n                self.centroids[min_idx] = v\n            else:\n                # align lengths (rare)\n                while len(self.centroids) <= min_idx:\n                    self.centroids.append(v)\n            if len(self.counts) > min_idx:\n                self.counts[min_idx] = int(count)\n            else:\n                while len(self.counts) < len(self.centroids):\n                    self.counts.append(1)\n                self.counts[min_idx] = int(count)\n            if len(self.creation_time) > min_idx:\n                self.creation_time[min_idx] = current_time\n            else:\n                while len(self.creation_time) < len(self.centroids):\n                    self.creation_time.append(current_time)\n\n    def update_prototype(self, idx, vector, eta=0.05, assignment_distance=None):\n        \"\"\"Update a prototype via online EMA and increment its count.\"\"\"\n        try:\n            if idx < 0 or idx >= len(self.centroids):\n                self.add_prototype(vector, time.time(), count=1)\n                return\n            old_centroid = self.centroids[idx]\n            new_vector = vector.detach().cpu()\n            try:\n                self.centroids[idx] = (1.0 - eta) * old_centroid + eta * new_vector\n            except Exception:\n                self.centroids[idx] = new_vector.clone()\n            # increment count safely\n            try:\n                self.counts[idx] = int(self.counts[idx]) + 1\n            except Exception:\n                # make lengths consistent\n                if len(self.counts) < len(self.centroids):\n                    self.counts = [max(1, int(c)) for c in self.counts] + [1] * (len(self.centroids) - len(self.counts))\n        except Exception:\n            # defensive: on any error, replace/add prototype\n            try:\n                self.add_prototype(vector, time.time(), count=1)\n            except Exception:\n                pass\n\n        if assignment_distance is not None:\n            try:\n                self.update_rolling_stats(float(assignment_distance))\n            except Exception:\n                pass\n\n    def update_rolling_stats(self, d):\n        \"\"\"Rolling mean and deviation for assignment distances.\"\"\"\n        try:\n            if not self.distances:\n                self.mu = float(d)\n                self.tau = 1e-6\n                self.distances = [float(d)]\n                return\n            prev_mu = self.mu\n            self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n            self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n            self.distances.append(float(d))\n            if len(self.distances) > 50:\n                self.distances.pop(0)\n        except Exception:\n            pass\n\n    def get_adaptive_threshold(self, lam=1.0):\n        \"\"\"Get adaptive threshold based on rolling statistics.\"\"\"\n        try:\n            return float(self.mu + lam * self.tau)\n        except Exception:\n            return float(self.mu)\n\n    def get_centroids(self, device):\n        \"\"\"Return centroids as a tensor on the requested device (or None).\"\"\"\n        if not self.centroids:\n            return None\n        try:\n            return torch.stack([c.to(device) for c in self.centroids], dim=0)\n        except Exception:\n            try:\n                return torch.stack([c.cpu() for c in self.centroids], dim=0).to(device)\n            except Exception:\n                return None\n\n    def get_valid_centroids(self, device, min_count=None):\n        \"\"\"Get centroids that have sufficient support (count >= min_count).\"\"\"\n        if min_count is None:\n            min_count = DSCD_N_MIN\n        idxs = [i for i, ct in enumerate(self.counts) if ct >= int(min_count)]\n        if not idxs:\n            return None, None\n        cents = [self.centroids[i].to(device) for i in idxs]\n        return torch.stack(cents, dim=0), idxs\n\n    def set_centroids_from_arrays(self, array_list, counts=None):\n        \"\"\"Set centroids from numpy arrays or tensors.\"\"\"\n        try:\n            self.centroids = [torch.from_numpy(np.asarray(a, dtype=np.float32)).cpu() for a in array_list]\n            if counts and len(counts) == len(array_list):\n                self.counts = [int(c) for c in counts]\n            else:\n                self.counts = [1 for _ in array_list]\n            self.creation_time = [time.time()] * len(array_list)\n        except Exception:\n            # best-effort fallback: clear\n            self.centroids = []\n            self.counts = []\n            self.creation_time = []\n\n    def size(self):\n        \"\"\"Return number of prototypes.\"\"\"\n        return len(self.centroids)\n\n\n# ==============================================================================\n# DSCD Online Class\n# ==============================================================================\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(self, embed_dim, tokenizer=None, buffer_size=None, max_protos=None,\n                 n_min=None, dispersion_threshold=None, language='bn',\n                 enable_training_clustering=False, max_clustering_points=None,\n                 max_candidates_per_step=2,\n                 dscd_min_letters: int = 2, dscd_min_letter_fraction: float = 0.6):\n        super().__init__()\n\n        if buffer_size is None:\n            buffer_size = DSCD_BUFFER_SIZE\n        if max_protos is None:\n            max_protos = DSCD_MAX_PROTOS\n        if n_min is None:\n            n_min = DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = DSCD_MAX_CLUSTERING_POINTS\n\n        self.embed_dim = int(embed_dim)\n        self.buffer_size = int(buffer_size)\n        self.max_protos = int(max_protos)\n        self.n_min = int(n_min)\n        self.dispersion_threshold = float(dispersion_threshold)\n        self.language = language\n        self.tokenizer = tokenizer\n\n        # token filtering parameters (for is_word_token)\n        self.dscd_min_letters = int(dscd_min_letters)\n        self.dscd_min_letter_fraction = float(dscd_min_letter_fraction)\n\n        # special tokens cache\n        try:\n            if tokenizer is not None and 'get_tokenizer_special_tokens' in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(getattr(tokenizer, 'all_special_tokens', []) if tokenizer is not None else [])\n        except Exception:\n            self.special_tokens = set()\n\n        # caches for token filtering decisions (avoid repeated unicode checks)\n        self._dscd_allowed_tokens = set()\n        self._dscd_ignored_tokens = set()\n\n        # storage\n        self.prototype_stores = {}\n        self.buffers = {}\n        self.discovery_log = []\n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n        # lock used to protect centroid reads/writes and cluster state updates\n        self.clustering_lock = threading.Lock()\n\n        # training-time clustering throttle controls\n        self.last_cluster_time = {}                  # token_key -> last clustering timestamp\n        self.cluster_cooldown_seconds = 60           # default cooldown per token (seconds)\n        self.enable_training_clustering = bool(enable_training_clustering)\n\n        # small heads for span prediction / gating (kept for compatibility)\n        self.span_head = nn.Sequential(\n            nn.Linear(self.embed_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1)\n        )\n        self.sigma_net = nn.Sequential(\n            nn.Linear(self.embed_dim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1)\n        )\n        self.gate_w = nn.Parameter(torch.tensor(1.0))\n        self.gate_b = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n\n        self.max_clustering_points = int(max_clustering_points)\n        self.max_candidates_per_step = int(max_candidates_per_step)\n\n        if VERBOSE_LOGGING:\n            print(f\"[DSCD-INIT] Initialized MemoryEfficientDSCDOnline:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - buffer_size: {self.buffer_size}\")\n            print(f\"  - max_protos: {self.max_protos}\")\n            print(f\"  - n_min: {self.n_min}\")\n            print(f\"  - dispersion_threshold: {self.dispersion_threshold}\")\n            print(f\"  - language: {self.language}\")\n            print(f\"  - enable_training_clustering: {self.enable_training_clustering}\")\n            print(f\"  - max_clustering_points: {self.max_clustering_points}\")\n            print(f\"  - min_letters: {self.dscd_min_letters}\")\n            print(f\"  - min_letter_fraction: {self.dscd_min_letter_fraction}\")\n\n    # ========================================================================\n    # ‚úÖ FIX #1: ADD state_dict() METHOD FOR PROTOTYPE SERIALIZATION\n    # ========================================================================\n    def state_dict(self):\n        \"\"\"\n        Save DSCD prototypes to a serializable dictionary.\n        This allows prototypes to be saved alongside model weights.\n        \n        Returns:\n            dict: Serializable state containing all prototype stores\n        \"\"\"\n        if VERBOSE_LOGGING:\n            print(f\"[DSCD] Saving state_dict with {len(self.prototype_stores)} token stores...\")\n        \n        state = {\n            'prototype_stores': {},\n            'discovery_log': self.discovery_log[-100:] if hasattr(self, 'discovery_log') else [],\n            'metadata': {\n                'embed_dim': self.embed_dim,\n                'max_protos': self.max_protos,\n                'n_min': self.n_min,\n                'language': self.language,\n                'total_tokens': len(self.prototype_stores),\n                'timestamp': time.time(),\n            }\n        }\n        \n        total_protos = 0\n        multi_sense = 0\n        \n        for token, store in self.prototype_stores.items():\n            try:\n                # Convert tensors to lists for JSON serialization\n                centroids_list = []\n                for c in store.centroids:\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            centroids_list.append(c.cpu().numpy().tolist())\n                        else:\n                            centroids_list.append(np.asarray(c, dtype=np.float32).tolist())\n                    except Exception:\n                        continue\n                \n                if not centroids_list:\n                    continue\n                \n                store_data = {\n                    'centroids': centroids_list,\n                    'counts': [int(c) for c in store.counts] if store.counts else [],\n                    'creation_time': [float(t) for t in store.creation_time] if store.creation_time else [],\n                    'mu': float(store.mu),\n                    'tau': float(store.tau),\n                    'num_prototypes': len(centroids_list),\n                }\n                \n                state['prototype_stores'][str(token)] = store_data\n                total_protos += len(centroids_list)\n                if len(centroids_list) >= 2:\n                    multi_sense += 1\n                    \n            except Exception as e:\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD] Warning: Failed to serialize store for token '{token}': {e}\")\n                continue\n        \n        state['metadata']['total_prototypes'] = total_protos\n        state['metadata']['multi_sense_tokens'] = multi_sense\n        \n        if VERBOSE_LOGGING:\n            print(f\"[DSCD] ‚úì state_dict created:\")\n            print(f\"       - Tokens: {len(state['prototype_stores'])}\")\n            print(f\"       - Total prototypes: {total_protos}\")\n            print(f\"       - Multi-sense tokens: {multi_sense}\")\n        \n        return state\n\n    # ========================================================================\n    # ‚úÖ FIX #2: ADD load_state_dict() METHOD FOR PROTOTYPE RESTORATION\n    # ========================================================================\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Load DSCD prototypes from a saved state dictionary.\n        Restores all prototype stores from checkpoint.\n        \n        Args:\n            state_dict (dict): State dictionary from checkpoint\n        \"\"\"\n        if not isinstance(state_dict, dict) or 'prototype_stores' not in state_dict:\n            print(\"[DSCD] ‚ö†Ô∏è WARNING: Invalid state_dict format - no prototype_stores found\")\n            return\n        \n        num_stores = len(state_dict['prototype_stores'])\n        print(f\"[DSCD] Loading {num_stores} prototype stores from checkpoint...\")\n        \n        self.prototype_stores = {}\n        total_protos = 0\n        multi_sense = 0\n        failed = 0\n        \n        for token, store_data in state_dict['prototype_stores'].items():\n            try:\n                # Create new store\n                store = MemoryEfficientPrototypeStore(self.embed_dim, self.max_protos)\n                \n                # Restore centroids\n                centroids_data = store_data.get('centroids', [])\n                if not centroids_data:\n                    failed += 1\n                    continue\n                \n                store.centroids = []\n                for c_list in centroids_data:\n                    try:\n                        c_tensor = torch.tensor(c_list, dtype=torch.float32).cpu()\n                        store.centroids.append(c_tensor)\n                    except Exception:\n                        continue\n                \n                if not store.centroids:\n                    failed += 1\n                    continue\n                \n                # Restore metadata\n                store.counts = [int(c) for c in store_data.get('counts', [])]\n                if len(store.counts) != len(store.centroids):\n                    store.counts = [1] * len(store.centroids)\n                \n                store.creation_time = [float(t) for t in store_data.get('creation_time', [])]\n                if len(store.creation_time) != len(store.centroids):\n                    store.creation_time = [time.time()] * len(store.centroids)\n                \n                store.mu = float(store_data.get('mu', 0.0))\n                store.tau = float(store_data.get('tau', 1e-6))\n                \n                # Store it\n                self.prototype_stores[token] = store\n                \n                num_protos = len(store.centroids)\n                total_protos += num_protos\n                if num_protos >= 2:\n                    multi_sense += 1\n                \n            except Exception as e:\n                failed += 1\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD] Warning: Failed to load store for token '{token}': {e}\")\n                continue\n        \n        # Restore discovery log if present\n        if 'discovery_log' in state_dict:\n            try:\n                self.discovery_log = list(state_dict['discovery_log'])\n            except Exception:\n                pass\n        \n        print(f\"[DSCD] ‚úì Prototypes restored:\")\n        print(f\"       - Tokens: {len(self.prototype_stores)} (failed: {failed})\")\n        print(f\"       - Total prototypes: {total_protos}\")\n        print(f\"       - Multi-sense tokens: {multi_sense}\")\n        \n        # Verify metadata matches\n        if 'metadata' in state_dict:\n            meta = state_dict['metadata']\n            print(f\"[DSCD] Checkpoint metadata:\")\n            print(f\"       - Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(meta.get('timestamp', 0)))}\")\n            print(f\"       - Language: {meta.get('language', 'unknown')}\")\n\n    # ========================================================================\n    # ‚úÖ FIX #3: ADD validate_prototypes() METHOD FOR QUALITY CHECKING\n    # ========================================================================\n    def validate_prototypes(self, homograph_list: Optional[List[str]] = None) -> Dict[str, Any]:\n        \"\"\"\n        Validate that prototypes were created correctly and check quality.\n        \n        Args:\n            homograph_list: List of known homographs to verify (defaults to HOMOGRAPH_WATCHLIST_BN)\n            \n        Returns:\n            dict: Validation metrics including quality score\n        \"\"\"\n        if homograph_list is None:\n            try:\n                homograph_list = list(HOMOGRAPH_WATCHLIST_BN)\n            except Exception:\n                homograph_list = [\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"]\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"[DSCD-VALIDATION] Prototype Quality Check\")\n        print(\"=\"*80)\n        \n        validation_results = {\n            'total_tokens': len(self.prototype_stores),\n            'total_prototypes': 0,\n            'multi_sense_tokens': 0,\n            'homographs_found': 0,\n            'homographs_missing': [],\n            'avg_prototypes_per_token': 0.0,\n            'avg_samples_per_prototype': 0.0,\n            'quality_score': 0.0,\n        }\n        \n        # Count prototypes and samples\n        total_samples = 0\n        for token, store in self.prototype_stores.items():\n            num_protos = len(store.centroids)\n            validation_results['total_prototypes'] += num_protos\n            if num_protos >= 2:\n                validation_results['multi_sense_tokens'] += 1\n            \n            # Count samples\n            try:\n                total_samples += sum(store.counts)\n            except Exception:\n                pass\n        \n        if validation_results['total_tokens'] > 0:\n            validation_results['avg_prototypes_per_token'] = (\n                validation_results['total_prototypes'] / validation_results['total_tokens']\n            )\n        \n        if validation_results['total_prototypes'] > 0:\n            validation_results['avg_samples_per_prototype'] = (\n                total_samples / validation_results['total_prototypes']\n            )\n        \n        # Check homographs\n        print(\"\\n[VALIDATION] Homograph Coverage:\")\n        print(\"-\" * 80)\n        \n        for homograph in homograph_list:\n            clean_h = homograph.replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n            \n            found = False\n            found_key = None\n            found_protos = 0\n            \n            # Check exact match\n            if homograph in self.prototype_stores:\n                found = True\n                found_key = homograph\n                found_protos = len(self.prototype_stores[homograph].centroids)\n            elif clean_h in self.prototype_stores:\n                found = True\n                found_key = clean_h\n                found_protos = len(self.prototype_stores[clean_h].centroids)\n            else:\n                # Check fuzzy match\n                for key in self.prototype_stores.keys():\n                    clean_key = str(key).replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n                    if clean_key == clean_h or clean_h in clean_key or clean_key in clean_h:\n                        found = True\n                        found_key = key\n                        found_protos = len(self.prototype_stores[key].centroids)\n                        break\n            \n            if found and found_protos >= 2:\n                validation_results['homographs_found'] += 1\n                try:\n                    counts = self.prototype_stores[found_key].counts\n                    print(f\"  ‚úì '{homograph}' ‚Üí {found_protos} prototypes (key='{found_key}', counts={counts})\")\n                except Exception:\n                    print(f\"  ‚úì '{homograph}' ‚Üí {found_protos} prototypes (key='{found_key}')\")\n            elif found and found_protos == 1:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  ‚ö†Ô∏è '{homograph}' ‚Üí Only 1 prototype (needs more clustering!)\")\n            else:\n                validation_results['homographs_missing'].append(homograph)\n                print(f\"  ‚úó '{homograph}' ‚Üí NOT FOUND (needs more training data)\")\n        \n        # Calculate quality score\n        homograph_coverage = validation_results['homographs_found'] / len(homograph_list) if homograph_list else 0.0\n        multi_sense_ratio = (\n            validation_results['multi_sense_tokens'] / validation_results['total_tokens']\n            if validation_results['total_tokens'] > 0 else 0.0\n        )\n        validation_results['quality_score'] = (homograph_coverage * 0.6 + multi_sense_ratio * 0.4)\n        \n        print(\"-\" * 80)\n        print(f\"\\n[VALIDATION] Summary:\")\n        print(f\"  - Total token types tracked: {validation_results['total_tokens']}\")\n        print(f\"  - Total prototypes: {validation_results['total_prototypes']}\")\n        print(f\"  - Multi-sense tokens (‚â•2 protos): {validation_results['multi_sense_tokens']}\")\n        print(f\"  - Avg prototypes/token: {validation_results['avg_prototypes_per_token']:.2f}\")\n        print(f\"  - Avg samples/prototype: {validation_results['avg_samples_per_prototype']:.1f}\")\n        print(f\"  - Homographs found: {validation_results['homographs_found']}/{len(homograph_list)}\")\n        print(f\"  - Quality Score: {validation_results['quality_score']:.2%}\")\n        \n        # Quality assessment\n        if validation_results['quality_score'] >= 0.7:\n            print(f\"\\n  ‚úÖ EXCELLENT: High-quality prototype clustering!\")\n        elif validation_results['quality_score'] >= 0.4:\n            print(f\"\\n  ‚úì GOOD: Acceptable prototype quality\")\n        else:\n            print(f\"\\n  ‚ö†Ô∏è WARNING: Low prototype quality - needs more training!\")\n        \n        if validation_results['homographs_missing']:\n            print(f\"\\n  ‚ö†Ô∏è Missing homographs: {', '.join(validation_results['homographs_missing'])}\")\n            print(f\"     ‚Üí These words will NOT be disambiguated during inference!\")\n        \n        print(\"=\"*80 + \"\\n\")\n        \n        return validation_results\n\n    # ========================================================================\n    # ‚úÖ FIX #4: ENHANCED should_track_token() FOR INFERENCE\n    # ========================================================================\n    def should_track_token(self, token_text: str) -> bool:\n        \"\"\"\n        Decide whether a token (canonicalized string) should be tracked and clustered.\n        Caches positive/negative results for speed.\n        \n        ‚úÖ FIX: During inference (self.training=False), ALWAYS check existing prototype_stores\n        to ensure tokens that were clustered during training are processed during inference.\n        \n        PRIORITY: Always tracks tokens in HOMOGRAPH_WATCHLIST_BN.\n        \"\"\"\n        if not token_text or not isinstance(token_text, str):\n            return False\n\n        # cache fast path\n        if token_text in self._dscd_allowed_tokens:\n            return True\n        if token_text in self._dscd_ignored_tokens:\n            return False\n\n        # ‚úÖ FIX #4: During inference, check if token already has prototypes\n        if not self.training:\n            # Direct check\n            if token_text in self.prototype_stores:\n                self._dscd_allowed_tokens.add(token_text)\n                return True\n            \n            # Check cleaned version\n            clean = token_text.replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n            if clean and clean in self.prototype_stores:\n                self._dscd_allowed_tokens.add(token_text)\n                return True\n\n        # PRIORITY: Always track homograph watchlist tokens\n        try:\n            clean = token_text.replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n            if clean in HOMOGRAPH_WATCHLIST_BN:\n                self._dscd_allowed_tokens.add(token_text)\n                if VERBOSE_LOGGING and len(self._dscd_allowed_tokens) <= 20:\n                    print(f\"[DSCD] ‚úÖ Homograph watchlist token tracked: '{clean}'\")\n                return True\n        except Exception:\n            pass\n\n        # skip special tokens quickly\n        if token_text in self.special_tokens:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        # remove markers and clean\n        clean = token_text.replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n        if clean == \"\":\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        # short tokens (common noise)\n        if len(clean) < 2:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        # must have alphabetic char somewhere\n        if not any(c.isalpha() for c in clean):\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        # skip pure numbers/punctuation\n        if clean.isdigit():\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n        if all(c in '.,!?;:()[]{}\"\\'-‚Äî‚Äì/\\\\' for c in clean):\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        # check bengali block presence to avoid over-filtering bengali words\n        try:\n            bengali_block = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            if bengali_block:\n                if len(clean) >= 2:\n                    self._dscd_allowed_tokens.add(token_text)\n                    return True\n        except Exception:\n            pass\n\n        # final Unicode-aware heuristic: ensure reasonable letter content\n        if is_word_token(clean, min_letters=self.dscd_min_letters, min_letter_fraction=self.dscd_min_letter_fraction):\n            self._dscd_allowed_tokens.add(token_text)\n            return True\n\n        # otherwise ignore\n        self._dscd_ignored_tokens.add(token_text)\n        return False\n\n    def _canonical_token_key(self, raw_token: str, token_word_map: Optional[dict], idx: int) -> str:\n        \"\"\"Prefer reconstructed whole-word (token_word_map) then cleaned token as key.\"\"\"\n        canonical = None\n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                canonical = str(token_word_map[idx]).strip()\n        except Exception:\n            canonical = None\n        if not canonical:\n            canonical = raw_token.replace('‚ñÅ', '').replace('ƒ†', '').replace('##', '').replace('@@', '').strip()\n        if not canonical:\n            canonical = raw_token\n        return canonical\n\n    # ------------------------\n    # forward: buffer embeddings & per-sequence processing\n    # ------------------------\n    def forward(self, token_embeddings, token_types=None, train_mode=True,\n                token_word_map=None, h_all=None, input_ids=None, attention_mask=None):\n        \"\"\"\n        Process token embeddings through DSCD.\n        Args:\n            token_embeddings: (batch, seq_len, embed_dim) tensor\n            token_types: list of lists of token strings (optional, will be generated from input_ids)\n            train_mode: bool, whether in training mode\n            token_word_map: list of dicts mapping token_idx -> word (optional)\n            h_all: alias for token_embeddings\n            input_ids: (batch, seq_len) tensor for generating token_types\n            attention_mask: (batch, seq_len) tensor (optional)\n        \"\"\"\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n\n        # generate token_types if not provided\n        if input_ids is not None and token_types is None:\n            batch_size, seq_len = input_ids.shape\n            token_types = []\n            for b in range(batch_size):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist()))\n                    except Exception:\n                        token_types.append([f'tok_{i}' for i in range(seq_len)])\n                else:\n                    token_types.append([f'tok_{i}' for i in range(seq_len)])\n\n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n\n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        seq_len = int(token_embeddings.size(1))\n\n        all_outputs = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': []\n        }\n\n        for b in range(batch_size):\n            word_map = token_word_map[b] if token_word_map and len(token_word_map) > b else None\n            batch_outputs = self.process_sequence(\n                token_embeddings[b],\n                token_types[b] if token_types and len(token_types) > b else [f'tok_{i}' for i in range(seq_len)],\n                device,\n                word_map=word_map,\n                train_mode=train_mode\n            )\n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n\n        # assemble h_augmented into tensor (batch, seq_len, embed_dim)\n        try:\n            h_aug_list = []\n            max_seq_len = seq_len\n            for b in range(batch_size):\n                h_batch_list = all_outputs['h_augmented'][b]\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        # pad rows (sequence length) at bottom\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = torch.zeros(max_seq_len, self.embed_dim, device=device)\n                h_aug_list.append(h_batch)\n            all_outputs['h_augmented'] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            # fallback to original embeddings shape (no augmentation)\n            all_outputs['h_augmented'] = token_embeddings\n\n        # coerce proto_assignments to stacked tensors when possible (left on CPU unless requested)\n        try:\n            proto_assign_tensor = []\n            for row in all_outputs['proto_assignments']:\n                # each row is a list of scalar tensors\n                try:\n                    stacked = torch.stack([x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in row], dim=0)\n                    proto_assign_tensor.append(stacked)\n                except Exception:\n                    # best-effort convert\n                    proto_assign_tensor.append(torch.tensor([int(x) if not isinstance(x, torch.Tensor) else int(x.item()) for x in row], dtype=torch.long))\n            all_outputs['proto_assignments'] = proto_assign_tensor\n        except Exception:\n            pass\n\n        return all_outputs\n\n    # ------------------------\n    # per-sequence processing: buffer, optionally assign, augment\n    # ------------------------\n    def process_sequence(self, token_embeddings, token_types, device, word_map=None, train_mode=True):\n        \"\"\"Process a single sequence through DSCD.\"\"\"\n        seq_len = int(token_embeddings.size(0))\n        outputs = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': []\n        }\n\n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f'tok_{j}'\n            token_key = self._canonical_token_key(raw_tok, word_map, j)\n            h_j = token_embeddings[j]\n\n            # filter by canonical key\n            if not self.should_track_token(token_key):\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n\n            # ensure store exists keyed by canonical word\n            if token_key not in self.buffers:\n                self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(self.embed_dim, self.max_protos)\n\n            # append embedding (cpu)\n            try:\n                self.buffers[token_key].append(h_j.detach().cpu())\n            except Exception:\n                try:\n                    self.buffers[token_key].append(h_j.cpu())\n                except Exception:\n                    pass\n\n            # -- background clustering trigger (throttled) --\n            try:\n                if self.enable_training_clustering and len(self.buffers[token_key]) >= max(self.n_min, 4):\n                    now = time.time()\n                    last_t = self.last_cluster_time.get(token_key, 0.0)\n                    if now - last_t > self.cluster_cooldown_seconds:\n                        # mark last time immediately to avoid double-spawn\n                        self.last_cluster_time[token_key] = now\n\n                        def _bg_cluster(tok=token_key):\n                            try:\n                                # Invoke clustering inside the lock to make updates atomic for readers\n                                with self.clustering_lock:\n                                    self._cluster_buffer_to_prototypes_hierarchical(tok)\n                            except Exception:\n                                if VERBOSE_LOGGING:\n                                    import traceback as _tb\n                                    print(f\"[DSCD] Background clustering error for token '{tok}': {_tb.format_exc().splitlines()[-1]}\")\n                        \n                        th = threading.Thread(target=_bg_cluster, daemon=True)\n                        th.start()\n            except Exception:\n                if VERBOSE_LOGGING:\n                    import traceback as _tb\n                    print(f\"[DSCD] Failed to trigger background clustering for token {token_key}: {_tb.format_exc().splitlines()[-1]}\")\n\n            store = self.prototype_stores[token_key]\n\n            # TAKE AN ATOMIC SNAPSHOT of centroids under the clustering_lock to avoid race\n            centroids_snapshot = None\n            with self.clustering_lock:\n                # Make a shallow copy of CPU tensors (clone to avoid mutation races)\n                try:\n                    centroids_snapshot = [c.clone() for c in getattr(store, \"centroids\", [])]\n                except Exception:\n                    centroids_snapshot = [c.clone() if isinstance(c, torch.Tensor) else torch.from_numpy(np.asarray(c)).cpu() for c in getattr(store, \"centroids\", [])]\n\n            assignment = -1\n            prob_list = []\n            uncertainty = 0.0\n            span_pred = 0.0\n            gate_val = 0.0\n            h_aug = h_j\n\n            # If we have a non-empty snapshot, compute distances safely from that snapshot\n            if centroids_snapshot and len(centroids_snapshot) >= 1:\n                try:\n                    # convert to numpy array for vectorized distance computation (all CPU)\n                    h_cpu = h_j.detach().cpu().numpy()\n                    cents_np = np.stack([c.numpy() for c in centroids_snapshot], axis=0)  # (K, H)\n                    # compute Euclidean distances\n                    dists_np = np.linalg.norm(cents_np - h_cpu[None, :], axis=1)\n                    if dists_np.size > 0:\n                        assignment = int(np.argmin(dists_np))\n                        min_dist = float(dists_np[assignment])\n                        # update store rolling stats using the chosen prototype (safe)\n                        try:\n                            store.update_rolling_stats(min_dist)\n                        except Exception:\n                            pass\n\n                        # convert distances to tensor on device for softmax\n                        try:\n                            dist_tensor = torch.from_numpy(dists_np).to(device)\n                            prob_list = F.softmax(-dist_tensor, dim=0).tolist()\n                            uncertainty = 1.0 - max(prob_list) if prob_list else 0.0\n                        except Exception:\n                            # fallback to numpy softmax\n                            exps = np.exp(-dists_np - np.max(-dists_np)) if dists_np.size > 0 else np.array([])\n                            if exps.size > 0:\n                                probs = exps / (exps.sum() + 1e-12)\n                                prob_list = probs.tolist()\n                                uncertainty = 1.0 - float(np.max(probs))\n                            else:\n                                prob_list = []\n                                uncertainty = 0.0\n\n                        try:\n                            span_pred = float(torch.sigmoid(self.span_head(h_j)).item())\n                        except Exception:\n                            try:\n                                span_pred = float(torch.sigmoid(self.span_head(h_j.cpu())).item())\n                            except Exception:\n                                span_pred = 0.0\n\n                        try:\n                            gate_val = float(torch.sigmoid(self.gate_w * torch.norm(h_j) + self.gate_b).item())\n                        except Exception:\n                            gate_val = 0.5\n\n                        # compute augmentation using the selected centroid (move centroid to device)\n                        if gate_val > 0.3 and 0 <= assignment < len(centroids_snapshot):\n                            centroid_t = centroids_snapshot[assignment].to(device)\n                            try:\n                                h_aug = h_j + 0.1 * (centroid_t - h_j)\n                            except Exception:\n                                h_aug = h_j\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD] Assignment error for '{token_key}': {str(e)[:200]}\")\n\n            outputs['proto_assignments'].append(torch.tensor(assignment))\n            outputs['proto_probs'].append(prob_list)\n            outputs['uncertainties'].append(uncertainty)\n            outputs['span_preds'].append(span_pred)\n            outputs['gates'].append(gate_val)\n            outputs['h_augmented'].append(h_aug)\n\n        # print summary in inference only (periodically)\n        if not train_mode and len(self.prototype_stores) > 0 and VERBOSE_LOGGING:\n            if self.last_periodic_check % PRINT_INTERVAL == 0:\n                self._print_clusters_summary()\n            self.last_periodic_check += 1\n\n        return outputs\n\n    # ------------------------\n    # improved cluster summary (inference-only)\n    # ------------------------\n    def _print_clusters_summary(self):\n        \"\"\"Print summary of cluster statistics.\"\"\"\n        try:\n            items = []\n            for token, store in self.prototype_stores.items():\n                try:\n                    proto_sample_count = sum(getattr(store, 'counts', []) or [])\n                except Exception:\n                    proto_sample_count = 0\n                buffer_len = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n            items.sort(key=lambda x: x[1], reverse=True)\n            top_5 = items[:5]\n\n            if VERBOSE_LOGGING:\n                print(\"\\n[CLUSTER] Top 5 clusters (by sample count or buffer size):\")\n                print(\"-\" * 100)\n                print(f\"{'Rank':<6} {'Token':<18} {'Count':<12} {'Protos':<8} {'BufLen':<8} {'Œº (mean)':<15} {'œÑ (dev)':<15}\")\n                print(\"-\" * 100)\n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(top_5, 1):\n                    tok_str = str(tok)[:18]\n                    print(f\"{rank:<6} {tok_str:<18} {cnt:<12} {prot:<8} {buflen:<8} {mu:<15.6f} {tau:<15.6f}\")\n                print(\"-\" * 100)\n                total_samples = sum(item[1] for item in items)\n                total_protos = sum(item[2] for item in items)\n                total_buffers = sum(item[5] for item in items)\n                print(f\"Total clusters: {len(items)} | Total samples: {total_samples} | Total protos: {total_protos} | Sum buffers: {total_buffers}\\n\")\n        except Exception as e:\n            if VERBOSE_LOGGING:\n                print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n\n    # ------------------------\n    # cleanup\n    # ------------------------\n    def cleanup_memory(self):\n        \"\"\"Periodic memory cleanup.\"\"\"\n        try:\n            for token_type, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            # encourage GC occasionally\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    # ------------------------\n    # robust clustering: hierarchical -> kmeans fallback\n    # ------------------------\n    def _cluster_buffer_to_prototypes_hierarchical(self, token_type):\n        \"\"\"\n        Robust clustering for a token_type buffer.\n        Returns True if any prototypes were created.\n        NOTE: This function expects the caller to hold self.clustering_lock when atomicity is required.\n        \"\"\"\n        try:\n            # skip non-word tokens (defensive)\n            if not self.should_track_token(token_type):\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD-CLUSTER] Skipping clustering for non-word token '{token_type}'\")\n                return False\n\n            if token_type not in self.buffers:\n                return False\n            buf = self.buffers[token_type]\n            if len(buf) < self.n_min:\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD-CLUSTER] '{token_type}' buffer size {len(buf)} < n_min {self.n_min}\")\n                return False\n\n            # assemble embeddings numpy (N, H) and sample if too large\n            emb_list = []\n            for e in buf:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        emb_list.append(e.numpy())\n                    else:\n                        emb_list.append(np.asarray(e))\n                except Exception:\n                    continue\n            if len(emb_list) == 0:\n                return False\n\n            # sample if buffer huge\n            if len(emb_list) > self.max_clustering_points:\n                # uniform random sample for clustering\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                embeddings = np.stack(emb_list, axis=0)\n\n            if embeddings.shape[0] < 2:\n                return False\n\n            if VERBOSE_LOGGING:\n                norms = np.linalg.norm(embeddings, axis=1)\n                print(f\"[DSCD-CLUSTER] Token '{token_type}' buffer={len(buf)} sampled={embeddings.shape[0]} mean_norm={norms.mean():.4f} std_norm={norms.std():.4f}\")\n\n            store = self.prototype_stores[token_type]\n\n            # Reset centroids & counts (we assume caller holds clustering_lock)\n            store.centroids = []\n            store.counts = []\n            store.creation_time = []\n\n            protos_added = 0\n\n            # hierarchical clustering (if available)\n            if HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric='euclidean')\n                    if condensed.size > 0:\n                        k_guess = min(self.max_protos, max(2, len(embeddings) // max(1, self.n_min)))\n                        if k_guess < 1:\n                            k_guess = 1\n                        Z = linkage(condensed, method='ward')\n                        clusters = fcluster(Z, t=k_guess, criterion='maxclust') - 1\n                        if clusters.size > 0:\n                            maxc = int(clusters.max())\n                            for cid in range(maxc + 1):\n                                mask = (clusters == cid)\n                                if mask.sum() >= self.n_min:\n                                    centroid = torch.from_numpy(embeddings[mask].mean(axis=0).astype(np.float32))\n                                    store.add_prototype(centroid, time.time(), count=int(mask.sum()))\n                                    protos_added += 1\n                    if VERBOSE_LOGGING and protos_added > 0:\n                        print(f\"[DSCD-CLUSTER] Hierarchical clustering created {protos_added} prototypes for '{token_type}'\")\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD-CLUSTER] Hierarchical clustering failed for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n\n            # fallback KMeans if hierarchical produced nothing\n            if protos_added == 0 and HAS_KMEANS:\n                try:\n                    k_guess = min(self.max_protos, max(1, len(embeddings) // max(1, self.n_min)))\n                    k_guess = min(k_guess, len(embeddings))\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n                        for c in range(k_guess):\n                            mask = (labels == c)\n                            if mask.sum() >= self.n_min:\n                                centroid = torch.from_numpy(embeddings[mask].mean(axis=0).astype(np.float32))\n                                store.add_prototype(centroid, time.time(), count=int(mask.sum()))\n                                protos_added += 1\n                        if VERBOSE_LOGGING and protos_added > 0:\n                            print(f\"[DSCD-CLUSTER] KMeans fallback created {protos_added} prototypes for '{token_type}'\")\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD-CLUSTER] KMeans fallback failed for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n\n            if VERBOSE_LOGGING:\n                print(f\"[DSCD-CLUSTER] Token '{token_type}': final_protos={store.size()} counts={store.counts}\")\n\n            return store.size() > 0\n\n        except Exception as e:\n            if VERBOSE_LOGGING:\n                print(f\"[DSCD-ERROR] Clustering error for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n            return False\n\n    def get_explanations(self, threshold_span=0.3):\n        \"\"\"Get disambiguation explanations for tokens with multiple senses.\"\"\"\n        expl = []\n        for token_type, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({'token': str(token_type), 'protos': store.size()})\n        return expl\n\n\n# ==============================================================================\n# VERIFICATION MESSAGE\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ Cell 3 (COMPLETELY FIXED): DSCD Ready with Save/Load/Validation\")\nprint(\"=\"*80)\nprint(\"üîß CRITICAL FIXES APPLIED:\")\nprint(\" ‚úÖ FIX #1: Added state_dict() method for prototype serialization\")\nprint(\" ‚úÖ FIX #2: Added load_state_dict() method for prototype restoration\")\nprint(\" ‚úÖ FIX #3: Added validate_prototypes() method for quality checking\")\nprint(\" ‚úÖ FIX #4: Enhanced should_track","metadata":{"id":"L25pcKUPH4J2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 replacement: ASBN module ‚Äî functional frozen-forward + device-safety\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Local fallbacks\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = 'bn'\n\n_has_is_valid_token = 'is_valid_token' in globals()\n_has_get_tokenizer_special_tokens = 'get_tokenizer_special_tokens' in globals()\n_has_get_cached_special_tokens = 'get_cached_special_tokens' in globals()\n\n\nclass LightweightDiscriminator(nn.Module):\n    \"\"\"Simple discriminator head for token-level signals (batchable).\"\"\"\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    \"\"\"\n    ASBN module: safe encoder-GRL using detached-cloned parameter tensors and\n    functional forward to avoid mutating original parameter objects (non-leaf).\n    Also ensures discriminators live on the same device as inputs during forward.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, tokenizer=None, language: str = 'bn'):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n\n        # discriminators (small)\n        self.d_freq = LightweightDiscriminator(embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(embed_dim)\n\n        # strengths & clipping\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 0.5, \"xl\": 0.8}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = 0.1\n\n        # Cache special tokens\n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n    def critic_parameters(self):\n        return list(self.d_freq.parameters()) + list(self.d_ctx.parameters()) + list(self.d_xl.parameters())\n\n    # -----------------------\n    # helpers\n    # -----------------------\n    def _ensure_discriminators_on_device(self, device: torch.device):\n        # Safely move discriminators to 'device' if not already there.\n        # We keep this best-effort (exceptions ignored) to avoid crashing if device move is impossible.\n        try:\n            for mod in (self.d_freq, self.d_ctx, self.d_xl):\n                # Quick check: if mod has parameters check their device first\n                try:\n                    p = next(mod.parameters(), None)\n                    if p is not None and p.device != device:\n                        mod.to(device)\n                except StopIteration:\n                    try:\n                        mod.to(device)\n                    except Exception:\n                        pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] _ensure_discriminators_on_device failed:\", traceback.format_exc().splitlines()[-1])\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n            if isinstance(proto_probs, torch.Tensor):\n                if proto_probs.dim() == 3:\n                    B, T, K = proto_probs.shape\n                    p = proto_probs.detach().to(device)\n                    pmax[:min(batch_size, B), :min(seq_len, T)] = p.max(dim=2)[0][:batch_size, :seq_len]\n                    return pmax\n                if proto_probs.dim() == 2:\n                    if batch_size >= 1:\n                        p = proto_probs.detach().to(device)\n                        pmax[0, :min(seq_len, p.size(0))] = p.max(dim=1)[0][:seq_len]\n                        return pmax\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            pmax[b, :min(seq_len, row.size(0))] = row.max(dim=1)[0][:seq_len].to(device)\n                        elif isinstance(row, (list, tuple)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        pmax[b, t] = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    pmax[0, t] = float(val.max().item())\n                                else:\n                                    pmax[0, t] = float(np.max(np.asarray(val, dtype=np.float32)))\n                            except Exception:\n                                pmax[0, t] = 0.5\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] parse_proto_probs exception:\", traceback.format_exc().splitlines()[-1])\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device, default: float = 0.0) -> torch.Tensor:\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n            if isinstance(mat, torch.Tensor):\n                if mat.dim() == 3:\n                    out[:min(batch_size, mat.size(0)), :min(seq_len, mat.size(1))] = mat[:, :seq_len, 0].to(device)\n                elif mat.dim() == 2:\n                    if mat.size(0) == batch_size:\n                        out[:, :min(seq_len, mat.size(1))] = mat[:, :seq_len].to(device)\n                    elif batch_size == 1:\n                        out[0, :min(seq_len, mat.size(0))] = mat[:seq_len].to(device)\n                elif mat.dim() == 1:\n                    if batch_size == 1:\n                        out[0, :min(seq_len, mat.size(0))] = mat[:seq_len].to(device)\n            elif isinstance(mat, (list, tuple)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor):\n                            if row.dim() >= 1:\n                                for t in range(min(seq_len, row.size(0))):\n                                    out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n                                except Exception:\n                                    out[b, t] = float(default)\n                else:\n                    if batch_size == 1:\n                        row = mat\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                v = row[t]\n                                out[0, t] = float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n                            except Exception:\n                                out[0, t] = float(default)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor, gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        base = float(self.lambda_base.get(lambda_type, 0.2))\n        lam = base * pmax * (1.0 - uncertainty) * gate\n        lam = torch.clamp(lam, 0.0, float(self.lambda_max))\n        lam = torch.where(torch.isfinite(lam), lam, torch.zeros_like(lam))\n        return lam\n\n    # -----------------------\n    # Monitor: run original discriminators under no_grad (device-safe)\n    # -----------------------\n    def forward_discriminators_simplified(\n        self,\n        h: torch.Tensor,\n        proto_probs: Any,\n        uncertainties: Any,\n        gates: Any,\n        token_word_map: Optional[List[Dict[int, str]]] = None\n    ) -> torch.Tensor:\n        if not self.training:\n            return torch.tensor(0.0, device=h.device)\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            return torch.tensor(0.0, device=h.device)\n\n        B, T, H = h.size()\n        device = h.device\n\n        # Ensure discriminators are on the same device as inputs (best-effort)\n        try:\n            self._ensure_discriminators_on_device(device)\n        except Exception:\n            pass\n\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)        # [B,T]\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)  # [B,T]\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)         # [B,T]\n\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            if _has_is_valid_token:\n                                try:\n                                    if not is_valid_token(wm[t], self.special_tokens, self.tokenizer, language=self.language):\n                                        sel_mask[b, t] = False\n                                except Exception:\n                                    sel_mask[b, t] = False\n                            else:\n                                w = str(wm[t])\n                                if len(w.strip()) < 2:\n                                    sel_mask[b, t] = False\n                        else:\n                            sel_mask[b, t] = sel_mask[b, t] & True\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[ASBN] token_word_map filter failed:\", traceback.format_exc().splitlines()[-1])\n\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        if sel_idx.numel() == 0:\n            return torch.tensor(0.0, device=device)\n\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n\n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1)\n\n        freq_input = torch.cat([sel_emb, freq_feature.to(device)], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature.to(device)], dim=1)\n        xl_input = sel_emb\n\n        # Use original discriminator modules for monitoring under no_grad\n        try:\n            with torch.no_grad():\n                freq_logits = self.d_freq(freq_input)\n                ctx_logits = self.d_ctx(ctx_input)\n                xl_logits = self.d_xl(xl_input)\n\n                freq_label = (pmax_flat > 0.7).long().to(device)\n                ctx_label = (U_flat < 0.3).long().to(device)\n                xl_label = (G_flat > 0.5).long().to(device)\n\n                loss_freq = F.cross_entropy(freq_logits, freq_label, reduction='none')\n                loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction='none')\n                loss_xl = F.cross_entropy(xl_logits, xl_label, reduction='none')\n\n                lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n                lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n                lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n                weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n                avg_loss = torch.mean(weighted)\n            return avg_loss\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] Monitor forward failed (device/param issue):\", traceback.format_exc().splitlines()[-1])\n            return torch.tensor(0.0, device=device)\n\n    # -----------------------\n    # Encoder GRL using detached-cloned param tensors and functional forward\n    # -----------------------\n    def forward_with_grl_simplified(\n        self,\n        h: torch.Tensor,\n        proto_probs: Any,\n        uncertainties: Any,\n        gates: Any,\n        token_word_map: Optional[List[Dict[int, str]]] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device('cpu')\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device('cpu')\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n\n        device = h.device\n\n        # Ensure discriminators are on same device for monitor stage\n        try:\n            self._ensure_discriminators_on_device(device)\n        except Exception:\n            pass\n\n        # Monitor loss computed with no_grad using discriminator modules directly\n        with torch.no_grad():\n            try:\n                disc_monitor_loss = self.forward_discriminators_simplified(h, proto_probs, uncertainties, gates, token_word_map)\n                if not isinstance(disc_monitor_loss, torch.Tensor):\n                    disc_monitor_loss = torch.tensor(float(disc_monitor_loss), device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[ASBN] forward_discriminators_simplified (monitor) failed:\", traceback.format_exc().splitlines()[-1])\n                disc_monitor_loss = torch.tensor(0.0, device=device)\n\n        # Now compute encoder loss using *detached cloned* weights (leaf Tensors) and functional forward.\n        try:\n            B, T, H = h.size()\n            pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n            U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n            G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n\n            sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n            if token_word_map:\n                try:\n                    for b in range(min(B, len(token_word_map))):\n                        wm = token_word_map[b] or {}\n                        for t in range(T):\n                            if t in wm:\n                                if _has_is_valid_token:\n                                    try:\n                                        if not is_valid_token(wm[t], self.special_tokens, self.tokenizer, language=self.language):\n                                            sel_mask[b, t] = False\n                                    except Exception:\n                                        sel_mask[b, t] = False\n                                else:\n                                    w = str(wm[t])\n                                    if len(w.strip()) < 2:\n                                        sel_mask[b, t] = False\n                            else:\n                                sel_mask[b, t] = sel_mask[b, t] & True\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(\"[ASBN] token_word_map filter (GRL) failed:\", traceback.format_exc().splitlines()[-1])\n\n            sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n            if sel_idx.numel() == 0:\n                encoder_loss = torch.tensor(0.0, device=device, requires_grad=True)\n            else:\n                h_flat = h.view(B * T, H)\n                sel_emb = h_flat[sel_idx]           # [N, H]\n                pmax_flat = pmax_mat.view(-1)[sel_idx]\n                U_flat = U_mat.view(-1)[sel_idx]\n                G_flat = G_mat.view(-1)[sel_idx]\n\n                max_len = max(int(_MAX_LENGTH), 1)\n                seq_len_feature = float(T) / float(max_len)\n                freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n                ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n\n                freq_input = torch.cat([sel_emb, freq_feature], dim=1)     # [N, Df]\n                ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)       # [N, Dc]\n                xl_input = sel_emb                                         # [N, H]\n\n                # Build frozen (detached.clone) param tensors for each discriminator (leaf tensors)\n                def get_frozen_params(module: nn.Module, device: torch.device):\n                    try:\n                        l0 = module.classifier[0]   # Linear in -> 64\n                        l1 = module.classifier[3]   # Linear 64 -> 2\n                        w0 = l0.weight.detach().clone().to(device)\n                        b0 = l0.bias.detach().clone().to(device) if l0.bias is not None else None\n                        w1 = l1.weight.detach().clone().to(device)\n                        b1 = l1.bias.detach().clone().to(device) if l1.bias is not None else None\n                        # ensure leaf and not requires grad\n                        w0.requires_grad = False\n                        if b0 is not None: b0.requires_grad = False\n                        w1.requires_grad = False\n                        if b1 is not None: b1.requires_grad = False\n                        return (w0, b0, w1, b1)\n                    except Exception:\n                        params = list(module.parameters())\n                        if len(params) >= 4:\n                            w0 = params[0].detach().clone().to(device)\n                            b0 = params[1].detach().clone().to(device) if params[1] is not None else None\n                            w1 = params[2].detach().clone().to(device)\n                            b1 = params[3].detach().clone().to(device) if params[3] is not None else None\n                            for t in (w0, b0, w1, b1):\n                                if t is not None:\n                                    try: t.requires_grad = False\n                                    except Exception: pass\n                            return (w0, b0, w1, b1)\n                        raise RuntimeError(\"Failed to extract frozen params from discriminator module\")\n\n                # get frozen params for freq/ctx/xl discriminators\n                frozen_freq = get_frozen_params(self.d_freq, device)\n                frozen_ctx = get_frozen_params(self.d_ctx, device)\n                frozen_xl = get_frozen_params(self.d_xl, device)\n\n                def functional_classifier_forward(x, frozen_params, dropout_p=0.1, training=False):\n                    w0, b0, w1, b1 = frozen_params\n                    y = F.linear(x, w0, b0)\n                    y = F.relu(y)\n                    y = F.dropout(y, p=dropout_p, training=training)\n                    y = F.linear(y, w1, b1)\n                    return y\n\n                freq_logits = functional_classifier_forward(freq_input, frozen_freq, dropout_p=0.1, training=False)\n                ctx_logits = functional_classifier_forward(ctx_input, frozen_ctx, dropout_p=0.1, training=False)\n                xl_logits = functional_classifier_forward(xl_input, frozen_xl, dropout_p=0.1, training=False)\n\n                freq_label = (pmax_flat > 0.7).long().to(device)\n                ctx_label = (U_flat < 0.3).long().to(device)\n                xl_label = (G_flat > 0.5).long().to(device)\n\n                loss_freq = F.cross_entropy(freq_logits, freq_label, reduction='none')\n                loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction='none')\n                loss_xl = F.cross_entropy(xl_logits, xl_label, reduction='none')\n\n                lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n                lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n                lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n                weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n                mean_weighted = torch.mean(weighted)\n                encoder_loss = -self.encoder_grl_scale * mean_weighted\n                encoder_loss = encoder_loss.to(device)\n                #encoder_loss.requires_grad = True\n\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] GRL computation failed:\", traceback.format_exc().splitlines()[-1])\n            encoder_loss = torch.tensor(0.0, device=device, requires_grad=True)\n\n        return encoder_loss, disc_monitor_loss, torch.tensor(0.0, device=device), torch.tensor(0.0, device=device)\n\nprint(\"‚úÖ Cell 4 (patched final, device-safe): ASBN module ready (functional frozen-forward + discriminator device safety)\")","metadata":{"id":"XrNq18UsH4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG EXPLANATION SYSTEM - COMPLETELY FIXED WITH DEBUG LOGGING\n# ==============================================================================\n# ‚úÖ FIXED: Lowered thresholds from 0.40/0.30 ‚Üí 0.20/0.20 for testing (ERROR #1 FIX)\n# ‚úÖ FIXED: Added debug logging for token filtering decisions (ERROR #2 FIX)\n# ‚úÖ FIXED: Enhanced statistics with evidence quality metrics (ERROR #3 FIX)\n# ‚úÖ FIXED: Added span value validation and logging (ERROR #4 FIX)\n# ‚úÖ ADDED: Homograph priority boost from HOMOGRAPH_WATCHLIST_BN (ERROR #5 FIX)\n# ‚úÖ ADDED: Comprehensive filtering report showing skip reasons\n# \n# Original fixes preserved:\n# ‚úÖ FIX #3: extract_evidence_from_target() bounds checking\n# ‚úÖ FIX #4: Verify homograph words detected\n# ‚úÖ FIX #6: compute_span() handles dict input correctly\n# \n# Features:\n# - Inference-only: returns no explanations during training\n# - Cached special tokens for performance\n# - Robust evidence extraction from DSCD outputs\n# - Minimal, dependency-safe fallbacks\n# ==============================================================================\n\nfrom typing import List, Dict, Tuple, Optional\nfrom collections import deque\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# Fallback defaults (do not hard-depend on other cells)\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\nexcept NameError:\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\nexcept NameError:\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\nexcept NameError:\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANGUAGE = 'bn'\n\n# ‚úÖ FIX #1: Lowered threshold from 0.40 ‚Üí 0.20 for testing\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept NameError:\n    _TRG_UNCERTAINTY_THRESHOLD = 0.20  # ‚Üê Changed from 0.40 for testing phase\n\n# ‚úÖ FIX #5: Import homograph watchlist for priority boosting\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\n    if _VERBOSE_LOGGING:\n        print(f\"[CELL5] ‚úÖ Loaded homograph watchlist: {_HOMOGRAPH_WATCHLIST}\")\nexcept NameError:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n    if _VERBOSE_LOGGING:\n        print(f\"[CELL5] ‚ö†Ô∏è Using default homograph watchlist: {_HOMOGRAPH_WATCHLIST}\")\n\n# Optional helper from other cells\n_has_is_valid_token = 'is_valid_token' in globals()\n_has_get_tokenizer_special_tokens = 'get_tokenizer_special_tokens' in globals()\n_has_get_cached_special_tokens = 'get_cached_special_tokens' in globals()\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    \"\"\"\n    Robust word-start detection:\n      - If token_word_map exists and contains this index -> treat as word\n      - If token starts with '‚ñÅ' or 'ƒ†' (SPM/BPE markers), treat as word-start\n      - If no offsets and no markers, allow tokens with clean length >= 2 as candidates\n    \"\"\"\n    try:\n        if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map:\n            w = token_word_map[idx]\n            return isinstance(w, str) and len(w.strip()) > 0\n\n        if isinstance(raw_token, str):\n            if raw_token.startswith('‚ñÅ') or raw_token.startswith('ƒ†'):\n                return True\n            clean = raw_token.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n            if len(clean) >= 2 and not all(ch in \".,;:!?\\\"'()[]{}-/\" for ch in clean):\n                return True\n    except Exception:\n        pass\n    return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    \"\"\"Templates to render explanation strings.\"\"\"\n\n    def __init__(self):\n        self.explanation_templates = {\n            'high_confidence': (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on contextual evidence: '{evidence}'. \"\n                \"This matches the learned pattern. {alternatives_text}\"\n            ),\n            'medium_confidence': (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty remains. {alternatives_text}\"\n            ),\n            'low_confidence': (\n                \"Uncertain between senses; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. {alternatives_text} Review recommended.\"\n            ),\n            'fallback': (\n                \"Token '{token}' processed with standard analysis. Context: '{evidence}'.\"\n            )\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        token = str(evidence.get('token', 'unknown')).replace('‚ñÅ', '')\n        sense_info = evidence.get('chosen_sense', ('unknown', 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = 'unknown', 0.5\n\n        evidence_tokens = evidence.get('evidence_tokens', [])\n        evidence_str = ', '.join([str(tok).replace('‚ñÅ', '') for tok in evidence_tokens[:_TRG_EVIDENCE_K]]) or 'limited context'\n\n        alternatives = evidence.get('alternatives', [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)} considered.\"\n\n        if confidence >= 0.65:\n            template_key = 'high_confidence'\n        elif confidence >= 0.4:\n            template_key = 'medium_confidence'\n        else:\n            template_key = 'low_confidence'\n\n        template = self.explanation_templates.get(template_key, self.explanation_templates['fallback'])\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token\n            )\n        except Exception:\n            return f\"Token '{token}' disambiguated as '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    \"\"\"Extracts evidence around a token for explanation rendering.\"\"\"\n\n    def __init__(self, tokenizer=None, language='bn'):\n        self.tokenizer = tokenizer\n        self.language = language\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #3: extract_evidence_from_target() - Comprehensive bounds checking\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor\n    ) -> Optional[Dict]:\n        \"\"\"\n        Extract evidence from a target span with COMPREHENSIVE bounds checking.\n        \n        ‚úÖ FIX: Multiple validation steps BEFORE any access\n        \"\"\"\n        \n        # Step 1: Type and value validation\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n        \n        # Step 2: Span bounds validation\n        if span_end > len(tgt_preds):\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Evidence extraction error: span_end {span_end} > tgt_preds length {len(tgt_preds)}\")\n            return None\n        \n        if span_start >= span_end:\n            return None\n        \n        # Step 3: Token index within span\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n        \n        # Step 4: Double-check token_idx against array length\n        if token_idx >= len(tgt_preds):\n            return None\n        \n        # NOW safe to access\n        try:\n            evidence = tgt_preds[token_idx]\n            if evidence is None:\n                return None\n            return {'evidence': evidence, 'span': (span_start, span_end)}\n            \n        except (IndexError, TypeError, AttributeError) as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Evidence extraction error at token {token_idx}: {e}\")\n            return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None\n    ) -> Dict:\n        \"\"\"Extract evidence safely with bounds checks and fallbacks.\"\"\"\n        if not isinstance(tokens, list) or token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(token_idx, tokens or [])\n\n        raw_token = tokens[token_idx]\n\n        # Token validity (basic)\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = (raw_token not in self.special_tokens and len(str(raw_token)) >= 2)\n        else:\n            is_valid = (raw_token not in self.special_tokens and len(str(raw_token)) >= 2)\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n\n            # Context window tokens\n            context_window = 2\n            start_idx = max(0, token_idx - context_window)\n            end_idx = min(len(tokens), token_idx + context_window + 1)\n\n            evidence_tokens = []\n            for i in range(start_idx, end_idx):\n                if i == token_idx or i >= len(tokens):\n                    continue\n                rtok = tokens[i]\n                clean_token = str(rtok).replace('‚ñÅ', '').replace('</w>', '').strip()\n                if not _is_word_start(rtok, token_word_map, i):\n                    if token_word_map is None and len(clean_token) >= 2:\n                        pass\n                    else:\n                        continue\n\n                # Validity check\n                if _has_is_valid_token:\n                    try:\n                        ok = is_valid_token(rtok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        ok = (rtok not in self.special_tokens and len(clean_token) > 1)\n                else:\n                    ok = (rtok not in self.special_tokens and len(clean_token) > 1)\n\n                if ok and len(clean_token) > 0:\n                    if token_word_map and isinstance(token_word_map.get(i, \"\"), str) and token_word_map[i].strip():\n                        evidence_tokens.append(token_word_map[i].strip())\n                    else:\n                        evidence_tokens.append(clean_token)\n\n            # Deduplicate and limit\n            seen = set()\n            dedup_evidence = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen.add(t)\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:_TRG_EVIDENCE_K]\n\n            # Sense alternatives from probabilities\n            top_senses = self._compute_sense_alternatives_fast(proto_probs)\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            # Prefer reconstructed word for main token\n            token_value = token_word_map[token_idx] if (token_word_map and token_idx in token_word_map and isinstance(token_word_map[token_idx], str) and token_word_map[token_idx].strip()) else raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Evidence extraction error at token {token_idx}: {e}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _safe_extract_proto_probs(self, token_idx: int, dscd_outputs: Dict) -> torch.Tensor:\n        \"\"\"Extract per-token prototype probabilities as a 1D tensor (safe).\"\"\"\n        try:\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all and len(pp_all) > 0:\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return row[token_idx].detach().cpu().flatten()\n                    return row.detach().cpu().flatten()\n                if isinstance(row, (list, tuple)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().flatten()\n                        elif isinstance(val, (list, tuple, np.ndarray)):\n                            return torch.as_tensor(val, dtype=torch.float32).flatten()\n                        else:\n                            return torch.tensor([float(val)], dtype=torch.float32)\n                    if len(row) > 0:\n                        maybe = row[0]\n                        if isinstance(maybe, torch.Tensor):\n                            return maybe.detach().cpu().flatten()\n        except Exception:\n            pass\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n        except Exception:\n            pass\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n        except Exception:\n            pass\n        return 0.0\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #6: _safe_extract_span() - Handle dict input correctly\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        \"\"\"\n        Extract span value safely. \n        \n        ‚úÖ FIX: Handles dict input to compute_span() correctly\n        (dict values must be extracted before sorting)\n        \"\"\"\n        try:\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return float(val.item()) if isinstance(val, torch.Tensor) else float(val)\n        except Exception:\n            pass\n        return 0.0\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #4: compute_span() - Properly handles dict input + validation\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    def compute_span(self, sense_probs) -> float:\n        \"\"\"\n        Compute span (confidence spread between top senses).\n        \n        ‚úÖ FIX: Properly handles dict input\n        ‚úÖ FIX: Validates span value is reasonable\n        (sorted() on dict sorts keys; must extract values first)\n        \"\"\"\n        try:\n            # Handle dict input\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n            \n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n            \n            if len(probs) < 2:\n                return 0.0\n            \n            # Sort numerically (descending)\n            sorted_probs = sorted(probs, reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n            \n            # ‚úÖ FIX #4: Validate span value\n            if span < 0.0 or span > 1.0:\n                if _VERBOSE_LOGGING:\n                    print(f\"[TRG] ‚ö†Ô∏è Suspicious span value: {span:.3f} (should be in [0,1])\")\n                span = max(0.0, min(1.0, span))\n            \n            return span\n            \n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] compute_span error: {e}\")\n            return 0.0\n\n    def _compute_sense_alternatives_fast(self, proto_probs: torch.Tensor) -> List[Tuple[str, float]]:\n        \"\"\"Return up to top-3 (sense_id, confidence).\"\"\"\n        try:\n            probs = proto_probs.flatten()\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [(f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item())) for i in range(top_k)]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(self, token_idx: int, tokens: List[str]) -> Dict:\n        \"\"\"Fallback evidence when extraction fails or token invalid.\"\"\"\n        token = tokens[token_idx] if isinstance(tokens, list) and 0 <= token_idx < len(tokens) else \"UNK\"\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    \"\"\"\n    Inference-only disambiguation and explanation component.\n    \"\"\"\n\n    def __init__(self, embed_dim: Optional[int] = None, tokenizer=None, language: str = 'bn'):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(_TRG_GEN_EMBED)\n        self.tokenizer = tokenizer\n        self.language = language\n\n        # Cache special tokens if available\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(tokenizer, language=language)\n\n        # Silver buffer for optional logging\n        self.silver_buffer = deque(maxlen=int(_MAX_SILVER_BUFFER))\n\n        # ‚úÖ FIX #3: Enhanced statistics with evidence quality metrics\n        self.stats = {\n            'explanations_generated': 0,\n            'high_confidence_explanations': 0,\n            'low_confidence_explanations': 0,\n            'empty_evidence_count': 0,  # ‚Üê New\n            'total_evidence_tokens': 0,  # ‚Üê New\n            'tokens_filtered_word_start': 0,  # ‚Üê New\n            'tokens_filtered_validity': 0,  # ‚Üê New\n            'tokens_filtered_ambiguity': 0,  # ‚Üê New\n        }\n\n        if _VERBOSE_LOGGING:\n            print(\"[TRG] System initialized (inference-only, testing thresholds: 0.20/0.20)\")\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None\n    ) -> Tuple[str, Dict]:\n        \"\"\"Generate an explanation string and its evidence for a single token.\"\"\"\n        # In eval mode only and feature flag must be enabled\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        # Bounds first\n        if not isinstance(tokens, list) or token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        # Token validity\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = (raw_token not in self.special_tokens and len(str(raw_token)) >= 2)\n        else:\n            is_valid = (raw_token not in self.special_tokens and len(str(raw_token)) >= 2)\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx, tokens, dscd_outputs, token_word_map=token_word_map\n            )\n            \n            # ‚úÖ FIX #3: Track evidence quality\n            if not evidence.get('evidence_tokens'):\n                self.stats['empty_evidence_count'] += 1\n            else:\n                self.stats['total_evidence_tokens'] += len(evidence['evidence_tokens'])\n            \n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] generate_explanation error at token {token_idx}: {e}\")\n            return \"\", {}\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1 + #2: Lowered thresholds + Added debug logging\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        uncertainty_threshold: Optional[float] = None,\n        top_k: int = 3\n    ) -> List[Dict]:\n        \"\"\"\n        Select up to top_k tokens and generate explanations for them.\n        \n        ‚úÖ FIX #1: Lowered thresholds from 0.40/0.30 ‚Üí 0.20/0.20\n        ‚úÖ FIX #2: Added debug logging for filtering decisions\n        \"\"\"\n        if self.training or not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n\n        # ‚úÖ FIX #1: Lowered from 0.40 ‚Üí 0.20 for testing phase\n        strict_uncertainty = max(0.20, uncertainty_threshold)  # ‚Üê Changed from 0.40\n\n        if _VERBOSE_LOGGING:\n            print(f\"[TRG] Using thresholds: uncertainty={strict_uncertainty:.2f}, span=0.20\")\n\n        explanations: List[Dict] = []\n        \n        # ‚úÖ FIX #2: Track filtering decisions\n        filter_stats = {\n            'total_tokens': 0,\n            'filtered_word_start': 0,\n            'filtered_validity': 0,\n            'filtered_ambiguity': 0,\n            'candidates_found': 0,\n        }\n        \n        try:\n            if not tokens or not isinstance(dscd_outputs, dict):\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n            if not U_all or not isinstance(U_all[0], (list, tuple, torch.Tensor)):\n                return explanations\n\n            # Normalize U and S to python lists\n            def _to_list(x):\n                if isinstance(x, torch.Tensor):\n                    if x.ndim == 2:\n                        return [float(v.item()) for v in x[0]]\n                    elif x.ndim == 1:\n                        return [float(v.item()) for v in x]\n                if isinstance(x, (list, tuple)):\n                    out = []\n                    for v in x:\n                        if isinstance(v, torch.Tensor):\n                            out.append(float(v.item()))\n                        else:\n                            out.append(float(v))\n                    return out\n                return []\n\n            U = _to_list(U_all[0])\n            S = _to_list(S_all[0]) if S_all else [0.0] * len(U)\n\n            # Collect candidates (idx, uncertainty, span) with real-ambiguity gating\n            candidates: List[Tuple[int, float, float, str]] = []  # ‚Üê Added token for debug\n            \n            for idx in range(min(len(tokens), len(U))):\n                filter_stats['total_tokens'] += 1\n                tok = tokens[idx]\n                clean_tok = tok.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n\n                # ‚úÖ FIX #2: Debug logging for first 5 tokens\n                debug_this = (idx < 5 and _VERBOSE_LOGGING)\n\n                # Only consider whole words (word-start or mapped full word)\n                if not _is_word_start(tok, token_word_map, idx):\n                    if token_word_map is None:\n                        if not isinstance(tok, str) or len(tok.replace('‚ñÅ', '').replace('ƒ†', '')) < 2:\n                            filter_stats['filtered_word_start'] += 1\n                            if debug_this:\n                                print(f\"[TRG-DEBUG] Token {idx} '{clean_tok}' SKIPPED: too short\")\n                            continue\n                    else:\n                        filter_stats['filtered_word_start'] += 1\n                        if debug_this:\n                            print(f\"[TRG-DEBUG] Token {idx} '{clean_tok}' SKIPPED: not word-start\")\n                        continue\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(tok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        valid = (tok not in self.special_tokens and len(str(tok)) >= 2)\n                else:\n                    valid = (tok not in self.special_tokens and len(str(tok)) >= 2)\n                    \n                if not valid:\n                    filter_stats['filtered_validity'] += 1\n                    if debug_this:\n                        print(f\"[TRG-DEBUG] Token {idx} '{clean_tok}' SKIPPED: invalid token\")\n                    continue\n\n                # Cast values\n                u = float(U[idx]) if idx < len(U) else 0.5\n                s = float(S[idx]) if idx < len(S) else 0.0\n\n                # Check multi-sense distribution size\n                probs = self.evidence_extractor._safe_extract_proto_probs(idx, dscd_outputs)\n                has_multi_sense = isinstance(probs, torch.Tensor) and probs.numel() >= 2\n\n                # ‚úÖ FIX #1: Lowered span threshold from 0.3 ‚Üí 0.2\n                # Consider ambiguous when one of the conditions hold:\n                # multi-sense OR span > 0.2 OR uncertainty > strict_uncertainty (0.20)\n                is_ambiguous = (has_multi_sense or (s > 0.2) or (u > strict_uncertainty))  # ‚Üê Changed from 0.3\n                \n                if not is_ambiguous:\n                    filter_stats['filtered_ambiguity'] += 1\n                    if debug_this:\n                        print(f\"[TRG-DEBUG] Token {idx} '{clean_tok}' SKIPPED: not ambiguous (multi={has_multi_sense}, s={s:.3f}, u={u:.3f})\")\n                    continue\n\n                filter_stats['candidates_found'] += 1\n                if debug_this:\n                    print(f\"[TRG-DEBUG] Token {idx} '{clean_tok}' ‚úì CANDIDATE (multi={has_multi_sense}, s={s:.3f}, u={u:.3f})\")\n                \n                candidates.append((idx, u, s, clean_tok))\n\n            # ‚úÖ FIX #2: Print filtering summary\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Filtering summary:\")\n                print(f\"  - Total tokens: {filter_stats['total_tokens']}\")\n                print(f\"  - Filtered (word-start): {filter_stats['filtered_word_start']}\")\n                print(f\"  - Filtered (validity): {filter_stats['filtered_validity']}\")\n                print(f\"  - Filtered (ambiguity): {filter_stats['filtered_ambiguity']}\")\n                print(f\"  - Candidates found: {filter_stats['candidates_found']}\")\n\n            if not candidates:\n                if _VERBOSE_LOGGING:\n                    print(f\"[TRG] ‚ö†Ô∏è No candidates found! Consider lowering thresholds further.\")\n                return explanations\n\n            # ‚úÖ FIX #5: Priority boost for known homographs\n            homograph_candidates = []\n            regular_candidates = []\n            \n            for (i, u, s, tok) in candidates:\n                if tok in _HOMOGRAPH_WATCHLIST:\n                    homograph_candidates.append((i, u, s, tok))\n                    if _VERBOSE_LOGGING:\n                        print(f\"[TRG] ‚úÖ Homograph priority boost: '{tok}' (u={u:.3f}, s={s:.3f})\")\n                else:\n                    regular_candidates.append((i, u, s, tok))\n\n            # Priority 1: Known homographs first\n            span_first = [(i, u, s, tok) for (i, u, s, tok) in homograph_candidates if s > 0.2]\n            span_first.sort(key=lambda t: (t[2], t[1]), reverse=True)\n\n            # Priority 2: tokens with higher span from regular candidates\n            regular_span_first = [(i, u, s, tok) for (i, u, s, tok) in regular_candidates if s > 0.2]\n            regular_span_first.sort(key=lambda t: (t[2], t[1]), reverse=True)\n\n            # Priority 3: tokens with uncertainty above threshold (homographs first)\n            uncertain_homographs = [(i, u, s, tok) for (i, u, s, tok) in homograph_candidates if u > strict_uncertainty]\n            uncertain_homographs.sort(key=lambda t: t[1], reverse=True)\n            \n            uncertain_regular = [(i, u, s, tok) for (i, u, s, tok) in regular_candidates if u > strict_uncertainty]\n            uncertain_regular.sort(key=lambda t: t[1], reverse=True)\n\n            selected: List[Tuple[int, float, float, str]] = []\n            \n            # Add in priority order\n            selected.extend(span_first)\n            selected.extend(regular_span_first)\n            \n            for t in uncertain_homographs:\n                if t not in selected:\n                    selected.append(t)\n                if len(selected) >= top_k:\n                    break\n            \n            for t in uncertain_regular:\n                if t not in selected and len(selected) < top_k:\n                    selected.append(t)\n\n            # Fallback: ensure at least 1 candidate if nothing selected\n            if not selected and candidates:\n                all_candidates_sorted = sorted(candidates, key=lambda t: (t[2], t[1]), reverse=True)\n                selected = all_candidates_sorted[:max(1, top_k)]\n\n            # Generate explanations\n            for (token_idx, u, s, clean_tok) in selected[:top_k]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx, tokens, dscd_outputs, token_word_map=token_word_map\n                    )\n                    if explanation_text and evidence:\n                        explanations.append({\n                            \"token_idx\": token_idx,\n                            \"token\": (token_word_map[token_idx] if token_word_map and token_idx in token_word_map else tokens[token_idx].replace('‚ñÅ', '')),\n                            \"explanation\": explanation_text,\n                            \"uncertainty\": u,\n                            \"span\": s\n                        })\n                        if _VERBOSE_LOGGING:\n                            print(f\"[TRG] ‚úì Generated explanation for '{clean_tok}' (u={u:.3f}, s={s:.3f})\")\n                except Exception as e:\n                    if _VERBOSE_LOGGING:\n                        print(f\"[TRG] Explanation generation failure @ idx {token_idx} '{clean_tok}': {e}\")\n                    continue\n\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[TRG] Sentence processing error: {e}\")\n\n        if _VERBOSE_LOGGING:\n            print(f\"[TRG] Final: {len(explanations)} explanations generated\")\n\n        return explanations\n\n    def _update_stats(self, evidence: Dict):\n        \"\"\"Update internal counters for generated explanations.\"\"\"\n        self.stats['explanations_generated'] += 1\n        confidence = 0.5\n        chosen = evidence.get('chosen_sense')\n        if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n            try:\n                confidence = float(chosen[1])\n            except Exception:\n                confidence = 0.5\n\n        if confidence >= 0.65:\n            self.stats['high_confidence_explanations'] += 1\n        elif confidence < 0.4:\n            self.stats['low_confidence_explanations'] += 1\n\n    def _add_to_silver_buffer(self, evidence: Dict, explanation: str, tokens: List[str]):\n        \"\"\"Append a compact silver entry for optional postprocessing.\"\"\"\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n            self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def get_statistics(self) -> Dict:\n        \"\"\"Return a snapshot of TRG statistics with evidence quality metrics.\"\"\"\n        total = max(self.stats['explanations_generated'], 1)\n        avg_evidence_tokens = (\n            self.stats['total_evidence_tokens'] / total \n            if self.stats['explanations_generated'] > 0 else 0.0\n        )\n        \n        return {\n            **self.stats,\n            \"high_confidence_rate\": self.stats['high_confidence_explanations'] / total,\n            \"low_confidence_rate\": self.stats['low_confidence_explanations'] / total,\n            \"empty_evidence_rate\": self.stats['empty_evidence_count'] / total,\n            \"avg_evidence_tokens\": avg_evidence_tokens,\n            \"silver_buffer_size\": len(self.silver_buffer),\n        }\n\n\nprint(\"=\"*80)\nprint(\"‚úÖ Cell 5: TRG explanation system ready (COMPLETELY FIXED)\")\nprint(\"=\"*80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Lowered thresholds from 0.40/0.30 ‚Üí 0.20/0.20 for testing\")\nprint(\" ‚úÖ FIX #2: Added debug logging for token filtering decisions\")\nprint(\" ‚úÖ FIX #3: Enhanced statistics with evidence quality metrics\")\nprint(\" ‚úÖ FIX #4: Added span value validation and logging\")\nprint(\" ‚úÖ FIX #5: Added homograph priority boost from watchlist\")\nprint(\" ‚úÖ FIX #6: Fixed compute_span() dict input handling\")\nprint(\"=\"*80)","metadata":{"id":"svk-wKO7H4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TATN MODEL - COMPLETELY FIXED WITH INFERENCE VALIDATION\n# ==============================================================================\n# ‚úÖ FIXED: Force word map reconstruction BEFORE DSCD forward (ERROR #1 FIX)\n# ‚úÖ FIXED: Add DSCD prototype validation after forward (ERROR #2 FIX)\n# ‚úÖ FIXED: Add comprehensive debug logging for inference (ERROR #3 FIX)\n# ‚úÖ FIXED: Fix span fallback to only trigger when NO prototypes (ERROR #4 FIX)\n# ‚úÖ ADDED: Homograph detection reporting during inference (ERROR #5 FIX)\n# ‚úÖ ADDED: Inference statistics summary\n# \n# Original features preserved:\n# - Gradient checkpointing support\n# - Multi-GPU compatibility\n# - Robust encoder-output handling\n# - Device-consistent normalization of DSCD outputs\n# - Training/inference path separation\n# ==============================================================================\nfrom typing import List, Dict, Optional, Any\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\n\n# ------------------------------------------------------------------------------\n# Defensive global fallbacks (use Exception where appropriate)\n# ------------------------------------------------------------------------------\ntry:\n    _EN_LANG = EN_LANG\nexcept Exception:\n    _EN_LANG = \"en\"\n\ndef _get_int_global(name, default):\n    try:\n        return int(globals().get(name))\n    except Exception:\n        return default\n\ndef _get_float_global(name, default):\n    try:\n        return float(globals().get(name))\n    except Exception:\n        return default\n\ndef _get_bool_global(name, default):\n    try:\n        return bool(globals().get(name))\n    except Exception:\n        return default\n\n_DSCD_BUFFER_SIZE = _get_int_global('DSCD_BUFFER_SIZE', 300)\n_DSCD_MAX_PROTOS = _get_int_global('DSCD_MAX_PROTOS', 3)\n_DSCD_N_MIN = _get_int_global('DSCD_N_MIN', 5)\n_DSCD_DISPERSION_THRESHOLD = _get_float_global('DSCD_DISPERSION_THRESHOLD', 0.25)\n\ntry:\n    _SOURCE_LANGUAGE = SOURCE_LANGUAGE\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\n_ENABLE_ASBN_TRAINING = _get_bool_global('ENABLE_ASBN_TRAINING', True)\n_ENABLE_TRG_INFERENCE = _get_bool_global('ENABLE_TRG_INFERENCE', True)\n_MEMORY_CLEANUP_FREQUENCY = _get_int_global('MEMORY_CLEANUP_FREQUENCY', 100)\n\n_NUM_GPUS = _get_int_global('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 1)\n_USE_GC = _get_bool_global('GRADIENT_CHECKPOINTING', False)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global('DSCD_ENABLE_TRAINING_CLUSTERING', False)\n_LAMBDA_ASBN = _get_float_global('LAMBDA_ASBN', 0.10)\n_LAMBDA_DSCD = _get_float_global('LAMBDA_DSCD', 0.05)\n_VERBOSE_LOGGING = _get_bool_global('VERBOSE_LOGGING', False)\n\n_has_reconstruct_word_spans = 'reconstruct_word_spans' in globals()\n\n# ‚úÖ FIX #5: Import homograph watchlist for detection reporting\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n# ------------------------------------------------------------------------------\n# Utility: safe extraction of encoder last hidden state (handles tuple or object)\n# ------------------------------------------------------------------------------\ndef _safe_get_last_hidden_state(enc_output):\n    \"\"\"\n    Accepts HF encoder outputs which could be BaseModelOutput-like with .last_hidden_state\n    or a tuple (last_hidden_state, ...) and returns the tensor.\n    \"\"\"\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, 'last_hidden_state'):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        return enc_output[0]\n    # unknown shape - return None\n    return None\n\n# ------------------------------------------------------------------------------\n# _normalize_dscd_outputs: robust parsing & device-consistent outputs\n# ------------------------------------------------------------------------------\ndef _normalize_dscd_outputs(raw: Dict[str, Any],\n                            batch_size: int,\n                            seq_len: int,\n                            device: torch.device,\n                            embed_dim: int) -> Dict[str, Any]:\n    \"\"\"\n    Defensive normalization of DSCD raw outputs into canonical forms:\n      - proto_probs: List[List[Tensor]] size [B][T]\n      - uncertainties/gates/span_preds: List[List[Tensor]] size [B][T]\n      - proto_assignments: List[Tensor] length B each [T]\n      - h_augmented: Tensor [B, T, H]\n    This function never raises; it logs when _VERBOSE_LOGGING is True.\n    \"\"\"\n    def _log(msg):\n        if _VERBOSE_LOGGING:\n            print(\"[DSCD-NORMALIZE]\", msg)\n\n    # defaults: create device-aware fallback structures\n    proto_probs = [[torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    uncertainties = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    gates = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    span_preds = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    proto_assignments = [torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)]\n    h_aug = None\n\n    try:\n        if not isinstance(raw, dict):\n            _log(\"raw DSCD output is not a dict; using fallbacks\")\n            raw = {} if raw is None else dict(raw)\n\n        # h_augmented: accept tensor or list-of-lists or None\n        h_raw = raw.get('h_augmented', None)\n        if isinstance(h_raw, torch.Tensor):\n            if h_raw.dim() == 3 and int(h_raw.size(0)) == batch_size and int(h_raw.size(1)) == seq_len:\n                h_aug = h_raw.to(device)\n            else:\n                # try to coerce rows into the returned shape\n                try:\n                    h_aug = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=h_raw.dtype)\n                    max_b = min(batch_size, int(h_raw.size(0)))\n                    for b in range(max_b):\n                        row = h_raw[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 2:\n                            L = min(seq_len, int(row.size(0)))\n                            h_aug[b, :L] = row[:L].to(device)\n                except Exception as e:\n                    _log(f\"h_aug coercion failed: {e}; fallback to zeros\")\n                    h_aug = None\n        elif isinstance(h_raw, (list, tuple)):\n            try:\n                # try to stack if shape matches\n                stacked = []\n                for b in range(min(batch_size, len(h_raw))):\n                    row = h_raw[b]\n                    if isinstance(row, torch.Tensor):\n                        stacked.append(row.to(device))\n                    elif isinstance(row, (list, tuple, np.ndarray)):\n                        stacked.append(torch.as_tensor(row, device=device))\n                if stacked:\n                    tensor = torch.stack(stacked, dim=0)\n                    if tensor.dim() == 3:\n                        # coerce shape\n                        h_aug = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=tensor.dtype)\n                        for b in range(min(batch_size, tensor.size(0))):\n                            L = min(seq_len, int(tensor.size(1)))\n                            h_aug[b, :L] = tensor[b, :L]\n            except Exception:\n                _log(\"h_aug list coercion failed\")\n                h_aug = None\n\n        # proto_probs: many possible layouts - normalize to [B][T] list-of-tensors\n        try:\n            pp = raw.get('proto_probs', None)\n            if pp is not None:\n                # helper to place tensor on device\n                def _to_tensor(v):\n                    if isinstance(v, torch.Tensor):\n                        return v.detach().to(device)\n                    try:\n                        return torch.as_tensor(v, dtype=torch.float32, device=device)\n                    except Exception:\n                        return torch.tensor([1.0], dtype=torch.float32, device=device)\n                if isinstance(pp, torch.Tensor):\n                    # [B, T, L], [T, L], or [L]\n                    if pp.dim() == 3:\n                        for b in range(min(batch_size, int(pp.size(0)))):\n                            for t in range(min(seq_len, int(pp.size(1)))):\n                                proto_probs[b][t] = _to_tensor(pp[b, t].flatten())\n                    elif pp.dim() == 2:\n                        # try treat as [B, T] or [T, L] if batch_size==1\n                        if int(pp.size(0)) == batch_size:\n                            for b in range(batch_size):\n                                for t in range(min(seq_len, int(pp.size(1)))):\n                                    proto_probs[b][t] = _to_tensor(pp[b, t].flatten())\n                        elif batch_size == 1:\n                            for t in range(min(seq_len, int(pp.size(0)))):\n                                proto_probs[0][t] = _to_tensor(pp[t].flatten())\n                    elif pp.dim() == 1 and batch_size == 1:\n                        # [L] -> assign to token 0 maybe\n                        for t in range(min(seq_len, int(pp.size(0)))):\n                            proto_probs[0][t] = _to_tensor(pp[t].unsqueeze(0))\n                elif isinstance(pp, (list, tuple)):\n                    # either list per-batch or single sequence for batch_size==1\n                    if len(pp) == batch_size:\n                        for b in range(batch_size):\n                            row = pp[b]\n                            if isinstance(row, (list, tuple, torch.Tensor, np.ndarray)):\n                                # row is either tensor [T,L] or list-of-L for each token\n                                if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                                    for t in range(min(seq_len, int(row.size(0)))):\n                                        proto_probs[b][t] = _to_tensor(row[t]).flatten()\n                                else:\n                                    for t in range(min(seq_len, len(row))):\n                                        proto_probs[b][t] = _to_tensor(row[t]).flatten()\n                    elif batch_size == 1:\n                        row = pp\n                        for t in range(min(seq_len, len(row))):\n                            proto_probs[0][t] = _to_tensor(row[t]).flatten()\n        except Exception as e:\n            _log(f\"proto_probs parsing failed: {e}\")\n\n        # scalar matrices: uncertainties / gates / span_preds -> normalize to List[List[tensor]]\n        def _normalize_scalar_matrix(key, target):\n            try:\n                val = raw.get(key, None)\n                if val is None:\n                    return\n                if isinstance(val, torch.Tensor):\n                    if val.dim() == 3 and int(val.size(0)) == batch_size:\n                        for b in range(batch_size):\n                            for t in range(min(seq_len, int(val.size(1)))):\n                                target[b][t] = torch.tensor(float(val[b, t].item()), device=device)\n                    elif val.dim() == 2 and int(val.size(0)) == batch_size:\n                        for b in range(batch_size):\n                            for t in range(min(seq_len, int(val.size(1)))):\n                                target[b][t] = torch.tensor(float(val[b, t].item()), device=device)\n                    elif val.dim() == 1 and batch_size == 1:\n                        for t in range(min(seq_len, int(val.size(0)))):\n                            target[0][t] = torch.tensor(float(val[t].item()), device=device)\n                elif isinstance(val, (list, tuple)):\n                    if len(val) == batch_size:\n                        for b in range(batch_size):\n                            row = val[b]\n                            if isinstance(row, torch.Tensor):\n                                for t in range(min(seq_len, int(row.size(0)))):\n                                    target[b][t] = torch.tensor(float(row[t].item()), device=device)\n                            else:\n                                for t in range(min(seq_len, len(row))):\n                                    try:\n                                        target[b][t] = torch.tensor(float(row[t]), device=device)\n                                    except Exception:\n                                        pass\n                    elif batch_size == 1:\n                        row = val\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                target[0][t] = torch.tensor(float(row[t]), device=device)\n                            except Exception:\n                                pass\n            except Exception as e:\n                _log(f\"{key} normalization failed: {e}\")\n\n        _normalize_scalar_matrix('uncertainties', uncertainties)\n        _normalize_scalar_matrix('gates', gates)\n        _normalize_scalar_matrix('span_preds', span_preds)\n\n        # proto_assignments: normalize to list of 1D long tensors length seq_len\n        try:\n            pa = raw.get('proto_assignments', None)\n            if pa is not None:\n                if isinstance(pa, list) and len(pa) == batch_size:\n                    for b in range(batch_size):\n                        row = pa[b]\n                        try:\n                            if isinstance(row, torch.Tensor):\n                                arr = row.detach().to(device).long()\n                                # pad/trim to seq_len\n                                if arr.numel() < seq_len:\n                                    pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                                    proto_assignments[b] = torch.cat([arr.view(-1), pad], dim=0)\n                                else:\n                                    proto_assignments[b] = arr.view(-1)[:seq_len]\n                            elif isinstance(row, (list, tuple, np.ndarray)):\n                                arr = torch.as_tensor(row, dtype=torch.long, device=device)\n                                if arr.numel() < seq_len:\n                                    pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                                    proto_assignments[b] = torch.cat([arr.view(-1), pad], dim=0)\n                                else:\n                                    proto_assignments[b] = arr.view(-1)[:seq_len]\n                        except Exception:\n                            proto_assignments[b] = torch.zeros(seq_len, dtype=torch.long, device=device)\n                elif isinstance(pa, torch.Tensor):\n                    if pa.dim() == 2 and int(pa.size(0)) == batch_size:\n                        for b in range(batch_size):\n                            arr = pa[b].detach().to(device).long()\n                            proto_assignments[b] = arr.view(-1)[:seq_len] if arr.numel() >= seq_len else torch.cat([arr.view(-1), torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)], dim=0)\n                    elif pa.dim() == 1 and batch_size == 1:\n                        arr = pa.detach().to(device).long()\n                        proto_assignments[0] = arr.view(-1)[:seq_len] if arr.numel() >= seq_len else torch.cat([arr.view(-1), torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)], dim=0)\n        except Exception as e:\n            _log(f\"proto_assignments parse failed: {e}\")\n\n    except Exception as e_outer:\n        _log(f\"overall normalization failure: {e_outer}\")\n        # return safe defaults below\n\n    if h_aug is None:\n        h_aug = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32)\n\n    return {\n        'proto_probs': proto_probs,\n        'uncertainties': uncertainties,\n        'gates': gates,\n        'span_preds': span_preds,\n        'proto_assignments': proto_assignments,\n        'h_augmented': h_aug\n    }\n\n# ------------------------------------------------------------------------------\n# Main model wrapper (MemoryOptimizedTATNWithExplanations)\n# ------------------------------------------------------------------------------\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.global_step = 0\n\n        # Load M2M100 backbone (fp32)\n        self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n            \"facebook/m2m100_418M\",\n            torch_dtype=torch.float32,\n            use_cache=False\n        )\n        try:\n            # ensure caching disabled for checkpointing\n            self.mbart.config.use_cache = False\n        except Exception:\n            pass\n\n        # force decoder BOS to English if possible\n        try:\n            forced_id = None\n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                forced_id = self.tokenizer.get_lang_id(_EN_LANG)\n            elif hasattr(self.tokenizer, \"lang_code_to_id\"):\n                forced_id = self.tokenizer.lang_code_to_id.get(_EN_LANG, None)\n            if forced_id is not None:\n                self.mbart.config.forced_bos_token_id = int(forced_id)\n                self.mbart.config.decoder_start_token_id = int(forced_id)\n        except Exception:\n            pass\n\n        # gradient checkpointing best-effort\n        try:\n            if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = int(self.mbart.config.d_model)\n\n        # Initialize DSCD (class must be defined in Cell 3)\n        self.dscd = MemoryEfficientDSCDOnline(\n            embed_dim=embed_dim,\n            tokenizer=tokenizer,\n            buffer_size=_DSCD_BUFFER_SIZE,\n            max_protos=_DSCD_MAX_PROTOS,\n            n_min=_DSCD_N_MIN,\n            language=_SOURCE_LANGUAGE,\n            dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n            enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n            max_clustering_points=500,\n            max_candidates_per_step=1\n        )\n\n        # ASBN and TRG placeholders: these must be present in your notebook as before\n        # Keep safe calls and device coercions where they are used.\n        self.asbn = globals().get('MemoryEfficientASBNModule', None)\n        if callable(self.asbn):\n            self.asbn = self.asbn(embed_dim, tokenizer, language=_SOURCE_LANGUAGE)\n        else:\n            # fallback stub if missing\n            class _StubASBN:\n                def forward_with_grl_simplified(self, *args, **kwargs):\n                    return torch.tensor(0.0, device=torch.device('cpu')), {}\n            self.asbn = _StubASBN()\n\n        self.trg_system = globals().get('CompleteTRGWithExplanations', None)\n        if callable(self.trg_system):\n            self.trg_system = self.trg_system(embed_dim, tokenizer, language=_SOURCE_LANGUAGE)\n        else:\n            # fallback stub\n            class _StubTRG:\n                def process_sentence_for_explanations(self, tokens, per_sent, token_word_map=None, uncertainty_threshold=0.1):\n                    return []\n            self.trg_system = _StubTRG()\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(proto_probs_list, gates_list=None, min_gate=0.0):\n        # Keep behavior similar to your original: compute average entropy across selected positions\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n        if dev is None:\n            return torch.tensor(0.0)\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    if float(gl[j]) < min_gate:\n                        continue\n                p = torch.clamp(probs.to(dev), 1e-8, 1.0)\n                H = -torch.sum(p * torch.log(p))\n                total = total + H\n                count += 1\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1: FORCE WORD MAP RECONSTRUCTION BEFORE DSCD FORWARD\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None\n    ) -> List[dict]:\n        \"\"\"\n        Force reconstruction of word maps BEFORE DSCD forward to ensure consistent keys.\n        \n        ‚úÖ FIX #1: This ensures DSCD uses proper word-level keys like \"‡¶ï‡¶≤\" instead of \"‚ñÅ‡¶ï‡¶≤\"\n        \"\"\"\n        word_maps_batch = []\n        \n        if token_word_map is not None and all(isinstance(m, dict) and len(m) > 0 for m in token_word_map):\n            # Already have valid word maps\n            if _VERBOSE_LOGGING:\n                total_words = sum(len(m) for m in token_word_map)\n                print(f\"[TATN-WORDMAP] Using provided word maps: {total_words} words across {batch_size} samples\")\n            return token_word_map\n        \n        # Need to reconstruct\n        if not _has_reconstruct_word_spans:\n            if _VERBOSE_LOGGING:\n                print(f\"[TATN-WORDMAP] ‚ö†Ô∏è reconstruct_word_spans() not available - using empty word maps\")\n            return [{} for _ in range(batch_size)]\n        \n        if _VERBOSE_LOGGING:\n            print(f\"[TATN-WORDMAP] Reconstructing word maps for {batch_size} samples...\")\n        \n        for b in range(batch_size):\n            try:\n                # Try to get original text\n                if src_texts and b < len(src_texts) and isinstance(src_texts[b], str) and src_texts[b].strip():\n                    orig_text = src_texts[b]\n                else:\n                    # Decode from input_ids\n                    try:\n                        orig_text = self.tokenizer.decode(input_ids[b], skip_special_tokens=True)\n                    except Exception:\n                        orig_text = \"\"\n                \n                if not orig_text.strip():\n                    word_maps_batch.append({})\n                    continue\n                \n                # Reconstruct word spans\n                wm, words = reconstruct_word_spans(self.tokenizer, orig_text, max_length=seq_len)\n                \n                if not isinstance(wm, dict):\n                    wm = {}\n                \n                word_maps_batch.append(wm)\n                \n                if _VERBOSE_LOGGING and b == 0:\n                    print(f\"[TATN-WORDMAP] Sample 0: {len(wm)} word spans reconstructed\")\n                    if wm:\n                        sample_words = [wm[k] for k in sorted(wm.keys())[:5]]\n                        print(f\"[TATN-WORDMAP] Sample words: {sample_words}\")\n                \n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[TATN-WORDMAP] Reconstruction failed for sample {b}: {e}\")\n                word_maps_batch.append({})\n        \n        total_words = sum(len(m) for m in word_maps_batch)\n        if _VERBOSE_LOGGING:\n            print(f\"[TATN-WORDMAP] ‚úì Reconstructed {total_words} words across {batch_size} samples\")\n        \n        return word_maps_batch\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n    ):\n        self.global_step += 1\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(f\"Expected 2D tensors, got {input_ids.shape}, {attention_mask.shape}\")\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        # periodic memory cleanup\n        if torch.cuda.is_available() and (self.global_step % _MEMORY_CLEANUP_FREQUENCY == 0):\n            for i in range(min(_NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n\n        # Encoder forward (safe extraction of last_hidden_state)\n        enc_outputs = None\n        try:\n            enc_outputs = self.mbart.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        except Exception:\n            # fallback to full model encoder via forward signature\n            try:\n                enc_outputs = self.mbart.get_encoder()(input_ids=input_ids, attention_mask=attention_mask)\n            except Exception:\n                enc_outputs = None\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            # fallback: run through input embeddings to construct a dummy encoding\n            try:\n                emb = self.mbart.get_input_embeddings()(input_ids).to(device)\n                h = emb\n            except Exception:\n                h = torch.zeros(batch_size, seq_len, int(self.mbart.config.d_model), device=device)\n\n        embed_dim = int(h.size(-1))\n\n        training_mode = (labels is not None and self.training)\n\n        # ‚úÖ FIX #1: FORCE WORD MAP RECONSTRUCTION BEFORE DSCD\n        token_word_map = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n\n        # DSCD forward (robust)\n        try:\n            raw_dscd = self.dscd.forward(\n                h, \n                token_types=None, \n                train_mode=self.training,\n                input_ids=input_ids, \n                attention_mask=attention_mask,\n                token_word_map=token_word_map  # ‚Üê Now guaranteed to have reconstructed maps\n            )\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] DSCD forward failed; using safe fallback. Trace:\", traceback.format_exc())\n            # safe fallback: minimal canonical structure\n            raw_dscd = {\n                'h_augmented': h.detach().clone(),\n                'proto_probs': [[torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)],\n                'uncertainties': [[torch.tensor(0.0, device=device) for _ in range(seq_len)] for _ in range(batch_size)],\n                'gates': [[torch.tensor(0.0, device=device) for _ in range(seq_len)] for _ in range(batch_size)],\n                'span_preds': [[torch.tensor(0.0, device=device) for _ in range(seq_len)] for _ in range(batch_size)],\n                'proto_assignments': [torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)],\n            }\n\n        # ‚úÖ FIX #2: VALIDATE DSCD PROTOTYPE STORES\n        if not self.training and _VERBOSE_LOGGING:\n            try:\n                num_stores = len(self.dscd.prototype_stores)\n                multi_sense = sum(1 for store in self.dscd.prototype_stores.values() if len(store.centroids) >= 2)\n                print(f\"[TATN-DSCD] Prototype stores: {num_stores} tokens, {multi_sense} multi-sense\")\n                \n                if num_stores == 0:\n                    print(f\"[TATN-DSCD] ‚ö†Ô∏è WARNING: NO PROTOTYPES EXIST! Explanations will be empty.\")\n                    print(f\"[TATN-DSCD]    ‚Üí Run discovery warmup or train more epochs\")\n                \n                # ‚úÖ FIX #5: Report homograph detection\n                homographs_found = []\n                for word in _HOMOGRAPH_WATCHLIST:\n                    clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                    # Check if word or any variant exists in stores\n                    for key in self.dscd.prototype_stores.keys():\n                        clean_key = str(key).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                        if clean_key == clean_word or clean_word in clean_key:\n                            num_protos = len(self.dscd.prototype_stores[key].centroids)\n                            homographs_found.append((clean_word, key, num_protos))\n                            break\n                \n                if homographs_found:\n                    print(f\"[TATN-DSCD] ‚úÖ Homographs detected:\")\n                    for clean_word, key, num_protos in homographs_found:\n                        print(f\"[TATN-DSCD]    - '{clean_word}' (key='{key}'): {num_protos} prototypes\")\n                else:\n                    print(f\"[TATN-DSCD] ‚ö†Ô∏è No homographs from watchlist found in prototype stores\")\n                    \n            except Exception as e:\n                print(f\"[TATN-DSCD] Validation failed: {e}\")\n\n        # Normalize DSCD outputs\n        dscd = _normalize_dscd_outputs(raw_dscd, batch_size, seq_len, device, embed_dim)\n        h_aug = dscd.get('h_augmented', h)\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            h_aug = h\n\n        # ‚úÖ FIX #4: Embedding-based span fallback ONLY when NO prototypes exist\n        try:\n            # Check if DSCD has ANY multi-sense prototypes\n            has_prototypes = False\n            if hasattr(self.dscd, 'prototype_stores'):\n                has_prototypes = any(\n                    len(store.centroids) >= 2 \n                    for store in self.dscd.prototype_stores.values()\n                )\n            \n            # Only apply fallback when NO prototypes at all\n            if not has_prototypes:\n                span_missing = True\n                for b in range(batch_size):\n                    row = dscd['span_preds'][b]\n                    if any(float(x) > 1e-6 for x in row):\n                        span_missing = False\n                        break\n                \n                if span_missing:\n                    norms = torch.norm(h_aug, dim=-1)  # [B, T]\n                    for b in range(batch_size):\n                        n = norms[b]\n                        if n.numel() == 0 or torch.all(n == 0):\n                            continue\n                        mn = float(n.min().item())\n                        mx = float(n.max().item())\n                        rng = mx - mn + 1e-8\n                        scaled = (n - mn) / rng\n                        for t in range(min(seq_len, scaled.size(0))):\n                            try:\n                                dscd['span_preds'][b][t] = torch.tensor(float(scaled[t].item()), device=device)\n                            except Exception:\n                                pass\n                    if _VERBOSE_LOGGING:\n                        print(\"[TATN] ‚ö†Ô∏è No prototypes exist - applied embedding-norm fallback for spans\")\n            elif _VERBOSE_LOGGING:\n                print(f\"[TATN] ‚úì Prototypes exist - using DSCD span values\")\n                \n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] Span fallback check failed:\", traceback.format_exc())\n\n        # TRAINING path\n        if training_mode:\n            # build encoder_outputs object for decoder\n            try:\n                enc_for_decoder = BaseModelOutput(last_hidden_state=h_aug)\n            except Exception:\n                # fallback to tuple\n                enc_for_decoder = (h_aug,)\n\n            try:\n                seq_outputs = self.mbart(encoder_outputs=enc_for_decoder,\n                                         attention_mask=attention_mask,\n                                         labels=labels,\n                                         use_cache=False,\n                                         return_dict=True)\n                translation_loss = getattr(seq_outputs, 'loss', None)\n                if translation_loss is None:\n                    translation_loss = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] Decoder forward failed during training:\", traceback.format_exc())\n                translation_loss = torch.tensor(0.0, device=device)\n\n            # ASBN loss (coerce to tensor)\n            try:\n                asbn_ret = self.asbn.forward_with_grl_simplified(h_aug, dscd.get('proto_probs', None),\n                                                                dscd.get('uncertainties', None),\n                                                                dscd.get('gates', None),\n                                                                token_word_map=token_word_map)\n                if isinstance(asbn_ret, tuple) or isinstance(asbn_ret, list):\n                    asbn_loss = asbn_ret[0]\n                else:\n                    asbn_loss = asbn_ret\n                if not isinstance(asbn_loss, torch.Tensor):\n                    asbn_loss = torch.tensor(float(asbn_loss), device=device)\n                else:\n                    asbn_loss = asbn_loss.to(device)\n                if not torch.isfinite(asbn_loss):\n                    asbn_loss = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] ASBN forward failed:\", traceback.format_exc())\n                asbn_loss = torch.tensor(0.0, device=device)\n\n            # DSCD entropy regularizer\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(dscd.get('proto_probs', []),\n                                                                     gates_list=dscd.get('gates', []),\n                                                                     min_gate=0.0)\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(float(dscd_reg), device=device)\n                else:\n                    dscd_reg = dscd_reg.to(device)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] DSCD reg computation failed:\", traceback.format_exc())\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            total_loss = translation_loss + _LAMBDA_ASBN * asbn_loss + _LAMBDA_DSCD * dscd_reg\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n            return total_loss\n\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ FIX #3: COMPREHENSIVE DEBUG LOGGING FOR INFERENCE PATH\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        # INFERENCE path: TRG explanations\n        explanations = {i: [] for i in range(batch_size)}\n        \n        if (not self.training) and _ENABLE_TRG_INFERENCE:\n            if _VERBOSE_LOGGING:\n                print(f\"\\n[TATN-INFERENCE] Starting TRG explanation generation for {batch_size} samples\")\n            \n            tokens_batch: List[List[str]] = []\n            \n            # Build tokens_batch\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    toks = self.tokenizer.convert_ids_to_tokens(ids_b) if hasattr(self.tokenizer, 'convert_ids_to_tokens') else ['UNK'] * seq_len\n                    if len(toks) < seq_len:\n                        toks = toks + [''] * (seq_len - len(toks))\n                    else:\n                        toks = toks[:seq_len]\n                except Exception:\n                    toks = ['UNK'] * seq_len\n                tokens_batch.append(toks)\n                \n                if _VERBOSE_LOGGING and b == 0:\n                    print(f\"[TATN-INFERENCE] Sample 0 tokens: {toks[:10]}...\")\n\n            # ‚úÖ FIX #3: Debug DSCD output stats\n            if _VERBOSE_LOGGING:\n                try:\n                    # Count non-default uncertainties\n                    uncertain_count = 0\n                    high_span_count = 0\n                    multi_proto_count = 0\n                    \n                    for b in range(batch_size):\n                        for t in range(seq_len):\n                            try:\n                                u = float(dscd['uncertainties'][b][t])\n                                s = float(dscd['span_preds'][b][t])\n                                p = dscd['proto_probs'][b][t]\n                                \n                                if u > 0.1:\n                                    uncertain_count += 1\n                                if s > 0.1:\n                                    high_span_count += 1\n                                if isinstance(p, torch.Tensor) and p.numel() >= 2:\n                                    multi_proto_count += 1\n                            except Exception:\n                                pass\n                    \n                    print(f\"[TATN-INFERENCE] DSCD stats:\")\n                    print(f\"  - Tokens with uncertainty > 0.1: {uncertain_count}/{batch_size * seq_len}\")\n                    print(f\"  - Tokens with span > 0.1: {high_span_count}/{batch_size * seq_len}\")\n                    print(f\"  - Tokens with multi-sense protos: {multi_proto_count}/{batch_size * seq_len}\")\n                    \n                except Exception as e:\n                    print(f\"[TATN-INFERENCE] Stats computation failed: {e}\")\n\n            # helper: produce per-token scalar tensors list length seq_len safely\n            def _safe_take_key(dscd_struct, key, b_index):\n                out = [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n                try:\n                    val = dscd_struct.get(key, None)\n                    if val is None:\n                        return out\n                    # layout 1: list of length batch_size, each a list of tensor scalars\n                    if isinstance(val, list) and len(val) == batch_size:\n                        row = val[b_index]\n                        if isinstance(row, list):\n                            for t in range(min(seq_len, len(row))):\n                                v = row[t]\n                                if isinstance(v, torch.Tensor):\n                                    out[t] = v.to(device)\n                                else:\n                                    try:\n                                        out[t] = torch.tensor(float(v), device=device)\n                                    except Exception:\n                                        out[t] = torch.tensor(0.0, device=device)\n                        elif isinstance(row, torch.Tensor):\n                            # 1D or scalar\n                            if row.dim() == 1:\n                                for t in range(min(seq_len, int(row.size(0)))):\n                                    out[t] = torch.tensor(float(row[t].item()), device=device)\n                            else:\n                                out[0] = torch.tensor(float(row.item()), device=device)\n                        else:\n                            # row is list-like of scalars\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    out[t] = torch.tensor(float(row[t]), device=device)\n                                except Exception:\n                                    out[t] = torch.tensor(0.0, device=device)\n                        return out\n                    # layout 2: val is tensor [B, T, ...] or [B, T]\n                    if isinstance(val, torch.Tensor):\n                        if val.dim() >= 2 and int(val.size(0)) == batch_size:\n                            for t in range(min(seq_len, int(val.size(1)))):\n                                try:\n                                    # pick scalar or 1-d tensor and reduce to scalar\n                                    if val.dim() == 3:\n                                        v = val[b_index, t]\n                                        if v.numel() == 1:\n                                            out[t] = torch.tensor(float(v.item()), device=device)\n                                        else:\n                                            out[t] = v.to(device)\n                                    else:\n                                        v = val[b_index, t]\n                                        out[t] = torch.tensor(float(v.item()), device=device)\n                                except Exception:\n                                    out[t] = torch.tensor(0.0, device=device)\n                            return out\n                        elif val.dim() == 1 and batch_size == 1:\n                            for t in range(min(seq_len, int(val.size(0)))):\n                                out[t] = torch.tensor(float(val[t].item()), device=device)\n                            return out\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(f\"[TATN] _safe_take_key error for key '{key}':\", traceback.format_exc())\n                return out\n\n            # build per-sentence inputs and call TRG\n            try:\n                total_explanations = 0\n                \n                for b in range(batch_size):\n                    per_sent = {\n                        'proto_probs': _safe_take_key(dscd, 'proto_probs', b),\n                        'uncertainties': _safe_take_key(dscd, 'uncertainties', b),\n                        'gates': _safe_take_key(dscd, 'gates', b),\n                        'span_preds': _safe_take_key(dscd, 'span_preds', b),\n                    }\n                    \n                    try:\n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=token_word_map[b],  # ‚Üê Now guaranteed to exist\n                            uncertainty_threshold=float(globals().get('TAU_LOW', 0.20)),  # ‚Üê Using lowered threshold\n                        )\n                        explanations[b] = exps if isinstance(exps, list) else []\n                        total_explanations += len(explanations[b])\n                        \n                        if _VERBOSE_LOGGING:\n                            print(f\"[TATN-INFERENCE] Sample {b}: {len(explanations[b])} explanations generated\")\n                            if explanations[b]:\n                                for exp in explanations[b][:2]:  # Show first 2\n                                    print(f\"[TATN-INFERENCE]    - Token: '{exp.get('token', 'UNK')}', u={exp.get('uncertainty', 0):.3f}, s={exp.get('span', 0):.3f}\")\n                        \n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(f\"[TATN-INFERENCE] TRG generation failed for sample {b}:\", traceback.format_exc())\n                        explanations[b] = []\n                \n                # ‚úÖ FIX #3: Summary statistics\n                if _VERBOSE_LOGGING:\n                    print(f\"\\n[TATN-INFERENCE] ‚úì Summary:\")\n                    print(f\"  - Total explanations: {total_explanations}/{batch_size * seq_len} tokens\")\n                    print(f\"  - Samples with explanations: {sum(1 for exps in explanations.values() if exps)}/{batch_size}\")\n                    \n                    if total_explanations == 0:\n                        print(f\"\\n[TATN-INFERENCE] ‚ö†Ô∏è WARNING: NO EXPLANATIONS GENERATED!\")\n                        print(f\"  Possible causes:\")\n                        print(f\"  1. DSCD prototype stores empty (run discovery warmup)\")\n                        print(f\"  2. Uncertainty/span thresholds too strict (check Cell 5)\")\n                        print(f\"  3. Word map reconstruction failed (check Cell 1)\")\n                        print(f\"  4. Token filtering too aggressive (check TRG system)\")\n                    \n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN-INFERENCE] TRG generation failed overall:\", traceback.format_exc())\n                explanations = {i: [] for i in range(batch_size)}\n\n        outputs = {\n            'encoder_outputs': enc_outputs,\n            'dscd_outputs': dscd,\n            'sense_augmented_embeddings': h_aug,\n            'explanations': [explanations.get(i, []) for i in range(batch_size)],\n            'asbn_loss': torch.tensor(0.0, device=device),\n        }\n        return outputs\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n        )\n\n# ------------------------------------------------------------------------------\n# Verification print\n# ------------------------------------------------------------------------------\nprint(\"=\" * 80)\nprint(\"‚úÖ Cell 6: TATN model ready (M2M100 418M) - COMPLETELY FIXED\")\nprint(\"=\" * 80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Force word map reconstruction BEFORE DSCD forward\")\nprint(\" ‚úÖ FIX #2: Add DSCD prototype validation after forward\")\nprint(\" ‚úÖ FIX #3: Add comprehensive debug logging for inference\")\nprint(\" ‚úÖ FIX #4: Fix span fallback to only trigger when NO prototypes\")\nprint(\" ‚úÖ FIX #5: Add homograph detection reporting\")\nprint(\"=\" * 80)\nprint(f\"‚úì Gradient checkpointing enabled: {_USE_GC}\")\nprint(f\"‚úì DSCD training clustering: {'ENABLED' if _DSCD_ENABLE_TRAINING_CLUSTERING else 'DISABLED (speed mode)'}\")\nprint(f\"‚úì DSCD buffer: {_DSCD_BUFFER_SIZE}, n_min: {_DSCD_N_MIN}, disp_th: {_DSCD_DISPERSION_THRESHOLD}\")\nprint(\"=\" * 80)","metadata":{"id":"KZbMDpIYH4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP - COMPLETELY FIXED WITH PER-EPOCH VALIDATION\n# ==============================================================================\n# ‚úÖ FIXED: Add comprehensive per-epoch validation (ERROR #1 FIX)\n# ‚úÖ FIXED: Add DSCD quality validation after each epoch (ERROR #2 FIX)\n# ‚úÖ FIXED: Enhanced validation to test explanations (ERROR #3 FIX)\n# ‚úÖ ADDED: Training metrics tracking (quality score, multi-sense ratio) (ERROR #4 FIX)\n# ‚úÖ ADDED: Homograph-specific detection logging (ERROR #5 FIX)\n# ‚úÖ ADDED: Epoch validation summary with quality trends\n# \n# Original features preserved:\n# - Data Parallel (DP) + AMP + Gradient Accumulation\n# - Robust error handling (OOM, runtime errors)\n# - Progress bar with live metrics\n# - Checkpoint saving\n# - Cluster tracking and debugging\n# ==============================================================================\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\n\n# ---------------- Debug control ----------------\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not _VERBOSE_LOGGING:\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\n\n# ---------------- Fallback globals ----------------\ntry:\n    _DEVICE = DEVICE\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept Exception:\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept Exception:\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept Exception:\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept Exception:\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept Exception:\n    _MEMORY_CLEANUP_FREQUENCY = 100\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept Exception:\n    _USE_AMP = True\n\ntry:\n    _BN_LANG = BN_LANG\n    _EN_LANG = EN_LANG\nexcept Exception:\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept Exception:\n    VALIDATION_CHECK_INTERVAL = 0\n\n# ‚úÖ FIX #5: Import homograph watchlist for targeted logging\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n# ---------------- Helpers ----------------\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        # iterate over actual CUDA devices\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n\ndef get_amp_ctx():\n    \"\"\"\n    Return a context manager for mixed-precision if enabled and available.\n    Otherwise return a nullcontext.\n    \"\"\"\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        # prefer torch.cuda.amp.autocast (aliased above)\n        return cuda_amp_autocast()\n    except Exception:\n        # fallback to no-op if autocast not available\n        return nullcontext()\n\n\ndef save_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, training_stats: Dict[str, Any],\n                    epoch: int, global_step: int, epoch_losses: List[float], ckpt_dir: str = \"checkpoints\"):\n    os.makedirs(ckpt_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    fname = f\"tatn_e{epoch}_s{global_step}_{timestamp}.pt\"\n    path = os.path.join(ckpt_dir, fname)\n    # pick the wrapped module if present\n    core_model = model.module if hasattr(model, \"module\") else model\n    \n    # ‚úÖ FIX #2: Include DSCD state in checkpoint\n    dscd_state = {}\n    try:\n        dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n        if dscd and hasattr(dscd, 'state_dict'):\n            dscd_state = dscd.state_dict()\n    except Exception as e:\n        print(f\"[CHECKPOINT] Warning: Could not save DSCD state: {e}\")\n    \n    ckpt = {\n        \"epoch\": epoch,\n        \"global_step\": global_step,\n        \"model_state_dict\": core_model.state_dict(),\n        \"dscd_state_dict\": dscd_state,  # ‚Üê Include DSCD prototypes\n        \"optimizer_state_dict\": optimizer.state_dict() if optimizer is not None else None,\n        \"training_stats\": training_stats,\n        \"avg_epoch_loss\": float(np.mean(epoch_losses)) if epoch_losses else 0.0,\n    }\n    try:\n        torch.save(ckpt, path)\n        print(f\"[CHECKPOINT] Saved {fname} avg_loss={ckpt['avg_epoch_loss']:.6f}\")\n        if dscd_state:\n            num_tokens = len(dscd_state.get('prototype_stores', {}))\n            print(f\"[CHECKPOINT] ‚úì DSCD state included: {num_tokens} tokens\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] Save failed: {type(e).__name__}: {str(e)[:200]}\")\n\n\n# ---------------- Validation (hardened) ----------------\n# Show protobuf hint only once per process if AttributeError observed during generation/decoding\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #3: ENHANCED VALIDATION - Tests translations AND explanations\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module, \n    tokenizer, \n    epoch: int,\n    global_step: int,\n    bn_lang: str, \n    en_lang: str, \n    max_length: int, \n    device: torch.device\n) -> Dict[str, Any]:\n    \"\"\"\n    Comprehensive validation after each epoch.\n    Tests both translation quality AND explanation generation.\n    Returns dict with validation metrics.\n    \n    ‚úÖ FIX #3: Enhanced to test DSCD + TRG pipeline during training\n    \"\"\"\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n    \n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n    core_model.eval()\n    \n    validation_results = {\n        'epoch': epoch,\n        'step': global_step,\n        'translations_success': 0,\n        'translations_failed': 0,\n        'explanations_generated': 0,\n        'homographs_with_explanations': 0,\n        'avg_explanation_confidence': 0.0,\n        'dscd_quality_score': 0.0,\n        'dscd_multi_sense_tokens': 0,\n        'dscd_total_prototypes': 0,\n    }\n    \n    # Validation sentences (Bengali ‚Üí English with expected homographs)\n    val_sentences = [\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap\", \"‡¶ï‡¶≤=tap/call\"),\n        (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"Tomorrow I will buy a book\", \"‡¶ï‡¶æ‡¶≤=tomorrow/yesterday\"),\n        (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"The leaf has fallen\", \"‡¶™‡¶æ‡¶§‡¶æ=leaf/page\"),\n        (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\", \"He went to the bank\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï=bank/embankment\"),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø‡•§\", \"I am fine\", \"No ambiguity\"),\n        (\"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"She speaks sweetly\", \"No ambiguity\"),\n        (\"‡¶è‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶á‡•§\", \"This is my book\", \"No ambiguity\"),\n        (\"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã‡•§\", \"Weather is good today\", \"No ambiguity\"),\n        (\"‡¶´‡¶≤ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶∏‡ßç‡¶¨‡¶æ‡¶¶‡ßÅ‡•§\", \"The fruit is delicious\", \"‡¶´‡¶≤=fruit/result\"),\n        (\"‡¶Æ‡¶æ‡¶•‡¶æ ‡¶¨‡ßç‡¶Ø‡¶•‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡•§\", \"Head is aching\", \"‡¶Æ‡¶æ‡¶•‡¶æ=head/top\"),\n    ]\n    \n    print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n    print(\"-\" * 80)\n    \n    confidences = []\n    homograph_words_detected = set()\n    \n    # try to find mbart submodule if present\n    gen_target = getattr(core_model, \"mbart\", core_model)\n\n    try:\n        # set language on tokenizer if supported\n        try:\n            tokenizer.src_lang = bn_lang\n        except Exception:\n            pass\n\n        # robust forced_id lookup (try several codes)\n        forced_id = None\n        try:\n            if hasattr(tokenizer, \"get_lang_id\"):\n                for code in (en_lang, \"en_XX\", \"en\", \"eng\"):\n                    try:\n                        lid = tokenizer.get_lang_id(code)\n                        if lid is not None:\n                            forced_id = lid\n                            break\n                    except Exception:\n                        continue\n            elif hasattr(tokenizer, \"lang_code_to_id\"):\n                forced_id = tokenizer.lang_code_to_id.get(en_lang, None)\n        except Exception:\n            forced_id = None\n\n        # try to enable use_cache for faster generation, restoring later\n        mbart_obj = getattr(core_model, \"mbart\", None)\n        orig_use_cache = None\n        try:\n            if mbart_obj is not None and hasattr(mbart_obj.config, \"use_cache\"):\n                orig_use_cache = mbart_obj.config.use_cache\n                mbart_obj.config.use_cache = True\n        except Exception:\n            orig_use_cache = None\n\n        for idx, (src, expected, note) in enumerate(val_sentences, 1):\n            try:\n                # Encode\n                enc = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n                enc = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in enc.items()}\n                \n                # Generate translation\n                if forced_id is not None:\n                    try:\n                        if mbart_obj is not None:\n                            mbart_obj.config.forced_bos_token_id = int(forced_id)\n                            mbart_obj.config.decoder_start_token_id = int(forced_id)\n                    except Exception:\n                        pass\n                \n                out_ids = None\n                try:\n                    gen_src = getattr(core_model, \"mbart\", None) or core_model\n                    if hasattr(gen_src, \"generate\"):\n                        out_ids = gen_src.generate(\n                            enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            max_length=max_length,\n                            num_beams=2,\n                            do_sample=False,\n                            early_stopping=True,\n                            pad_token_id=int(getattr(tokenizer, \"pad_token_id\", 1)),\n                            forced_bos_token_id=int(forced_id) if forced_id is not None else None\n                        )\n                except AttributeError as ae:\n                    if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                        print(\"[VALIDATION] Warning: generation raised AttributeError (often protobuf incompatibility).\")\n                        print(\"  Suggestion: pip install 'protobuf==3.20.3' and restart the kernel.\")\n                        _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                    out_ids = None\n                except Exception as e:\n                    print(f\"[VALIDATION] Generation error: {type(e).__name__}: {str(e)[:100]}\")\n                    out_ids = None\n\n                if out_ids is not None:\n                    try:\n                        if isinstance(out_ids, (list, tuple)):\n                            translation = tokenizer.batch_decode(out_ids, skip_special_tokens=True)[0]\n                        else:\n                            translation = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n                    except AttributeError:\n                        if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                            print(\"[VALIDATION] Warning: decode raised AttributeError (protobuf). Pin protobuf and restart.\")\n                            _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                        translation = \"\"\n                    except Exception as e:\n                        print(f\"[VALIDATION] Decode error: {type(e).__name__}: {str(e)[:100]}\")\n                        translation = \"\"\n                else:\n                    translation = \"\"\n                \n                if translation:\n                    validation_results['translations_success'] += 1\n                else:\n                    validation_results['translations_failed'] += 1\n                    print(f\"  {idx:2d}. ‚úó {note[:30]:30s} ‚Üí Translation failed\")\n                    continue\n                \n                # ‚úÖ FIX #3: Test explanation generation\n                explanation_status = \"\"\n                try:\n                    # Check if translate_with_explanations exists\n                    if 'translate_with_explanations' in globals():\n                        res = translate_with_explanations(model, tokenizer, src)\n                        exps = res.get('explanations', [])\n                        validation_results['explanations_generated'] += len(exps)\n                        \n                        if exps:\n                            explanation_status = f\"‚úì {len(exps)} expl\"\n                            # Track confidence\n                            for exp in exps:\n                                try:\n                                    conf = exp.get('confidence', 0.5)\n                                    confidences.append(float(conf))\n                                    \n                                    # Check if homograph from watchlist\n                                    word = exp.get('ambiguous_word', '').strip()\n                                    clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '')\n                                    if clean_word in _HOMOGRAPH_WATCHLIST:\n                                        validation_results['homographs_with_explanations'] += 1\n                                        homograph_words_detected.add(clean_word)\n                                except Exception:\n                                    pass\n                        else:\n                            explanation_status = \"‚óã\"\n                    else:\n                        explanation_status = \"?\"\n                except Exception as e:\n                    explanation_status = f\"‚úó {type(e).__name__}\"\n                \n                print(f\"  {idx:2d}. {explanation_status} {note[:30]:30s} ‚Üí {translation[:40]}\")\n                \n            except Exception as e:\n                validation_results['translations_failed'] += 1\n                print(f\"  {idx:2d}. ‚úó {note[:30]:30s} ‚Üí ERROR: {type(e).__name__}\")\n    \n    finally:\n        try:\n            if mbart_obj is not None and orig_use_cache is not None:\n                mbart_obj.config.use_cache = orig_use_cache\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            try:\n                torch.cuda.synchronize()\n            except Exception:\n                pass\n        clear_all_gpu_caches()\n        if was_training:\n            core_model.train()\n    \n    # ‚úÖ FIX #2: DSCD Quality Validation\n    print(\"\\n\" + \"-\" * 80)\n    print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n    try:\n        dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n        if dscd and hasattr(dscd, 'validate_prototypes'):\n            quality_results = dscd.validate_prototypes()\n            validation_results['dscd_quality_score'] = quality_results['quality_score']\n            validation_results['dscd_multi_sense_tokens'] = quality_results['multi_sense_tokens']\n            validation_results['dscd_total_prototypes'] = quality_results['total_prototypes']\n            print(f\"  - Quality Score: {quality_results['quality_score']:.1%}\")\n        else:\n            print(f\"  - Validation not available (DSCD has no validate_prototypes method)\")\n            validation_results['dscd_quality_score'] = 0.0\n    except Exception as e:\n        print(f\"  - Validation failed: {type(e).__name__}\")\n        validation_results['dscd_quality_score'] = 0.0\n    \n    # Compute averages\n    if confidences:\n        validation_results['avg_explanation_confidence'] = sum(confidences) / len(confidences)\n    \n    print(\"-\" * 80)\n    print(f\"\\n[VALIDATION] Summary:\")\n    print(f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\")\n    print(f\"  - Explanations generated: {validation_results['explanations_generated']}\")\n    print(f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\")\n    print(f\"  - Homographs with explanations: {validation_results['homographs_with_explanations']}\")\n    if homograph_words_detected:\n        print(f\"  - Homographs detected: {', '.join(sorted(homograph_words_detected))}\")\n    print(f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\")\n    print(f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\")\n    print(f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\")\n    \n    # Health warnings\n    warnings = []\n    if validation_results['translations_failed'] > len(val_sentences) // 2:\n        warnings.append(\"‚ö†Ô∏è High translation failure rate!\")\n    if validation_results['explanations_generated'] == 0:\n        warnings.append(\"‚ö†Ô∏è No explanations generated - check TRG thresholds!\")\n    if validation_results['dscd_quality_score'] < 0.3:\n        warnings.append(\"‚ö†Ô∏è Low DSCD quality score - needs more training!\")\n    if validation_results['dscd_multi_sense_tokens'] < 10:\n        warnings.append(\"‚ö†Ô∏è Very few multi-sense tokens - increase training data!\")\n    \n    if warnings:\n        print(f\"\\n[VALIDATION] Health Warnings:\")\n        for w in warnings:\n            print(f\"  {w}\")\n    else:\n        print(f\"\\n[VALIDATION] ‚úì All systems healthy\")\n    \n    print(\"=\" * 80 + \"\\n\")\n    \n    return validation_results\n\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else model.dscd\n        return len(getattr(dscd, \"prototype_stores\", {}) or {})\n    except Exception:\n        return 0\n\n\n# ---- Safe DSCD retriever (handles DP wrappers)\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        return getattr(core, \"dscd\", None)\n    except Exception:\n        return None\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #5: HOMOGRAPH-SPECIFIC CLUSTER LOGGING\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    \"\"\"Print top clusters with homograph highlighting.\"\"\"\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        if _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] No DSCD instance attached to model.\")\n        return\n    try:\n        items = []\n        homograph_items = []\n        \n        for token, store in dscd.prototype_stores.items():\n            total_count = sum(getattr(store, \"counts\", []) or [])\n            protos = store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n            \n            # Check if homograph\n            clean_token = str(token).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n            is_homograph = clean_token in _HOMOGRAPH_WATCHLIST\n            \n            item = (token, total_count, protos, len(dscd.buffers.get(token, [])), is_homograph)\n            items.append(item)\n            if is_homograph:\n                homograph_items.append(item)\n        \n        items.sort(key=lambda x: x[1], reverse=True)\n        \n        if _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen, is_homo) in enumerate(items[:top_n], 1):\n                marker = \"üéØ\" if is_homo else \"  \"\n                print(f\"{marker}{i:2d}. {str(tok)[:20]:20s} samples={cnt:4d} protos={prot} buf={buflen}\")\n            \n            if homograph_items:\n                print(\"[CLUSTER-DBG] Homograph status:\")\n                for tok, cnt, prot, buflen, _ in homograph_items:\n                    print(f\"  üéØ {str(tok)[:20]:20s} samples={cnt:4d} protos={prot}\")\n    except Exception as e:\n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}: {str(e)[:200]}\")\n\n\ndef _print_cluster_stats(model: torch.nn.Module):\n    \"\"\"Print cluster statistics with multi-sense ratio.\"\"\"\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n    try:\n        total_tokens = len(dscd.prototype_stores)\n        total_protos = 0\n        total_samples = 0\n        total_buffers = 0\n        multi_sense = 0\n        \n        for token, store in dscd.prototype_stores.items():\n            num_protos = store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n            total_protos += num_protos\n            total_samples += sum(getattr(store, \"counts\", []) or [])\n            total_buffers += len(dscd.buffers.get(token, []))\n            if num_protos >= 2:\n                multi_sense += 1\n        \n        multi_sense_ratio = multi_sense / total_tokens if total_tokens > 0 else 0.0\n        \n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] tokens={total_tokens} protos={total_protos} samples={total_samples} buffers={total_buffers} multi_sense={multi_sense} ({multi_sense_ratio:.1%})\")\n    except Exception as e:\n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_cluster_stats error: {type(e).__name__}: {str(e)[:200]}\")\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #1: MAIN TRAINING LOOP WITH PER-EPOCH VALIDATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = VALIDATION_CHECK_INTERVAL\n\n    print(f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, accum_steps={accumulation_steps}\")\n    print(f\"[TRAIN] Validation: {'enabled' if enable_validation and validate_every > 0 else 'disabled'}\")\n    print(f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\")\n\n    model.train()\n    clear_all_gpu_caches()\n    # GradScaler enabled only if AMP requested and CUDA present\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    # ‚úÖ FIX #4: Enhanced training statistics\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],  # ‚Üê Track per-epoch averages\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],  # ‚Üê Store validation results per epoch\n        \"dscd_quality_history\": [],  # ‚Üê Track DSCD quality over time\n        \"multi_sense_ratio_history\": [],  # ‚Üê Track multi-sense discovery\n    }\n\n    skip_reasons = defaultdict(int)\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        try:\n            optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n        progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", ncols=180, dynamic_ncols=False)\n\n        for batch_idx, batch in enumerate(progress):\n            global_step += 1\n            training_stats[\"batches_processed\"] += 1\n\n            # Periodic debug print\n            if _VERBOSE_LOGGING and global_step % DEBUG_PRINT_INTERVAL == 0:\n                print(f\"[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} GlobalStep {global_step}\")\n\n            # Validation scheduling (in-loop)\n            if enable_validation and validate_every and validate_every > 0 and (global_step % validate_every == 0):\n                if accumulated_steps == 0:\n                    comprehensive_epoch_validation(model, tokenizer, epoch, global_step, _BN_LANG, _EN_LANG, _MAX_LENGTH, _DEVICE)\n                else:\n                    pending_validation = True\n\n            # Validate batch\n            if batch is None:\n                training_stats[\"skipped_batches\"] += 1\n                skip_reasons[\"batch_none\"] += 1\n                cell7_dbg(\"batch_none\", f\"Batch is None at idx={batch_idx}\")\n                continue\n\n            try:\n                # Required tensors (assume collate returns them)\n                input_ids = batch[\"input_ids\"]\n                attention_mask = batch[\"attention_mask\"]\n                labels = batch[\"labels\"]\n\n                # DP-divisible truncation safety (should already be handled by collate)\n                if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                    bsz = int(input_ids.size(0))\n                    keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                    if keep == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"dp_keep_zero\"] += 1\n                        cell7_dbg(\"dp_keep_zero\", f\"DP keep==0 bsz={bsz}, gpus={_NUM_GPUS}\")\n                        continue\n                    if keep != bsz:\n                        input_ids = input_ids[:keep]\n                        attention_mask = attention_mask[:keep]\n                        labels = labels[:keep]\n\n                # Move tensors to device\n                input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                labels = labels.to(_DEVICE, non_blocking=True)\n\n                if input_ids.size(0) == 0:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"empty_batch\"] += 1\n                    continue\n\n                # Optional debugging: check token_word_map presence in the batch (non-tensor)\n                if _VERBOSE_LOGGING and 'token_word_map' in batch:\n                    try:\n                        sample_map = batch['token_word_map'][:2]\n                        cell7_dbg(\"tokmap_sample\", f\"token_word_map sample lens: {[len(x) if x else 0 for x in sample_map]}\", limit=3)\n                    except Exception:\n                        pass\n\n                # Build forward kwargs: pass token_word_map and src_texts so DSCD can use canonical word keys\n                forward_kwargs = {\n                    \"input_ids\": input_ids,\n                    \"attention_mask\": attention_mask,\n                    \"labels\": labels,\n                    # pass non-tensor args if available (may be None)\n                    \"src_texts\": batch.get(\"src_text\", None),\n                    \"token_word_map\": batch.get(\"token_word_map\", None),\n                }\n\n                amp_ctx = get_amp_ctx()\n                with amp_ctx:\n                    forward_out = model(**forward_kwargs)\n\n                    # Model may return scalar loss or dict with 'loss' or full outputs (Cell 6 style)\n                    if isinstance(forward_out, torch.Tensor):\n                        loss_tensor = forward_out\n                    elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                        loss_tensor = forward_out[\"loss\"]\n                    else:\n                        # try to coerce if model returns a tensor-like first element\n                        if isinstance(forward_out, (list, tuple)) and len(forward_out) > 0 and isinstance(forward_out[0], torch.Tensor):\n                            loss_tensor = forward_out[0]\n                        else:\n                            raise RuntimeError(\"Model forward did not return a recognizable loss tensor\")\n\n                    # Ensure scalar and on device\n                    if not isinstance(loss_tensor, torch.Tensor):\n                        loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE)\n                    else:\n                        loss_tensor = loss_tensor.to(_DEVICE)\n\n                    if loss_tensor.numel() > 1:\n                        loss_val = float(loss_tensor.mean().item())\n                        loss_tensor = loss_tensor.mean()\n                    else:\n                        loss_val = float(loss_tensor.item())\n\n                    last_forward_loss = loss_val\n                    epoch_losses.append(loss_val)\n                    training_stats[\"total_loss\"].append(loss_val)\n\n                # backward + accumulation\n                loss_scaled = loss_tensor / max(1, accumulation_steps)\n                last_backward_loss = float(loss_scaled.item())\n\n                # Backward: use scaler only if enabled\n                if scaler.is_enabled():\n                    scaler.scale(loss_scaled).backward()\n                else:\n                    loss_scaled.backward()\n\n                accumulated_steps += 1\n\n                # optimizer step\n                if accumulated_steps >= accumulation_steps:\n                    try:\n                        # unscale & clip/step depending on scaler\n                        if scaler.is_enabled():\n                            scaler.unscale_(optimizer)\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                            scaler.step(optimizer)\n                            scaler.update()\n                        else:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                            optimizer.step()\n                        optimizer.zero_grad(set_to_none=True)\n                        training_stats[\"optimizer_updates\"] += 1\n                    except RuntimeError as e:\n                        # OOM handling\n                        if \"out of memory\" in str(e).lower():\n                            training_stats[\"oom_errors\"] += 1\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"oom\"] += 1\n                            print(f\"[OOM] OOM at step {global_step}: {str(e)[:200]}\")\n                            optimizer.zero_grad(set_to_none=True)\n                            for p in model.parameters():\n                                p.grad = None\n                            clear_all_gpu_caches()\n                            accumulated_steps = 0\n                            continue\n                        else:\n                            training_stats[\"runtime_errors\"] += 1\n                            skip_reasons[\"opt_runtime\"] += 1\n                            print(f\"[ERROR] Runtime error during optimizer step: {type(e).__name__}: {str(e)[:200]}\")\n                    except Exception as e:\n                        training_stats[\"exceptions\"] += 1\n                        skip_reasons[\"opt_exception\"] += 1\n                        print(f\"[ERROR] Exception during optimizer step: {type(e).__name__}: {str(e)[:200]}\")\n                    finally:\n                        accumulated_steps = 0\n                        # pending validation if previously deferred\n                        if pending_validation:\n                            comprehensive_epoch_validation(model, tokenizer, epoch, global_step, _BN_LANG, _EN_LANG, _MAX_LENGTH, _DEVICE)\n                            pending_validation = False\n\n                # periodic housekeeping & logs\n                if global_step % DEBUG_PRINT_INTERVAL == 0:\n                    _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                    try:\n                        cluster_count = _get_cluster_count(model)\n                    except Exception:\n                        cluster_count = 0\n                    print(f\"[TRAIN-DEBUG] step={global_step} loss={last_forward_loss:.4f} opt_updates={training_stats['optimizer_updates']} clusters={cluster_count}\")\n                    _print_top_clusters(model, top_n=5)\n                    _print_cluster_stats(model)\n\n                if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                    clear_all_gpu_caches()\n\n\n            except RuntimeError as e:\n                # OOM and other runtime errors\n                if \"out of memory\" in str(e).lower():\n                    training_stats[\"oom_errors\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"oom\"] += 1\n                    print(f\"[OOM] Caught OOM at step {global_step}: {str(e)[:200]}\")\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    for p in model.parameters():\n                        p.grad = None\n                    clear_all_gpu_caches()\n                    accumulated_steps = 0\n                    continue\n                else:\n                    training_stats[\"runtime_errors\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"runtime\"] += 1\n                    print(f\"[RUNTIME] RuntimeError at step {global_step}: {type(e).__name__}: {str(e)[:200]}\")\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    continue\n            except Exception as e:\n                training_stats[\"exceptions\"] += 1\n                training_stats[\"skipped_batches\"] += 1\n                skip_reasons[\"exceptions\"] += 1\n                print(f\"[EXCEPTION] Exception at step {global_step}: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    print(traceback.format_exc())\n                try:\n                    optimizer.zero_grad(set_to_none=True)\n                except Exception:\n                    pass\n                accumulated_steps = 0\n                continue\n\n            # update progress bar postfix\n            processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n            expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n            success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n            cluster_count = _get_cluster_count(model)\n            progress.set_postfix_str(\n                f\"fwd_loss={last_forward_loss:.4f} bwd_loss={last_backward_loss:.6f} rate={success_rate:.1f}% proc={processed_batches} skip={training_stats['skipped_batches']} clusters={cluster_count}\"\n            )\n\n        # end epoch: flush remaining grads if any\n        if accumulated_steps > 0:\n            try:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                    optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}: {str(e)[:200]}\")\n            finally:\n                accumulated_steps = 0\n\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ FIX #1 + #2: COMPREHENSIVE PER-EPOCH VALIDATION\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n        expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n        success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n        cluster_count = _get_cluster_count(model)\n        \n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"Epoch {epoch} Training Summary:\")\n        print(f\"  duration (min): {epoch_duration_min:.2f}\")\n        print(f\"  optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\"  batches processed: {training_stats['batches_processed']} (processed={processed_batches}, skipped={training_stats['skipped_batches']})\")\n        print(f\"  success rate (updates/expected): {success_rate:.1f}%\")\n        print(f\"  clustered token types: {cluster_count}\")\n        print(f\"  avg epoch loss: {avg_epoch_loss:.6f}\")\n        if skip_reasons:\n            print(\"  skip reasons:\")\n            for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"    - {k}: {v}\")\n        print(\"=\" * 80)\n\n        # ‚úÖ FIX #1: Run comprehensive validation after each epoch\n        try:\n            print(f\"\\n[TRAIN] Running comprehensive validation after epoch {epoch}...\")\n            validation_results = comprehensive_epoch_validation(\n                model=model,\n                tokenizer=tokenizer,\n                epoch=epoch,\n                global_step=global_step,\n                bn_lang=_BN_LANG,\n                en_lang=_EN_LANG,\n                max_length=_MAX_LENGTH,\n                device=_DEVICE\n            )\n            \n            # ‚úÖ FIX #4: Store validation results for tracking\n            training_stats['epoch_validations'].append(validation_results)\n            training_stats['dscd_quality_history'].append(validation_results['dscd_quality_score'])\n            \n            # Compute multi-sense ratio\n            try:\n                dscd = model.module.dscd if hasattr(model, 'module') else model.dscd\n                total_tokens = len(dscd.prototype_stores)\n                multi_sense = validation_results['dscd_multi_sense_tokens']\n                ratio = multi_sense / total_tokens if total_tokens > 0 else 0.0\n                training_stats['multi_sense_ratio_history'].append(ratio)\n            except Exception:\n                training_stats['multi_sense_ratio_history'].append(0.0)\n            \n        except Exception as e:\n            print(f\"[TRAIN] Epoch validation failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n        # save checkpoint at epoch end\n        try:\n            save_checkpoint(model, optimizer, training_stats, epoch, global_step, epoch_losses)\n        except Exception as e:\n            print(f\"[CHECKPOINT] Save at epoch end failed: {type(e).__name__}: {str(e)[:200]}\")\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1: FINAL TRAINING SUMMARY WITH QUALITY TRENDS\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"[TRAIN] TRAINING COMPLETED\")\n    print(\"=\" * 80)\n    \n    processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n    expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n    success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n    \n    print(f\"[TRAIN] Success Rate (updates/expected): {success_rate:.1f}%\")\n    print(f\"[TRAIN] Batches processed={processed_batches} skipped={training_stats['skipped_batches']}\")\n    print(f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\")\n    \n    # Show quality trends\n    if training_stats['dscd_quality_history']:\n        print(f\"\\n[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats['dscd_quality_history'], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n        \n        initial_score = training_stats['dscd_quality_history'][0]\n        final_score = training_stats['dscd_quality_history'][-1]\n        improvement = final_score - initial_score\n        print(f\"  Improvement: {improvement:+.1%}\")\n    \n    if training_stats['multi_sense_ratio_history']:\n        print(f\"\\n[TRAIN] Multi-Sense Ratio Trend:\")\n        for i, ratio in enumerate(training_stats['multi_sense_ratio_history'], 1):\n            print(f\"  Epoch {i}: {ratio:.1%}\")\n    \n    print(\"=\" * 80)\n    return model\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Cell 7: Training loop ready (COMPLETELY FIXED)\")\nprint(\"=\" * 80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Added comprehensive per-epoch validation\")\nprint(\" ‚úÖ FIX #2: Added DSCD quality validation after each epoch\")\nprint(\" ‚úÖ FIX #3: Enhanced validation to test explanations\")\nprint(\" ‚úÖ FIX #4: Added training metrics tracking (quality scores, ratios)\")\nprint(\" ‚úÖ FIX #5: Added homograph-specific detection logging\")\nprint(\" ‚úÖ Added: Epoch validation summary with quality trends\")\nprint(\"=\" * 80)","metadata":{"id":"coTb4Fi4H4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: INFERENCE PIPELINE - COMPLETELY FIXED WITH DEBUG LOGGING\n# ==============================================================================\n# ‚úÖ FIXED: Add comprehensive inference debug logging (ERROR #1 FIX)\n# ‚úÖ FIXED: Add DSCD validation before inference (ERROR #2 FIX)\n# ‚úÖ FIXED: Add threshold logging (ERROR #3 FIX)\n# ‚úÖ ADDED: Explanation quality metrics (ERROR #4 FIX)\n# ‚úÖ ADDED: Warmup validation with homograph checking (ERROR #5 FIX)\n# ‚úÖ ADDED: Inference statistics tracker (ERROR #6 FIX)\n# \n# Original features preserved:\n# - Safer device handling for tokenizer outputs\n# - Robust extraction of DSCD/TRG outputs\n# - VRAM-safe generation with fallback\n# - Subword token filtering\n# - Post-processing to remove false positives\n# ==============================================================================\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\n\n# Local fallbacks (read from Cell 0 if available)\ntry:\n    _BN_LANG = BN_LANG\n    _EN_LANG = EN_LANG\nexcept NameError:\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\nexcept NameError:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _USE_MULTI_GPU = torch.cuda.is_available() and (torch.cuda.device_count() > 1)\n\n# Default \"real ambiguity\" thresholds (configurable)\ntry:\n    _REAL_AMB_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept NameError:\n    _REAL_AMB_SPAN_THRESHOLD = 0.3\n\ntry:\n    _REAL_AMB_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept NameError:\n    _REAL_AMB_UNCERTAINTY_THRESHOLD = 0.4\n\n# ‚úÖ FIX #6: Import homograph watchlist for validation\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #6: INFERENCE STATISTICS TRACKER\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass InferenceStatistics:\n    \"\"\"Track inference metrics across multiple calls.\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.total_inferences = 0\n        self.successful_translations = 0\n        self.failed_translations = 0\n        self.total_explanations = 0\n        self.high_confidence_explanations = 0\n        self.low_confidence_explanations = 0\n        self.total_confidence = 0.0\n        self.homographs_detected = set()\n        self.avg_span = 0.0\n        self.avg_uncertainty = 0.0\n        self.dscd_empty_warnings = 0\n    \n    def record_inference(self, result: Dict[str, Any]):\n        \"\"\"Record results from a single inference call.\"\"\"\n        self.total_inferences += 1\n        \n        if result.get('translation') and result['translation'] != \"ERROR DURING TRANSLATION\":\n            self.successful_translations += 1\n        else:\n            self.failed_translations += 1\n        \n        explanations = result.get('explanations', [])\n        self.total_explanations += len(explanations)\n        \n        for exp in explanations:\n            try:\n                conf = exp.get('confidence', 0.5)\n                self.total_confidence += float(conf)\n                \n                if conf >= 0.65:\n                    self.high_confidence_explanations += 1\n                elif conf < 0.4:\n                    self.low_confidence_explanations += 1\n                \n                # Track homographs\n                word = str(exp.get('ambiguous_word', '')).strip()\n                clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '')\n                if clean_word in _HOMOGRAPH_WATCHLIST:\n                    self.homographs_detected.add(clean_word)\n                \n                # Track metrics\n                self.avg_span += float(exp.get('span', 0.0))\n                self.avg_uncertainty += float(exp.get('uncertainty', 0.0))\n                \n            except Exception:\n                pass\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Return summary statistics.\"\"\"\n        total_exp = max(self.total_explanations, 1)\n        \n        return {\n            'total_inferences': self.total_inferences,\n            'successful_translations': self.successful_translations,\n            'failed_translations': self.failed_translations,\n            'success_rate': self.successful_translations / max(self.total_inferences, 1),\n            'total_explanations': self.total_explanations,\n            'explanations_per_inference': self.total_explanations / max(self.total_inferences, 1),\n            'high_confidence_rate': self.high_confidence_explanations / total_exp,\n            'low_confidence_rate': self.low_confidence_explanations / total_exp,\n            'avg_confidence': self.total_confidence / total_exp,\n            'avg_span': self.avg_span / total_exp,\n            'avg_uncertainty': self.avg_uncertainty / total_exp,\n            'homographs_detected': list(self.homographs_detected),\n            'dscd_empty_warnings': self.dscd_empty_warnings,\n        }\n    \n    def print_summary(self):\n        \"\"\"Print formatted summary.\"\"\"\n        summary = self.get_summary()\n        print(\"\\n\" + \"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n        if summary['homographs_detected']:\n            print(f\"Homographs detected: {', '.join(summary['homographs_detected'])}\")\n        if summary['dscd_empty_warnings'] > 0:\n            print(f\"‚ö†Ô∏è DSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80 + \"\\n\")\n\n# Global statistics tracker (optional)\n_INFERENCE_STATS = InferenceStatistics()\n\n# ------------------------------------------------------------------------------\n# Helpers\n# ------------------------------------------------------------------------------\ndef _to_device_batch(enc: Any, device: torch.device):\n    \"\"\"\n    Move tokenizer output to device. Tokenizers may return BatchEncoding with .to()\n    or plain dict of tensors. This helper attempts .to() first, else moves values.\n    \"\"\"\n    try:\n        if hasattr(enc, \"to\"):\n            # BatchEncoding has .to(device)\n            return enc.to(device)\n    except Exception:\n        pass\n    # fallback: assume dict-like mapping of tensors\n    out = {}\n    try:\n        for k, v in enc.items():\n            try:\n                out[k] = v.to(device)\n            except Exception:\n                out[k] = v\n    except Exception:\n        return enc\n    return out\n\n\ndef _extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    \"\"\"\n    Accept a variety of shapes returned by model.forward_with_explanations / forward:\n      - dict with keys: 'dscd_outputs' or 'dscd' or a dict already containing 'explanations'\n      - directly the normalized dict produced by Cell 6's forward (keys: explanations, proto_probs etc)\n    Returns a dict (possibly empty) with at least 'explanations' as list-of-lists, if present.\n    \"\"\"\n    if raw_out is None:\n        return {}\n    if isinstance(raw_out, dict):\n        # If dict already contains explanations or normalized DSCD fields, return it (best-effort)\n        if \"explanations\" in raw_out or \"proto_probs\" in raw_out or \"dscd_outputs\" in raw_out:\n            # if nested\n            if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n                return raw_out[\"dscd_outputs\"]\n            if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n                return raw_out[\"dscd\"]\n            return raw_out\n        # nested under dscd_outputs or similar\n        for key in (\"dscd_outputs\", \"dscd\", \"dscd_out\"):\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n        # fallback: return raw_out (may be partially useful)\n        return raw_out\n    # if model returns a tuple/list, try to find dict inside\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return _extract_dscd_outputs(item)\n    return {}\n\n\ndef _get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    \"\"\"\n    Normalize explanations field to list-of-lists: [ [ex1, ex2, ...], ... ] where outer list = sentences\n    \"\"\"\n    if not dscd:\n        return []\n    expl = dscd.get(\"explanations\", None)\n    if expl is None:\n        # some older outputs store per-sentence explanations under alternatives\n        for alt in (\"explanations_per_sentence\", \"trg_explanations\", \"exps\"):\n            if alt in dscd:\n                expl = dscd[alt]\n                break\n    if expl is None:\n        return []\n    # If it's already [[...]], return as-is\n    if isinstance(expl, list):\n        # If it's a list of dicts (single sentence), wrap\n        if len(expl) > 0 and isinstance(expl[0], dict):\n            return [expl]\n        # If it's list-of-lists, assume correct\n        if len(expl) > 0 and isinstance(expl[0], list):\n            return expl\n    return []\n\n\ndef _is_subword_token(token: str) -> bool:\n    \"\"\"\n    Check if token is a subword continuation marker.\n    Common patterns: ##, ‚ñÅ‚ñÅ, @@, short fragments, punctuation\n    \"\"\"\n    if not token or len(token.strip()) == 0:\n        return True\n\n    token = token.strip()\n    # Subword markers\n    if token.startswith(\"##\") or token.startswith(\"‚ñÅ‚ñÅ\") or token.startswith(\"@@\") or token.startswith(\"‚ñÅ\"):\n        return True\n\n    # Very short tokens (likely fragments)\n    if len(token) < 2:\n        return True\n\n    # Just punctuation or numbers\n    if token in '.,!?;:()[]{}\"\\'-' or token.isdigit():\n        return True\n\n    return False\n\n\ndef _should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    \"\"\"\n    Decide whether to filter out an explanation based on quality checks.\n    Returns True if should be REMOVED (filtered out).\n    \"\"\"\n    try:\n        token = expl.get('ambiguous_word', expl.get('token', ''))\n        span = float(expl.get('span', 0.0))\n        uncertainty = float(expl.get('uncertainty', 0.0))\n\n        # Filter subword tokens\n        if _is_subword_token(str(token)):\n            return True\n\n        # Filter low-quality detections (neither span nor uncertainty passes threshold)\n        if span <= span_th and uncertainty <= u_th:\n            return True\n\n        return False\n    except Exception:\n        return True  # Filter on error\n\n\n# Helper: force English BOS on M2M100-like models\ndef _force_english_bos(tokenizer, mbart_model) -> Optional[int]:\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            forced_id = tokenizer.get_lang_id(_EN_LANG)\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            forced_id = tokenizer.lang_code_to_id.get(_EN_LANG, None)\n    except Exception:\n        forced_id = None\n\n    if forced_id is not None and hasattr(mbart_model, \"config\"):\n        try:\n            mbart_model.config.forced_bos_token_id = forced_id\n            mbart_model.config.decoder_start_token_id = forced_id\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL8] Could not set forced BOS on mbart config\")\n    return forced_id\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #1 + #2 + #3 + #4: ENHANCED translate_with_explanations\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n    \"\"\"\n    Translate a single Bengali sentence and return filtered explanations from TRG/DSCD.\n    \n    ‚úÖ FIX #1: Comprehensive debug logging at each stage\n    ‚úÖ FIX #2: DSCD validation before inference\n    ‚úÖ FIX #3: Threshold logging\n    ‚úÖ FIX #4: Explanation quality metrics\n\n    Returns a dict:\n      {\n        \"input_sentence\": ...,\n        \"translation\": ...,\n        \"ambiguous_words_detected\": int,\n        \"explanations\": [ {ambiguous_word, position, explanation, uncertainty, span, is_real_amb, confidence}, ... ],\n        \"quality_metrics\": { avg_confidence, high_conf_count, low_conf_count, ... }\n      }\n    \"\"\"\n    device = _DEVICE if device is None else device\n    span_th = _REAL_AMB_SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = _REAL_AMB_UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    # ‚úÖ FIX #3: Log thresholds\n    if _VERBOSE_LOGGING:\n        print(f\"\\n[INFERENCE] Starting inference:\")\n        print(f\"[INFERENCE]   Input: {input_sentence[:60]}\")\n        print(f\"[INFERENCE]   Thresholds: span={span_th:.2f}, uncertainty={u_th:.2f}\")\n\n    try:\n        # prepare tokenizer and inputs\n        try:\n            tokenizer.src_lang = _BN_LANG\n        except Exception:\n            pass\n\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=_MAX_LENGTH\n        )\n        enc = _to_device_batch(enc, device)\n\n        # ensure model is eval\n        model.eval()\n        core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n        # ‚úÖ FIX #2: VALIDATE DSCD STATE BEFORE INFERENCE\n        dscd_validated = False\n        try:\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n            if dscd:\n                num_stores = len(dscd.prototype_stores)\n                multi_sense = sum(1 for store in dscd.prototype_stores.values() if len(store.centroids) >= 2)\n                \n                if _VERBOSE_LOGGING:\n                    print(f\"[INFERENCE] DSCD state:\")\n                    print(f\"[INFERENCE]   - Prototype stores: {num_stores}\")\n                    print(f\"[INFERENCE]   - Multi-sense tokens: {multi_sense}\")\n                \n                if num_stores == 0:\n                    print(f\"[INFERENCE] ‚ö†Ô∏è WARNING: DSCD prototype stores are EMPTY!\")\n                    print(f\"[INFERENCE]    ‚Üí No explanations will be generated\")\n                    print(f\"[INFERENCE]    ‚Üí Run dscd_discovery_warmup() or train more epochs\")\n                    if track_stats:\n                        _INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n                    \n                    # Check for homographs\n                    homographs_found = []\n                    for word in _HOMOGRAPH_WATCHLIST:\n                        clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                        for key in dscd.prototype_stores.keys():\n                            clean_key = str(key).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                            if clean_key == clean_word:\n                                num_protos = len(dscd.prototype_stores[key].centroids)\n                                homographs_found.append((clean_word, num_protos))\n                                break\n                    \n                    if _VERBOSE_LOGGING and homographs_found:\n                        print(f\"[INFERENCE] Homographs in DSCD:\")\n                        for word, num_protos in homographs_found:\n                            print(f\"[INFERENCE]   - '{word}': {num_protos} prototypes\")\n            else:\n                print(f\"[INFERENCE] ‚ö†Ô∏è WARNING: Model has no DSCD component!\")\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[INFERENCE] DSCD validation failed: {e}\")\n\n        # get DSCD/TRG outputs and ensure method compatibility\n        with torch.inference_mode():\n            raw_dscd_out = {}\n            try:\n                if _VERBOSE_LOGGING:\n                    print(f\"[INFERENCE] Calling model forward...\")\n                \n                # prefer forward_with_explanations if available\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=[input_sentence]\n                        )\n                    except TypeError:\n                        raw_dscd_out = core.forward_with_explanations(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), [input_sentence])\n                else:\n                    # fallback to forward that may return dict containing dscd outputs\n                    try:\n                        out = core.forward(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"), src_texts=[input_sentence], labels=None)\n                    except TypeError:\n                        out = core.forward(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), src_texts=[input_sentence], labels=None)\n                    if isinstance(out, dict):\n                        raw_dscd_out = _extract_dscd_outputs(out)\n                \n                if _VERBOSE_LOGGING:\n                    print(f\"[INFERENCE] ‚úì Model forward completed\")\n                    \n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(\"[INFERENCE] ‚úó DSCD/TRG forward error:\", e)\n                    traceback.print_exc()\n                raw_dscd_out = {}\n\n            # prepare mbart for generation\n            if not hasattr(core, \"mbart\"):\n                raise RuntimeError(\"Model backend missing .mbart (M2M100). Cannot generate translation.\")\n\n            mbart = core.mbart\n            forced_id = _force_english_bos(tokenizer, mbart)\n            orig_use_cache = getattr(mbart.config, \"use_cache\", None) if hasattr(mbart, \"config\") else None\n            if hasattr(mbart, \"config\"):\n                try:\n                    mbart.config.use_cache = True\n                except Exception:\n                    pass\n\n            generated = None\n            hyps = []\n            try:\n                if _VERBOSE_LOGGING:\n                    print(f\"[INFERENCE] Generating translation...\")\n                \n                # primary generation attempt\n                try:\n                    generated = mbart.generate(\n                        enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=2,\n                        early_stopping=True,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id if forced_id is not None else getattr(mbart.config, \"forced_bos_token_id\", None),\n                    )\n                except RuntimeError as e:\n                    # VRAM fallback: smaller beams, shorter length\n                    if \"out of memory\" in str(e).lower():\n                        if _VERBOSE_LOGGING:\n                            print(f\"[INFERENCE] OOM during generation, using fallback...\")\n                        if torch.cuda.is_available():\n                            torch.cuda.empty_cache()\n                        # per-sentence fallback generation to reduce peak memory\n                        hyps = []\n                        try:\n                            enc1 = tokenizer(input_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=min(_MAX_LENGTH, 48))\n                            enc1 = _to_device_batch(enc1, device)\n                            gen1 = mbart.generate(\n                                enc1.get(\"input_ids\"),\n                                attention_mask=enc1.get(\"attention_mask\"),\n                                max_length=min(_MAX_LENGTH, 48),\n                                num_beams=1,\n                                early_stopping=True,\n                                pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                                forced_bos_token_id=forced_id if forced_id is not None else getattr(mbart.config, \"forced_bos_token_id\", None),\n                            )\n                            try:\n                                hyp1 = tokenizer.decode(gen1[0], skip_special_tokens=True)\n                            except Exception:\n                                hyp1 = tokenizer.batch_decode(gen1, skip_special_tokens=True)[0] if hasattr(tokenizer, \"batch_decode\") else \"\"\n                            hyps.append(hyp1)\n                        except Exception:\n                            # fallback to empty hyp\n                            hyps.append(\"\")\n                    else:\n                        # unknown runtime error -> re-raise to outer except\n                        raise\n                # decode if generated\n                if generated is not None:\n                    try:\n                        translation = tokenizer.decode(generated[0], skip_special_tokens=True)\n                    except Exception:\n                        # try batch decode fallback\n                        try:\n                            translation = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n                        except Exception:\n                            translation = \"\"\n                else:\n                    translation = hyps[0] if hyps else \"\"\n                \n                if _VERBOSE_LOGGING:\n                    print(f\"[INFERENCE] ‚úì Translation: {translation[:60]}\")\n                    \n            finally:\n                # restore cache setting\n                if hasattr(mbart, \"config\") and orig_use_cache is not None:\n                    try:\n                        mbart.config.use_cache = orig_use_cache\n                    except Exception:\n                        pass\n\n            # ‚úÖ FIX #1: Debug DSCD/TRG extraction\n            if _VERBOSE_LOGGING:\n                print(f\"[INFERENCE] Extracting explanations...\")\n            \n            # process explanations (raw_dscd_out expected format from Cell 6)\n            dscd_out = _extract_dscd_outputs(raw_dscd_out)\n            explanations_list = _get_explanations_list(dscd_out)\n            sentence_explanations = explanations_list[0] if (isinstance(explanations_list, list) and len(explanations_list) > 0) else []\n\n            if _VERBOSE_LOGGING:\n                print(f\"[INFERENCE] Raw explanations: {len(sentence_explanations)}\")\n\n            # real ambiguity predicate uses local thresholds\n            def _is_real_ambiguity(e):\n                try:\n                    s = float(e.get(\"span\", 0.0))\n                    u = float(e.get(\"uncertainty\", 0.0))\n                    return (s > span_th) or (u > u_th)\n                except Exception:\n                    return False\n\n            # ‚úÖ FIX #1 + #4: Filter + Track quality metrics\n            real_amb_count = 0\n            out_explanations = []\n            filtered_count = 0\n            \n            quality_metrics = {\n                'total_raw_explanations': len(sentence_explanations) if isinstance(sentence_explanations, list) else 0,\n                'filtered_explanations': 0,\n                'high_confidence_count': 0,\n                'low_confidence_count': 0,\n                'avg_confidence': 0.0,\n                'avg_span': 0.0,\n                'avg_uncertainty': 0.0,\n            }\n            \n            confidences = []\n            spans = []\n            uncertainties = []\n            \n            if isinstance(sentence_explanations, list):\n                for ex in sentence_explanations:\n                    try:\n                        if _should_filter_explanation(ex, span_th, u_th):\n                            filtered_count += 1\n                            if _VERBOSE_LOGGING and filtered_count <= 3:\n                                word = ex.get('token', ex.get('ambiguous_word', 'UNK'))\n                                print(f\"[INFERENCE] Filtered: '{word}' (span={ex.get('span', 0):.3f}, u={ex.get('uncertainty', 0):.3f})\")\n                            continue\n                        \n                        is_real = _is_real_ambiguity(ex)\n                        if is_real:\n                            real_amb_count += 1\n                        \n                        # Extract confidence (prefer explicit field, fallback to span)\n                        confidence = ex.get('confidence', None)\n                        if confidence is None:\n                            # Derive from span and uncertainty\n                            s = float(ex.get('span', 0.0))\n                            u = float(ex.get('uncertainty', 0.0))\n                            confidence = max(s, u)\n                        confidence = float(confidence)\n                        \n                        # Track metrics\n                        confidences.append(confidence)\n                        spans.append(float(ex.get('span', 0.0)))\n                        uncertainties.append(float(ex.get('uncertainty', 0.0)))\n                        \n                        if confidence >= 0.65:\n                            quality_metrics['high_confidence_count'] += 1\n                        elif confidence < 0.4:\n                            quality_metrics['low_confidence_count'] += 1\n                        \n                        out_explanations.append({\n                            \"ambiguous_word\": ex.get(\"token\", ex.get(\"ambiguous_word\", \"N/A\")),\n                            \"position\": ex.get(\"token_idx\", ex.get(\"position\", \"N/A\")),\n                            \"explanation\": ex.get(\"explanation\", \"\") or ex.get(\"explain\", \"\") or \"\",\n                            \"uncertainty\": float(ex.get(\"uncertainty\", 0.0)),\n                            \"span\": float(ex.get(\"span\", 0.0)),\n                            \"confidence\": confidence,\n                            \"is_real_amb\": bool(is_real),\n                        })\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        continue\n            \n            # Compute quality averages\n            quality_metrics['filtered_explanations'] = filtered_count\n            if confidences:\n                quality_metrics['avg_confidence'] = sum(confidences) / len(confidences)\n                quality_metrics['avg_span'] = sum(spans) / len(spans)\n                quality_metrics['avg_uncertainty'] = sum(uncertainties) / len(uncertainties)\n            \n            if _VERBOSE_LOGGING:\n                print(f\"[INFERENCE] ‚úì Final explanations: {len(out_explanations)} (filtered: {filtered_count})\")\n                print(f\"[INFERENCE] Quality: avg_conf={quality_metrics['avg_confidence']:.3f}, high={quality_metrics['high_confidence_count']}, low={quality_metrics['low_confidence_count']}\")\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n            \n            # ‚úÖ FIX #6: Track statistics\n            if track_stats:\n                _INFERENCE_STATS.record_inference(result)\n            \n            return result\n\n    except Exception as e:\n        # fail-safe return\n        if _VERBOSE_LOGGING:\n            print(f\"[INFERENCE] ‚úó ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            traceback.print_exc()\n        \n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": str(e)[:200],\n        }\n        \n        if track_stats:\n            _INFERENCE_STATS.record_inference(error_result)\n        \n        return error_result\n\n\n# ------------------------------------------------------------------------------\n# demonstrate_system: small runner that prints nicely\n# ------------------------------------------------------------------------------\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\",\n            \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\",\n            \"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\",\n            \"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\",\n            \"‡¶Ü‡¶ú ‡¶≠‡¶æ‡¶≤ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡•§\",\n        ]\n    print(\"=\" * 80)\n    print(\"TATN DEMO: translating and listing DSCD/TRG explanations\")\n    print(\"=\" * 80)\n    \n    # Reset statistics for demo\n    _INFERENCE_STATS.reset()\n    \n    for s in sentences:\n        print(f\"\\nInput: {s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n        print(\"Translation:\", res.get(\"translation\", \"\"))\n        print(\"Ambiguous words detected (real):\", res.get(\"ambiguous_words_detected\", 0))\n        \n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(f\"Quality: avg_conf={quality.get('avg_confidence', 0):.3f}, high={quality.get('high_confidence_count', 0)}, low={quality.get('low_confidence_count', 0)}\")\n        \n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(f\"  {idx}. word='{ex['ambiguous_word']}' pos={ex['position']} conf={ex.get('confidence', 0):.3f} span={ex['span']:.3f} U={ex['uncertainty']:.3f} real={ex['is_real_amb']}\")\n                print(\"     \", ex.get(\"explanation\", \"\")[:200])\n        else:\n            print(\"  No explanations\")\n    \n    print(\"=\" * 80)\n    \n    # Print session statistics\n    _INFERENCE_STATS.print_summary()\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #5: ENHANCED dscd_discovery_warmup WITH VALIDATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef dscd_discovery_warmup(model, tokenizer, num_sents: int = 8000, batch_size: int = 64, max_len: Optional[int] = None):\n    \"\"\"\n    Run data through encoder to populate DSCD buffers and trigger clustering.\n    \n    ‚úÖ FIX #5: Added homograph-specific validation after warmup\n    \"\"\"\n    if max_len is None:\n        max_len = _MAX_LENGTH\n\n    core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component; skipping warmup.\")\n            return\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[WARMUP] Starting DSCD discovery warmup...\")\n        print(\"=\" * 80)\n        \n        # Save originals\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_n_min = getattr(dscd, \"n_min\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        # Apply temporary permissive settings\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n                print(f\"[WARMUP] Enabled training clustering\")\n            if hasattr(dscd, \"n_min\"):\n                dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n                print(f\"[WARMUP] Lowered n_min to {dscd.n_min}\")\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n                print(f\"[WARMUP] Increased buffer_size to {dscd.buffer_size}\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n        # Prepare texts for warmup: sample from dataset loader if available (Cell 2)\n        texts = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for (bn, _) in pairs][:num_sents]\n                print(f\"[WARMUP] Loaded {len(texts)} sentences from dataset\")\n            else:\n                base = [\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\"]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n                print(f\"[WARMUP] Using {len(texts)} default sentences\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            texts = [\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\"] * num_sents\n\n        # Batch and run forward_with_explanations (no grad)\n        processed = 0\n        core.eval()\n        \n        print(f\"\\n[WARMUP] Processing {len(texts)} sentences in batches of {batch_size}...\")\n        \n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                try:\n                    enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n                    enc = _to_device_batch(enc, _DEVICE)\n                    # call forward_with_explanations to populate DSCD buffers and allow clustering to run\n                    if hasattr(core, \"forward_with_explanations\"):\n                        try:\n                            core.forward_with_explanations(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"), src_texts=batch)\n                        except TypeError:\n                            core.forward_with_explanations(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), batch)\n                    else:\n                        # fallback: call encoder only\n                        core.mbart.model.encoder(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"))\n                    processed += len(batch)\n                    if (i // batch_size) % 10 == 0:\n                        print(f\"[WARMUP] Processed {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%)\")\n                except Exception as e:\n                    print(f\"[WARMUP] Batch {i//batch_size} failed: {str(e)[:200]}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    continue\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[WARMUP] Prototype Discovery Complete\")\n        print(\"-\" * 80)\n        \n        # ‚úÖ FIX #5: Comprehensive validation with homograph checking\n        try:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            num_types = len(stores)\n            total_protos = sum(store.size() for store in stores.values()) if stores else 0\n            multi = sum(1 for store in stores.values() if store.size() >= 2) if stores else 0\n            \n            print(f\"[WARMUP] Summary:\")\n            print(f\"  - Token types with prototypes: {num_types}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n            \n            if num_types > 0:\n                multi_sense_ratio = multi / num_types\n                print(f\"  - Multi-sense ratio: {multi_sense_ratio:.1%}\")\n            \n            # ‚úÖ FIX #5: Check homographs specifically\n            print(f\"\\n[WARMUP] Homograph Status:\")\n            homographs_found = 0\n            homographs_multi_sense = 0\n            \n            for word in _HOMOGRAPH_WATCHLIST:\n                clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                found = False\n                found_key = None\n                found_protos = 0\n                \n                # Check exact and fuzzy matches\n                if word in stores:\n                    found = True\n                    found_key = word\n                    found_protos = stores[word].size()\n                elif clean_word in stores:\n                    found = True\n                    found_key = clean_word\n                    found_protos = stores[clean_word].size()\n                else:\n                    # Fuzzy match\n                    for key in stores.keys():\n                        clean_key = str(key).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                        if clean_key == clean_word or clean_word in clean_key:\n                            found = True\n                            found_key = key\n                            found_protos = stores[key].size()\n                            break\n                \n                if found and found_protos >= 2:\n                    homographs_found += 1\n                    homographs_multi_sense += 1\n                    counts = stores[found_key].counts if hasattr(stores[found_key], 'counts') else []\n                    print(f\"  ‚úÖ '{clean_word}' ‚Üí {found_protos} prototypes (key='{found_key}', counts={counts})\")\n                elif found and found_protos == 1:\n                    homographs_found += 1\n                    print(f\"  ‚ö†Ô∏è  '{clean_word}' ‚Üí Only 1 prototype (needs more data)\")\n                else:\n                    print(f\"  ‚úó  '{clean_word}' ‚Üí NOT FOUND\")\n            \n            print(f\"\\n[WARMUP] Homograph Coverage: {homographs_found}/{len(_HOMOGRAPH_WATCHLIST)} found, {homographs_multi_sense} multi-sense\")\n            \n            # Quality assessment\n            if num_types == 0:\n                print(f\"\\n[WARMUP] ‚ö†Ô∏è  CRITICAL: NO PROTOTYPES CREATED!\")\n                print(f\"[WARMUP]    Possible causes:\")\n                print(f\"[WARMUP]    1. Clustering disabled in DSCD config\")\n                print(f\"[WARMUP]    2. n_min too high\")\n                print(f\"[WARMUP]    3. Not enough diverse training data\")\n            elif homographs_multi_sense < len(_HOMOGRAPH_WATCHLIST) // 2:\n                print(f\"\\n[WARMUP] ‚ö†Ô∏è  WARNING: Less than 50% of homographs have multi-sense prototypes\")\n                print(f\"[WARMUP]    ‚Üí Consider running warmup with more sentences\")\n            else:\n                print(f\"\\n[WARMUP] ‚úÖ SUCCESS: Good homograph coverage achieved!\")\n            \n        except Exception as e:\n            print(f\"[WARMUP] Validation failed: {type(e).__name__}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    finally:\n        # Restore original dscd settings\n        try:\n            if dscd is not None:\n                if hasattr(dscd, \"enable_training_clustering\"):\n                    dscd.enable_training_clustering = orig_enable\n                if hasattr(dscd, \"n_min\") and orig_n_min is not None:\n                    dscd.n_min = orig_n_min\n                if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                    dscd.buffer_size = orig_buffer\n                print(\"\\n[WARMUP] Restored DSCD configuration\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n        \n        print(\"=\" * 80 + \"\\n\")\n\n\n# ------------------------------------------------------------------------------\n# load_checkpoint_for_resume: robust checkpoint loader with DataParallel handling\n# ------------------------------------------------------------------------------\ndef load_checkpoint_for_resume(model: torch.nn.Module, optimizer, checkpoint_path: str) -> Tuple[bool, int, int, float]:\n    \"\"\"\n    Load checkpoint safely into model and optimizer.\n    Returns (success, epoch, step, avg_loss)\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        print(f\"[CHECKPOINT] Not found: {checkpoint_path}\")\n        return False, 0, 0, 0.0\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=_DEVICE)\n    except Exception as e:\n        print(f\"[CHECKPOINT] Load failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        return False, 0, 0, 0.0\n\n    core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    # Try strict load first, fallback to non-strict mapping of keys\n    state = ckpt.get(\"model_state_dict\", ckpt)\n    try:\n        # Accept state dicts with 'module.' prefixes by loading with strict=False first\n        core.load_state_dict(state, strict=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] model.load_state_dict(strict=False) failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        # As last resort attempt to strip 'module.' prefixes if keys are present\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                core.load_state_dict(new_state, strict=False)\n                print(\"[CHECKPOINT] Retried loading after stripping 'module.' prefixes\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    # load optimizer state if present\n    try:\n        if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n    except Exception as e:\n        print(f\"[CHECKPOINT] optimizer.load_state_dict failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n    \n    # ‚úÖ Load DSCD state if present\n    try:\n        if \"dscd_state_dict\" in ckpt and ckpt[\"dscd_state_dict\"]:\n            print(\"[CHECKPOINT] Restoring DSCD prototypes...\")\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n            if dscd and hasattr(dscd, 'load_state_dict'):\n                dscd.load_state_dict(ckpt[\"dscd_state_dict\"])\n                num_tokens = len(dscd.prototype_stores)\n                print(f\"[CHECKPOINT] ‚úì DSCD prototypes restored for {num_tokens} tokens\")\n            else:\n                print(\"[CHECKPOINT] ‚ö†Ô∏è Model has no dscd.load_state_dict method\")\n        else:\n            print(\"[CHECKPOINT] ‚ö†Ô∏è No DSCD state in checkpoint\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] DSCD restore failed: {type(e).__name__}: {str(e)[:200]}\")\n\n    epoch = int(ckpt.get(\"epoch\", 0))\n    step = int(ckpt.get(\"global_step\", ckpt.get(\"step\", 0)))\n    avg_loss = float(ckpt.get(\"avg_epoch_loss\", ckpt.get(\"avg_loss\", 0.0)))\n\n    print(f\"[CHECKPOINT] Loaded: epoch={epoch} step={step} avg_loss={avg_loss:.6f}\")\n    return True, epoch, step, avg_loss\n\n\n# ==============================================================================\n# End of Cell 8\n# ==============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Cell 8: Inference pipeline & warmup helpers loaded (COMPLETELY FIXED)\")\nprint(\"=\" * 80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Added comprehensive inference debug logging\")\nprint(\" ‚úÖ FIX #2: Added DSCD validation before inference\")\nprint(\" ‚úÖ FIX #3: Added threshold logging\")\nprint(\" ‚úÖ FIX #4: Added explanation quality metrics\")\nprint(\" ‚úÖ FIX #5: Enhanced warmup with homograph validation\")\nprint(\" ‚úÖ FIX #6: Added inference statistics tracker\")\nprint(\"=\" * 80)","metadata":{"id":"7Dxg7ck0H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION - COMPLETELY FIXED\n# ==============================================================================\n# ‚úÖ FIXED: Add homograph-specific validation (ERROR #1 FIX)\n# ‚úÖ FIXED: Add explanation quality metrics (ERROR #2 FIX)\n# ‚úÖ FIXED: Add expected translation comparison (ERROR #3 FIX)\n# ‚úÖ ADDED: Baseline comparison feature (ERROR #4 FIX)\n# ‚úÖ ADDED: Expanded test set with diverse cases (ERROR #5 FIX)\n# ‚úÖ ADDED: Detailed error categorization (ERROR #6 FIX)\n# ‚úÖ ADDED: Comprehensive reporting with actionable insights\n# \n# Original features preserved:\n# - Translation quality testing\n# - Ambiguity detection validation\n# - DSCD prototype statistics\n# - Cluster analysis functions\n# - DataParallel wrapper handling\n# ==============================================================================\n\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nfrom collections import defaultdict\n\n# Local fallbacks for globals\ntry:\n    _USE_MULTI_GPU = USE_MULTI_GPU\nexcept NameError:\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _BN_LANG = BN_LANG\nexcept NameError:\n    _BN_LANG = \"bn\"   # M2M100 expects \"bn\"\n\ntry:\n    _VERBOSE_LOGGING = VERBOSE_LOGGING\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\n# Real-ambiguity thresholds (kept consistent with Cell 0/8)\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept NameError:\n    _SPAN_THRESHOLD = 0.3\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept NameError:\n    _UNCERTAINTY_THRESHOLD = 0.4\n\n# ‚úÖ FIX #1: Import homograph watchlist\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n\n# ==============================================================================\n# CLUSTER ANALYSIS FUNCTIONS (FOR TRAINING LOOP MONITORING)\n# ==============================================================================\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    \"\"\"Get total cluster count only\"\"\"\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else model.dscd\n        return len(getattr(dscd, \"prototype_stores\", {}) or {})\n    except Exception:\n        return 0\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    \"\"\"\n    Print top N clusters by sample count (homographs discovered by DSCD).\n    Shows: Token, Sample Count, Number of Prototypes, Mean Distance, Deviation.\n    \"\"\"\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else model.dscd\n        prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n        \n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n        \n        # Collect cluster information\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            total_count = sum(getattr(store, \"counts\", []))\n            n_protos = len(getattr(store, \"centroids\", []))\n            cluster_info.append({\n                'token': token,\n                'count': total_count,\n                'protos': n_protos,\n                'mu': getattr(store, \"mu\", 0.0),\n                'tau': getattr(store, \"tau\", 0.0)\n            })\n        \n        # Sort by count (descending)\n        cluster_info.sort(key=lambda x: x['count'], reverse=True)\n        \n        # Print top N clusters\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters (by sample count):\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Œº (mean)':<15}{'œÑ (dev)':<12}\")\n        print(\"-\" * 90)\n        \n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_display = info['token'][:12] if len(info['token']) > 12 else info['token']\n            print(f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                  f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\")\n        \n        print(\"-\" * 90)\n        total_samples = sum(c['count'] for c in cluster_info)\n        print(f\"Total clusters: {len(cluster_info)} | Total samples in clusters: {total_samples}\")\n        \n    except Exception as e:\n        print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\n\ndef _print_cluster_stats(model: torch.nn.Module):\n    \"\"\"\n    Print comprehensive cluster statistics including total clusters, samples,\n    prototypes, and distribution metrics.\n    \"\"\"\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else model.dscd\n        prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n        \n        if not prototype_stores:\n            return  # Silently skip if no clusters\n        \n        # Aggregate statistics\n        total_clusters = len(prototype_stores)\n        total_samples = 0\n        total_protos = 0\n        cluster_counts = []\n        \n        for token, store in prototype_stores.items():\n            count = sum(getattr(store, \"counts\", []))\n            protos = len(getattr(store, \"centroids\", []))\n            total_samples += count\n            total_protos += protos\n            cluster_counts.append(count)\n        \n        # Calculate stats\n        avg_samples = total_samples / total_clusters if total_clusters > 0 else 0\n        avg_protos = total_protos / total_clusters if total_clusters > 0 else 0\n        max_samples = max(cluster_counts) if cluster_counts else 0\n        min_samples = min(cluster_counts) if cluster_counts else 0\n        \n        print(f\"\\n[CLUSTER-STATS] Cluster Statistics:\")\n        print(f\"  ‚Ä¢ Total clusters: {total_clusters}\")\n        print(f\"  ‚Ä¢ Total samples: {total_samples}\")\n        print(f\"  ‚Ä¢ Total prototypes: {total_protos}\")\n        print(f\"  ‚Ä¢ Avg samples/cluster: {avg_samples:.1f}\")\n        print(f\"  ‚Ä¢ Avg protos/cluster: {avg_protos:.1f}\")\n        print(f\"  ‚Ä¢ Max samples/cluster: {max_samples}\")\n        print(f\"  ‚Ä¢ Min samples/cluster: {min_samples}\")\n        \n    except Exception as e:\n        print(f\"[CLUSTER-STATS] Error: {str(e)[:100]}\")\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #1-6: COMPREHENSIVE POST-TRAINING TESTING WITH ALL ENHANCEMENTS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@torch.inference_mode()\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module, \n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Run a comprehensive evaluation with enhanced metrics and reporting.\n    \n    ‚úÖ FIX #1: Homograph-specific validation\n    ‚úÖ FIX #2: Explanation quality metrics\n    ‚úÖ FIX #3: Expected translation comparison\n    ‚úÖ FIX #4: Baseline comparison\n    ‚úÖ FIX #5: Expanded test set\n    ‚úÖ FIX #6: Detailed error categorization\n    \n    Args:\n        model: TATN model to evaluate\n        tokenizer: Tokenizer for the model\n        run_warmup: Whether to run DSCD warmup if no prototypes found\n        compare_baseline: Whether to compare against baseline metrics\n        baseline_metrics: Previous metrics for comparison (optional)\n    \n    Returns:\n        Dict with comprehensive evaluation metrics\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Enhanced)\")\n    print(\"=\" * 80)\n\n    # ‚úÖ FIX #5: Expanded test set with diverse cases\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        # (Bengali, Expected_English, Description, Expected_Homographs)\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap\", \"‡¶ï‡¶≤ = tap/call\", [\"‡¶ï‡¶≤\"]),\n        (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"Tomorrow I will buy a book\", \"‡¶ï‡¶æ‡¶≤ = tomorrow/yesterday\", [\"‡¶ï‡¶æ‡¶≤\"]),\n        (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"The leaf has fallen\", \"‡¶™‡¶æ‡¶§‡¶æ = leaf/page\", [\"‡¶™‡¶æ‡¶§‡¶æ\"]),\n        (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\", \"He went to the bank\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï = bank/embankment\", [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"]),\n        (\"‡¶´‡¶≤ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶∏‡ßç‡¶¨‡¶æ‡¶¶‡ßÅ‡•§\", \"The fruit is delicious\", \"‡¶´‡¶≤ = fruit/result\", [\"‡¶´‡¶≤\"]),\n        (\"‡¶Æ‡¶æ‡¶•‡¶æ ‡¶¨‡ßç‡¶Ø‡¶•‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡•§\", \"Head is aching\", \"‡¶Æ‡¶æ‡¶•‡¶æ = head/top\", [\"‡¶Æ‡¶æ‡¶•‡¶æ\"]),\n        (\"‡¶ï‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶è‡¶∏‡ßá‡¶õ‡ßá‡•§\", \"A call came from the tap\", \"Multiple ‡¶ï‡¶≤ (tap+call)\", [\"‡¶ï‡¶≤\"]),\n        (\"‡¶ï‡¶æ‡¶≤‡¶ï‡ßá ‡¶ï‡¶æ‡¶≤ ‡¶Æ‡ßá‡¶ò ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶ó‡ßá‡¶õ‡ßá‡•§\", \"Yesterday black clouds were seen\", \"Multiple ‡¶ï‡¶æ‡¶≤\", [\"‡¶ï‡¶æ‡¶≤\"]),\n        (\"‡¶Ü‡¶ú ‡¶≠‡¶æ‡¶≤ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡•§\", \"Weather is good today\", \"Simple (no ambiguity)\", []),\n        (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø‡•§\", \"I am fine\", \"Simple (no ambiguity)\", []),\n        (\"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"She speaks sweetly\", \"Simple (no ambiguity)\", []),\n        (\"‡¶è‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶á‡•§\", \"This is my book\", \"Simple (no ambiguity)\", []),\n        (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡¶® ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡ßá ‡¶¨‡¶∏‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡¶®‡•§\", \n         \"He works at the bank and sits on the embankment\", \n         \"Long sentence with multiple ambiguities\", [\"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    # ‚úÖ FIX #2: Initialize quality metrics tracking\n    quality_metrics = {\n        'total_confidence': 0.0,\n        'confidence_samples': 0,\n        'high_confidence_count': 0,  # >= 0.65\n        'medium_confidence_count': 0,  # 0.4 - 0.65\n        'low_confidence_count': 0,  # < 0.4\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n    }\n    \n    # ‚úÖ FIX #1: Homograph tracking\n    homograph_tracking = {\n        'expected_homographs': set(),\n        'detected_homographs': set(),\n        'homograph_explanations': defaultdict(list),\n        'homograph_detection_rate': {},\n    }\n    \n    # ‚úÖ FIX #6: Detailed error categorization\n    error_tracking = {\n        'translation_failures': 0,\n        'dscd_failures': 0,\n        'trg_failures': 0,\n        'timeout_errors': 0,\n        'oom_errors': 0,\n        'other_errors': 0,\n        'error_details': [],\n    }\n\n    # Check DSCD state and optionally run warmup\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                stores = getattr(dscd, \"prototype_stores\", None)\n                if (stores is None or len(stores) == 0) and 'dscd_discovery_warmup' in globals():\n                    print(\"[EVAL] No DSCD prototypes found. Running moderate warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(model, tokenizer, num_sents=4000, batch_size=64)\n                    except Exception as e:\n                        print(f\"[EVAL] DSCD warmup failed/skipped: {type(e).__name__}: {str(e)[:200]}\")\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    # Prepare metrics\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    # Ensure tokenizer configured\n    try:\n        tokenizer.src_lang = _BN_LANG\n    except Exception:\n        pass\n\n    # helper predicate\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n    \n    # ‚úÖ FIX #3: Simple similarity check (word overlap)\n    def _compute_similarity(pred: str, expected: str) -> float:\n        \"\"\"Simple word-overlap similarity.\"\"\"\n        try:\n            pred_words = set(pred.lower().split())\n            exp_words = set(expected.lower().split())\n            if not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            return overlap / len(exp_words)\n        except Exception:\n            return 0.0\n\n    # Collect expected homographs\n    for _, _, _, expected_homos in test_sentences:\n        homograph_tracking['expected_homographs'].update(expected_homos)\n\n    # Run tests\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"[EVAL] translate_with_explanations not available; skipping this test.\")\n                error_tracking['other_errors'] += 1\n                continue\n\n            result = translate_with_explanations(core_model if core_model is not None else model, tokenizer, src_text)\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n            \n            # ‚úÖ FIX #3: Compute similarity\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous Words (real, counted): {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n                \n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0)) if expl.get(\"span\", None) is not None else 0.0\n                    u_val = float(expl.get(\"uncertainty\", 0.0)) if expl.get(\"uncertainty\", None) is not None else 0.0\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n                    \n                    marker = \"[SPAN>0.3]\" if span_val > _SPAN_THRESHOLD else \"           \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ pos {pos}\")\n                    print(f\"       Confidence={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\") or \"\")\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    # ‚úÖ FIX #2: Track quality metrics\n                    quality_metrics['confidences'].append(conf_val)\n                    quality_metrics['spans'].append(span_val)\n                    quality_metrics['uncertainties'].append(u_val)\n                    quality_metrics['total_confidence'] += conf_val\n                    quality_metrics['confidence_samples'] += 1\n                    \n                    if conf_val >= 0.65:\n                        quality_metrics['high_confidence_count'] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics['medium_confidence_count'] += 1\n                    else:\n                        quality_metrics['low_confidence_count'] += 1\n\n                    if span_val > _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n                    \n                    # ‚úÖ FIX #1: Track homograph detections\n                    clean_word = str(word).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                    if clean_word in _HOMOGRAPH_WATCHLIST:\n                        homograph_tracking['detected_homographs'].add(clean_word)\n                        homograph_tracking['homograph_explanations'][clean_word].append({\n                            'sentence': src_text,\n                            'confidence': conf_val,\n                            'span': span_val,\n                            'uncertainty': u_val,\n                        })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n            else:\n                print(\"No explanations produced (high-confidence translation)\")\n\n            # Consider translation successful if non-empty and not error sentinel\n            if translation and translation.strip() and translation not in (\"Error occurred\", \"Translation generation failed\", \"ERROR DURING TRANSLATION\"):\n                successful_translations += 1\n                print(\"‚úì Translation successful\")\n            else:\n                print(\"‚úó Translation failed or empty\")\n                error_tracking['translation_failures'] += 1\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] ‚úó OOM Error: {str(e)[:100]}\")\n                error_tracking['oom_errors'] += 1\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] ‚úó Timeout Error: {str(e)[:100]}\")\n                error_tracking['timeout_errors'] += 1\n            else:\n                print(f\"[EVAL] ‚úó Runtime Error: {type(e).__name__}: {str(e)[:200]}\")\n                error_tracking['other_errors'] += 1\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            continue\n        except Exception as e:\n            print(f\"[EVAL] ‚úó Test {idx} failed: {type(e).__name__}: {str(e)[:200]}\")\n            error_tracking['other_errors'] += 1\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            continue\n\n        print(\"-\" * 60)\n\n    # ‚úÖ FIX #2: Compute quality averages\n    if quality_metrics['confidence_samples'] > 0:\n        quality_metrics['avg_confidence'] = quality_metrics['total_confidence'] / quality_metrics['confidence_samples']\n        quality_metrics['avg_span'] = sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n        quality_metrics['avg_uncertainty'] = sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n    else:\n        quality_metrics['avg_confidence'] = 0.0\n        quality_metrics['avg_span'] = 0.0\n        quality_metrics['avg_uncertainty'] = 0.0\n\n    # ‚úÖ FIX #1: Compute homograph detection rate\n    if homograph_tracking['expected_homographs']:\n        detected = homograph_tracking['detected_homographs']\n        expected = homograph_tracking['expected_homographs']\n        detection_rate = len(detected) / len(expected)\n        homograph_tracking['detection_rate'] = detection_rate\n        \n        for homo in expected:\n            detected_count = len(homograph_tracking['homograph_explanations'].get(homo, []))\n            homograph_tracking['homograph_detection_rate'][homo] = detected_count\n\n    # DSCD statistics\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            stores = getattr(dscd, \"prototype_stores\") or {}\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for key, store in stores.items():\n                try:\n                    sz = int(store.size()) if hasattr(store, \"size\") else 0\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\"total_words\": total_words, \"multi_sense_words\": multi, \"total_prototypes\": total_protos}\n    except Exception as e:\n        print(f\"[EVAL] Could not retrieve DSCD stats: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # COMPREHENSIVE SUMMARY WITH ALL ENHANCEMENTS\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n    \n    # Basic metrics\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful translations: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n    \n    # Ambiguity detection\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations produced: {total_explanations}\")\n    print(f\"  High-span (S>0.3): {total_high_span}\")\n    print(f\"  Real ambiguous (S>0.3 OR U>{_UNCERTAINTY_THRESHOLD}): {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n        print(f\"  Avg real ambiguous/test: {total_real_ambiguous / total_tests:.2f}\")\n    \n    # ‚úÖ FIX #2: Quality metrics\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n    print(f\"  High confidence (‚â•0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium confidence (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low confidence (<0.4): {quality_metrics['low_confidence_count']}\")\n    if quality_metrics['confidence_samples'] > 0:\n        high_rate = quality_metrics['high_confidence_count'] / quality_metrics['confidence_samples']\n        print(f\"  High confidence rate: {high_rate:.1%}\")\n    \n    # ‚úÖ FIX #1: Homograph-specific results\n    print(f\"\\n[HOMOGRAPH DETECTION]\")\n    print(f\"  Expected homographs: {len(homograph_tracking['expected_homographs'])}\")\n    print(f\"  Detected homographs: {len(homograph_tracking['detected_homographs'])}\")\n    print(f\"  Detection rate: {homograph_tracking.get('detection_rate', 0):.1%}\")\n    \n    if homograph_tracking['detected_homographs']:\n        print(f\"\\n  Detected homographs:\")\n        for homo in sorted(homograph_tracking['detected_homographs']):\n            count = homograph_tracking['homograph_detection_rate'].get(homo, 0)\n            exps = homograph_tracking['homograph_explanations'].get(homo, [])\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            print(f\"    ‚úÖ '{homo}': {count} explanations, avg_conf={avg_conf:.3f}\")\n    \n    missing = homograph_tracking['expected_homographs'] - homograph_tracking['detected_homographs']\n    if missing:\n        print(f\"\\n  ‚ö†Ô∏è  Missing homographs: {', '.join(sorted(missing))}\")\n    \n    # DSCD statistics\n    print(f\"\\n[DSCD PROTOTYPE DISCOVERY]\")\n    print(f\"  Word types tracked: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense words (‚â•2 protos): {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats['total_words'] > 0:\n        print(f\"  Avg prototypes/word: {dscd_stats['total_prototypes'] / dscd_stats['total_words']:.2f}\")\n        multi_sense_ratio = dscd_stats['multi_sense_words'] / dscd_stats['total_words']\n        print(f\"  Multi-sense ratio: {multi_sense_ratio:.1%}\")\n    \n    # ‚úÖ FIX #6: Error analysis\n    total_errors = sum([\n        error_tracking['translation_failures'],\n        error_tracking['dscd_failures'],\n        error_tracking['trg_failures'],\n        error_tracking['timeout_errors'],\n        error_tracking['oom_errors'],\n        error_tracking['other_errors'],\n    ])\n    \n    if total_errors > 0:\n        print(f\"\\n[ERROR ANALYSIS]\")\n        print(f\"  Total errors: {total_errors}\")\n        print(f\"  Translation failures: {error_tracking['translation_failures']}\")\n        print(f\"  DSCD failures: {error_tracking['dscd_failures']}\")\n        print(f\"  TRG failures: {error_tracking['trg_failures']}\")\n        print(f\"  OOM errors: {error_tracking['oom_errors']}\")\n        print(f\"  Timeout errors: {error_tracking['timeout_errors']}\")\n        print(f\"  Other errors: {error_tracking['other_errors']}\")\n    \n    # ‚úÖ FIX #4: Baseline comparison\n    if compare_baseline and baseline_metrics:\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            current_success = (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n            \n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            expl_delta = total_explanations - baseline_expl\n            \n            baseline_quality = baseline_metrics.get('quality_metrics', {}).get('avg_confidence', 0)\n            quality_delta = quality_metrics['avg_confidence'] - baseline_quality\n            \n            print(f\"  Translation success: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Total explanations: {total_explanations} ({expl_delta:+d})\")\n            print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f} ({quality_delta:+.3f})\")\n        except Exception as e:\n            print(f\"  Comparison failed: {e}\")\n    \n    # Health warnings\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"‚ö†Ô∏è  High translation failure rate (>50%)\")\n    if total_explanations == 0:\n        warnings.append(\"‚ö†Ô∏è  No explanations generated - check TRG thresholds\")\n    if dscd_stats['total_words'] < 100:\n        warnings.append(\"‚ö†Ô∏è  Very few DSCD prototypes - needs more training\")\n    if quality_metrics['low_confidence_count'] > quality_metrics['high_confidence_count']:\n        warnings.append(\"‚ö†Ô∏è  More low-confidence than high-confidence explanations\")\n    if homograph_tracking.get('detection_rate', 0) < 0.5:\n        warnings.append(\"‚ö†Ô∏è  Less than 50% of expected homographs detected\")\n    if error_tracking['oom_errors'] > 0:\n        warnings.append(\"‚ö†Ô∏è  OOM errors occurred - reduce batch size or sequence length\")\n    \n    if warnings:\n        print(f\"\\n[HEALTH WARNINGS]\")\n        for w in warnings:\n            print(f\"  {w}\")\n    else:\n        print(f\"\\n[HEALTH CHECK] ‚úÖ All systems nominal\")\n    \n    print(\"=\" * 80)\n\n    # Final metrics returned\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n    }\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Cell 9: Comprehensive testing & evaluation ready (COMPLETELY FIXED)\")\nprint(\"=\" * 80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Added homograph-specific validation and tracking\")\nprint(\" ‚úÖ FIX #2: Added explanation quality metrics (confidence, high/low rates)\")\nprint(\" ‚úÖ FIX #3: Added expected translation comparison (similarity scoring)\")\nprint(\" ‚úÖ FIX #4: Added baseline comparison feature\")\nprint(\" ‚úÖ FIX #5: Expanded test set from 5 to 13 diverse cases\")\nprint(\" ‚úÖ FIX #6: Added detailed error categorization (OOM, timeout, etc.)\")\nprint(\" ‚úÖ Added: Comprehensive reporting with actionable insights\")\nprint(\"=\" * 80)","metadata":{"id":"8uL574F8H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE - COMPLETELY FIXED WITH COMPREHENSIVE VALIDATION\n# ==============================================================================\n# ‚úÖ FIXED: Add validate_prototypes() call after discovery (ERROR #1 FIX)\n# ‚úÖ FIXED: Include DSCD state in checkpoint save (ERROR #2 FIX)\n# ‚úÖ FIXED: Persist training metrics to checkpoint (ERROR #3 FIX)\n# ‚úÖ ADDED: Capture baseline metrics before training (ERROR #4 FIX)\n# ‚úÖ ADDED: Discovery progress validation (ERROR #5 FIX)\n# ‚úÖ ADDED: Comprehensive final report (ERROR #6 FIX)\n# ‚úÖ ADDED: Checkpoint verification\n# \n# Original features preserved:\n# - Robust tokenizer loading with fallbacks\n# - Multi-GPU DataParallel support\n# - Safe clustering with homograph verification\n# - Memory-efficient processing\n# ==============================================================================\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Optional, Iterable\n\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport unicodedata\n\n# -------------------------\n# Safe defaults (if Cell 0 not executed)\n# -------------------------\nFREEZE_ENCODER = False\n\ndef _g(name, default):\n    \"\"\"Defensive global getter.\"\"\"\n    return globals().get(name, default)\n\n# Pull globals defensively (fall back to sane defaults)\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _BN_LANG = _g(\"BN_LANG\", \"bn\")\n    _EN_LANG = _g(\"EN_LANG\", \"en\")\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 48))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 1e-5))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", False))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 0))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 8000))\n    _VERBOSE_LOGGING = bool(_g(\"VERBOSE_LOGGING\", True))\n    _HOMOGRAPH_WATCHLIST_BN = set(_g(\"HOMOGRAPH_WATCHLIST_BN\", {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"}))\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 48\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 1e-5\n    _ENABLE_ASBN_TRAINING = False\n    _VALIDATION_CHECK_INTERVAL = 0\n    _DSCD_WARMUP_SAMPLES = 8000\n    _VERBOSE_LOGGING = True\n    _HOMOGRAPH_WATCHLIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\"}\n\n# DSCD clustering thresholds (defensive)\nDSCD_MIN_CLUSTER_SAMPLES = globals().get(\"DSCD_MIN_CLUSTER_SAMPLES\", None)\nDSCD_N_MIN = int(globals().get(\"DSCD_N_MIN\", 5))\nDEFAULT_CLUSTER_MIN_SAMPLES = 20\n_CLUSTER_MIN_SAMPLES = int(DSCD_MIN_CLUSTER_SAMPLES or max(DEFAULT_CLUSTER_MIN_SAMPLES, DSCD_N_MIN * 2))\n\n# -------------------------\n# Helper: Clear GPU caches safely\n# -------------------------\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            try:\n                clear_all_gpu_caches()\n            except Exception:\n                pass\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n# -------------------------\n# Small normalization helpers used for homograph matching\n# -------------------------\ndef _norm_clean_token(tok: Optional[str]) -> str:\n    if tok is None:\n        return \"\"\n    s = str(tok)\n    # remove common subword markers and normalize to NFKC\n    for marker in ('‚ñÅ', '##', 'ƒ†', '@@'):\n        s = s.replace(marker, '')\n    s = s.strip()\n    s = unicodedata.normalize('NFKC', s)\n    # Do not lowercase (Bengali script should be preserved)\n    return s\n\ndef _token_matches_homograph(token_key: str, homograph: str) -> bool:\n    clean_tok = _norm_clean_token(token_key)\n    clean_h = _norm_clean_token(homograph)\n    if not clean_tok or not clean_h:\n        return False\n    if clean_tok == clean_h:\n        return True\n    if clean_h in clean_tok:\n        return True\n    if clean_tok in clean_h:\n        return True\n    return False\n\n# -------------------------\n# Robust tokenizer loader (lazy imports + helpful errors + fallback)\n# -------------------------\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False, prefer_fast: bool = True):\n    \"\"\"\n    Robustly load a tokenizer. If transformers missing, return a whitespace fallback\n    that implements the key methods used downstream (decode, convert_ids_to_tokens, __len__, vocab_size).\n    \"\"\"\n    # 1) Lazy import to avoid protobuf/transformers import-time issues\n    try:\n        import transformers as _tf\n        from transformers import AutoTokenizer\n    except Exception as e_tf:\n        # Transformers not importable: return a richer whitespace fallback\n        class _WhitespaceFallback:\n            def __init__(self):\n                self.pad_token = \"<pad>\"\n                self.pad_token_id = None\n                self.vocab_size = 0\n            def __len__(self):\n                return int(self.vocab_size)\n            def encode(self, text, add_special_tokens=True):\n                # return list of \"ids\" represented as tokens (strings)\n                if text is None:\n                    return []\n                return text.split()\n            def convert_ids_to_tokens(self, ids):\n                # If ids are strings, return them; if ints, return their str repr\n                if ids is None:\n                    return []\n                out = []\n                for x in ids:\n                    if isinstance(x, str):\n                        out.append(x)\n                    else:\n                        out.append(str(x))\n                return out\n            def decode(self, ids, skip_special_tokens=True, **kwargs):\n                if ids is None:\n                    return \"\"\n                if isinstance(ids, (list, tuple)):\n                    return \" \".join([str(t) for t in ids])\n                return str(ids)\n            def __call__(self, texts, padding=False, truncation=False, return_tensors=None, max_length=None, add_special_tokens=True):\n                # ensure consistent return structure similar to HF tokenizer\n                if isinstance(texts, str):\n                    texts = [texts]\n                input_ids = []\n                attention_mask = []\n                for t in texts:\n                    toks = (t or \"\").split()\n                    input_ids.append(toks)\n                    attention_mask.append([1] * len(toks))\n                if return_tensors == \"pt\":\n                    # convert to padded tensors with pad token id = 0\n                    maxlen = max((len(x) for x in input_ids), default=0)\n                    import torch as _torch\n                    ids_t = _torch.zeros((len(input_ids), maxlen), dtype=_torch.long)\n                    mask_t = _torch.zeros((len(input_ids), maxlen), dtype=_torch.long)\n                    for i, row in enumerate(input_ids):\n                        for j, tok in enumerate(row):\n                            # token ids unknown, use 0\n                            ids_t[i, j] = 0\n                            mask_t[i, j] = 1\n                    return {\"input_ids\": ids_t, \"attention_mask\": mask_t}\n                return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n        if _VERBOSE_LOGGING:\n            print(\"WARNING: 'transformers' import failed in _safe_tokenizer_from_pretrained(). Using whitespace fallback.\")\n            print(f\"         Original error: {type(e_tf).__name__}: {e_tf}\")\n        return _WhitespaceFallback()\n\n    # 2) Try to load an appropriate tokenizer\n    tried = []\n    try:\n        from transformers import M2M100TokenizerFast as _M2MFast\n    except Exception:\n        _M2MFast = None\n\n    if _M2MFast is not None:\n        try:\n            return _M2MFast.from_pretrained(model_name, local_files_only=local_files_only)\n        except Exception as e:\n            tried.append((\"M2M100TokenizerFast\", e))\n\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=prefer_fast, local_files_only=local_files_only)\n        return tok\n    except Exception as e_auto:\n        tried.append((\"AutoTokenizer(use_fast=%s)\" % prefer_fast, e_auto))\n        msg = str(e_auto).lower()\n        if \"sentencepiece\" in msg or \"tokenizers\" in msg or \"sacremoses\" in msg:\n            raise RuntimeError(\n                f\"Failed to instantiate tokenizer for '{model_name}'. This often happens because 'sentencepiece' or 'tokenizers' is not installed or incompatible.\\n\"\n                \"Please run: pip install transformers==4.30.2 sentencepiece tokenizers\\n\"\n                \"Then RESTART the kernel and re-run cells 0‚Üí10.\\n\"\n                f\"Original tokenizer error: {e_auto}\"\n            ) from e_auto\n        # try slow tokenizer as fallback\n        try:\n            tok = AutoTokenizer.from_pretrained(model_name, use_fast=False, local_files_only=local_files_only)\n            return tok\n        except Exception as e_slow:\n            tried.append((\"AutoTokenizer(use_fast=False)\", e_slow))\n            summary = \"; \".join([f\"{name}:{type(exc).__name__}\" for name, exc in tried])\n            raise RuntimeError(\n                f\"No usable tokenizer class available for '{model_name}'. Tried: {summary}.\\n\"\n                \"Make sure you have a compatible 'transformers' installed and the optional dependencies (sentencepiece, tokenizers) for the model.\\n\"\n                \"Suggested command:\\n\"\n                \"  pip install transformers==4.30.2 sentencepiece tokenizers\\n\"\n                \"Then RESTART the kernel and re-run the notebook.\\n\"\n                f\"Last error: {e_slow}\"\n            ) from e_slow\n\n# -------------------------\n# Main pipeline\n# -------------------------\ndef initialize_environment():\n    print(\"[CELL10] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[CELL10] GPUs available: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n            except Exception:\n                name = \"Unknown GPU\"\n            try:\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3\n                print(f\"  - GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  - GPU {i}: {name} (mem unknown)\")\n        _safe_clear_gpu_caches()\n        if gcnt > 1:\n            print(\"[CELL10] Multi-GPU detected\")\n    else:\n        print(\"[CELL10] No GPU detected - running on CPU\")\n    return True\n\ndef main_pipeline() -> Tuple[object, object]:\n    \"\"\"\n    End-to-end orchestration. Returns (trained_model, tokenizer).\n    \n    ‚úÖ FIXED: Complete implementation with all validation and checkpoint features\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"CELL10: TATN MAIN PIPELINE (COMPLETELY FIXED)\")\n    print(\"=\" * 80)\n\n    initialize_environment()\n\n    # -----------------------\n    # Step 1: Tokenizer\n    # -----------------------\n    print(\"[CELL10] Step 1: Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    try:\n        tokenizer.src_lang = _BN_LANG\n    except Exception:\n        pass\n\n    # Ensure pad token exists (best-effort)\n    try:\n        pad_id = getattr(tokenizer, \"pad_token_id\", None)\n        if pad_id is None and hasattr(tokenizer, \"add_special_tokens\"):\n            try:\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # compute a useful vocab_info for logging\n    vocab_info = \"unknown\"\n    try:\n        if hasattr(tokenizer, \"vocab_size\") and getattr(tokenizer, \"vocab_size\") is not None:\n            vocab_info = int(getattr(tokenizer, \"vocab_size\"))\n        elif hasattr(tokenizer, \"__len__\"):\n            try:\n                vocab_info = int(len(tokenizer))\n            except Exception:\n                vocab_info = \"unknown\"\n        else:\n            vocab_info = \"unknown\"\n    except Exception:\n        vocab_info = \"unknown\"\n    print(f\"[CELL10] Tokenizer loaded (vocab size approx {vocab_info})\")\n\n    # -----------------------\n    # Step 2: Data loading\n    # -----------------------\n    print(f\"[CELL10] Step 2: Loading/preprocessing up to {_NUM_SAMPLES} samples...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception:\n            print(\"[CELL10] load_and_preprocess_optimized failed; using fallback single example\")\n            pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"i turned off the tap.\")]\n    else:\n        print(\"[CELL10] Warning: load_and_preprocess_optimized not found; using small fallback dataset\")\n        pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"i turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals():\n        raise RuntimeError(\"MemoryEfficientDataset not present - run Cell 2 first\")\n    dataset = MemoryEfficientDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n\n    batch_size = int(_BATCH_SIZE)\n    active_device_ids = list(range(_NUM_GPUS)) if (_USE_MULTI_GPU and _NUM_GPUS > 1) else []\n    if active_device_ids and batch_size < len(active_device_ids):\n        # don't try to use more GPUs than the batch size allows\n        usable = max(1, batch_size)\n        active_device_ids = active_device_ids[:usable]\n        print(f\"[CELL10] Adjusting DataParallel devices to {len(active_device_ids)} due to small batch_size\")\n\n    # synchronize global BATCH_SIZE for compatibility with other cells\n    try:\n        global BATCH_SIZE\n        BATCH_SIZE = batch_size\n    except Exception:\n        pass\n\n    # collate function if provided\n    collate_fn = globals().get(\"safe_collate\", None)\n    collate_fn = collate_fn if callable(collate_fn) else None\n\n    # Prefer an optimized dataloader if available, else fallback to vanilla DataLoader\n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = create_optimized_dataloader(dataset, batch_size=batch_size, shuffle=True)\n        except Exception:\n            print(\"[CELL10] create_optimized_dataloader failed; falling back to DataLoader\")\n            train_loader = DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=True,\n                num_workers=0,\n                pin_memory=torch.cuda.is_available(),\n                collate_fn=collate_fn,\n                drop_last=False\n            )\n    else:\n        train_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=0,\n            pin_memory=torch.cuda.is_available(),\n            collate_fn=collate_fn,\n            drop_last=False\n        )\n\n    try:\n        batches_count = len(train_loader)\n    except Exception:\n        batches_count = \"unknown\"\n    print(f\"[CELL10] Dataset: {len(dataset)} examples, {batches_count} batches (batch_size={batch_size})\")\n\n    # -----------------------\n    # Step 3: Model initialization\n    # -----------------------\n    print(\"[CELL10] Step 3: Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        raise RuntimeError(\"Model class MemoryOptimizedTATNWithExplanations not found (Cell 6)\")\n    model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n\n    # Wrap into DataParallel if multiple device ids chosen\n    if active_device_ids and len(active_device_ids) > 1:\n        print(f\"[CELL10] Wrapping model in DataParallel on devices {active_device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=active_device_ids)\n    else:\n        model = model_core\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] Single-GPU / CPU mode (no DataParallel)\")\n\n    # Move to device carefully (avoid .to on DataParallel in some setups)\n    try:\n        model = model.to(_DEVICE)\n    except Exception:\n        try:\n            core = model.module if hasattr(model, \"module\") else model\n            core.to(_DEVICE)\n        except Exception:\n            pass\n\n    core_model = model.module if hasattr(model, \"module\") else model\n\n    # Resize embeddings if tokenizer vocabulary differs from model embedding size\n    try:\n        mb = getattr(core_model, \"mbart\", None)\n        if mb is not None and hasattr(mb, \"get_input_embeddings\"):\n            emb = mb.get_input_embeddings()\n            current_emb = getattr(emb, \"num_embeddings\", None) or getattr(emb, \"weight\", None).shape[0] if hasattr(emb, \"weight\") else None\n            new_size = None\n            try:\n                if hasattr(tokenizer, \"vocab_size\") and getattr(tokenizer, \"vocab_size\") is not None:\n                    new_size = int(getattr(tokenizer, \"vocab_size\"))\n                elif hasattr(tokenizer, \"__len__\"):\n                    new_size = int(len(tokenizer))\n            except Exception:\n                new_size = None\n            if new_size and current_emb and int(current_emb) != int(new_size):\n                try:\n                    mb.resize_token_embeddings(new_size)\n                    print(f\"[CELL10] Resized token embeddings: {current_emb} -> {new_size}\")\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(\"[CELL10] Warning: resize_token_embeddings failed; continuing\")\n    except Exception:\n        pass\n\n    # Optional encoder freeze\n    if FREEZE_ENCODER:\n        try:\n            for p in core_model.mbart.model.encoder.parameters():\n                p.requires_grad = False\n            print(\"[CELL10] Encoder frozen for faster training\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL10] Encoder freeze failed; continuing\")\n\n    # -----------------------\n    # Step 4: Optimizers\n    # -----------------------\n    print(\"[CELL10] Step 4: Preparing optimizers...\")\n    try:\n        critic_params = list(core_model.asbn.critic_parameters()) if hasattr(core_model, \"asbn\") and hasattr(core_model.asbn, \"critic_parameters\") else []\n    except Exception:\n        critic_params = []\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n\n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n    phi_optimizer = None\n    if critic_params and _ENABLE_ASBN_TRAINING:\n        try:\n            phi_optimizer = torch.optim.AdamW([p for p in critic_params if p.requires_grad], lr=_LR_PHI)\n            print(f\"[CELL10] ASBN critic optimizer created (params: {len([p for p in critic_params if p.requires_grad])})\")\n        except Exception:\n            phi_optimizer = None\n            print(\"[CELL10] ASBN critic optimizer creation failed; continuing without it\")\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] ASBN critic optimizer disabled\")\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #4: CAPTURE BASELINE METRICS BEFORE TRAINING\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    print(\"\\n[CELL10] Step 5: Baseline Evaluation (Pre-Training)\")\n    baseline_metrics = None\n    try:\n        if \"comprehensive_post_training_testing\" in globals():\n            print(\"[CELL10] Running baseline evaluation...\")\n            baseline_metrics = comprehensive_post_training_testing(\n                model, \n                tokenizer,\n                run_warmup=False  # Don't run warmup before training\n            )\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            print(f\"[CELL10] ‚úì Baseline captured:\")\n            print(f\"[CELL10]   - Success rate: {baseline_success:.1f}%\")\n            print(f\"[CELL10]   - Explanations: {baseline_expl}\")\n        else:\n            print(\"[CELL10] Skipping baseline (evaluation function not found)\")\n    except Exception as e:\n        print(f\"[CELL10] Baseline evaluation failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    # -----------------------\n    # Step 6: Training\n    # -----------------------\n    print(\"\\n[CELL10] Step 6: Training phase...\")\n    trained_model = model\n    training_stats = {}  # ‚úÖ FIX #3: Store training statistics\n    \n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=bool(_VALIDATION_CHECK_INTERVAL > 0)\n            )\n            \n            # ‚úÖ FIX #3: Extract training statistics\n            try:\n                core_for_stats = trained_model.module if hasattr(trained_model, 'module') else trained_model\n                if hasattr(core_for_stats, 'training_stats'):\n                    training_stats = core_for_stats.training_stats\n                    total_batches = len(training_stats.get('total_loss', []))\n                    print(f\"[CELL10] ‚úì Training stats captured: {total_batches} batches\")\n            except Exception:\n                pass\n                \n        except Exception as e:\n            print(f\"[CELL10] Training failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            trained_model = model\n    else:\n        print(\"[CELL10] Training function not found (Cell 7). Skipping training.\")\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1 + #5: DISCOVERY PHASE WITH COMPREHENSIVE VALIDATION\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    print(\"\\n\" + \"=\" * 80)\n    print(\"STEP 7: DISCOVERY PHASE - Clustering DSCD buffers to create prototypes\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    discovery_success = False\n    total_prototypes = 0\n    multi_sense_words = 0\n\n    try:\n        core_for_discovery = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n        if not hasattr(core_for_discovery, \"dscd\"):\n            raise RuntimeError(\"Trained model does not have a .dscd attribute (DSCD instance)\")\n\n        dscd = core_for_discovery.dscd\n\n        # Collect clusterable tokens using a conservative threshold\n        buffers_iter = getattr(dscd, \"buffers\", {}) or {}\n        clusterable_tokens = []\n        for token_type, buffer in buffers_iter.items():\n            try:\n                buf_len = len(buffer)\n            except Exception:\n                buf_len = 0\n            if buf_len >= _CLUSTER_MIN_SAMPLES:\n                clusterable_tokens.append((token_type, buf_len))\n\n        # Relax threshold if nothing meets strict threshold\n        if len(clusterable_tokens) == 0:\n            relaxed = []\n            for token_type, buffer in buffers_iter.items():\n                try:\n                    buf_len = len(buffer)\n                except Exception:\n                    buf_len = 0\n                if buf_len >= DSCD_N_MIN:\n                    relaxed.append((token_type, buf_len))\n            if relaxed:\n                print(f\"[DISCOVERY] No tokens >= {_CLUSTER_MIN_SAMPLES}. Relaxing threshold to DSCD_N_MIN={DSCD_N_MIN} (found {len(relaxed)})\")\n                clusterable_tokens = relaxed\n\n        # Sort by buffer size (descending) and limit to top K\n        clusterable_tokens.sort(key=lambda x: x[1], reverse=True)\n        MAX_TO_CLUSTER = min(500, max(1, len(clusterable_tokens)))\n        clusterable_tokens = clusterable_tokens[:MAX_TO_CLUSTER]\n\n        print(f\"[DISCOVERY] Found {len(clusterable_tokens)} tokens meeting threshold for clustering (threshold={_CLUSTER_MIN_SAMPLES})\")\n\n        if len(clusterable_tokens) == 0:\n            print(\"[DISCOVERY] ‚ö†Ô∏è WARNING: No tokens with sufficient samples! DSCD will not work reliably.\")\n        else:\n            clustered_count = 0\n            failed_count = 0\n            start_time = time.time()\n\n            # ‚úÖ FIX #5: Add periodic validation checkpoints\n            VALIDATION_INTERVAL = 100\n            last_validation_idx = 0\n\n            for idx, (token_type, buffer_size) in enumerate(clusterable_tokens):\n                try:\n                    success = False\n                    if hasattr(dscd, \"_cluster_buffer_to_prototypes_hierarchical\"):\n                        try:\n                            success = dscd._cluster_buffer_to_prototypes_hierarchical(token_type)\n                        except Exception as e:\n                            if _VERBOSE_LOGGING:\n                                print(f\"  [WARN] Clustering failed for '{token_type}': {type(e).__name__}\")\n                            success = False\n                    else:\n                        if _VERBOSE_LOGGING:\n                            print(\"  [WARN] DSCD has no _cluster_buffer_to_prototypes_hierarchical method\")\n                        success = False\n                    \n                    if success:\n                        clustered_count += 1\n                    else:\n                        failed_count += 1\n\n                    # Progress reporting\n                    if (idx + 1) % 50 == 0:\n                        elapsed = time.time() - start_time\n                        print(f\"  Progress: {idx + 1}/{len(clusterable_tokens)} tokens processed \"\n                              f\"({clustered_count} successful, {failed_count} failed) \"\n                              f\"[{elapsed:.1f}s elapsed]\")\n                    \n                    # ‚úÖ FIX #5: Periodic validation during discovery\n                    if (idx + 1) % VALIDATION_INTERVAL == 0 and (idx + 1) - last_validation_idx >= VALIDATION_INTERVAL:\n                        try:\n                            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                            current_multi_sense = sum(1 for store in prototype_stores.values() \n                                                    if ((store.size() if hasattr(store, \"size\") and callable(store.size) \n                                                        else len(store) if hasattr(store, \"__len__\") else 0) >= 2))\n                            print(f\"  [CHECKPOINT] Tokens: {len(prototype_stores)}, Multi-sense: {current_multi_sense}\")\n                            last_validation_idx = idx + 1\n                        except Exception:\n                            pass\n\n                except Exception as e:\n                    failed_count += 1\n                    if failed_count <= 10:\n                        token_str = str(token_type)[:40]\n                        print(f\"  [WARN] Clustering failed for token '{token_str}': {type(e).__name__}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    continue\n\n            # Final discovery statistics\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            try:\n                total_prototypes = 0\n                for store in prototype_stores.values():\n                    try:\n                        if hasattr(store, \"size\") and callable(store.size):\n                            total_prototypes += int(store.size())\n                        elif hasattr(store, \"__len__\"):\n                            total_prototypes += int(len(store))\n                        else:\n                            total_prototypes += int(getattr(store, \"n_prototypes\", 0) or 0)\n                    except Exception:\n                        pass\n            except Exception:\n                total_prototypes = 0\n\n            try:\n                multi_sense_words = sum(1 for store in prototype_stores.values() \n                                       if ((store.size() if hasattr(store, \"size\") and callable(store.size) \n                                           else len(store) if hasattr(store, \"__len__\") else 0) >= 2))\n            except Exception:\n                multi_sense_words = 0\n\n            elapsed_total = time.time() - start_time\n\n            print(\"=\" * 80)\n            print(\"‚úì DISCOVERY PHASE COMPLETE\")\n            print(\"=\" * 80)\n            print(f\"  ‚Ä¢ Tokens processed: {len(clusterable_tokens)}\")\n            print(f\"  ‚Ä¢ Successfully clustered: {clustered_count}\")\n            print(f\"  ‚Ä¢ Failed: {failed_count}\")\n            print(f\"  ‚Ä¢ Total prototypes created: {total_prototypes}\")\n            print(f\"  ‚Ä¢ Multi-sense words (‚â•2 prototypes): {multi_sense_words}\")\n            print(f\"  ‚Ä¢ Time elapsed: {elapsed_total:.2f}s ({elapsed_total/60:.2f} min)\")\n            print(\"=\" * 80)\n\n            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n            # ‚úÖ FIX #1: USE BUILT-IN VALIDATION FROM CELL 3\n            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n            print(\"\\n[DISCOVERY] Running comprehensive prototype validation...\")\n            try:\n                if hasattr(dscd, 'validate_prototypes'):\n                    print(\"[DISCOVERY] Calling dscd.validate_prototypes()...\")\n                    validation_results = dscd.validate_prototypes(list(_HOMOGRAPH_WATCHLIST_BN))\n                    \n                    # Extract key metrics\n                    quality_score = validation_results.get('quality_score', 0.0)\n                    homographs_found = validation_results.get('homographs_found', 0)\n                    total_homographs = len(_HOMOGRAPH_WATCHLIST_BN)\n                    \n                    # Quality assessment\n                    print(\"\\n[DISCOVERY] Quality Assessment:\")\n                    if quality_score < 0.3:\n                        print(\"  ‚ö†Ô∏è WARNING: Prototype quality score is LOW!\")\n                        print(\"     Recommendations:\")\n                        print(\"     1. Increase NUM_SAMPLES in Cell 0\")\n                        print(\"     2. Run more training epochs\")\n                        print(\"     3. Check that homograph words are in training data\")\n                    elif quality_score >= 0.7:\n                        print(\"  ‚úÖ EXCELLENT: High-quality prototype clustering achieved!\")\n                        discovery_success = True\n                    else:\n                        print(\"  ‚úì GOOD: Prototypes have acceptable quality\")\n                        discovery_success = True\n                    \n                    print(f\"\\n[DISCOVERY] Homograph Coverage: {homographs_found}/{total_homographs} detected\")\n                    \n                    if homographs_found < total_homographs:\n                        missing = validation_results.get('homographs_missing', [])\n                        print(f\"[DISCOVERY] ‚ö†Ô∏è Missing homographs: {', '.join(missing)}\")\n                    \n                else:\n                    print(\"\\n‚ö†Ô∏è DSCD has no validate_prototypes() method - using basic verification\")\n                    # Fallback to basic homograph verification\n                    print(\"\\n[DISCOVERY] Basic homograph verification:\")\n                    print(\"-\" * 80)\n                    \n                    homographs_found_count = 0\n                    homographs_missing_count = 0\n                    \n                    # Build normalized map\n                    proto_map = {}\n                    for token_key, store in prototype_stores.items():\n                        try:\n                            nk = _norm_clean_token(token_key)\n                        except Exception:\n                            nk = str(token_key)\n                        if nk not in proto_map:\n                            proto_map[nk] = (token_key, store)\n                    \n                    for homograph in (list(_HOMOGRAPH_WATCHLIST_BN) if _HOMOGRAPH_WATCHLIST_BN else []):\n                        matched_store = None\n                        matched_key = None\n                        nh = _norm_clean_token(homograph)\n                        \n                        if nh and nh in proto_map:\n                            matched_key, matched_store = proto_map[nh]\n                        else:\n                            for nk, (orig_k, store) in proto_map.items():\n                                try:\n                                    if _token_matches_homograph(orig_k, homograph):\n                                        matched_key, matched_store = orig_k, store\n                                        break\n                                except Exception:\n                                    continue\n                        \n                        try:\n                            store_size = 0\n                            if matched_store is not None:\n                                if hasattr(matched_store, \"size\") and callable(matched_store.size):\n                                    store_size = int(matched_store.size())\n                                elif hasattr(matched_store, \"__len__\"):\n                                    store_size = int(len(matched_store))\n                                else:\n                                    store_size = int(getattr(matched_store, \"n_prototypes\", 0) or 0)\n                            \n                            if matched_store is not None and store_size >= 2:\n                                counts = getattr(matched_store, \"counts\", None)\n                                print(f\"  ‚úì '{homograph}' ‚Üí {store_size} prototypes (key='{matched_key}') counts={counts}\")\n                                homographs_found_count += 1\n                            else:\n                                print(f\"  ‚úó WARNING: '{homograph}' has NO multi-sense prototypes\")\n                                print(f\"            This word will NOT be disambiguated in inference!\")\n                                homographs_missing_count += 1\n                        except Exception:\n                            print(f\"  ‚úó WARNING: '{homograph}' verification encountered an error\")\n                            homographs_missing_count += 1\n                    \n                    print(\"-\" * 80)\n                    print(f\"Homograph verification: {homographs_found_count}/{len(list(_HOMOGRAPH_WATCHLIST_BN))} detected\")\n                    \n                    if homographs_missing_count > 0:\n                        print(f\"\\n‚ö†Ô∏è WARNING: {homographs_missing_count} known homographs were NOT properly clustered!\")\n                        print(\"Possible causes:\")\n                        print(\"  1. Not enough training samples containing these words\")\n                        print(\"  2. Words were filtered out by should_track_token() (Cell 3)\")\n                        print(\"  3. Buffer/cluster thresholds too strict\")\n                        print(\"  4. Clustering backend unavailable or failed (SciPy/sklearn)\")\n                    else:\n                        print(\"\\n‚úÖ All homographs successfully clustered! Disambiguation ready.\")\n                        discovery_success = True\n                    \n            except Exception as e:\n                print(f\"\\n‚ö†Ô∏è Validation failed: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n\n            # Clear buffers if prototypes created\n            if total_prototypes > 0:\n                if _VERBOSE_LOGGING:\n                    print(\"\\n[DISCOVERY] Clearing DSCD buffers to save memory.\")\n                try:\n                    if hasattr(dscd, \"buffers\") and hasattr(dscd.buffers, \"clear\"):\n                        dscd.buffers.clear()\n                    else:\n                        dscd.buffers = {}\n                except Exception:\n                    try:\n                        dscd.buffers = {}\n                    except Exception:\n                        pass\n                _safe_clear_gpu_caches()\n            else:\n                if _VERBOSE_LOGGING:\n                    print(\"\\n[DISCOVERY] Not clearing buffers (no prototypes) for debugging.\")\n\n    except Exception as e:\n        print(f\"\\n[DISCOVERY] CRITICAL ERROR: Discovery phase failed!\")\n        print(f\"  Error type: {type(e).__name__}\")\n        print(f\"  Error message: {str(e)[:300]}\")\n        if _VERBOSE_LOGGING:\n            print(\"\\n[DISCOVERY] Full traceback:\")\n            traceback.print_exc()\n        print(\"\\n[DISCOVERY] ‚ö†Ô∏è WARNING: DSCD homograph detection will NOT work!\")\n        print(\"  The model will function but only at baseline M2M100 quality.\")\n\n    # Optional: Run additional warmup inference\n    if \"dscd_discovery_warmup\" in globals():\n        try:\n            print(\"\\n[CELL10] Step 7.5: Additional DSCD inference warmup...\")\n            warmup_samples = min(1000, int(_DSCD_WARMUP_SAMPLES))\n            dscd_discovery_warmup(trained_model, tokenizer, num_sents=warmup_samples, max_len=_MAX_LENGTH)\n            print(f\"[CELL10] ‚úì Inference warmup complete\")\n        except Exception as e:\n            print(f\"[CELL10] Warmup failed: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #4: POST-TRAINING EVALUATION WITH BASELINE COMPARISON\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    print(\"\\n[CELL10] Step 8: Post-Training Evaluation\")\n    _safe_clear_gpu_caches()\n    \n    eval_results = {}\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            print(\"[CELL10] Running post-training evaluation...\")\n            eval_results = comprehensive_post_training_testing(\n                trained_model, \n                tokenizer,\n                run_warmup=False,  # Already ran warmup\n                compare_baseline=(baseline_metrics is not None),\n                baseline_metrics=baseline_metrics\n            )\n            \n            final_success = eval_results.get('success_rate_pct', 0)\n            final_expl = eval_results.get('total_explanations', 0)\n            print(f\"[CELL10] ‚úì Evaluation complete:\")\n            print(f\"[CELL10]   - Success rate: {final_success:.1f}%\")\n            print(f\"[CELL10]   - Explanations: {final_expl}\")\n            \n        except Exception as e:\n            print(f\"[CELL10] Evaluation failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n    else:\n        print(\"[CELL10] Skipping evaluation (function not found)\")\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #2 + #3: SAVE MODEL WITH DSCD STATE AND TRAINING METRICS\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    print(\"\\n[CELL10] Step 9: Saving model checkpoint...\")\n    has_model = False\n    has_dscd = False\n    has_training = False\n    num_tokens = 0\n    save_path = \"tatn_kaggle_final.pt\"\n    \n    try:\n        core_for_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        \n        # Collect model state\n        print(\"[CELL10] Collecting model state...\")\n        model_state = core_for_save.state_dict()\n        has_model = len(model_state) > 0\n        \n        # ‚úÖ FIX #2: Collect DSCD state\n        print(\"[CELL10] Collecting DSCD state...\")\n        dscd_state = {}\n        if hasattr(core_for_save, 'dscd') and hasattr(core_for_save.dscd, 'state_dict'):\n            dscd_state = core_for_save.dscd.state_dict()\n            num_tokens = len(dscd_state.get('prototype_stores', {}))\n            has_dscd = num_tokens > 0\n            print(f\"[CELL10] ‚úì DSCD state collected ({num_tokens} tokens)\")\n        else:\n            print(\"[CELL10] ‚ö†Ô∏è WARNING: DSCD has no state_dict() method!\")\n        \n        # ‚úÖ FIX #3: Include training statistics and all metrics\n        has_training = len(training_stats) > 0\n        \n        checkpoint = {\n            'model_state_dict': model_state,\n            'dscd_state_dict': dscd_state,\n            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n            'training_stats': training_stats,\n            'baseline_metrics': baseline_metrics,\n            'eval_results': eval_results,\n            'discovery_success': discovery_success,\n            'total_prototypes': total_prototypes,\n            'multi_sense_words': multi_sense_words,\n            'training_complete': True,\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'user': 'manas0003',\n            'config': {\n                'epochs': _EPOCHS,\n                'batch_size': _BATCH_SIZE,\n                'num_samples': _NUM_SAMPLES,\n                'max_length': _MAX_LENGTH,\n                'accumulation_steps': _ACCUMULATION_STEPS,\n                'lr_nmt': _LR_NMT,\n                'lr_phi': _LR_PHI,\n            }\n        }\n        \n        print(\"[CELL10] Writing checkpoint to disk...\")\n        torch.save(checkpoint, save_path)\n        \n        # ‚úÖ FIX #2: VERIFY CHECKPOINT\n        print(\"[CELL10] Verifying checkpoint...\")\n        verify_ckpt = torch.load(save_path, map_location='cpu')\n        \n        has_model = 'model_state_dict' in verify_ckpt and len(verify_ckpt['model_state_dict']) > 0\n        has_dscd = 'dscd_state_dict' in verify_ckpt and len(verify_ckpt['dscd_state_dict']) > 0\n        has_training = 'training_stats' in verify_ckpt and verify_ckpt['training_stats']\n        \n        print(f\"[CELL10] ‚úì Checkpoint saved to {save_path}\")\n        print(f\"[CELL10] Verification:\")\n        print(f\"  - Model state: {'‚úì Present (%d params)' % len(verify_ckpt['model_state_dict']) if has_model else '‚úó MISSING'}\")\n        print(f\"  - DSCD state: {'‚úì Present' if has_dscd else '‚úó MISSING'}\")\n        print(f\"  - Training stats: {'‚úì Present' if has_training else '‚úó MISSING'}\")\n        print(f\"  - Baseline metrics: {'‚úì Present' if verify_ckpt.get('baseline_metrics') else '‚úó Missing'}\")\n        print(f\"  - Eval results: {'‚úì Present' if verify_ckpt.get('eval_results') else '‚úó Missing'}\")\n        \n        if has_dscd:\n            num_tokens = len(verify_ckpt['dscd_state_dict'].get('prototype_stores', {}))\n            print(f\"  - DSCD tokens: {num_tokens}\")\n            if num_tokens == 0:\n                print(\"  ‚ö†Ô∏è WARNING: DSCD state is empty! Prototypes were not saved!\")\n        else:\n            print(\"  ‚ö†Ô∏è CRITICAL: DSCD state missing from checkpoint!\")\n            print(\"     ‚Üí Inference will NOT work after loading this checkpoint!\")\n        \n    except Exception as e:\n        print(f\"[CELL10] Save failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #6: COMPREHENSIVE FINAL REPORT\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN PIPELINE COMPLETE - COMPREHENSIVE SUMMARY\")\n    print(\"=\" * 80)\n    \n    # Phase 1: Training\n    print(\"\\n[PHASE 1: TRAINING]\")\n    if training_stats:\n        total_loss = training_stats.get('total_loss', [])\n        optimizer_updates = training_stats.get('optimizer_updates', 0)\n        batches_processed = training_stats.get('batches_processed', 0)\n        skipped = training_stats.get('skipped_batches', 0)\n        \n        print(f\"  ‚úì Training completed\")\n        print(f\"  - Batches processed: {batches_processed} (skipped: {skipped})\")\n        print(f\"  - Optimizer updates: {optimizer_updates}\")\n        if total_loss:\n            avg_loss = sum(total_loss) / len(total_loss)\n            final_loss = sum(total_loss[-100:]) / len(total_loss[-100:]) if len(total_loss) >= 100 else avg_loss\n            print(f\"  - Avg loss: {avg_loss:.6f}\")\n            print(f\"  - Final loss (last 100): {final_loss:.6f}\")\n    else:\n        print(f\"  ‚ö†Ô∏è No training stats available\")\n    \n    # Phase 2: Discovery\n    print(\"\\n[PHASE 2: DISCOVERY]\")\n    if discovery_success:\n        print(f\"  ‚úì Discovery completed successfully\")\n        print(f\"  - Token types: {len(getattr(dscd, 'prototype_stores', {}) or {})}\")\n        print(f\"  - Total prototypes: {total_prototypes}\")\n        print(f\"  - Multi-sense tokens: {multi_sense_words}\")\n        if len(getattr(dscd, 'prototype_stores', {}) or {}) > 0:\n            ratio = multi_sense_words / len(dscd.prototype_stores)\n            print(f\"  - Multi-sense ratio: {ratio:.1%}\")\n    else:\n        print(f\"  ‚ö†Ô∏è Discovery had issues (check logs above)\")\n        print(f\"  - Total prototypes: {total_prototypes}\")\n        print(f\"  - Multi-sense tokens: {multi_sense_words}\")\n    \n    # Phase 3: Evaluation\n    print(\"\\n[PHASE 3: EVALUATION]\")\n    if baseline_metrics and eval_results:\n        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n        final_success = eval_results.get('success_rate_pct', 0)\n        improvement = final_success - baseline_success\n        \n        print(f\"  ‚úì Baseline: {baseline_success:.1f}% success rate\")\n        print(f\"  ‚úì Final: {final_success:.1f}% success rate\")\n        print(f\"  ‚úì Improvement: {improvement:+.1f}%\")\n        \n        baseline_expl = baseline_metrics.get('total_explanations', 0)\n        final_expl = eval_results.get('total_explanations', 0)\n        print(f\"  - Explanations: {baseline_expl} ‚Üí {final_expl} ({final_expl - baseline_expl:+d})\")\n        \n        # Quality metrics\n        quality = eval_results.get('quality_metrics', {})\n        if quality:\n            print(f\"  - Avg confidence: {quality.get('avg_confidence', 0):.3f}\")\n            print(f\"  - High confidence rate: {quality.get('high_confidence_count', 0)}/{quality.get('confidence_samples', 0)}\")\n        \n        # Homograph detection\n        homo_tracking = eval_results.get('homograph_tracking', {})\n        if homo_tracking:\n            detected = len(homo_tracking.get('detected_homographs', set()))\n            expected = len(homo_tracking.get('expected_homographs', set()))\n            print(f\"  - Homographs detected: {detected}/{expected}\")\n        \n    elif eval_results:\n        print(f\"  ‚úì Success rate: {eval_results.get('success_rate_pct', 0):.1f}%\")\n        print(f\"  - Total explanations: {eval_results.get('total_explanations', 0)}\")\n        dscd_stats = eval_results.get('dscd_stats', {})\n        if dscd_stats:\n            print(f\"  - DSCD tokens: {dscd_stats.get('total_words', 0)}\")\n            print(f\"  - Multi-sense: {dscd_stats.get('multi_sense_words', 0)}\")\n    else:\n        print(f\"  ‚ö†Ô∏è No evaluation metrics available\")\n    \n    # Phase 4: Checkpoint\n    print(\"\\n[PHASE 4: CHECKPOINT]\")\n    if has_model and has_dscd:\n        print(f\"  ‚úÖ Checkpoint saved successfully\")\n        print(f\"  - File: {save_path}\")\n        print(f\"  - Model params: {len(verify_ckpt.get('model_state_dict', {}))}\")\n        print(f\"  - DSCD prototypes: {num_tokens} tokens\")\n        print(f\"  - Training stats: {'Included' if has_training else 'Not included'}\")\n        \n        # File size\n        try:\n            file_size = os.path.getsize(save_path) / (1024**2)\n            print(f\"  - Checkpoint size: {file_size:.1f} MB\")\n        except Exception:\n            pass\n            \n    else:\n        print(f\"  ‚ö†Ô∏è Checkpoint incomplete!\")\n        if not has_model:\n            print(f\"     ‚Üí Model state missing\")\n        if not has_dscd:\n            print(f\"     ‚Üí DSCD state missing - CRITICAL!\")\n    \n    # Overall health assessment\n    print(\"\\n[OVERALL HEALTH CHECK]\")\n    warnings = []\n    successes = []\n    \n    # Check each phase\n    if training_stats and optimizer_updates > 0:\n        successes.append(\"Training completed with optimizer updates\")\n    else:\n        warnings.append(\"Training may not have completed properly\")\n    \n    if discovery_success and total_prototypes > 0:\n        successes.append(f\"Discovery created {total_prototypes} prototypes\")\n    else:\n        warnings.append(\"Discovery phase had issues or created no prototypes\")\n    \n    if has_dscd and num_tokens > 0:\n        successes.append(f\"DSCD state saved with {num_tokens} tokens\")\n    else:\n        warnings.append(\"CRITICAL: DSCD state not saved - inference will fail!\")\n    \n    if eval_results and eval_results.get('success_rate_pct', 0) >= 70:\n        successes.append(\"High translation success rate achieved\")\n    elif eval_results and eval_results.get('success_rate_pct', 0) >= 50:\n        successes.append(\"Acceptable translation success rate\")\n    else:\n        warnings.append(\"Low translation success rate\")\n    \n    if baseline_metrics and eval_results:\n        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n        final_success = eval_results.get('success_rate_pct', 0)\n        if final_success > baseline_success:\n            successes.append(f\"Model improved by {final_success - baseline_success:.1f}%\")\n        else:\n            warnings.append(\"Model did not improve over baseline\")\n    \n    # Print assessment\n    if successes:\n        print(\"\\n‚úÖ Successes:\")\n        for s in successes:\n            print(f\"  ‚Ä¢ {s}\")\n    \n    if warnings:\n        print(\"\\n‚ö†Ô∏è Warnings:\")\n        for w in warnings:\n            print(f\"  ‚Ä¢ {w}\")\n    \n    if not warnings:\n        print(\"\\nüéâ EXCELLENT: All systems nominal - model ready for production!\")\n    elif len(warnings) == 1 and \"baseline\" in warnings[0].lower():\n        print(\"\\n‚úì GOOD: Model trained successfully with minor issues\")\n    else:\n        print(\"\\n‚ö†Ô∏è ATTENTION NEEDED: Review warnings above before deployment\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"Pipeline execution complete.\")\n    print(\"=\" * 80)\n    print(\"\\nNext steps:\")\n    print(\"  1. Review the checkpoint verification above\")\n    print(\"  2. Test inference with: translate_with_explanations(model, tokenizer, '‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§')\")\n    print(\"  3. If DSCD state is missing, re-run discovery phase\")\n    print(\"  4. Save checkpoint again with proper DSCD state\")\n    print(\"=\" * 80)\n\n    # Clear caches and return\n    _safe_clear_gpu_caches()\n    return trained_model, tokenizer\n\n# When this cell is executed, the user can call main_pipeline() to execute.\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Cell 10 (COMPLETELY FIXED): Main pipeline ready\")\nprint(\"=\" * 80)\nprint(\"Fixes applied:\")\nprint(\" ‚úÖ FIX #1: Calls validate_prototypes() after discovery\")\nprint(\" ‚úÖ FIX #2: Saves DSCD state in checkpoint + verification\")\nprint(\" ‚úÖ FIX #3: Persists training metrics to checkpoint\")\nprint(\" ‚úÖ FIX #4: Captures baseline metrics before training\")\nprint(\" ‚úÖ FIX #5: Adds discovery progress validation\")\nprint(\" ‚úÖ FIX #6: Comprehensive 4-phase final report\")\nprint(\"=\" * 80)\nprint(\"\\nTo execute: trained_model, tokenizer = main_pipeline()\")\nprint(\"=\" * 80)","metadata":{"id":"kEux2BVXH4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER - COMPLETELY FIXED\n# ==============================================================================\n# ‚úÖ FIXED: Add execution time tracking (ERROR #1 FIX)\n# ‚úÖ FIXED: Add checkpoint validation (ERROR #2 FIX)\n# ‚úÖ FIXED: Add comprehensive metrics summary (ERROR #3 FIX)\n# ‚úÖ FIXED: Test homograph disambiguation (ERROR #4 FIX)\n# ‚úÖ ADDED: Failure categorization and recovery (ERROR #5 FIX)\n# ‚úÖ ADDED: Next steps guidance (ERROR #6 FIX)\n# \n# Original features preserved:\n# - Hardened fallbacks for missing Cell 0 globals\n# - Multi-GPU aware reporting\n# - Controlled verbose tracebacks\n# - Robust error handling\n# ==============================================================================\n\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\n\n# Robust fallbacks for Cell 0 globals (do not crash if Cell 0 not run)\ntry:\n    _NUM_SAMPLES = NUM_SAMPLES\n    _EPOCHS = EPOCHS\n    _BATCH_SIZE = BATCH_SIZE\n    _ACCUMULATION_STEPS = ACCUMULATION_STEPS\n    _DEVICE = DEVICE\n    _ENABLE_ASBN_TRAINING = ENABLE_ASBN_TRAINING\n    _ENABLE_TRG_INFERENCE = ENABLE_TRG_INFERENCE\n    _PERIODIC_DISCOVERY_FREQUENCY = PERIODIC_DISCOVERY_FREQUENCY\n    _VERBOSE_LOGGING = VERBOSE_LOGGING\n    _USE_MULTI_GPU = USE_MULTI_GPU\n    _NUM_GPUS = NUM_GPUS\n    _HOMOGRAPH_WATCHLIST_BN = HOMOGRAPH_WATCHLIST_BN\nexcept NameError:\n    # sensible defaults\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 5000\n    _VERBOSE_LOGGING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _HOMOGRAPH_WATCHLIST_BN = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n    print(\"[CELL11] Using fallback configuration (Cell 0 not executed)\")\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    \"\"\"Return ceil(a/b) when both ints and b>0, else 0.\"\"\"\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\ndef _format_duration(seconds: float) -> str:\n    \"\"\"Format duration in human-readable form.\"\"\"\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}m\"\n    else:\n        return f\"{seconds/3600:.2f}h\"\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ MAIN EXECUTION WITH COMPREHENSIVE TRACKING\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN FOR KAGGLE T4√ó2 (COMPLETE EXECUTION)\")\n    print(\"=\" * 80)\n\n    # user and timestamp\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    # Configuration summary\n    print(\"\\nConfiguration:\")\n    print(f\"   ‚Ä¢ Samples: {_NUM_SAMPLES}\")\n    print(f\"   ‚Ä¢ Epochs: {_EPOCHS}\")\n    print(f\"   ‚Ä¢ Batch Size: {_BATCH_SIZE}\")\n    print(f\"   ‚Ä¢ Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"   ‚Ä¢ Device: {_DEVICE}\")\n    print(f\"   ‚Ä¢ Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPU(s))\")\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"   ‚Ä¢ Batch per GPU: {per_gpu}\")\n    print(f\"   ‚Ä¢ ASBN Training: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"   ‚Ä¢ TRG Inference: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"   ‚Ä¢ Periodic Discovery: Every {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    # Require main_pipeline defined by Cell 10\n    if 'main_pipeline' not in globals():\n        print(\"\\n‚ùå ERROR: main_pipeline not found - please run Cell 10 before executing this cell.\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 (main_pipeline) not executed\"\n    else:\n        try:\n            print(\"\\nüöÄ Starting full pipeline (this may take a while)...\")\n            print(\"   Expected duration: ~15-45 minutes depending on configuration\")\n            \n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n            \n            print(f\"\\n‚úÖ Pipeline completed in {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n            \n        except KeyboardInterrupt:\n            print(\"\\n‚ö†Ô∏è Execution interrupted by user (KeyboardInterrupt).\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"User manually stopped execution\"\n            \n        except RuntimeError as e:\n            msg = str(e).lower()\n            \n            # Tokenizer-related errors\n            if \"no usable tokenizer class available\" in msg or \"failed to instantiate tokenizer\" in msg or \"sentencepiece\" in msg or \"tokenizers\" in msg:\n                print(f\"\\n‚ùå Pipeline execution failed: {type(e).__name__}\")\n                print(f\"   Error: {str(e)[:400]}\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = \"Tokenizer dependencies missing or incompatible\"\n                \n                print(\"\\nüìã This error indicates the tokenizer could not be instantiated.\")\n                print(\"   Common causes and fixes:\")\n                print(\"   ‚Ä¢ Missing or incompatible 'transformers' package\")\n                print(\"   ‚Ä¢ Missing optional dependencies (sentencepiece, tokenizers)\")\n                print(\"\\nüîß Suggested fix:\")\n                print(\"   Run in a notebook cell:\")\n                print(\"     !pip install transformers==4.30.2 sentencepiece tokenizers --quiet\")\n                print(\"   Then RESTART the kernel and re-run Cells 0‚Üí11 in order.\")\n                \n            # OOM errors\n            elif \"out of memory\" in msg:\n                print(f\"\\n‚ùå Pipeline execution failed: Out of Memory (OOM)\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU ran out of memory during training\"\n                \n                print(\"\\nüîß Suggested fixes:\")\n                print(\"   1. Reduce BATCH_SIZE in Cell 0 (try 2 or 4)\")\n                print(\"   2. Reduce NUM_SAMPLES (try 10000-20000)\")\n                print(\"   3. Increase ACCUMULATION_STEPS to 32 or 64\")\n                print(\"   4. Reduce MAX_LENGTH to 32\")\n                \n            # Generic runtime error\n            else:\n                print(f\"\\n‚ùå Pipeline execution failed: {type(e).__name__}\")\n                print(f\"   Error: {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n                \n            if _VERBOSE_LOGGING:\n                print(\"\\nüìú Full traceback (VERBOSE):\")\n                traceback.print_exc()\n            else:\n                print(\"\\nüí° Set VERBOSE_LOGGING = True in Cell 0 to see full traceback.\")\n                \n        except Exception as e:\n            print(f\"\\n‚ùå Pipeline execution failed: {type(e).__name__}\")\n            print(f\"   Error: {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n            \n            if _VERBOSE_LOGGING:\n                print(\"\\nüìú Full traceback (VERBOSE):\")\n                traceback.print_exc()\n            else:\n                print(\"\\nüí° Set VERBOSE_LOGGING = True in Cell 0 to see full traceback.\")\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #2 + #3: POST-RUN VALIDATION AND METRICS SUMMARY\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"‚úÖ SYSTEM INITIALIZATION SUCCEEDED\")\n        print(\"=\" * 80)\n        \n        # ‚úÖ FIX #2: CHECKPOINT VALIDATION\n        print(\"\\n[CHECKPOINT VALIDATION]\")\n        checkpoint_valid = False\n        checkpoint_path = \"tatn_kaggle_final.pt\"\n        \n        try:\n            if os.path.exists(checkpoint_path):\n                checkpoint_size = os.path.getsize(checkpoint_path) / (1024**2)\n                print(f\"  ‚úì Checkpoint file exists: {checkpoint_path}\")\n                print(f\"  ‚úì Size: {checkpoint_size:.1f} MB\")\n                \n                # Verify checkpoint contents\n                try:\n                    ckpt = torch.load(checkpoint_path, map_location='cpu')\n                    \n                    has_model = 'model_state_dict' in ckpt and len(ckpt['model_state_dict']) > 0\n                    has_dscd = 'dscd_state_dict' in ckpt and len(ckpt['dscd_state_dict']) > 0\n                    has_training = 'training_stats' in ckpt and ckpt['training_stats']\n                    \n                    print(f\"  ‚úì Model state: {'Present' if has_model else '‚ùå MISSING'}\")\n                    print(f\"  ‚úì DSCD state: {'Present' if has_dscd else '‚ùå MISSING'}\")\n                    print(f\"  ‚úì Training stats: {'Present' if has_training else 'Missing'}\")\n                    \n                    if has_dscd:\n                        num_tokens = len(ckpt['dscd_state_dict'].get('prototype_stores', {}))\n                        print(f\"  ‚úì DSCD tokens: {num_tokens}\")\n                        \n                        if num_tokens > 0:\n                            checkpoint_valid = True\n                            print(f\"  ‚úÖ Checkpoint is VALID and ready for inference\")\n                        else:\n                            print(f\"  ‚ö†Ô∏è WARNING: Checkpoint has EMPTY DSCD state!\")\n                            print(f\"     ‚Üí Model can translate but won't disambiguate homographs\")\n                    else:\n                        print(f\"  ‚ùå CRITICAL: Checkpoint missing DSCD state!\")\n                        print(f\"     ‚Üí Inference will fail - need to re-run discovery phase\")\n                    \n                except Exception as e:\n                    print(f\"  ‚ö†Ô∏è Could not verify checkpoint contents: {type(e).__name__}\")\n                    \n            else:\n                print(f\"  ‚ùå Checkpoint file NOT FOUND: {checkpoint_path}\")\n                print(f\"     ‚Üí Pipeline may have failed during save phase\")\n                \n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è Checkpoint validation failed: {type(e).__name__}\")\n        \n        # ‚úÖ FIX #3: COMPREHENSIVE METRICS SUMMARY\n        print(\"\\n[PERFORMANCE METRICS]\")\n        \n        try:\n            # Try to extract metrics from the checkpoint\n            if os.path.exists(checkpoint_path):\n                ckpt = torch.load(checkpoint_path, map_location='cpu')\n                \n                # Training metrics\n                training_stats = ckpt.get('training_stats', {})\n                if training_stats:\n                    total_loss = training_stats.get('total_loss', [])\n                    optimizer_updates = training_stats.get('optimizer_updates', 0)\n                    \n                    print(f\"  Training:\")\n                    print(f\"    ‚Ä¢ Optimizer updates: {optimizer_updates}\")\n                    if total_loss:\n                        avg_loss = sum(total_loss) / len(total_loss)\n                        final_loss = sum(total_loss[-100:]) / len(total_loss[-100:]) if len(total_loss) >= 100 else avg_loss\n                        print(f\"    ‚Ä¢ Avg loss: {avg_loss:.6f}\")\n                        print(f\"    ‚Ä¢ Final loss: {final_loss:.6f}\")\n                \n                # Discovery metrics\n                total_prototypes = ckpt.get('total_prototypes', 0)\n                multi_sense_words = ckpt.get('multi_sense_words', 0)\n                discovery_success = ckpt.get('discovery_success', False)\n                \n                print(f\"\\n  Discovery:\")\n                print(f\"    ‚Ä¢ Status: {'‚úì SUCCESS' if discovery_success else '‚ö†Ô∏è Had issues'}\")\n                print(f\"    ‚Ä¢ Total prototypes: {total_prototypes}\")\n                print(f\"    ‚Ä¢ Multi-sense words: {multi_sense_words}\")\n                if total_prototypes > 0:\n                    ratio = multi_sense_words / total_prototypes\n                    print(f\"    ‚Ä¢ Multi-sense ratio: {ratio:.1%}\")\n                \n                # Evaluation metrics\n                eval_results = ckpt.get('eval_results', {})\n                baseline_metrics = ckpt.get('baseline_metrics', {})\n                \n                if eval_results:\n                    print(f\"\\n  Evaluation:\")\n                    final_success = eval_results.get('success_rate_pct', 0)\n                    total_expl = eval_results.get('total_explanations', 0)\n                    \n                    if baseline_metrics:\n                        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n                        improvement = final_success - baseline_success\n                        print(f\"    ‚Ä¢ Baseline: {baseline_success:.1f}% success rate\")\n                        print(f\"    ‚Ä¢ Final: {final_success:.1f}% success rate\")\n                        print(f\"    ‚Ä¢ Improvement: {improvement:+.1f}%\")\n                    else:\n                        print(f\"    ‚Ä¢ Success rate: {final_success:.1f}%\")\n                    \n                    print(f\"    ‚Ä¢ Total explanations: {total_expl}\")\n                    \n                    # Quality metrics\n                    quality = eval_results.get('quality_metrics', {})\n                    if quality:\n                        avg_conf = quality.get('avg_confidence', 0)\n                        high_conf = quality.get('high_confidence_count', 0)\n                        conf_samples = quality.get('confidence_samples', 1)\n                        print(f\"    ‚Ä¢ Avg confidence: {avg_conf:.3f}\")\n                        print(f\"    ‚Ä¢ High confidence rate: {high_conf}/{conf_samples} ({high_conf/conf_samples:.1%})\")\n                    \n                    # Homograph detection\n                    homo_tracking = eval_results.get('homograph_tracking', {})\n                    if homo_tracking:\n                        detected = len(homo_tracking.get('detected_homographs', set()))\n                        expected = len(homo_tracking.get('expected_homographs', set()))\n                        print(f\"    ‚Ä¢ Homographs detected: {detected}/{expected}\")\n                        \n                        if detected > 0:\n                            detected_words = homo_tracking.get('detected_homographs', set())\n                            print(f\"      ‚Üí Words: {', '.join(sorted(detected_words))}\")\n                \n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è Could not extract metrics: {type(e).__name__}\")\n        \n        # System capabilities\n        print(\"\\n[SYSTEM CAPABILITIES]\")\n        print(\"  ‚úì Bengali ‚Üí English translation\")\n        print(\"  ‚úì Automatic homograph disambiguation (DSCD + TRG)\")\n        print(\"  ‚úì Dynamic prototype discovery (hierarchical clustering)\")\n        if _USE_MULTI_GPU:\n            print(f\"  ‚úì Multi-GPU acceleration ({_NUM_GPUS} GPUs)\")\n        print(\"=\" * 80)\n\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ FIX #4: COMPREHENSIVE INFERENCE VALIDATION WITH HOMOGRAPHS\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        print(\"\\n[INFERENCE VALIDATION]\")\n        print(\"Testing homograph disambiguation with known ambiguous words...\")\n        print(\"-\" * 80)\n        \n        inference_success_count = 0\n        inference_failed_count = 0\n        homographs_detected = set()\n        \n        test_sentences = [\n            (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"‡¶ï‡¶≤ (tap/call)\"),\n            (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"‡¶ï‡¶æ‡¶≤ (tomorrow/yesterday)\"),\n            (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"‡¶™‡¶æ‡¶§‡¶æ (leaf/page)\"),\n        ]\n        \n        try:\n            if 'translate_with_explanations' in globals():\n                for idx, (sentence, description) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}. {description}\")\n                        print(f\"   Input: {sentence}\")\n                        \n                        res = translate_with_explanations(trained_model, tokenizer, sentence)\n                        \n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations', []) or []\n                            \n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous words: {amb_count}\")\n                            \n                            if exs:\n                                for exp in exs:\n                                    word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                    clean_word = str(word).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                                    \n                                    # Track detected homographs\n                                    if clean_word in _HOMOGRAPH_WATCHLIST_BN:\n                                        homographs_detected.add(clean_word)\n                                    \n                                    try:\n                                        conf = float(exp.get('confidence', 0.5))\n                                        span = float(exp.get('span', 0.0))\n                                        u = float(exp.get('uncertainty', 0.0))\n                                        print(f\"   ‚Üí '{word}': conf={conf:.3f}, span={span:.3f}, u={u:.3f}\")\n                                    except Exception:\n                                        print(f\"   ‚Üí '{word}': (metrics unavailable)\")\n                                \n                                inference_success_count += 1\n                            else:\n                                print(f\"   ‚ö†Ô∏è No explanations (high-confidence or filtering)\")\n                                inference_success_count += 1  # Still successful translation\n                        else:\n                            print(f\"   ‚ö†Ô∏è Unexpected result format\")\n                            inference_failed_count += 1\n                            \n                    except Exception as e:\n                        print(f\"   ‚ùå Failed: {type(e).__name__}: {str(e)[:100]}\")\n                        inference_failed_count += 1\n                \n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Inference validation: {inference_success_count}/{len(test_sentences)} successful\")\n                \n                if homographs_detected:\n                    print(f\"‚úÖ Homographs detected: {', '.join(sorted(homographs_detected))}\")\n                else:\n                    print(f\"‚ö†Ô∏è No homographs detected - check TRG thresholds or DSCD state\")\n                \n            else:\n                print(\"‚ö†Ô∏è translate_with_explanations not available - ensure Cell 8 is run\")\n                \n        except Exception as e:\n            print(f\"‚ùå Inference validation failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ FIX #6: NEXT STEPS GUIDANCE\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"üìö NEXT STEPS - HOW TO USE YOUR TRAINED MODEL\")\n        print(\"=\" * 80)\n        \n        print(\"\\n1Ô∏è‚É£ SINGLE SENTENCE TRANSLATION:\")\n        print(\"   ```python\")\n        print(\"   result = translate_with_explanations(trained_model, tokenizer, '‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§')\")\n        print(\"   print(result['translation'])\")\n        print(\"   print(result['explanations'])\")\n        print(\"   ```\")\n        \n        print(\"\\n2Ô∏è‚É£ BATCH TRANSLATION:\")\n        print(\"   ```python\")\n        print(\"   sentences = ['‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§', '‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§']\")\n        print(\"   for sent in sentences:\")\n        print(\"       res = translate_with_explanations(trained_model, tokenizer, sent)\")\n        print(\"       print(f'{sent} ‚Üí {res[\\\"translation\\\"]}')\")\n        print(\"   ```\")\n        \n        print(\"\\n3Ô∏è‚É£ LOAD CHECKPOINT (for later use):\")\n        print(\"   ```python\")\n        print(\"   checkpoint = torch.load('tatn_kaggle_final.pt', map_location='cpu')\")\n        print(\"   model.load_state_dict(checkpoint['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(checkpoint['dscd_state_dict'])\")\n        print(\"   model.eval()\")\n        print(\"   ```\")\n        \n        print(\"\\n4Ô∏è‚É£ RUN COMPREHENSIVE EVALUATION:\")\n        print(\"   ```python\")\n        print(\"   eval_results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n        print(\"   print(eval_results['success_rate_pct'])\")\n        print(\"   ```\")\n        \n        print(\"\\n5Ô∏è‚É£ DEMONSTRATE SYSTEM:\")\n        print(\"   ```python\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n        print(\"   ```\")\n        \n        if not checkpoint_valid:\n            print(\"\\n‚ö†Ô∏è WARNING: Checkpoint validation had issues!\")\n            print(\"   Before deployment, re-run Cell 10 to regenerate checkpoint with valid DSCD state.\")\n        \n        print(\"\\n\" + \"=\" * 80)\n    \n    else:\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ FIX #5: DETAILED FAILURE CATEGORIZATION AND RECOVERY\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"‚ùå SYSTEM INITIALIZATION FAILED\")\n        print(\"=\" * 80)\n        \n        print(f\"\\nFailure Category: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details}\")\n        \n        print(\"\\n[COMPONENT DIAGNOSTICS]\")\n        \n        # Check which components are available\n        print(\"\\nChecking prerequisites:\")\n        \n        components = {\n            'Cell 0 (Configuration)': 'NUM_SAMPLES' in globals(),\n            'Cell 1 (Utilities)': 'reconstruct_word_spans' in globals(),\n            'Cell 2 (Dataset)': 'MemoryEfficientDataset' in globals(),\n            'Cell 3 (DSCD)': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4 (ASBN)': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5 (TRG)': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6 (Model)': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7 (Training)': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8 (Inference)': 'translate_with_explanations' in globals(),\n            'Cell 9 (Evaluation)': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10 (Pipeline)': 'main_pipeline' in globals(),\n        }\n        \n        all_present = True\n        for comp, present in components.items():\n            status = \"‚úì\" if present else \"‚ùå\"\n            print(f\"  {status} {comp}\")\n            if not present:\n                all_present = False\n        \n        if not all_present:\n            print(\"\\n‚ö†Ô∏è Some components are missing!\")\n            print(\"   ‚Üí Run all cells 0-10 in order before executing Cell 11\")\n        \n        print(\"\\n[TARGETED RECOVERY STEPS]\")\n        \n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\nüîß Recovery: Run Cells 0-10 in sequence\")\n            print(\"   1. Execute Cell 0 (Configuration)\")\n            print(\"   2. Execute Cells 1-9 (Components)\")\n            print(\"   3. Execute Cell 10 (Pipeline)\")\n            print(\"   4. Re-run this cell (Cell 11)\")\n            \n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\nüîß Recovery: Install tokenizer dependencies\")\n            print(\"   1. Run in a notebook cell:\")\n            print(\"      !pip install transformers==4.30.2 sentencepiece tokenizers --quiet\")\n            print(\"   2. RESTART the kernel (important!)\")\n            print(\"   3. Re-run all cells 0-11 in order\")\n            \n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\nüîß Recovery: Reduce memory usage\")\n            print(\"   1. In Cell 0, reduce BATCH_SIZE to 2 or 4\")\n            print(\"   2. Reduce NUM_SAMPLES to 10000-20000\")\n            print(\"   3. Increase ACCUMULATION_STEPS to 32 or 64\")\n            print(\"   4. Reduce MAX_LENGTH to 32\")\n            print(\"   5. Re-run all cells 0-11\")\n            \n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\nüîß Recovery: Debug runtime error\")\n            print(\"   1. Set VERBOSE_LOGGING = True in Cell 0\")\n            print(\"   2. Re-run Cell 11 to see full traceback\")\n            print(\"   3. Check the specific error message\")\n            print(\"   4. Verify GPU availability: torch.cuda.is_available()\")\n            \n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\nüîß Recovery: Resume from checkpoint (if available)\")\n            print(\"   1. Check if checkpoint exists: 'tatn_kaggle_final.pt'\")\n            print(\"   2. If yes, you can load it and skip training:\")\n            print(\"      model.load_state_dict(torch.load('tatn_kaggle_final.pt')['model_state_dict'])\")\n            print(\"   3. If no, re-run Cell 11 and let it complete\")\n            \n        else:\n            print(\"\\nüîß General recovery steps:\")\n            print(\"   1. Set VERBOSE_LOGGING = True in Cell 0 to see detailed errors\")\n            print(\"   2. Re-run all cells 0-11 in order\")\n            print(\"   3. Check that GPUs are available and CUDA is working\")\n            print(\"   4. Verify training data loaded successfully\")\n        \n        print(\"\\n[ADDITIONAL TROUBLESHOOTING]\")\n        print(\"  ‚Ä¢ Ensure Cells 0-10 executed without errors\")\n        print(\"  ‚Ä¢ Check GPU availability: torch.cuda.is_available()\")\n        print(\"  ‚Ä¢ Verify CUDA version matches PyTorch installation\")\n        print(\"  ‚Ä¢ Check disk space for checkpoint saving\")\n        print(\"  ‚Ä¢ If persistent issues, try reducing configuration parameters\")\n        \n        print(\"\\n\" + \"=\" * 80)\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1: EXECUTION TIME SUMMARY\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    total_duration = time.time() - start_time\n    end_time_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_time_utc}\")\n    print(f\"Total duration: {_format_duration(total_duration)}\")\n    \n    if pipeline_success:\n        print(f\"Status: ‚úÖ SUCCESS\")\n        if checkpoint_valid:\n            print(f\"Checkpoint: ‚úÖ VALID\")\n        else:\n            print(f\"Checkpoint: ‚ö†Ô∏è NEEDS VERIFICATION\")\n    else:\n        print(f\"Status: ‚ùå FAILED ({failure_category or 'UNKNOWN'})\")\n    \n    print(\"=\" * 80)\n    print(\"\\nCELL 11: Execution wrapper finished.\")\n    print(\"=\" * 80)","metadata":{"id":"9n4Hrn1wH4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Smoke test\nasbn = MemoryEfficientASBNModule(embed_dim=1024)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# create a fake batch [B, T, H]\nh = torch.randn(1, 10, 1024).to(device)\nproto_probs = None\nuncertainties = None\ngates = None\nasbn.train()\nenc_loss, mon_loss, _, _ = asbn.forward_with_grl_simplified(h, proto_probs, uncertainties, gates, token_word_map=None)\nprint(\"enc_loss:\", enc_loss, \"monitor_loss:\", mon_loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12: EXTENDED INFERENCE TESTING - COMPLETELY FIXED\n# ==============================================================================\n# ‚úÖ FIXED: Load DSCD state from checkpoint (ERROR #1 FIX)\n# ‚úÖ FIXED: Validate checkpoint has DSCD data (ERROR #2 FIX)\n# ‚úÖ FIXED: Track quality metrics (confidence, span, uncertainty) (ERROR #3 FIX)\n# ‚úÖ FIXED: Track homograph detection against watchlist (ERROR #4 FIX)\n# ‚úÖ ADDED: Validate warmup success (ERROR #5 FIX)\n# ‚úÖ ADDED: Compare translations to expected outputs (ERROR #6 FIX)\n# ‚úÖ ADDED: Comprehensive quality report\n# \n# Original features preserved:\n# - Robust checkpoint loading with multiple fallbacks\n# - Safe device mapping and embedding resize\n# - Optional warmup when prototypes empty\n# - Controlled verbose tracebacks\n# ==============================================================================\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Any, Dict, List, Optional\nfrom collections import defaultdict\n\nimport torch\n\n# -------------------------\n# Local fallbacks for globals (safe)\n# -------------------------\ntry:\n    _DEVICE = DEVICE\n    _USE_MULTI_GPU = USE_MULTI_GPU\n    _NUM_GPUS = NUM_GPUS\n    _VERBOSE_LOGGING = VERBOSE_LOGGING\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _VERBOSE_LOGGING = False\n    print(\"[CELL12] Warning: using fallback device/settings\")\n\n# ‚úÖ Import homograph watchlist\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n# Helpers -----------------------------------------------------------------------\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING:\n        traceback.print_exc()\n    else:\n        _safe_print(\"   (set VERBOSE_LOGGING = True for full traceback)\")\n\n\n# ‚úÖ FIX #6: Translation similarity helper\ndef _compute_similarity(translation: str, expected: str) -> float:\n    \"\"\"Compute word-overlap similarity between translation and expected.\"\"\"\n    try:\n        trans_words = set(translation.lower().split())\n        exp_words = set(expected.lower().split())\n        if not exp_words:\n            return 0.0\n        overlap = len(trans_words & exp_words)\n        return overlap / len(exp_words)\n    except Exception:\n        return 0.0\n\n\n# ------------------------------------------------------------------------------\n# Check runtime prerequisites (informational)\ntrained_model_available = 'trained_model' in globals() and globals().get('trained_model') is not None\ntokenizer_available = 'tokenizer' in globals() and globals().get('tokenizer') is not None\ntranslate_available = 'translate_with_explanations' in globals()\n\nif not trained_model_available:\n    _safe_print(\"‚ö†Ô∏è trained_model not found in globals. You can load a saved checkpoint if available.\")\nif not tokenizer_available:\n    _safe_print(\"‚ö†Ô∏è tokenizer not found in globals. Please run the pipeline or load a tokenizer first.\")\nif not translate_available:\n    _safe_print(\"‚ö†Ô∏è translate_with_explanations not found. Ensure Cell 8 (inference utilities) has been executed.\")\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #1 + #2: ENHANCED CHECKPOINT LOADER WITH DSCD STATE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef try_load_checkpoint(checkpoint_path: str, tokenizer) -> Tuple[bool, Any]:\n    \"\"\"\n    Try to load a checkpoint file into a freshly instantiated model.\n    \n    ‚úÖ FIX #1: Loads DSCD state from checkpoint\n    ‚úÖ FIX #2: Validates DSCD state exists and is non-empty\n    \n    Returns (success, model_instance_or_error).\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        return False, f\"Checkpoint path not found: {checkpoint_path}\"\n\n    if 'MemoryOptimizedTATNWithExplanations' not in globals():\n        return False, \"Model class MemoryOptimizedTATNWithExplanations not available in current session.\"\n\n    _safe_print(f\"[CELL12] Loading checkpoint from: {checkpoint_path}\")\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n    except Exception as e:\n        _safe_print(f\"[CELL12] Failed to load checkpoint file: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        return False, e\n\n    # ‚úÖ FIX #2: VALIDATE CHECKPOINT STRUCTURE\n    _safe_print(\"[CELL12] Validating checkpoint structure...\")\n    \n    # Check for model state\n    state = None\n    try:\n        if isinstance(ckpt, dict):\n            for k in (\"model_state_dict\", \"state_dict\", \"model\", \"model_state\"):\n                if k in ckpt and isinstance(ckpt[k], dict):\n                    state = ckpt[k]\n                    break\n            if state is None:\n                sample_vals = list(ckpt.values())[:10]\n                if any(torch.is_tensor(v) for v in sample_vals):\n                    state = ckpt\n        else:\n            state = ckpt\n    except Exception as e:\n        _safe_print(f\"[CELL12] Error inspecting checkpoint: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    if state is None:\n        return False, \"Could not find model state-dict in checkpoint.\"\n    \n    _safe_print(f\"[CELL12] ‚úì Model state found ({len(state)} keys)\")\n    \n    # ‚úÖ FIX #2: CHECK FOR DSCD STATE\n    dscd_state = None\n    if isinstance(ckpt, dict) and 'dscd_state_dict' in ckpt:\n        dscd_state = ckpt['dscd_state_dict']\n        if dscd_state and isinstance(dscd_state, dict):\n            num_tokens = len(dscd_state.get('prototype_stores', {}))\n            _safe_print(f\"[CELL12] ‚úì DSCD state found ({num_tokens} tokens)\")\n            \n            if num_tokens == 0:\n                _safe_print(\"[CELL12] ‚ö†Ô∏è WARNING: DSCD state is EMPTY!\")\n                _safe_print(\"[CELL12]    Model will load but homograph detection won't work\")\n                _safe_print(\"[CELL12]    Consider running warmup after loading\")\n        else:\n            _safe_print(\"[CELL12] ‚ö†Ô∏è WARNING: DSCD state exists but is not valid dict\")\n    else:\n        _safe_print(\"[CELL12] ‚ö†Ô∏è WARNING: No DSCD state in checkpoint!\")\n        _safe_print(\"[CELL12]    Homograph detection will NOT work without warmup\")\n\n    # Instantiate model\n    try:\n        model_inst = MemoryOptimizedTATNWithExplanations(tokenizer)\n    except Exception as e:\n        _safe_print(f\"[CELL12] Failed to instantiate model: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False, e\n\n    # Resize embeddings if needed\n    try:\n        mbart = getattr(model_inst, \"mbart\", None)\n        if mbart is not None and hasattr(mbart, \"get_input_embeddings\"):\n            emb = mbart.get_input_embeddings()\n            cur = getattr(emb, \"num_embeddings\", None)\n            tok_len = None\n            \n            try:\n                if tokenizer is None:\n                    tok_len = None\n                elif hasattr(tokenizer, \"vocab_size\") and tokenizer.vocab_size:\n                    tok_len = int(tokenizer.vocab_size)\n                elif hasattr(tokenizer, \"__len__\"):\n                    tok_len = int(len(tokenizer))\n            except Exception:\n                tok_len = None\n\n            if cur is not None and tok_len is not None and int(cur) != int(tok_len) and int(tok_len) > 0:\n                _safe_print(f\"[CELL12] Resizing embeddings: {cur} -> {tok_len}\")\n                try:\n                    mbart.resize_token_embeddings(tok_len)\n                except Exception as ex:\n                    _safe_print(f\"[CELL12] Embedding resize failed: {type(ex).__name__}\")\n                    _maybe_traceback(ex)\n    except Exception as e:\n        _safe_print(f\"[CELL12] Embedding resize warning: {type(e).__name__}\")\n\n    # Load model state\n    def _load_and_report(state_dict: Dict[str, Any]) -> Tuple[bool, List[str], List[str]]:\n        try:\n            res = model_inst.load_state_dict(state_dict, strict=False)\n            missing, unexpected = [], []\n            \n            if hasattr(res, \"missing_keys\") or hasattr(res, \"unexpected_keys\"):\n                missing = list(getattr(res, \"missing_keys\", []) or [])\n                unexpected = list(getattr(res, \"unexpected_keys\", []) or [])\n            else:\n                try:\n                    if isinstance(res, (tuple, list)) and len(res) == 2:\n                        missing = list(res[0]) or []\n                        unexpected = list(res[1]) or []\n                except Exception:\n                    missing, unexpected = [], []\n            return True, missing, unexpected\n        except Exception as e:\n            return False, [str(e)], []\n\n    # Load model state with fallback\n    try:\n        ok, missing, unexpected = _load_and_report(state)\n        if not ok:\n            raise RuntimeError(f\"Primary load_state_dict failed: {missing}\")\n        _safe_print(f\"[CELL12] ‚úì Model state loaded (missing: {len(missing)}, unexpected: {len(unexpected)})\")\n        \n        if _VERBOSE_LOGGING and missing:\n            _safe_print(f\"  Missing keys (first 10): {missing[:10]}\")\n            \n    except Exception as e:\n        _safe_print(f\"[CELL12] load_state_dict raised: {type(e).__name__}\")\n        _maybe_traceback(e)\n        \n        # Retry with stripped prefixes\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\", 1) if isinstance(k, str) and k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                ok, missing, unexpected = _load_and_report(new_state)\n                if ok:\n                    _safe_print(\"[CELL12] ‚úì Loaded after stripping 'module.' prefixes\")\n                else:\n                    raise RuntimeError(f\"Retry failed: {missing}\")\n            else:\n                raise RuntimeError(\"State-dict not a dict; cannot strip prefixes\")\n        except Exception as e2:\n            _safe_print(f\"[CELL12] Retry failed: {type(e2).__name__}\")\n            _maybe_traceback(e2)\n            return False, e2\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #1: LOAD DSCD STATE\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    if dscd_state is not None:\n        _safe_print(\"[CELL12] Loading DSCD state...\")\n        try:\n            dscd = model_inst.dscd if hasattr(model_inst, 'dscd') else None\n            \n            if dscd and hasattr(dscd, 'load_state_dict'):\n                dscd.load_state_dict(dscd_state)\n                \n                # Verify loaded successfully\n                num_tokens = len(dscd.prototype_stores) if hasattr(dscd, 'prototype_stores') else 0\n                _safe_print(f\"[CELL12] ‚úÖ DSCD state loaded successfully ({num_tokens} tokens)\")\n                \n                if num_tokens == 0:\n                    _safe_print(\"[CELL12] ‚ö†Ô∏è WARNING: DSCD loaded but has 0 tokens!\")\n                    _safe_print(\"[CELL12]    Warmup will be needed for homograph detection\")\n                    \n            elif dscd:\n                _safe_print(\"[CELL12] ‚ö†Ô∏è DSCD exists but has no load_state_dict method\")\n                _safe_print(\"[CELL12]    Attempting manual state restoration...\")\n                \n                # Manual restoration fallback\n                try:\n                    if 'prototype_stores' in dscd_state:\n                        dscd.prototype_stores = dscd_state['prototype_stores']\n                        _safe_print(\"[CELL12] ‚úì Manually restored prototype_stores\")\n                except Exception as e:\n                    _safe_print(f\"[CELL12] Manual restoration failed: {type(e).__name__}\")\n            else:\n                _safe_print(\"[CELL12] ‚ö†Ô∏è Model has no DSCD component!\")\n                \n        except Exception as e:\n            _safe_print(f\"[CELL12] DSCD state loading failed: {type(e).__name__}\")\n            _maybe_traceback(e)\n            _safe_print(\"[CELL12] ‚ö†Ô∏è Model loaded but DSCD state NOT restored\")\n            _safe_print(\"[CELL12]    Homograph detection will require warmup\")\n    else:\n        _safe_print(\"[CELL12] ‚ö†Ô∏è No DSCD state to load - warmup will be required\")\n\n    # Move to device and set eval\n    try:\n        model_inst.to(_DEVICE)\n        model_inst.eval()\n    except Exception as e:\n        try:\n            core = model_inst.module if hasattr(model_inst, \"module\") else model_inst\n            core.to(_DEVICE)\n            core.eval()\n            model_inst = core\n        except Exception:\n            _safe_print(f\"[CELL12] Failed to move to device: {type(e).__name__}\")\n            _maybe_traceback(e)\n            return False, e\n\n    _safe_print(f\"[CELL12] ‚úÖ Model ready on device: {_DEVICE}\")\n    return True, model_inst\n\n\n# ------------------------------------------------------------------------------\n# If checkpoint exists, load it\nif os.path.exists(\"tatn_kaggle_final.pt\") and tokenizer_available:\n    succ, model_or_err = try_load_checkpoint(\"tatn_kaggle_final.pt\", globals().get(\"tokenizer\"))\n    if succ:\n        globals()['trained_model'] = model_or_err\n        trained_model_available = True\n        _safe_print(\"[CELL12] ‚úÖ Checkpoint loaded for inference testing\")\n    else:\n        _safe_print(\"[CELL12] ‚ùå Checkpoint load failed; falling back to trained_model from runtime\")\n        if isinstance(model_or_err, Exception):\n            _maybe_traceback(model_or_err)\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #5: ENHANCED WARMUP WITH VALIDATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef maybe_run_warmup_if_needed(model, tokenizer, warmup_sents: int = 4000):\n    \"\"\"\n    If DSCD prototype stores are empty, run warmup and VALIDATE success.\n    \n    ‚úÖ FIX #5: Validates that prototypes were actually created\n    \"\"\"\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        \n        if dscd is None:\n            _safe_print(\"[CELL12] No DSCD component - skipping warmup\")\n            return False\n        \n        proto_stores = getattr(dscd, \"prototype_stores\", None)\n        initial_count = len(proto_stores) if proto_stores else 0\n        \n        if initial_count > 0:\n            _safe_print(f\"[CELL12] ‚úì DSCD already has {initial_count} prototype tokens - skipping warmup\")\n            return True\n        \n        # Need warmup\n        _safe_print(\"[CELL12] ‚ö†Ô∏è DSCD prototype stores are EMPTY\")\n        _safe_print(\"[CELL12] Running warmup to build prototypes...\")\n        \n        if 'dscd_discovery_warmup' not in globals():\n            _safe_print(\"[CELL12] ‚ùå dscd_discovery_warmup not available\")\n            return False\n        \n        try:\n            dscd_discovery_warmup(model, tokenizer, num_sents=warmup_sents, max_len=globals().get(\"MAX_LENGTH\", 48))\n            \n            # ‚úÖ FIX #5: VALIDATE WARMUP SUCCESS\n            proto_stores_after = getattr(dscd, \"prototype_stores\", None)\n            final_count = len(proto_stores_after) if proto_stores_after else 0\n            \n            if final_count > 0:\n                multi_sense = sum(1 for store in proto_stores_after.values() \n                                 if len(getattr(store, 'centroids', [])) >= 2)\n                _safe_print(f\"[CELL12] ‚úÖ Warmup successful!\")\n                _safe_print(f\"[CELL12]    Tokens: {final_count}, Multi-sense: {multi_sense}\")\n                return True\n            else:\n                _safe_print(\"[CELL12] ‚ö†Ô∏è Warmup completed but NO prototypes created\")\n                _safe_print(\"[CELL12]    Homograph detection will NOT work\")\n                return False\n                \n        except Exception as e:\n            _safe_print(f\"[CELL12] ‚ùå Warmup failed: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return False\n            \n    except Exception as e:\n        _safe_print(f\"[CELL12] Warmup probe failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n        return False\n\n\n# Prepare test sentences -------------------------------------------------------\ntest_sentences: List[Tuple[str, str, str]] = [\n    (\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I turned off the tap\", \"‡¶ï‡¶≤ = tap/call\"),\n    (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"Tomorrow I will buy a book\", \"‡¶ï‡¶æ‡¶≤ = tomorrow/yesterday\"),\n    (\"‡¶™‡¶æ‡¶§‡¶æ ‡¶ù‡¶∞‡ßá ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§\", \"The leaf has fallen\", \"‡¶™‡¶æ‡¶§‡¶æ = leaf/page\"),\n    (\"‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï ‡¶ó‡ßá‡¶õ‡ßá‡¶®‡•§\", \"He went to the bank\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï = bank/embankment\"),\n    (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø‡•§\", \"I am fine\", \"Simple (no ambiguity)\"),\n    (\"‡¶∏‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶Æ‡¶ø‡¶∑‡ßç‡¶ü‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡•§\", \"She speaks sweetly\", \"Adjective usage\"),\n    (\"‡¶è‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶á‡•§\", \"This is my book\", \"Demonstrative pronoun\"),\n    (\"‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã?\", \"Can you help me?\", \"Question form\"),\n    (\"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã‡•§\", \"The weather is good today\", \"Simple\"),\n    (\"‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶¨‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶ø‡•§\", \"We live in Bangladesh\", \"Country name\"),\n    (\"‡¶∏‡ßÇ‡¶∞‡ßç‡¶Ø ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ì‡¶†‡ßá‡•§\", \"The sun rises in the east\", \"Directional\"),\n    (\"‡¶™‡¶æ‡¶ñ‡¶ø ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá ‡¶â‡¶°‡¶º‡ßá‡•§\", \"Birds fly in the sky\", \"Simple present\"),\n    (\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡•§\", \"She is going to school\", \"Present continuous\"),\n]\n\n# Verify prerequisites ---------------------------------------------------------\nif not (trained_model_available and tokenizer_available and translate_available):\n    _safe_print(\"\\n‚ùå Cannot run extended inference tests. Missing prerequisites.\")\n    _safe_print(\"   Please run the full pipeline (Cells 0-11) or load a checkpoint.\")\nelse:\n    # ‚úÖ FIX #5: Run warmup with validation\n    warmup_success = False\n    try:\n        warmup_success = maybe_run_warmup_if_needed(\n            globals().get('trained_model'), \n            globals().get(\"tokenizer\"), \n            warmup_sents=4000\n        )\n    except Exception as e:\n        _safe_print(f\"[CELL12] Warmup invocation failed: {type(e).__name__}\")\n        _maybe_traceback(e)\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #3 + #4 + #6: COMPREHENSIVE TESTING WITH QUALITY METRICS\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    total = len(test_sentences)\n    successes = 0\n    tests_with_explanations = 0\n    total_ambiguous_detected = 0\n    \n    # ‚úÖ FIX #3: Quality metric tracking\n    quality_metrics = {\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n        'similarities': [],\n    }\n    \n    # ‚úÖ FIX #4: Homograph tracking\n    homographs_detected = set()\n    homograph_explanations = defaultdict(list)\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"CELL 12: EXTENDED INFERENCE TESTING - START\")\n    _safe_print(\"=\" * 80)\n    \n    if not warmup_success:\n        _safe_print(\"\\n‚ö†Ô∏è WARNING: Warmup failed or not needed\")\n        _safe_print(\"   Homograph detection may not work properly\\n\")\n\n    for idx, (sent, expected, note) in enumerate(test_sentences, 1):\n        _safe_print(\"\\n\" + \"-\" * 70)\n        _safe_print(f\"Test {idx}/{total}: {note}\")\n        _safe_print(f\"Input: {sent}\")\n        _safe_print(f\"Expected: {expected}\")\n        \n        try:\n            model_for_infer = globals().get('trained_model')\n            tokenizer = globals().get('tokenizer')\n            \n            if model_for_infer is None or tokenizer is None:\n                raise RuntimeError(\"trained_model or tokenizer missing\")\n\n            try:\n                res = translate_with_explanations(model_for_infer, tokenizer, sent)\n            except Exception as e:\n                _safe_print(f\"[CELL12] translate_with_explanations raised: {type(e).__name__}\")\n                _maybe_traceback(e)\n                res = None\n\n            if res is None:\n                _safe_print(\"[CELL12] Translation returned None - skipping\")\n                continue\n\n            if not isinstance(res, dict):\n                _safe_print(f\"[CELL12] Warning: non-dict result, coercing\")\n                res = {\"translation\": str(res)}\n\n            translation = str(res.get(\"translation\", \"\") or \"\")\n            amb_count = int(res.get(\"ambiguous_words_detected\", 0) or 0)\n            explanations = res.get(\"explanations\", []) or []\n\n            _safe_print(f\"Translation: {translation}\")\n            \n            # ‚úÖ FIX #6: Compute similarity\n            similarity = _compute_similarity(translation, expected)\n            quality_metrics['similarities'].append(similarity)\n            _safe_print(f\"Similarity to expected: {similarity:.1%}\")\n            \n            _safe_print(f\"Ambiguous words detected: {amb_count}\")\n\n            if amb_count > 0:\n                tests_with_explanations += 1\n                total_ambiguous_detected += amb_count\n                _safe_print(\"Explanations:\")\n                \n                for j, e in enumerate(explanations, 1):\n                    try:\n                        word = e.get(\"ambiguous_word\", e.get(\"token\", \"N/A\"))\n                        conf = float(e.get(\"confidence\", 0.5) or 0.5)\n                        u = float(e.get(\"uncertainty\", 0.0) or 0.0)\n                        s = float(e.get(\"span\", 0.0) or 0.0)\n                        \n                        # ‚úÖ FIX #3: Track quality metrics\n                        quality_metrics['confidences'].append(conf)\n                        quality_metrics['spans'].append(s)\n                        quality_metrics['uncertainties'].append(u)\n                        \n                        # ‚úÖ FIX #4: Track homographs\n                        clean_word = str(word).replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                        if clean_word in _HOMOGRAPH_WATCHLIST:\n                            homographs_detected.add(clean_word)\n                            homograph_explanations[clean_word].append({\n                                'sentence': sent,\n                                'confidence': conf,\n                                'span': s,\n                                'uncertainty': u,\n                            })\n                        \n                        marker = \"üî•\" if s > 0.3 else \"  \"\n                        _safe_print(f\"  {j}. {marker} '{word}'  conf={conf:.3f}  U={u:.3f}  S={s:.3f}\")\n                        \n                        expl_text = e.get('explanation', '')\n                        if expl_text:\n                            _safe_print(f\"       {expl_text[:100]}{'...' if len(expl_text) > 100 else ''}\")\n                            \n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        continue\n            else:\n                _safe_print(\"No ambiguity detected\")\n\n            if translation and translation.strip():\n                successes += 1\n                _safe_print(\"‚úì Translation successful\")\n            else:\n                _safe_print(\"‚úó Translation empty/failed\")\n\n        except Exception as e:\n            _safe_print(f\"Test {idx} failed: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ‚úÖ FIX #3 + #4: COMPREHENSIVE QUALITY SUMMARY\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    \n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"CELL 12: COMPREHENSIVE TEST SUMMARY\")\n    _safe_print(\"=\" * 80)\n    \n    # Basic metrics\n    _safe_print(f\"\\n[TRANSLATION QUALITY]\")\n    _safe_print(f\"  Total tests: {total}\")\n    if total > 0:\n        _safe_print(f\"  Successful: {successes} ({successes/total*100:.1f}%)\")\n        _safe_print(f\"  Failed: {total - successes} ({(total-successes)/total*100:.1f}%)\")\n        \n        # ‚úÖ FIX #6: Similarity metrics\n        if quality_metrics['similarities']:\n            avg_sim = sum(quality_metrics['similarities']) / len(quality_metrics['similarities'])\n            _safe_print(f\"  Avg similarity to expected: {avg_sim:.1%}\")\n    \n    # Ambiguity detection\n    _safe_print(f\"\\n[AMBIGUITY DETECTION]\")\n    _safe_print(f\"  Tests with explanations: {tests_with_explanations}/{total} ({tests_with_explanations/total*100:.1f}%)\")\n    _safe_print(f\"  Total ambiguous words: {total_ambiguous_detected}\")\n    if total > 0:\n        _safe_print(f\"  Avg ambiguous per sentence: {total_ambiguous_detected/total:.2f}\")\n    \n    # ‚úÖ FIX #3: Quality metrics\n    if quality_metrics['confidences']:\n        _safe_print(f\"\\n[EXPLANATION QUALITY]\")\n        avg_conf = sum(quality_metrics['confidences']) / len(quality_metrics['confidences'])\n        avg_span = sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n        avg_u = sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n        \n        high_conf = sum(1 for c in quality_metrics['confidences'] if c >= 0.65)\n        low_conf = sum(1 for c in quality_metrics['confidences'] if c < 0.4)\n        \n        _safe_print(f\"  Avg confidence: {avg_conf:.3f}\")\n        _safe_print(f\"  Avg span: {avg_span:.3f}\")\n        _safe_print(f\"  Avg uncertainty: {avg_u:.3f}\")\n        _safe_print(f\"  High confidence (‚â•0.65): {high_conf}/{len(quality_metrics['confidences'])} ({high_conf/len(quality_metrics['confidences']):.1%})\")\n        _safe_print(f\"  Low confidence (<0.4): {low_conf}/{len(quality_metrics['confidences'])} ({low_conf/len(quality_metrics['confidences']):.1%})\")\n    else:\n        _safe_print(f\"\\n[EXPLANATION QUALITY]\")\n        _safe_print(f\"  No explanations generated!\")\n        _safe_print(f\"  ‚ö†Ô∏è This indicates:\")\n        _safe_print(f\"     1. DSCD prototypes are empty (warmup failed)\")\n        _safe_print(f\"     2. TRG thresholds too strict\")\n        _safe_print(f\"     3. No ambiguous words in test set\")\n    \n    # ‚úÖ FIX #4: Homograph detection\n    _safe_print(f\"\\n[HOMOGRAPH DETECTION]\")\n    _safe_print(f\"  Watchlist size: {len(_HOMOGRAPH_WATCHLIST)}\")\n    _safe_print(f\"  Detected: {len(homographs_detected)}\")\n    _safe_print(f\"  Detection rate: {len(homographs_detected)/len(_HOMOGRAPH_WATCHLIST):.1%}\")\n    \n    if homographs_detected:\n        _safe_print(f\"\\n  Detected homographs:\")\n        for homo in sorted(homographs_detected):\n            exps = homograph_explanations[homo]\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps)\n            _safe_print(f\"    ‚úÖ '{homo}': {len(exps)} occurrences, avg_conf={avg_conf:.3f}\")\n    \n    missing = _HOMOGRAPH_WATCHLIST - homographs_detected\n    if missing:\n        _safe_print(f\"\\n  ‚ö†Ô∏è Missing homographs: {', '.join(sorted(missing))}\")\n        _safe_print(f\"     ‚Üí These words were not detected in test sentences\")\n        _safe_print(f\"     ‚Üí Either not in test set or DSCD has no prototypes for them\")\n    \n    # Health assessment\n    _safe_print(f\"\\n[HEALTH ASSESSMENT]\")\n    warnings = []\n    \n    if successes < total * 0.7:\n        warnings.append(\"Low translation success rate (<70%)\")\n    if tests_with_explanations == 0:\n        warnings.append(\"NO explanations generated - DSCD/TRG not working\")\n    if not quality_metrics['confidences']:\n        warnings.append(\"No quality metrics - explanation generation failed\")\n    elif avg_conf < 0.5:\n        warnings.append(\"Low average confidence (<0.5)\")\n    if len(homographs_detected) < len(_HOMOGRAPH_WATCHLIST) * 0.5:\n        warnings.append(\"Less than 50% of homographs detected\")\n    \n    if warnings:\n        for w in warnings:\n            _safe_print(f\"  ‚ö†Ô∏è {w}\")\n    else:\n        _safe_print(f\"  ‚úÖ All systems performing well!\")\n    \n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"Thresholds used: span > 0.3 OR uncertainty > 0.5\")\n    _safe_print(\"Cell 12 testing complete.\")\n    _safe_print(\"=\" * 80)","metadata":{"id":"zWd0uRn7H4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13: LARGE-SCALE EVALUATION - COMPLETELY FIXED WITH RESEARCH METRICS\n# ==============================================================================\n# ‚úÖ FIXED: Add homograph detection metrics (ERROR #1 FIX)\n# ‚úÖ FIXED: Add explanation quality assessment (ERROR #2 FIX)\n# ‚úÖ ADDED: Baseline comparison feature (ERROR #3 FIX)\n# ‚úÖ ADDED: Per-homograph accuracy tracking (ERROR #4 FIX)\n# ‚úÖ FIXED: Enhanced CSV with quality columns (ERROR #5 FIX)\n# ‚úÖ ADDED: Execution time breakdown (ERROR #6 FIX)\n# ‚úÖ ADDED: Comprehensive research report\n# \n# Original features preserved:\n# - Batched generation (VRAM-friendly)\n# - Safe DataParallel handling\n# - BLEU/CHRF/COMET metrics\n# - Progress reporting\n# ==============================================================================\nimport os\nimport sys\nimport warnings\nimport numpy as np\nimport torch\nimport time\nimport csv\nimport traceback\nfrom typing import List, Dict, Tuple, Optional, Any, Iterable\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nwarnings.filterwarnings(\"ignore\")\n\n# Try to import metrics libraries, with safe fallbacks\nHAS_COMET = False\nHAS_BLEU = False\nHAS_CHRF = False\n\ntry:\n    from comet import download_model, load_from_checkpoint\n    HAS_COMET = True\nexcept Exception:\n    HAS_COMET = False\n\ntry:\n    import sacrebleu\n    if hasattr(sacrebleu, \"corpus_bleu\"):\n        HAS_BLEU = True\n    if hasattr(sacrebleu, \"corpus_chrf\"):\n        HAS_CHRF = True\nexcept Exception:\n    HAS_BLEU = False\n    HAS_CHRF = False\n    print(\"[EVAL] SacreBLEU not available: BLEU/CHRF will be skipped (pip install sacrebleu).\")\n\n# Fallbacks\ntry:\n    _DEVICE = DEVICE\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = VERBOSE_LOGGING\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\n# ‚úÖ Import homograph watchlist\ntry:\n    _HOMOGRAPH_WATCHLIST = set(HOMOGRAPH_WATCHLIST_BN)\nexcept Exception:\n    _HOMOGRAPH_WATCHLIST = {\"‡¶ï‡¶≤\", \"‡¶ï‡¶æ‡¶≤\", \"‡¶™‡¶æ‡¶§‡¶æ\", \"‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï\", \"‡¶´‡¶≤\", \"‡¶Æ‡¶æ‡¶•‡¶æ\"}\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING:\n        traceback.print_exc()\n    else:\n        print(\"   (set VERBOSE_LOGGING = True in Cell 0 for full traceback)\")\n\ndef _unwrap_model(model: torch.nn.Module) -> torch.nn.Module:\n    \"\"\"Return core model (unwrap DataParallel/DistributedDataParallel if needed).\"\"\"\n    return model.module if hasattr(model, \"module\") else model\n\ndef _get_forced_bos_id(tokenizer, core_mbart) -> Optional[int]:\n    \"\"\"Try several tokenizer/model attributes to find an English forced BOS id.\"\"\"\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in (\"en\", \"en_XX\", \"en-XX\"):\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = lid\n                        break\n                except Exception:\n                    continue\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            for code in (\"en\", \"en_XX\", \"en-XX\"):\n                forced_id = getattr(tokenizer, \"lang_code_to_id\", {}).get(code, None)\n                if forced_id is not None:\n                    break\n    except Exception:\n        forced_id = None\n    \n    try:\n        if forced_id is None and core_mbart is not None and hasattr(core_mbart, \"config\"):\n            forced_id = getattr(core_mbart.config, \"forced_bos_token_id\", None)\n            if forced_id is None:\n                forced_id = getattr(core_mbart.config, \"decoder_start_token_id\", None)\n    except Exception:\n        forced_id = None\n    return forced_id\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #1 + #2 + #4: RESEARCH METRICS CLASS (HOMOGRAPH + EXPLANATION QUALITY)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass ResearchMetrics:\n    \"\"\"\n    Compute research-specific metrics:\n    - Homograph detection accuracy\n    - Explanation generation rate\n    - Per-word disambiguation accuracy\n    \n    ‚úÖ FIX #1: Measures homograph disambiguation effectiveness\n    ‚úÖ FIX #2: Tracks explanation generation quality\n    ‚úÖ FIX #4: Per-homograph accuracy breakdown\n    \"\"\"\n    \n    def __init__(self, homograph_watchlist: set):\n        self.watchlist = homograph_watchlist\n        self.reset()\n    \n    def reset(self):\n        self.total_sentences = 0\n        self.sentences_with_explanations = 0\n        self.total_explanations = 0\n        self.homographs_detected = set()\n        self.homograph_occurrences = defaultdict(int)\n        self.homograph_detections = defaultdict(int)\n        self.quality_metrics = {\n            'confidences': [],\n            'spans': [],\n            'uncertainties': [],\n        }\n    \n    def record_sentence(self, sentence: str, explanations: List[Dict[str, Any]]):\n        \"\"\"Record explanations for a single sentence.\"\"\"\n        self.total_sentences += 1\n        \n        if explanations:\n            self.sentences_with_explanations += 1\n            self.total_explanations += len(explanations)\n            \n            for exp in explanations:\n                try:\n                    # Track quality\n                    conf = float(exp.get('confidence', 0.5))\n                    span = float(exp.get('span', 0.0))\n                    u = float(exp.get('uncertainty', 0.0))\n                    \n                    self.quality_metrics['confidences'].append(conf)\n                    self.quality_metrics['spans'].append(span)\n                    self.quality_metrics['uncertainties'].append(u)\n                    \n                    # Track homograph detection\n                    word = str(exp.get('ambiguous_word', exp.get('token', '')))\n                    clean_word = word.replace('‚ñÅ', '').replace('ƒ†', '').strip()\n                    \n                    if clean_word in self.watchlist:\n                        self.homographs_detected.add(clean_word)\n                        self.homograph_detections[clean_word] += 1\n                        \n                except Exception:\n                    pass\n        \n        # Track homograph occurrences in source (simple word matching)\n        for word in self.watchlist:\n            if word in sentence:\n                self.homograph_occurrences[word] += 1\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Return comprehensive research metrics.\"\"\"\n        summary = {\n            'total_sentences': self.total_sentences,\n            'sentences_with_explanations': self.sentences_with_explanations,\n            'explanation_rate': self.sentences_with_explanations / max(self.total_sentences, 1),\n            'total_explanations': self.total_explanations,\n            'avg_explanations_per_sentence': self.total_explanations / max(self.total_sentences, 1),\n        }\n        \n        # Quality metrics\n        if self.quality_metrics['confidences']:\n            summary['avg_confidence'] = np.mean(self.quality_metrics['confidences'])\n            summary['avg_span'] = np.mean(self.quality_metrics['spans'])\n            summary['avg_uncertainty'] = np.mean(self.quality_metrics['uncertainties'])\n            summary['high_confidence_rate'] = sum(1 for c in self.quality_metrics['confidences'] if c >= 0.65) / len(self.quality_metrics['confidences'])\n        else:\n            summary['avg_confidence'] = 0.0\n            summary['avg_span'] = 0.0\n            summary['avg_uncertainty'] = 0.0\n            summary['high_confidence_rate'] = 0.0\n        \n        # Homograph detection\n        summary['homographs_detected'] = list(self.homographs_detected)\n        summary['detection_rate'] = len(self.homographs_detected) / len(self.watchlist) if self.watchlist else 0.0\n        \n        # Per-word accuracy\n        summary['per_word_accuracy'] = {}\n        for word in self.watchlist:\n            occurrences = self.homograph_occurrences.get(word, 0)\n            detections = self.homograph_detections.get(word, 0)\n            if occurrences > 0:\n                summary['per_word_accuracy'][word] = {\n                    'occurrences': occurrences,\n                    'detections': detections,\n                    'detection_rate': detections / occurrences,\n                }\n        \n        return summary\n\n\n# -----------------------------------------------------------------------------\n# Large scale metrics class (BLEU/CHRF/COMET)\n# -----------------------------------------------------------------------------\nclass LargeScaleEvaluationMetrics:\n    \"\"\"Compute standard MT metrics on 2000+ samples efficiently.\"\"\"\n\n    def __init__(self, device: Optional[torch.device] = None, batch_size: int = 32):\n        self.device = device or _DEVICE\n        self.batch_size = int(batch_size)\n        self.comet_model = None\n        self.metrics_available = {\"comet\": HAS_COMET, \"bleu\": HAS_BLEU, \"chrf\": HAS_CHRF}\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"INITIALIZING EVALUATION METRICS\")\n        print(\"=\" * 80)\n        print(f\"Device: {self.device}\")\n        print(f\"Batch Size: {self.batch_size}\")\n        print(f\"MT Metrics: BLEU={HAS_BLEU}, CHRF={HAS_CHRF}, COMET={HAS_COMET}\")\n        print(f\"Research Metrics: Homograph Detection, Explanation Quality\")\n        print(\"=\" * 80 + \"\\n\")\n\n        if HAS_COMET:\n            try:\n                print(\"[EVAL] Loading COMET model (this may take some time)...\")\n                try:\n                    model_path = download_model(\"Unbabel/wmt22-comet-da\", saving_directory=\".comet_cache\")\n                    self.comet_model = load_from_checkpoint(model_path)\n                    print(\"[EVAL] ‚úì COMET model loaded successfully\\n\")\n                except Exception:\n                    print(\"[EVAL] COMET automatic load failed; disabling COMET for this run.\")\n                    self.metrics_available[\"comet\"] = False\n                    self.comet_model = None\n            except Exception:\n                self.metrics_available[\"comet\"] = False\n                self.comet_model = None\n\n    def compute_bleu_large(self, references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        if not self.metrics_available[\"bleu\"] or not references or not hypotheses:\n            return {\"bleu\": None, \"error\": \"BLEU unavailable or empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            print(f\"\\n[BLEU] Computing BLEU score on {len(hypotheses)} samples...\")\n            start_time = time.time()\n            try:\n                import sacrebleu\n                score = sacrebleu.corpus_bleu(hypotheses, [references])\n                elapsed = time.time() - start_time\n                result = {\n                    \"bleu\": float(score.score),\n                    \"num_samples\": len(hypotheses),\n                    \"computation_time_sec\": elapsed,\n                }\n                print(f\"[BLEU] ‚úì Score computed in {elapsed:.2f}s\")\n                print(f\"  BLEU Score: {score.score:.2f}/100\")\n                return result\n            except Exception:\n                from sacrebleu import BLEU\n                bleu = BLEU()\n                score = bleu.corpus_score(hypotheses, [references])\n                elapsed = time.time() - start_time\n                result = {\"bleu\": float(score.score), \"num_samples\": len(hypotheses), \"computation_time_sec\": elapsed}\n                print(f\"[BLEU] ‚úì Score computed in {elapsed:.2f}s\")\n                print(f\"  BLEU Score: {score.score:.2f}/100\")\n                return result\n        except Exception as e:\n            print(f\"[BLEU] ‚úó Error computing BLEU: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"bleu\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_chrf_large(self, references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        if not self.metrics_available[\"chrf\"] or not references or not hypotheses:\n            return {\"chrf\": None, \"error\": \"CHRF unavailable or empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            print(f\"\\n[CHRF++] Computing CHRF++ score on {len(hypotheses)} samples...\")\n            start_time = time.time()\n            try:\n                import sacrebleu\n                score = sacrebleu.corpus_chrf(hypotheses, [references], beta=3.0)\n                elapsed = time.time() - start_time\n                result = {\"chrf\": float(score.score), \"num_samples\": len(hypotheses), \"computation_time_sec\": elapsed}\n                print(f\"[CHRF++] ‚úì Score computed in {elapsed:.2f}s\")\n                print(f\"  CHRF++ Score: {score.score:.2f}/100\")\n                return result\n            except Exception:\n                from sacrebleu import CHRF\n                chrf = CHRF(char_order=6, beta=3.0)\n                score = chrf.corpus_score(hypotheses, [references])\n                elapsed = time.time() - start_time\n                result = {\"chrf\": float(score.score), \"num_samples\": len(hypotheses), \"computation_time_sec\": elapsed}\n                print(f\"[CHRF++] ‚úì Score computed in {elapsed:.2f}s\")\n                print(f\"  CHRF++ Score: {score.score:.2f}/100\")\n                return result\n        except Exception as e:\n            print(f\"[CHRF++] ‚úó Error computing CHRF++: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"chrf\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_comet_large(\n        self, source_texts: List[str], references: List[str], hypotheses: List[str]\n    ) -> Dict[str, Any]:\n        if not self.metrics_available[\"comet\"] or self.comet_model is None:\n            return {\"comet\": None, \"error\": \"COMET model unavailable\", \"num_samples\": len(hypotheses)}\n        if not source_texts or not references or not hypotheses:\n            return {\"comet\": None, \"error\": \"Empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            print(f\"\\n[COMET] Computing COMET score on {len(hypotheses)} samples (may take several minutes)...\")\n            start_time = time.time()\n            data = [{\"src\": s, \"ref\": r, \"mt\": h} for s, r, h in zip(source_texts, references, hypotheses)]\n            \n            try:\n                if torch.cuda.is_available():\n                    self.comet_model.to(self.device)\n            except Exception:\n                pass\n            \n            with torch.no_grad():\n                if hasattr(self.comet_model, \"predict\"):\n                    output = self.comet_model.predict(data, batch_size=self.batch_size, gpus=1 if torch.cuda.is_available() else 0)\n                    scores = np.asarray(getattr(output, \"scores\", []) or [], dtype=np.float32)\n                    system_score = getattr(output, \"system_score\", None)\n                else:\n                    scores = []\n                    for i in range(0, len(data), self.batch_size):\n                        batch = data[i : i + self.batch_size]\n                        try:\n                            out = self.comet_model.predict(batch)\n                            scores.extend(getattr(out, \"scores\", []) or [])\n                        except Exception:\n                            break\n                    scores = np.asarray(scores, dtype=np.float32) if scores else np.array([])\n                    system_score = np.mean(scores) if scores.size else None\n            \n            elapsed = time.time() - start_time\n            result = {\n                \"comet\": float(system_score) if system_score is not None else None,\n                \"comet_mean\": float(np.mean(scores)) if scores.size else None,\n                \"comet_median\": float(np.median(scores)) if scores.size else None,\n                \"comet_std\": float(np.std(scores)) if scores.size else None,\n                \"num_samples\": len(hypotheses),\n                \"computation_time_sec\": elapsed,\n            }\n            print(f\"[COMET] ‚úì Score computed in {elapsed:.2f}s ({elapsed/60:.2f} min)\")\n            return result\n        except Exception as e:\n            print(f\"[COMET] ‚úó Error computing COMET: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"comet\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_all_metrics_large(\n        self, source_texts: List[str], references: List[str], hypotheses: List[str]\n    ) -> Dict[str, Any]:\n        results = {\"num_samples\": len(hypotheses), \"metrics\": {}, \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n        if self.metrics_available.get(\"bleu\"):\n            results[\"metrics\"][\"bleu\"] = self.compute_bleu_large(references, hypotheses)\n        if self.metrics_available.get(\"chrf\"):\n            results[\"metrics\"][\"chrf\"] = self.compute_chrf_large(references, hypotheses)\n        if self.metrics_available.get(\"comet\"):\n            results[\"metrics\"][\"comet\"] = self.compute_comet_large(source_texts, references, hypotheses)\n        return results\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ FIX #6: TIMING TRACKER\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass TimingTracker:\n    \"\"\"Track execution time for each phase.\"\"\"\n    def __init__(self):\n        self.timings = {}\n        self.start_times = {}\n    \n    def start(self, phase: str):\n        self.start_times[phase] = time.time()\n    \n    def end(self, phase: str):\n        if phase in self.start_times:\n            elapsed = time.time() - self.start_times[phase]\n            self.timings[phase] = elapsed\n            del self.start_times[phase]\n    \n    def get_summary(self) -> Dict[str, float]:\n        return self.timings.copy()\n    \n    def print_summary(self):\n        total = sum(self.timings.values())\n        print(\"\\n[TIMING BREAKDOWN]\")\n        for phase, elapsed in sorted(self.timings.items(), key=lambda x: -x[1]):\n            percentage = (elapsed / total * 100) if total > 0 else 0\n            print(f\"  {phase:30s}: {elapsed:7.2f}s ({percentage:5.1f}%)\")\n        print(f\"  {'TOTAL':30s}: {total:7.2f}s (100.0%)\")\n\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# ‚úÖ MAIN EVALUATION FUNCTION WITH ALL FIXES\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef evaluate_on_large_dataset(\n    model: torch.nn.Module,\n    tokenizer,\n    dataset: Optional[List[Tuple[str, str]]] = None,\n    num_samples: int = 2000,\n    batch_size: int = 32,\n    save_results: bool = True,\n    max_length: int = 512,\n    compute_research_metrics: bool = True,  # ‚úÖ NEW PARAMETER\n) -> Dict[str, Any]:\n    \"\"\"\n    Evaluate model on large dataset with comprehensive metrics.\n    \n    ‚úÖ FIX #1: Computes homograph detection accuracy\n    ‚úÖ FIX #2: Tracks explanation generation quality\n    ‚úÖ FIX #4: Per-homograph accuracy breakdown\n    ‚úÖ FIX #5: Enhanced CSV output\n    ‚úÖ FIX #6: Detailed timing breakdown\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"LARGE-SCALE COMPREHENSIVE EVALUATION\")\n    print(\"=\" * 80 + \"\\n\")\n    \n    # ‚úÖ FIX #6: Initialize timing tracker\n    timer = TimingTracker()\n    timer.start('total')\n\n    try:\n        # Step 1: Prepare dataset\n        timer.start('data_preparation')\n        print(f\"[PREP] Preparing dataset (requested {num_samples} samples)...\")\n        \n        if dataset is None or len(dataset) == 0:\n            if \"load_and_preprocess_optimized\" in globals():\n                print(\"[PREP] Loading via load_and_preprocess_optimized()\")\n                try:\n                    pairs = load_and_preprocess_optimized(num_samples)\n                except Exception as e:\n                    print(f\"[PREP] Failed: {type(e).__name__}, using dummy data\")\n                    sample_pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I stopped the call.\"), \n                                   (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"I will buy a book tomorrow.\")]\n                    pairs = (sample_pairs * ((num_samples // len(sample_pairs)) + 1))[:num_samples]\n            else:\n                print(\"[PREP] No data loader found; using dummy data\")\n                sample_pairs = [(\"‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶≤ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\", \"I stopped the call.\"),\n                               (\"‡¶ï‡¶æ‡¶≤ ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶ï‡¶ø‡¶®‡¶¨‡•§\", \"I will buy a book tomorrow.\")]\n                pairs = (sample_pairs * ((num_samples // len(sample_pairs)) + 1))[:num_samples]\n        else:\n            pairs = dataset\n\n        pairs = pairs[:num_samples]\n        print(f\"[PREP] ‚úì Loaded {len(pairs)} samples\")\n        timer.end('data_preparation')\n\n        source_texts = [s for s, _ in pairs]\n        references = [r for _, r in pairs]\n        hypotheses: List[str] = []\n        \n        # ‚úÖ FIX #1 + #2: Initialize research metrics tracker\n        research_metrics = ResearchMetrics(_HOMOGRAPH_WATCHLIST) if compute_research_metrics else None\n        explanation_data = []  # ‚úÖ FIX #5: Store for CSV\n\n        # Prepare model\n        core = _unwrap_model(model)\n        core.eval()\n        try:\n            core.to(_DEVICE)\n        except Exception:\n            pass\n\n        gen_callable = None\n        mbart = getattr(core, \"mbart\", None)\n        if mbart is not None and hasattr(mbart, \"generate\"):\n            gen_callable = mbart.generate\n        elif hasattr(core, \"generate\"):\n            gen_callable = core.generate\n        else:\n            raise RuntimeError(\"No generate() found on model or model.mbart\")\n\n        forced_bos = _get_forced_bos_id(tokenizer, mbart)\n\n        # Step 2: Generate translations + explanations\n        timer.start('generation')\n        print(f\"\\n[GEN] Generating predictions with explanations (batch_size={batch_size})...\")\n        \n        n = len(source_texts)\n        batch_size_gen = max(1, int(batch_size))\n        \n        with torch.no_grad():\n            for start in tqdm(range(0, n, batch_size_gen), desc=\"[GEN] Batches\", unit=\"batch\"):\n                batch_srcs = source_texts[start : start + batch_size_gen]\n                \n                # Generate translations (standard pipeline)\n                try:\n                    try:\n                        tokenizer.src_lang = \"bn\"\n                    except Exception:\n                        pass\n\n                    enc = tokenizer(batch_srcs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n                    enc = {k: v.to(_DEVICE) for k, v in enc.items() if isinstance(v, torch.Tensor)}\n                    \n                    gen_kwargs = {\n                        \"max_length\": 256,\n                        \"num_beams\": 5,\n                        \"early_stopping\": True,\n                    }\n                    if forced_bos is not None:\n                        gen_kwargs[\"forced_bos_token_id\"] = int(forced_bos)\n\n                    generated_ids = gen_callable(**enc, **gen_kwargs)\n                    \n                    if isinstance(generated_ids, (list, tuple)):\n                        if len(generated_ids) > 0 and isinstance(generated_ids[0], torch.Tensor):\n                            gen_ids_tensor = generated_ids[0]\n                        else:\n                            try:\n                                gen_ids_tensor = torch.stack([torch.tensor(x) for x in generated_ids], dim=0)\n                            except Exception:\n                                gen_ids_tensor = generated_ids\n                    else:\n                        gen_ids_tensor = generated_ids\n\n                    try:\n                        batch_hyps = tokenizer.batch_decode(gen_ids_tensor, skip_special_tokens=True)\n                    except Exception:\n                        batch_hyps = []\n                        seqs = gen_ids_tensor.cpu().tolist() if isinstance(gen_ids_tensor, torch.Tensor) else list(gen_ids_tensor)\n                        for seq in seqs:\n                            try:\n                                batch_hyps.append(tokenizer.decode(seq, skip_special_tokens=True))\n                            except Exception:\n                                batch_hyps.append(\"\")\n                    \n                    hypotheses.extend(batch_hyps)\n                    \n                    # ‚úÖ FIX #2: Generate explanations for research metrics\n                    if compute_research_metrics and 'translate_with_explanations' in globals():\n                        for src in batch_srcs:\n                            try:\n                                res = translate_with_explanations(core, tokenizer, src)\n                                explanations = res.get('explanations', []) if isinstance(res, dict) else []\n                                research_metrics.record_sentence(src, explanations)\n                                explanation_data.append(explanations)\n                            except Exception:\n                                research_metrics.record_sentence(src, [])\n                                explanation_data.append([])\n                    else:\n                        # No explanations available\n                        for src in batch_srcs:\n                            explanation_data.append([])\n\n                except Exception as e:\n                    print(f\"\\n[GEN] Batch error at start={start}: {type(e).__name__}\")\n                    # Fallback: per-sentence generation\n                    for src in batch_srcs:\n                        try:\n                            tokenizer.src_lang = \"bn\"\n                        except Exception:\n                            pass\n                        \n                        try:\n                            enc1 = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=max_length)\n                            enc1 = {k: v.to(_DEVICE) for k, v in enc1.items() if isinstance(v, torch.Tensor)}\n                            gen_kwargs1 = {\"max_length\": 128, \"num_beams\": 1, \"early_stopping\": True}\n                            if forced_bos is not None:\n                                gen_kwargs1[\"forced_bos_token_id\"] = int(forced_bos)\n                            gen_ids = gen_callable(**enc1, **gen_kwargs1)\n                            seq = gen_ids[0] if isinstance(gen_ids, (list, tuple)) else gen_ids\n                            try:\n                                hyp = tokenizer.decode(seq[0] if isinstance(seq, (list, tuple)) else seq, skip_special_tokens=True)\n                            except Exception:\n                                hyp = \"\"\n                            hypotheses.append(hyp)\n                            \n                            # Explanations\n                            if compute_research_metrics and 'translate_with_explanations' in globals():\n                                try:\n                                    res = translate_with_explanations(core, tokenizer, src)\n                                    explanations = res.get('explanations', []) if isinstance(res, dict) else []\n                                    research_metrics.record_sentence(src, explanations)\n                                    explanation_data.append(explanations)\n                                except Exception:\n                                    research_metrics.record_sentence(src, [])\n                                    explanation_data.append([])\n                            else:\n                                explanation_data.append([])\n                                \n                        except Exception:\n                            hypotheses.append(\"\")\n                            explanation_data.append([])\n\n        if len(hypotheses) < len(source_texts):\n            hypotheses.extend([\"\"] * (len(source_texts) - len(hypotheses)))\n            explanation_data.extend([[]] * (len(source_texts) - len(explanation_data)))\n\n        print(f\"\\n[GEN] ‚úì Generated {len(hypotheses)} predictions\")\n        timer.end('generation')\n\n        # Step 3: Compute MT metrics\n        timer.start('mt_metrics')\n        print(\"\\n\" + \"=\" * 80)\n        print(\"COMPUTING MT METRICS\")\n        print(\"=\" * 80)\n\n        metrics_computer = LargeScaleEvaluationMetrics(device=_DEVICE, batch_size=batch_size)\n        mt_metrics = metrics_computer.compute_all_metrics_large(source_texts, references, hypotheses)\n        timer.end('mt_metrics')\n\n        # ‚úÖ FIX #1: Get research metrics summary\n        research_summary = research_metrics.get_summary() if research_metrics else {}\n\n        # ‚úÖ FIX #5: Save enhanced CSV\n        timer.start('save_results')\n        csv_path = None\n        if save_results:\n            csv_path = \"evaluation_results_comprehensive.csv\"\n            print(f\"\\n[SAVE] Saving comprehensive results to {csv_path}...\")\n            try:\n                with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n                    writer = csv.writer(f)\n                    # ‚úÖ FIX #5: Enhanced headers with quality columns\n                    writer.writerow([\n                        \"Index\", \"Source\", \"Reference\", \"Hypothesis\",\n                        \"Num_Explanations\", \"Avg_Confidence\", \"Avg_Span\", \"Avg_Uncertainty\",\n                        \"Homographs_Detected\"\n                    ])\n                    \n                    for idx, (s, r, h, exps) in enumerate(zip(source_texts, references, hypotheses, explanation_data), 1):\n                        # Compute row quality metrics\n                        num_exps = len(exps) if exps else 0\n                        if num_exps > 0:\n                            avg_conf = np.mean([float(e.get('confidence', 0.5)) for e in exps])\n                            avg_span = np.mean([float(e.get('span', 0.0)) for e in exps])\n                            avg_u = np.mean([float(e.get('uncertainty', 0.0)) for e in exps])\n                            homos = \", \".join([e.get('ambiguous_word', '') for e in exps])\n                        else:\n                            avg_conf = avg_span = avg_u = 0.0\n                            homos = \"\"\n                        \n                        writer.writerow([idx, s, r, h, num_exps, f\"{avg_conf:.3f}\", f\"{avg_span:.3f}\", f\"{avg_u:.3f}\", homos])\n                \n                print(f\"[SAVE] ‚úì Saved {len(hypotheses)} predictions with quality metrics\")\n            except Exception as e:\n                print(f\"[SAVE] ‚úó Error: {type(e).__name__}: {str(e)[:200]}\")\n        timer.end('save_results')\n\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        # ‚úÖ COMPREHENSIVE FINAL REPORT\n        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n        \n        timer.end('total')\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"COMPREHENSIVE EVALUATION REPORT\")\n        print(\"=\" * 80 + \"\\n\")\n\n        print(f\"Dataset: {len(hypotheses)} samples\")\n        print(f\"Timestamp: {mt_metrics.get('timestamp', '')}\\n\")\n\n        # MT Metrics\n        print(\"[MACHINE TRANSLATION METRICS]\")\n        print(\"-\" * 80)\n        if \"bleu\" in mt_metrics[\"metrics\"]:\n            bleu_data = mt_metrics[\"metrics\"][\"bleu\"]\n            if bleu_data.get(\"bleu\") is not None:\n                print(f\"  BLEU:   {bleu_data['bleu']:>7.2f}/100\")\n            else:\n                print(f\"  BLEU:   ERROR - {bleu_data.get('error', 'Unknown')}\")\n        \n        if \"chrf\" in mt_metrics[\"metrics\"]:\n            chrf_data = mt_metrics[\"metrics\"][\"chrf\"]\n            if chrf_data.get(\"chrf\") is not None:\n                print(f\"  CHRF++: {chrf_data['chrf']:>7.2f}/100\")\n            else:\n                print(f\"  CHRF++: ERROR - {chrf_data.get('error', 'Unknown')}\")\n        \n        if \"comet\" in mt_metrics[\"metrics\"]:\n            comet_data = mt_metrics[\"metrics\"][\"comet\"]\n            if comet_data.get(\"comet\") is not None:\n                print(f\"  COMET:  {comet_data['comet']:>7.4f}/1.0\")\n            else:\n                print(f\"  COMET:  ERROR - {comet_data.get('error', 'Unknown')}\")\n        print(\"-\" * 80)\n\n        # ‚úÖ FIX #1 + #2 + #4: Research metrics\n        if research_summary:\n            print(\"\\n[RESEARCH METRICS - HOMOGRAPH DISAMBIGUATION]\")\n            print(\"-\" * 80)\n            print(f\"  Explanation generation rate: {research_summary['explanation_rate']:.1%}\")\n            print(f\"  Avg explanations per sentence: {research_summary['avg_explanations_per_sentence']:.2f}\")\n            print(f\"  Avg confidence: {research_summary['avg_confidence']:.3f}\")\n            print(f\"  High confidence rate: {research_summary['high_confidence_rate']:.1%}\")\n            print(f\"  Homographs detected: {len(research_summary['homographs_detected'])}/{len(_HOMOGRAPH_WATCHLIST)}\")\n            print(f\"  Detection rate: {research_summary['detection_rate']:.1%}\")\n            \n            if research_summary['homographs_detected']:\n                print(f\"\\n  Detected words: {', '.join(sorted(research_summary['homographs_detected']))}\")\n            \n            # ‚úÖ FIX #4: Per-word accuracy\n            if research_summary['per_word_accuracy']:\n                print(f\"\\n  Per-word disambiguation accuracy:\")\n                for word, stats in sorted(research_summary['per_word_accuracy'].items()):\n                    print(f\"    '{word}': {stats['detections']}/{stats['occurrences']} ({stats['detection_rate']:.1%})\")\n            \n            print(\"-\" * 80)\n\n        # ‚úÖ FIX #6: Timing breakdown\n        timer.print_summary()\n\n        # Sample outputs\n        print(\"\\n[SAMPLE TRANSLATIONS - First 5]\")\n        print(\"-\" * 80)\n        for i, (s, r, h) in enumerate(zip(source_texts[:5], references[:5], hypotheses[:5]), 1):\n            print(f\"\\n{i}. Source:    {s}\")\n            print(f\"   Reference: {r}\")\n            print(f\"   Hypothesis: {h}\")\n            if i <= len(explanation_data) and explanation_data[i-1]:\n                print(f\"   Explanations: {len(explanation_data[i-1])}\")\n        print(\"\\n\" + \"=\" * 80)\n\n        return {\n            \"mt_metrics\": mt_metrics[\"metrics\"],\n            \"research_metrics\": research_summary,\n            \"num_samples\": len(hypotheses),\n            \"csv_file\": csv_path,\n            \"timing\": timer.get_summary(),\n        }\n        \n    except Exception as e:\n        print(f\"\\n[ERROR] Evaluation failed: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        return {\"error\": str(e), \"metrics\": {}}\n\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\n        \"\"\"\n    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n    ‚ïë    LARGE-SCALE COMPREHENSIVE EVALUATION (2000+ SAMPLES) - READY       ‚ïë\n    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n    \n    Metrics computed:\n      ‚Ä¢ BLEU, CHRF++, COMET (translation quality)\n      ‚Ä¢ Homograph detection accuracy\n      ‚Ä¢ Explanation generation rate\n      ‚Ä¢ Per-word disambiguation accuracy\n      ‚Ä¢ Quality metrics (confidence, span, uncertainty)\n    \n    Usage:\n      eval_results = evaluate_on_large_dataset(\n          model=trained_model,\n          tokenizer=tokenizer,\n          num_samples=2000,\n          batch_size=32,\n          save_results=True,\n          compute_research_metrics=True\n      )\n    \"\"\"\n    )\n    \n    # Check if model available\n    if 'trained_model' in globals() and 'tokenizer' in globals():\n        print(\"\\n‚úì Model and tokenizer available - ready to run\")\n        print(\"\\nTo execute: eval_results = evaluate_on_large_dataset(trained_model, tokenizer)\")\n    else:\n        print(\"\\n‚ö†Ô∏è trained_model or tokenizer not found\")\n        print(\"   Run Cells 0-11 first, or load a checkpoint\")\n    \n    print(\"\\n‚úÖ Cell 13: Comprehensive large-scale evaluation ready\")","metadata":{"id":"hZw0m3uEH4J6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}