{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14014111,"sourceType":"datasetVersion","datasetId":8927824}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers sentence-transformers\n!pip install transformers==4.30.2 --no-deps --force-reinstall\n!pip install sentencepiece tokenizers sacremoses\n!pip install scipy scikit-learn\n!pip install --upgrade \"protobuf==3.20.3\"\n# optional:\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"id":"W8IIWAEHH4Jy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: TATN CONFIGURATION (BENGALI → ENGLISH)\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom pathlib import Path\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union, Set, Any\nfrom types import SimpleNamespace\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\n# ------------------------------------------------------------------------------\n# Optional dependencies\n# ------------------------------------------------------------------------------\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    _HAS_PANDAS = False\n    print(\"[WARN] pandas not available; CSV loading will fail\")\n\ntry:\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\n    _HAS_M2M_TOKENIZER = True\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer\n        _HAS_M2M_TOKENIZER = True\n    except Exception:\n        M2M100Tokenizer = None\n        _HAS_M2M_TOKENIZER = False\n        print(\"[WARN] M2M100Tokenizer not available\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# ------------------------------------------------------------------------------\n# Device / GPU configuration\n# ------------------------------------------------------------------------------\n\nNUM_GPUS = torch.cuda.device_count()\nUSE_MULTI_GPU = NUM_GPUS > 1\n\nif USE_MULTI_GPU:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available\")\n    DEVICE = torch.device(\"cuda:0\")\nelse:\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    mode = \"Single GPU Mode\" if torch.cuda.is_available() else \"CPU Mode\"\n    print(f\"[Cell 0] {mode}\")\n\nprint(f\"[Cell 0] Device: {DEVICE} (visible GPUs: {NUM_GPUS})\")\n\n# ------------------------------------------------------------------------------\n# Dataset path (NOTE: CSV is en→bn but model trains bn→en; swap happens in Cell 2)\n# ------------------------------------------------------------------------------\n\nDATASET_CSV_PATH = os.environ.get(\n    \"DATASET_PATH\",\n    \"/kaggle/input/bn-homo/bn_homograph_complete_dataset.csv\"\n)\n\nif not os.path.exists(DATASET_CSV_PATH):\n    print(f\"[WARN] Dataset CSV not found at: {DATASET_CSV_PATH}\")\n    print(\"[WARN] Training will use fallback dataset if file is not accessible\")\nelse:\n    print(f\"[INFO] Dataset CSV found: {DATASET_CSV_PATH}\")\n    if _HAS_PANDAS:\n        try:\n            _test_df = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n            if \"src\" not in _test_df.columns or \"tgt\" not in _test_df.columns:\n                print(\"[ERROR] CSV missing required columns 'src' and/or 'tgt'\")\n                print(f\"[ERROR] Found columns: {list(_test_df.columns)}\")\n            else:\n                print(f\"[INFO] CSV validation passed (columns: {list(_test_df.columns)})\")\n            del _test_df\n        except Exception as e:\n            print(f\"[WARN] Could not validate CSV structure: {e}\")\n\n# ------------------------------------------------------------------------------\n# Core training hyperparameters\n# ------------------------------------------------------------------------------\n\nBATCH_SIZE = 100\nNUM_SAMPLES = 30000\nMAX_LENGTH = 50\n\nLR_NMT = 2e-5\nLR_TRG = 1e-5\nLR_PHI = 1e-5\n\nEPOCHS = 1\nGRAD_CLIP_NORM = 1.0\nUSE_AMP = True\nPRINT_INTERVAL = 300\nSEED = 42\n\nACCUMULATION_STEPS = 16\n\n# ------------------------------------------------------------------------------\n# TRG / MC-dropout / data loader / memory\n# ------------------------------------------------------------------------------\n\nMC_DROPOUT_PASSES = 5\nTRG_EVIDENCE_K = 3\nMAX_SILVER_BUFFER = 50\n\nNUM_WORKERS = 2\nPIN_MEMORY = True\nPREFETCH_FACTOR = 2\nGRADIENT_CHECKPOINTING = True\n\n# ------------------------------------------------------------------------------\n# Debug flags (keep DSCD internal logs off by default to avoid spam)\n# ------------------------------------------------------------------------------\n\nDEBUG_DISCOVERY = False\nDEBUG_TIMING = True\nDEBUG_VERBOSE = False\n\n# ------------------------------------------------------------------------------\n# DSCD configuration (more permissive for multi-sense)\n# ------------------------------------------------------------------------------\n\nDSCD_BUFFER_SIZE = 50\nDSCD_MAX_PROTOS = 8\nDSCD_N_MIN = 3\nDSCD_DISPERSION_THRESHOLD = 0.20\nDSCD_EMBED_DIM = 1024\nDSCD_TEMPERATURE = 0.7\nDSCD_DROPOUT = 0.1\nDSCD_AUGMENT_SCALE = 0.1\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_WARMUP_SAMPLES = 8000\n\nPERIODIC_DISCOVERY_FREQUENCY = 50\n_MAX_TOKENS_PER_DISCOVERY = 150\n\n# ------------------------------------------------------------------------------\n# ASBN / TRG configuration\n# ------------------------------------------------------------------------------\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = True\n\nENABLE_TRG_TRAINING = True\nENABLE_TRG_INFERENCE = True\n\nCLUSTERING_TIMEOUT = 5\nMEMORY_CLEANUP_FREQUENCY = 100\nVALIDATION_CHECK_INTERVAL = 200\nVERBOSE_LOGGING = False\n\n# ------------------------------------------------------------------------------\n# Checkpoint configuration\n# ------------------------------------------------------------------------------\n\nCHECKPOINT_DIR = \"/kaggle/working/\"\nCHECKPOINT_SAVE_AFTER_TRAINING = True\nCHECKPOINT_FILENAME = \"tatn_final.pt\"\nCHECKPOINT_INTERVAL = 99999999\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = 25000\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\n\n# ------------------------------------------------------------------------------\n# TRG uncertainty / span thresholds\n# ------------------------------------------------------------------------------\n\nTAU_LOW = 0.15\nTAU_HIGH = 0.85\nTAU_ACCEPT = 0.8\n\nTRG_MAX_GEN_LEN = 16\nTRG_GEN_EMBED = 64\nTRG_GEN_HID = 64\n\nSPAN_THRESHOLD = 0.12\nUNCERTAINTY_THRESHOLD = 0.15\nTRG_TEMPERATURE = 1.0\n\n# ------------------------------------------------------------------------------\n# ASBN loss weights\n# ------------------------------------------------------------------------------\n\nASBN_HIDDEN_DIM = 64\nASBN_LAMBDA = 0.1\nASBN_DROPOUT = 0.1\n\nLAMBDA_ASBN = 0.05\nLAMBDA_DSCD = 0.15\n\n# ------------------------------------------------------------------------------\n# Domain labels / GRL schedule\n# ------------------------------------------------------------------------------\n\nTRAIN_DOMAIN = 0\nTEST_DOMAIN = 1\nUSE_DOMAIN_LABELS = True\n\nGRL_ALPHA_START = 0.0\nGRL_ALPHA_END = 1.0\nGRL_ALPHA_SCHEDULE = \"linear\"\nGRL_ALPHA_STEPS = (\n    NUM_SAMPLES // (BATCH_SIZE * ACCUMULATION_STEPS) * EPOCHS\n    if BATCH_SIZE * ACCUMULATION_STEPS > 0\n    else 10000\n)\n\n# ------------------------------------------------------------------------------\n# Language configuration\n# ------------------------------------------------------------------------------\n\nSOURCE_LANGUAGE = \"bn\"\nTARGET_LANGUAGE = \"en\"\n\nM2M100_BN_TOKEN_ID = 128025\nM2M100_EN_TOKEN_ID = 128022\n\n# ------------------------------------------------------------------------------\n# Reference homograph list (evaluation only; DSCD unsupervised)\n# ------------------------------------------------------------------------------\n\nHOMOGRAPH_REFERENCE_LIST_BN: Set[str] = {\n    \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n    \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\",\n    \"দিন\", \"রাত\", \"জল\", \"বাড়ি\", \"পার্ক\", \"নদী\", \"বন\", \"ফুল\", \"গাছ\",\n    \"চোখ\", \"মুখ\", \"পা\", \"কান\", \"গলা\", \"নাক\", \"দাঁত\", \"কোমর\",\n    \"পড়া\", \"দেখা\", \"যাওয়া\", \"আসা\", \"খেলা\", \"লেখা\", \"বলা\", \"শোনা\",\n    \"চলা\", \"ধরা\", \"দেওয়া\", \"নেওয়া\",\n    \"সময়\", \"বছর\", \"মাস\", \"সাল\", \"ঘন্টা\", \"মুহূর্ত\",\n    \"গরম\", \"শীত\", \"বাতাস\", \"আগুন\", \"পাথর\", \"মাটি\",\n    \"ভাব\", \"রং\", \"আলো\", \"ছায়া\", \"শব্দ\", \"অর্থ\",\n}\n\nHOMOGRAPH_WATCHLIST_BN: Set[str] = set()\nHOMOGRAPH_WATCHLIST: Set[str] = set()\nUSE_WATCHLIST_PRIORITIZATION = False\nWATCHLIST_ONLY_FOR_TRG = False\n\n# ------------------------------------------------------------------------------\n# Normalization utilities\n# ------------------------------------------------------------------------------\n\ndef normalize_bengali(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t)\n    t = t.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    return t\n\ndef normalize_english(t: str) -> str:\n    if not t:\n        return \"\"\n    t = unicodedata.normalize(\"NFKC\", t).lower().strip()\n    return t\n\n# ------------------------------------------------------------------------------\n# CUDA helpers\n# ------------------------------------------------------------------------------\n\ndef empty_cuda_cache() -> None:\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize() -> None:\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage() -> None:\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        print(f\"\\n[GPU MONITOR] Checking {visible_gpus} GPU(s):\")\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024 ** 3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n                print(\n                    f\"  GPU {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\"\n                )\n            except Exception:\n                print(f\"  GPU {i}: memory stats unavailable\")\n    else:\n        print(\"[GPU MONITOR] No CUDA devices available\")\n\n# ------------------------------------------------------------------------------\n# Checkpoint helpers\n# ------------------------------------------------------------------------------\n\ndef get_checkpoint_path() -> str:\n    return os.path.join(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n\ndef should_save_checkpoint(global_step: int, epoch: int, is_final: bool = False) -> bool:\n    if is_final and CHECKPOINT_SAVE_AFTER_TRAINING:\n        return True\n    if (\n        CHECKPOINT_INTERVAL < 99999999\n        and global_step >= CHECKPOINT_INTERVAL\n        and global_step % CHECKPOINT_INTERVAL == 0\n    ):\n        return True\n    return False\n\n# ------------------------------------------------------------------------------\n# Function timeout utility\n# ------------------------------------------------------------------------------\n\nclass FunctionTimeoutError(Exception):\n    pass\n\ndef with_timeout(seconds: int):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\n# ------------------------------------------------------------------------------\n# Token utilities for DSCD / tokenizer\n# ------------------------------------------------------------------------------\n\ndef get_special_tokens(tokenizer) -> Set[str]:\n    try:\n        s = set(getattr(tokenizer, \"all_special_tokens\", []))\n    except Exception:\n        s = {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\"}\n    s.update({SOURCE_LANGUAGE, TARGET_LANGUAGE})\n    return s\n\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(\n    token,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\",\n) -> bool:\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        min_len = 2\n        if len(clean) < min_len:\n            result = False\n        else:\n            has_bengali_chars = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if not has_bengali_chars:\n                result = False\n            else:\n                bengali_count = sum(1 for c in clean if \"\\u0980\" <= c <= \"\\u09FF\")\n                alphanum_count = sum(1 for c in clean if c.isalnum())\n                if alphanum_count == 0:\n                    result = False\n                else:\n                    bengali_ratio = bengali_count / alphanum_count\n                    result = bengali_ratio >= 0.5\n\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n\n    return result\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    try:\n        encoded = tokenizer(\n            text,\n            return_offsets_mapping=True,\n            max_length=max_length,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        toks = tokenizer.convert_ids_to_tokens(encoded.get(\"input_ids\", []))\n        offsets = encoded.get(\"offset_mapping\", [(0, 0)] * len(toks))\n        return toks, offsets\n    except Exception:\n        return None, None\n\n# ------------------------------------------------------------------------------\n# Discovery timing helper used by DSCD\n# ------------------------------------------------------------------------------\n\nclass DiscoveryTimer:\n    def __init__(self):\n        self.discovery_times: List[float] = []\n        self.discovery_steps: List[int] = []\n\n    def record(self, step: int, duration: float) -> None:\n        self.discovery_times.append(duration)\n        self.discovery_steps.append(step)\n\n    def get_stats(self) -> Dict[str, float]:\n        if not self.discovery_times:\n            return {\"count\": 0, \"total\": 0.0, \"avg\": 0.0, \"max\": 0.0}\n        total = sum(self.discovery_times)\n        return {\n            \"count\": len(self.discovery_times),\n            \"total\": total,\n            \"avg\": total / len(self.discovery_times),\n            \"max\": max(self.discovery_times),\n        }\n\n_discovery_timer = DiscoveryTimer()\ndiscoverytimer = _discovery_timer\n\n# ------------------------------------------------------------------------------\n# Seeding and CuDNN behaviour\n# ------------------------------------------------------------------------------\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\n# ------------------------------------------------------------------------------\n# Summary printout\n# ------------------------------------------------------------------------------\n\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nif USE_MULTI_GPU:\n    effective_batch *= NUM_GPUS\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TATN CONFIGURATION (Bengali to English)\")\nprint(\"=\" * 80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs)\")\nprint(f\"Dataset: {DATASET_CSV_PATH}\")\nprint(f\"Samples: {NUM_SAMPLES:,} | Batch: {BATCH_SIZE} | Accum: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch: {effective_batch}\")\nprint(f\"Max length: {MAX_LENGTH} | Epochs: {EPOCHS} | AMP: {USE_AMP}\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer: {DSCD_BUFFER_SIZE} | n_min: {DSCD_N_MIN} | Max protos: {DSCD_MAX_PROTOS}\")\nprint(f\"  Dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  Periodic discovery: Every {PERIODIC_DISCOVERY_FREQUENCY} steps\")\nprint(f\"  Max tokens per discovery: {_MAX_TOKENS_PER_DISCOVERY}\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  MC Dropout passes: {MC_DROPOUT_PASSES} | TAU_LOW: {TAU_LOW}\")\nprint(f\"  SPAN_THRESHOLD: {SPAN_THRESHOLD} | UNCERTAINTY_THRESHOLD: {UNCERTAINTY_THRESHOLD}\")\nprint(f\"  TAU_HIGH: {TAU_HIGH} | Temperature: {TRG_TEMPERATURE}\")\nprint()\nprint(\"ASBN / Loss:\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN} | LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint(f\"  Domain labels: {USE_DOMAIN_LABELS} | GRL: {GRL_ALPHA_SCHEDULE}\")\nprint(f\"  GRL steps: {GRL_ALPHA_STEPS}\")\nprint()\nprint(\"Debug Flags:\")\nprint(f\"  Discovery logging: {DEBUG_DISCOVERY}\")\nprint(f\"  Timing monitoring: {DEBUG_TIMING}\")\nprint(f\"  Verbose mode: {DEBUG_VERBOSE}\")\nprint()\nprint(\"Validation:\")\nprint(f\"  Check interval: {VALIDATION_CHECK_INTERVAL} steps\")\nprint()\nprint(\"Language Tokens:\")\nprint(f\"  Bengali (bn): {M2M100_BN_TOKEN_ID}\")\nprint(f\"  English (en): {M2M100_EN_TOKEN_ID}\")\nprint()\nprint(\"Checkpoint:\")\nprint(f\"  Path: {get_checkpoint_path()}\")\nprint(f\"  Save strategy: Final only\")\nprint()\nprint(\"Discovery Mode:\")\nprint(\"  PURE UNSUPERVISED (no watchlist bias)\")\nprint(f\"  Reference list: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words (evaluation only)\")\nprint(\"  Watchlist prioritization: DISABLED\")\nprint(\"=\" * 80)\n\nif not _HAS_PANDAS:\n    print(\"[ERROR] pandas not available - CSV loading will fail!\")\nif not _HAS_M2M_TOKENIZER:\n    print(\"[ERROR] M2M100Tokenizer not available - tokenization will fail!\")\n\ntry:\n    test_file = os.path.join(CHECKPOINT_DIR, \".test_write\")\n    with open(test_file, \"w\") as f:\n        f.write(\"test\")\n    os.remove(test_file)\n    print(f\"Checkpoint directory writable: {CHECKPOINT_DIR}\")\nexcept Exception as e:\n    print(f\"Checkpoint directory not writable: {e}\")\n\nmonitor_gpu_usage()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 0: Configuration loaded successfully\")\nprint(\"=\" * 80)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"5jMPDi9xH4Jz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1: TOKENIZER UTILITIES + PROTOTYPE DATABASE (BENGALI-FOCUSED)\n# ===========================================================================================\n\nimport threading\nimport json\nimport pickle\nfrom typing import Tuple, List, Dict, Optional, Set, Any\nfrom pathlib import Path\nfrom collections import defaultdict\nimport numpy as np\nimport torch\nimport datetime\n\ntry:\n    if isinstance(MAX_LENGTH, (int, float)) and MAX_LENGTH > 0:\n        SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\n    else:\n        SAFE_OFFSET_MAX_LEN = 48\nexcept (NameError, ValueError, TypeError):\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n\ntry:\n    _DEBUG_VERBOSE = DEBUG_VERBOSE\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\ntry:\n    _DEBUG_DISCOVERY = DEBUG_DISCOVERY\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\n_SPECIAL_TOKENS_CACHE: Dict[str, Set[str]] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n_LANGUAGE_WARNING_COUNT = 0\n_MAX_LANGUAGE_WARNINGS = 3\n\ndef _special_token_cache_key(tokenizer) -> str:\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None)\n    if not name:\n        name = \"unknown_tokenizer\"\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    return f\"{name}__vocab={vocab}\"\n\ndef get_tokenizer_special_tokens(tokenizer) -> Set[str]:\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens: Set[str] = set()\n        try:\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"all_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    result = getattr(tokenizer, \"additional_special_tokens\")\n                    if isinstance(result, (list, tuple, set)):\n                        special_tokens.update(x for x in result if x)\n                except Exception:\n                    pass\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\",\n                         \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            try:\n                stm = (\n                    getattr(tokenizer, \"special_tokens_map\", None)\n                    or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                )\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n        except Exception:\n            special_tokens = set()\n\n        special_tokens.update({\n            \"__bn__\", \"__en__\",\n            \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\",\n        })\n\n        try:\n            vocab = tokenizer.get_vocab() if hasattr(tokenizer, \"get_vocab\") else {}\n            special_tokens = {\n                tok\n                for tok in special_tokens\n                if tok in vocab or tok in {\"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n            }\n        except Exception:\n            pass\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            try:\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in arr[0]\n                        ]\n                        return enc\n                if isinstance(off, (list, tuple)):\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [\n                            (x[0], x[1])\n                            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                            else (None, None)\n                            for x in off[0]\n                        ]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    try:\n        data = getattr(enc, \"data\", None)\n        if (\n            data\n            and isinstance(data, dict)\n            and \"offset_mapping\" in data\n            and data[\"offset_mapping\"] is not None\n        ):\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [\n                    (x[0], x[1])\n                    if (isinstance(x, (list, tuple)) and len(x) >= 2)\n                    else (None, None)\n                    for x in om[0]\n                ]\n                return enc\n    except Exception:\n        pass\n\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            if hasattr(input_ids, \"shape\") and len(input_ids.shape) > 0:\n                seq_len = int(input_ids.shape[-1])\n            elif (\n                isinstance(input_ids, (list, tuple))\n                and len(input_ids) > 0\n                and isinstance(input_ids[0], (list, tuple))\n            ):\n                seq_len = len(input_ids[0])\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\ndef safe_offsets_tokenize(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n    include_special_tokens: bool = False,\n) -> dict:\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    try:\n        if not isinstance(text, str):\n            text = \"\" if text is None else str(text)\n    except Exception:\n        if _DEBUG_VERBOSE:\n            print(\"[WARN] Failed to convert input to string, using empty string\")\n        text = \"\"\n\n    char_limit = min(eff_max * 30, 8000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    if is_fast:\n        try:\n            enc = tokenizer(\n                sample_text,\n                return_offsets_mapping=True,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=False,\n                max_length=eff_max,\n                add_special_tokens=include_special_tokens,\n            )\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            pass\n\n    try:\n        enc = tokenizer(\n            sample_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=eff_max,\n            add_special_tokens=include_special_tokens,\n        )\n    except Exception as e:\n        if _DEBUG_VERBOSE:\n            print(f\"[WARN] Tokenization failed: {e}, returning empty encoding\")\n        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n        enc = {\n            \"input_ids\": torch.tensor([[pad_id]], dtype=torch.long),\n            \"attention_mask\": torch.tensor([[1]], dtype=torch.long),\n        }\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    try:\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n\n        tokens: List[str] = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n\n        offsets_list: List[Tuple[Optional[int], Optional[int]]] = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            token_text = (tok or \"\").replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n\n        enc[\"offset_mapping\"] = offsets_list\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\ndef reconstruct_word_spans(\n    tokenizer,\n    text: str,\n    max_length: Optional[int] = None,\n) -> Tuple[Dict[int, Optional[str]], List[str]]:\n    global _LANGUAGE_WARNING_COUNT\n\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    has_bengali = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in text)\n    has_english = any(\"a\" <= c.lower() <= \"z\" for c in text)\n\n    if _DEBUG_VERBOSE and _DEBUG_DISCOVERY:\n        bengali_pct = (\n            sum(1 for c in text if \"\\u0980\" <= c <= \"\\u09FF\")\n            / max(1, len(text))\n            * 100.0\n        )\n        print(f\"[TOKENIZER] Text sample: {text[:50]}\")\n        print(\n            f\"[TOKENIZER] Bengali: {has_bengali} ({bengali_pct:.1f}%), \"\n            f\"English: {has_english}\"\n        )\n\n    if not has_bengali and has_english and _LANGUAGE_WARNING_COUNT < _MAX_LANGUAGE_WARNINGS:\n        if _DEBUG_DISCOVERY:\n            print(\"[TOKENIZER WARNING] Text appears to be ENGLISH, not BENGALI\")\n            print(f\"  Sample: {text[:80]}\")\n        _LANGUAGE_WARNING_COUNT += 1\n        if _LANGUAGE_WARNING_COUNT == _MAX_LANGUAGE_WARNINGS:\n            print(\"[TOKENIZER] Suppressing further language warnings\")\n\n    char_limit = min(eff_max * 30, 8000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    try:\n        encoded = safe_offsets_tokenize(\n            tokenizer, text, max_length=eff_max, include_special_tokens=False\n        )\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    if isinstance(offsets, list) and len(offsets) > 0 and all(\n        isinstance(x, tuple) for x in offsets\n    ):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(\n        offsets[0], (list, tuple)\n    ):\n        offsets_list = [\n            (x[0], x[1])\n            if (isinstance(x, (list, tuple)) and len(x) >= 2)\n            else (None, None)\n            for x in offsets[0]\n        ]\n    else:\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, Optional[str]] = {}\n    words: List[str] = []\n\n    used_any_offset = any(\n        isinstance(o, tuple) and o[0] is not None and o[1] is not None\n        for o in offsets_list\n    )\n    if used_any_offset:\n        word_start: Optional[int] = None\n        word_end: Optional[int] = None\n\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start = int(off[0]) if off[0] is not None else None\n                off_end = int(off[1]) if off[1] is not None else None\n            except Exception:\n                off_start, off_end = None, None\n\n            if off_start is not None and off_end is not None:\n                if off_start < 0 or off_end < 0:\n                    if _DEBUG_VERBOSE:\n                        print(\n                            f\"[WARN] Negative offset detected: \"\n                            f\"({off_start}, {off_end}), skipping\"\n                        )\n                    off_start, off_end = None, None\n                else:\n                    off_start = max(0, min(off_start, text_len))\n                    off_end = max(off_start, min(off_end, text_len))\n\n            if off_start is None or off_end is None:\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                token_word_map[idx] = None\n                continue\n\n            if tok in special_tokens:\n                token_word_map[idx] = None\n                continue\n\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                if off_start > word_end:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            try:\n                current_word = text[word_start:word_end].strip()\n                token_word_map[idx] = current_word if current_word else None\n            except Exception:\n                token_word_map[idx] = None\n\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    token_word_map = {}\n    assembled: List[str] = []\n    current_parts: List[str] = []\n    running_word = \"\"\n    max_word_len = 100\n\n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            token_word_map[i] = None\n            continue\n        clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n        if not clean:\n            token_word_map[i] = None\n            continue\n\n        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n            if current_parts:\n                word = \"\".join(current_parts)\n                if len(word) <= max_word_len:\n                    assembled.append(word)\n            current_parts = [clean]\n            running_word = clean\n        else:\n            current_parts.append(clean)\n            running_word = \"\".join(current_parts)\n            if len(running_word) > max_word_len:\n                if current_parts[:-1]:\n                    word = \"\".join(current_parts[:-1])\n                    assembled.append(word)\n                current_parts = [clean]\n                running_word = clean\n\n        token_word_map[i] = running_word if running_word else None\n\n    if current_parts:\n        word = \"\".join(current_parts)\n        if len(word) <= max_word_len:\n            assembled.append(word)\n\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n\n        if tokens and word_list:\n            word_idx = 0\n\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n                if not clean or tok in special_tokens:\n                    token_word_map[i] = None\n                    continue\n\n                if word_idx < len(word_list):\n                    current_word = word_list[word_idx]\n                    if clean in current_word or current_word.startswith(clean):\n                        token_word_map[i] = current_word\n                    else:\n                        word_idx = min(word_idx + 1, len(word_list) - 1)\n                        token_word_map[i] = word_list[word_idx]\n                else:\n                    token_word_map[i] = word_list[-1] if word_list else None\n\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\ndef is_valid_token(\n    token: str,\n    special_tokens: Optional[Set[str]] = None,\n    tokenizer=None,\n    language: str = \"bn\"\n) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    \n    token = token.strip()\n    if not token:\n        return False\n    \n    if special_tokens and token in special_tokens:\n        return False\n    \n    clean = token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").replace(\",\", \"\").strip()\n    if len(clean) < 2:\n        return False\n    \n    if not any(c.isalpha() for c in clean):\n        return False\n    \n    punct_set = set(\".,!?;:—-\")\n    if all(c in punct_set for c in clean):\n        return False\n    \n    if clean.isdigit():\n        return False\n    \n    return True\n\ndef map_subwords_to_words(tokens: List[str], tokenizer) -> Dict[int, str]:\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n    word_map = {}\n    current_word = \"\"\n    word_start_idx = 0\n    \n    for i, tok in enumerate(tokens):\n        if tok in special_tokens:\n            word_map[i] = None\n            continue\n        \n        clean = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip()\n        \n        if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n            if current_word:\n                for j in range(word_start_idx, i):\n                    word_map[j] = current_word\n            current_word = clean\n            word_start_idx = i\n        else:\n            current_word += clean\n    \n    for j in range(word_start_idx, len(tokens)):\n        word_map[j] = current_word if current_word else None\n    \n    return word_map\n\nclass PrototypeDatabase:\n    def __init__(self, save_dir='./dscd_prototypes'):\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(exist_ok=True, parents=True)\n        \n        self.prototype_file = self.save_dir / 'prototypes.pkl'\n        self.metadata_file = self.save_dir / 'metadata.json'\n        \n        self.prototypes = {}\n        self.metadata = {\n            'total_words': 0,\n            'total_prototypes': 0,\n            'last_updated': None,\n            'version': '1.0'\n        }\n    \n    def load(self):\n        if self.prototype_file.exists():\n            try:\n                with open(self.prototype_file, 'rb') as f:\n                    self.prototypes = pickle.load(f)\n                print(f\"✓ Loaded {len(self.prototypes)} words from {self.prototype_file}\")\n            except Exception as e:\n                print(f\"⚠ Failed to load prototypes: {e}\")\n                self.prototypes = {}\n        \n        if self.metadata_file.exists():\n            try:\n                with open(self.metadata_file, 'r', encoding='utf-8') as f:\n                    self.metadata = json.load(f)\n            except Exception as e:\n                print(f\"⚠ Failed to load metadata: {e}\")\n    \n    def save(self):\n        self.metadata['total_words'] = len(self.prototypes)\n        self.metadata['total_prototypes'] = sum(len(senses) for senses in self.prototypes.values())\n        self.metadata['last_updated'] = datetime.datetime.now().isoformat()\n        \n        try:\n            with open(self.prototype_file, 'wb') as f:\n                pickle.dump(self.prototypes, f)\n            \n            with open(self.metadata_file, 'w', encoding='utf-8') as f:\n                json.dump(self.metadata, f, indent=2, ensure_ascii=False)\n            \n            print(f\"✓ Saved {self.metadata['total_words']} words, {self.metadata['total_prototypes']} prototypes\")\n            print(f\"  → {self.prototype_file}\")\n        except Exception as e:\n            print(f\"⚠ Failed to save prototypes: {e}\")\n    \n    def add_or_update_prototype(self, word: str, sense_id: int, centroid, count: int, validity: bool = True):\n        word = str(word).strip().lower()\n        \n        if word not in self.prototypes:\n            self.prototypes[word] = {}\n            if _DEBUG_DISCOVERY:\n                print(f\"  NEW WORD: '{word}'\")\n        \n        if sense_id not in self.prototypes[word]:\n            if _DEBUG_DISCOVERY:\n                print(f\"    NEW SENSE: '{word}' → sense {sense_id}\")\n        \n        self.prototypes[word][sense_id] = {\n            'centroid': centroid.detach().cpu() if torch.is_tensor(centroid) else centroid,\n            'count': count,\n            'validity': validity\n        }\n    \n    def get_prototypes(self, word: str):\n        word = str(word).strip().lower()\n        return self.prototypes.get(word, None)\n    \n    def sync_from_dscd(self, dscd_module):\n        print(\"Syncing prototypes from DSCD module...\")\n        \n        lock = None\n        if hasattr(dscd_module, 'buffer_lock'):\n            lock = dscd_module.buffer_lock\n        elif hasattr(dscd_module, 'clustering_lock'):\n            lock = dscd_module.clustering_lock\n        \n        if lock:\n            with lock:\n                stores = dict(dscd_module.prototype_stores)\n        else:\n            stores = dict(dscd_module.prototype_stores)\n        \n        for word, store in stores.items():\n            if not hasattr(store, 'centroids') or not hasattr(store, 'counts'):\n                continue\n            \n            centroids = store.centroids\n            counts = store.counts\n            \n            for sense_id, (centroid, count) in enumerate(zip(centroids, counts)):\n                validity = True\n                if hasattr(store, 'size') and store.size >= 2:\n                    validity = True\n                \n                self.add_or_update_prototype(\n                    word=word,\n                    sense_id=sense_id,\n                    centroid=centroid,\n                    count=count,\n                    validity=validity\n                )\n        \n        self.save()\n        return len(stores)\n    \n    def load_into_dscd(self, dscd_module):\n        print(\"Loading prototypes into DSCD module...\")\n        \n        loaded_count = 0\n        \n        for word, senses in self.prototypes.items():\n            centroids = []\n            counts = []\n            \n            for sense_id in sorted(senses.keys()):\n                sense_data = senses[sense_id]\n                if not sense_data['validity']:\n                    continue\n                \n                centroids.append(sense_data['centroid'].to(dscd_module.device))\n                counts.append(sense_data['count'])\n            \n            if len(centroids) == 0:\n                continue\n            \n            from types import SimpleNamespace\n            store = SimpleNamespace(\n                centroids=centroids,\n                counts=counts,\n                size=len(centroids),\n                mu=0.5,\n                tau=1.0\n            )\n            \n            dscd_module.prototype_stores[word] = store\n            loaded_count += 1\n        \n        print(f\"✓ Loaded {loaded_count} words into DSCD\")\n        return loaded_count\n\ndef inspect_prototypes(word: str = None, proto_db=None):\n    if proto_db is None:\n        try:\n            proto_db = PROTOTYPE_DB\n        except NameError:\n            print(\"Error: PROTOTYPE_DB not found. Create instance first.\")\n            return\n    \n    proto_db.load()\n    \n    if word:\n        word = word.strip().lower()\n        senses = proto_db.get_prototypes(word)\n        if senses is None:\n            print(f\"Word '{word}' not found in database\")\n            return\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"WORD: '{word}' ({len(senses)} senses)\")\n        print(f\"{'='*80}\")\n        \n        for sense_id, data in sorted(senses.items()):\n            print(f\"\\n  Sense {sense_id}:\")\n            print(f\"    Count: {data['count']}\")\n            print(f\"    Valid: {data['validity']}\")\n            print(f\"    Centroid shape: {data['centroid'].shape}\")\n    else:\n        print(f\"\\n{'='*80}\")\n        print(\"PROTOTYPE DATABASE SUMMARY\")\n        print(f\"{'='*80}\")\n        print(f\"Total words: {len(proto_db.prototypes)}\")\n        print(f\"Total prototypes: {sum(len(s) for s in proto_db.prototypes.values())}\")\n        print(f\"\\nTop 20 words by sense count:\")\n        \n        word_sense_counts = [(w, len(s)) for w, s in proto_db.prototypes.items()]\n        word_sense_counts.sort(key=lambda x: x[1], reverse=True)\n        \n        for word, num_senses in word_sense_counts[:20]:\n            counts = [proto_db.prototypes[word][sid]['count'] for sid in proto_db.prototypes[word]]\n            print(f\"  '{word}': {num_senses} senses, counts={counts}\")\n\ndef test_tokenizer_utilities_quick(tokenizer=None) -> bool:\n    sample_bn = \"কাল আমি বাজারে যাব।\"\n    sample_en = \"Tomorrow I will go to the market.\"\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOKENIZER UTILITIES TEST\")\n    print(\"=\" * 60)\n\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: skipping test\")\n            return True\n\n        print(\"\\n[TEST 1] Bengali text processing:\")\n        print(f\"  Input: {sample_bn}\")\n        enc_bn = safe_offsets_tokenize(\n            tokenizer, sample_bn, max_length=32, include_special_tokens=False\n        )\n        enc_len = (\n            int(enc_bn[\"input_ids\"].shape[-1])\n            if isinstance(enc_bn, dict) and \"input_ids\" in enc_bn\n            else \"N/A\"\n        )\n        print(f\"  Encoded length: {enc_len}\")\n        offsets_bn = enc_bn.get(\"offset_mapping\") or []\n        print(f\"  Offsets (first 5): {offsets_bn[:5]}\")\n\n        token_map_bn, words_bn = reconstruct_word_spans(tokenizer, sample_bn, max_length=32)\n        print(f\"  Reconstructed words: {words_bn}\")\n        print(f\"  Token map sample: {dict(list(token_map_bn.items())[:3])}\")\n\n        has_bengali_words = any(\n            any(\"\\u0980\" <= c <= \"\\u09FF\" for c in w) for w in words_bn\n        )\n        print(f\"  Contains Bengali words: {has_bengali_words}\")\n\n        print(\"\\n[TEST 2] English text processing (should show warning):\")\n        print(f\"  Input: {sample_en}\")\n        token_map_en, words_en = reconstruct_word_spans(tokenizer, sample_en, max_length=32)\n        print(f\"  Reconstructed words: {words_en}\")\n\n        has_english_words = any(\n            any(\"a\" <= c.lower() <= \"z\" for c in w) for w in words_en\n        )\n        print(f\"  Contains English words: {has_english_words}\")\n\n        print(\"\\n[TEST 3] Token-to-word mapping:\")\n        tokens_test = [\"▁আমি\", \"▁\", \"ক\", \"ল\", \"▁বন্ধ\"]\n        word_map = map_subwords_to_words(tokens_test, tokenizer)\n        print(f\"  Tokens: {tokens_test}\")\n        print(f\"  Word map: {word_map}\")\n        print(f\"  '▁কল' split correctly: {word_map.get(2) == 'কল' and word_map.get(3) == 'কল'}\")\n\n        if has_bengali_words and not any(\n            \"a\" <= c.lower() <= \"z\" for c in \"\".join(words_bn)\n        ):\n            print(\"\\nTest PASSED: Bengali processing works correctly\")\n            return True\n        else:\n            print(\"\\nTest WARNING: Check language detection logic\")\n            return False\n\n    except Exception as e:\n        print(f\"\\nTest FAILED: {repr(e)}\")\n        import traceback\n        traceback.print_exc()\n        return False\n    finally:\n        print(\"=\" * 60 + \"\\n\")\n\nsafeoffsetstokenize = safe_offsets_tokenize\nreconstructwordspans = reconstruct_word_spans\ngettokenizerspecialtokens = get_tokenizer_special_tokens\nisvalidtoken = is_valid_token\nmapsubwordstowords = map_subwords_to_words\n\ntry:\n    PROTOTYPE_DB = PrototypeDatabase(save_dir='/kaggle/working')\nexcept Exception as e:\n    print(f\"Warning: Could not initialize PROTOTYPE_DB: {e}\")\n    PROTOTYPE_DB = None\n\nprint(\"Cell 1: Tokenizer utilities + PrototypeDatabase loaded\")\n","metadata":{"id":"WZE9PkHyH4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (BENGALI → ENGLISH TASK)\n# ==============================================================================\n\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_VERBOSE = bool(DEBUG_VERBOSE)\nexcept NameError:\n    _DEBUG_VERBOSE = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING) or bool(_DEBUG_VERBOSE)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT) -> None:\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _SOURCE_LANG = str(SOURCE_LANGUAGE)\n    _TARGET_LANG = str(TARGET_LANGUAGE)\nexcept NameError:\n    _SOURCE_LANG = \"bn\"\n    _TARGET_LANG = \"en\"\n    print(\"[CELL2] WARNING: SOURCE_LANGUAGE/TARGET_LANGUAGE not defined, using defaults bn/en\")\n\ntry:\n    _M2M_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\n    _M2M_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept NameError:\n    _M2M_BN_TOKEN_ID = 128025\n    _M2M_EN_TOKEN_ID = 128022\n    print(\"[CELL2] WARNING: M2M100 token IDs not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\n    _USE_DOMAIN_LABELS = bool(USE_DOMAIN_LABELS)\nexcept NameError:\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n    _USE_DOMAIN_LABELS = False\n    print(\"[CELL2] WARNING: Domain label config not found, disabling domain labels\")\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n_has_safe_offsets_tokenize = \"safe_offsets_tokenize\" in globals()\n_has_map_subwords_to_words = \"map_subwords_to_words\" in globals()\n\n_BENGALI_CHAR_RE = re.compile(r\"[\\u0980-\\u09FF]\")\n\ndef is_bengali_text(s: str) -> bool:\n    if s is None:\n        return False\n    if not isinstance(s, str) or not s:\n        return False\n    return bool(_BENGALI_CHAR_RE.search(s))\n\ndef normalize_bengali(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[।\\.]$', '', text)\n    return text\n\ndef normalize_english(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    text = text.strip().lower()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[\\.!?]+$', '', text)\n    return text\n\ndef _dataloader_worker_init_fn(worker_id: int) -> None:\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    try:\n        if dataset is not None and hasattr(dataset, \"_tokenizer_name_or_path\") and dataset._tokenizer_name_or_path:\n            try:\n                from transformers import M2M100Tokenizer\n                dataset.tokenizer = M2M100Tokenizer.from_pretrained(dataset._tokenizer_name_or_path)\n                dataset.is_fast = getattr(dataset.tokenizer, \"is_fast\", False)\n                if DEBUG_CELL2:\n                    print(f\"[CELL2-WORKER-{worker_id}] Tokenizer reloaded successfully\")\n            except Exception as e:\n                cell2_dbg(\"worker_tokenizer_reload\", f\"Worker {worker_id} tokenizer reload failed: {e}\")\n                dataset.tokenizer = None\n                dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] Tokenizer rebind failed in worker {worker_id}\")\n\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\ndef load_and_preprocess_optimized(\n    num_samples: Optional[int] = None,\n    split: str = \"train\",\n) -> List[Tuple[str, str]]:\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback dataset for debugging.\")\n        return _get_fallback_dataset()\n\n    try:\n        print(\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        if df.empty:\n            print(\"[CELL2] ERROR: CSV file is empty\")\n            return _get_fallback_dataset()\n\n        if \"src\" not in df.columns or \"tgt\" not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing required columns. Found columns: {list(df.columns)}\")\n            print(\"[CELL2] Expected format: src (Bengali), tgt (English) OR src (English), tgt (Bengali)\")\n            return _get_fallback_dataset()\n\n        sample_src = str(df[\"src\"].iloc[0]) if len(df) > 0 else \"\"\n        sample_tgt = str(df[\"tgt\"].iloc[0]) if len(df) > 0 else \"\"\n\n        src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n        tgt_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_tgt))\n        src_is_english = bool(re.search(r\"[a-zA-Z]\", sample_src)) and not src_is_bengali\n        tgt_is_english = bool(re.search(r\"[a-zA-Z]\", sample_tgt)) and not tgt_is_bengali\n\n        if src_is_english and tgt_is_bengali:\n            print(\"[CELL2] Detected src=English, tgt=Bengali: Swapping columns for bn→en task.\")\n            df = df.rename(columns={\"src\": \"_temp_tgt\", \"tgt\": \"_temp_src\"})\n            df = df.rename(columns={\"_temp_src\": \"src\", \"_temp_tgt\": \"tgt\"})\n            sample_src = str(df[\"src\"].iloc[0]) if len(df) > 0 else \"\"\n            sample_tgt = str(df[\"tgt\"].iloc[0]) if len(df) > 0 else \"\"\n            src_is_bengali = bool(_BENGALI_CHAR_RE.search(sample_src))\n            tgt_is_english = bool(re.search(r\"[a-zA-Z]\", sample_tgt)) and not bool(_BENGALI_CHAR_RE.search(sample_tgt))\n            if not src_is_bengali or not tgt_is_english:\n                print(\"[CELL2] ERROR: Swap failed, after swap src is not Bengali or tgt is not English.\")\n                return _get_fallback_dataset()\n            else:\n                print(\"[CELL2] Swap successful: src=Bengali, tgt=English\")\n        elif not src_is_bengali or not tgt_is_english:\n            print(\"[CELL2] WARNING: After column check, src not Bengali or tgt not English. Proceeding but output may be incorrect.\")\n\n        df = df.head(num_samples)\n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n\n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n\n        for row_tuple in tqdm(df.itertuples(index=False), total=len(df), desc=\"Loading dataset\"):\n            try:\n                src_val = row_tuple.src\n                tgt_val = row_tuple.tgt\n                if pd.isna(src_val) or pd.isna(tgt_val):\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", \"NaN value detected\")\n                    continue\n                bn = str(src_val).strip()\n                en = str(tgt_val).strip()\n                if not bn or not en:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", \"Empty src/tgt field\")\n                    continue\n                if not is_bengali_text(bn):\n                    skipped += 1\n                    cell2_dbg(\"not_bengali_src\", \"src field not Bengali\")\n                    continue\n                if not re.search(r\"[a-zA-Z]\", en):\n                    skipped += 1\n                    cell2_dbg(\"not_english_tgt\", \"tgt field not English\")\n                    continue\n                max_words = max(20, _MAX_LENGTH // 2)\n                if len(bn.split()) > max_words or len(en.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", \"Text too long\")\n                    continue\n                bn_norm = normalize_bengali(bn)\n                en_norm = normalize_english(en)\n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", \"Empty after normalization\")\n                    continue\n                pairs.append((bn_norm, en_norm))\n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception: {type(e).__name__}\")\n                continue\n\n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            print(\"[CELL2] Check that src column contains Bengali and tgt column contains English.\")\n            return _get_fallback_dataset()\n\n        return pairs\n\n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        traceback.print_exc()\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    print(\"[CELL2] Using fallback dataset (50 unique samples)\")\n    fallback_pairs = [\n        (\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\"),\n        (\"সে আমাকে পরে কল করবে।\", \"he will call me later.\"),\n        (\"আমরা প্রতিদিন তাজা ফল খাই।\", \"we eat fresh fruits every day.\"),\n        (\"তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\", \"his hard work has brought good results.\"),\n        (\"গাছে নতুন পাতাগুলো গজিয়েছে।\", \"new leaves have sprouted on the tree.\"),\n        (\"আমি বইয়ের পাতা উল্টাচ্ছি।\", \"i am turning the pages of the book.\"),\n        (\"কাল আমি বাজারে গিয়েছিলাম।\", \"yesterday i went to the market.\"),\n        (\"কাল আমি তোমার সাথে দেখা করব।\", \"tomorrow i will meet you.\"),\n        (\"তারা আকাশে উজ্জ্বল।\", \"the stars are bright in the sky.\"),\n        (\"তারা বাড়িতে নেই।\", \"they are not at home.\"),\n        (\"ব্যাংক নদীর ধারে ভেঙে গেছে।\", \"the bank by the river has collapsed.\"),\n        (\"আমি ব্যাংকে টাকা জমা দিয়েছি।\", \"i deposited money in the bank.\"),\n        (\"বার বার চেষ্টা করতে হবে।\", \"you have to try again and again.\"),\n        (\"আমি বার খুলে ভিতরে ঢুকলাম।\", \"i opened the bar and entered.\"),\n        (\"তার মাথা ব্যথা করছে।\", \"his head is hurting.\"),\n        (\"আমি মাথা নেড়ে সম্মতি দিলাম।\", \"i nodded my head in agreement.\"),\n        (\"সে হার মেনে নিয়েছে।\", \"he accepted defeat.\"),\n        (\"আমি গলায় সোনার হার পরেছি।\", \"i am wearing a gold necklace.\"),\n        (\"পানি খুব ঠান্ডা।\", \"the water is very cold.\"),\n        (\"আমি পানি খাচ্ছি।\", \"i am drinking water.\"),\n        (\"দল খেলায় জিতেছে।\", \"the team won the game.\"),\n        (\"আমি মাটি দল দিয়ে ফেললাম।\", \"i trampled the soil.\"),\n        (\"বাজার থেকে সবজি কিনলাম।\", \"i bought vegetables from the market.\"),\n        (\"বাজার অনেক ভিড় ছিল।\", \"the market was very crowded.\"),\n        (\"তার নাম আহমেদ।\", \"his name is ahmed.\"),\n        (\"নাম না করে কাজ করো।\", \"work without making a name.\"),\n        (\"কথা বলা বন্ধ করো।\", \"stop talking.\"),\n        (\"তার কথা শুনে ভালো লাগল।\", \"i felt good hearing his words.\"),\n        (\"বই পড়তে ভালো লাগে।\", \"i like reading books.\"),\n        (\"আমি একটি নতুন বই কিনেছি।\", \"i bought a new book.\"),\n        (\"ঘর পরিষ্কার করা হয়েছে।\", \"the house has been cleaned.\"),\n        (\"আমি ঘরে বসে আছি।\", \"i am sitting at home.\"),\n        (\"মন ভালো নেই।\", \"my mind is not good.\"),\n        (\"আমার মন চায় বেড়াতে যেতে।\", \"my mind wants to go for a walk.\"),\n        (\"হাত ধুয়ে নাও।\", \"wash your hands.\"),\n        (\"আমি তার হাত ধরলাম।\", \"i held his hand.\"),\n        (\"দিন কেটে যাচ্ছে।\", \"the day is passing by.\"),\n        (\"আজ কি দিন?\", \"what day is today?\"),\n        (\"রাত হয়ে এসেছে।\", \"night has come.\"),\n        (\"আমি রাত জেগে পড়েছি।\", \"i studied staying up at night.\"),\n        (\"জল খুব গরম।\", \"the water is very hot.\"),\n        (\"আমি জল দিয়ে গাছ সিঞ্চন করেছি।\", \"i watered the plants.\"),\n        (\"বাড়ি যাচ্ছি।\", \"i am going home.\"),\n        (\"আমার বাড়ি ঢাকায়।\", \"my house is in dhaka.\"),\n        (\"পার্কে অনেক মানুষ।\", \"there are many people in the park.\"),\n        (\"আমি প্রতিদিন পার্কে হাঁটি।\", \"i walk in the park every day.\"),\n        (\"নদী বইছে।\", \"the river is flowing.\"),\n        (\"আমি নদীর ধারে দাঁড়িয়ে আছি।\", \"i am standing by the river.\"),\n        (\"বন খুব সুন্দর।\", \"the forest is very beautiful.\"),\n        (\"আমি বন দেখতে গিয়েছিলাম।\", \"i went to see the forest.\"),\n    ]\n    return [(normalize_bengali(bn), normalize_english(en)) for bn, en in fallback_pairs]\n\nclass MemoryEfficientDataset(Dataset):\n    def __init__(\n        self,\n        pairs: List[Tuple[str, str]],\n        tokenizer: Any = None,\n        max_length: Optional[int] = None,\n        split: str = \"train\",\n    ):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        self.split = split\n\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                src, tgt = p\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                self.pairs.append((src, tgt))\n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n\n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid\")\n\n        try:\n            if \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {\n                f\"__{_SOURCE_LANG}__\",\n                f\"__{_TARGET_LANG}__\",\n                \"</s>\",\n                \"<pad>\",\n                \"<s>\",\n                \"<unk>\",\n            }\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\"tokenizer\"] = None\n        state[\"_tokenizer_name_or_path\"] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self.tokenizer = None\n        self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n                self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                try:\n                    if isinstance(enc[\"input_ids\"], torch.Tensor):\n                        input_ids = enc[\"input_ids\"].squeeze(0)\n                    else:\n                        input_ids = torch.tensor(enc[\"input_ids\"][0])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0])\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids))\n                if isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=False,\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict) and wm:\n                        token_word_map = wm\n                except Exception as e:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {e}\")\n\n            if not token_word_map and tokens and _has_map_subwords_to_words:\n                try:\n                    token_word_map = map_subwords_to_words(tokens, self.tokenizer)\n                except Exception as e:\n                    cell2_dbg(\"map_subwords_exc\", f\"map_subwords_to_words failed: {e}\")\n\n            if not token_word_map and tokens:\n                try:\n                    current_word: List[str] = []\n                    for idx, tok in enumerate(tokens):\n                        if isinstance(tok, str) and tok not in self.special_tokens:\n                            clean = (tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").replace(\"##\", \"\").strip())\n                            if clean:\n                                if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n                                    current_word = [clean]\n                                else:\n                                    current_word.append(clean)\n                                token_word_map[idx] = \"\".join(current_word)\n                except Exception as e:\n                    cell2_dbg(\"fallback_wm\", f\"Fallback word map failed: {e}\")\n\n            return input_ids, attention_mask, tokens, token_word_map\n\n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}\")\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get(\"tokenizer\", None)\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=False,\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer is not None else 1\n            labels[labels == int(pad_id)] = -100\n            return labels\n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\") -> Dict[str, Any]:\n        try:\n            src = \"আমি\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = random.choice([_TRAIN_DOMAIN, _TEST_DOMAIN])\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception:\n            pad_id = 1\n            domain_label = random.choice([_TRAIN_DOMAIN, _TEST_DOMAIN])\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": [],\n                \"domain_label\": domain_label,\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx}\")\n                return self._make_safe_sample(\"oob\")\n\n            src, tgt = self.pairs[idx]\n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            if DEBUG_CELL2 and idx < 3:\n                has_bengali = is_bengali_text(src)\n                has_english = any(\"a\" <= c.lower() <= \"z\" for c in src)\n                print(f\"[CELL2-GETITEM-{idx}] src sample: {src[:50]}\")\n                print(f\"[CELL2-GETITEM-{idx}] Bengali: {has_bengali}, English: {has_english}\")\n                if not has_bengali:\n                    print(f\"[CELL2] WARNING: src_text is NOT Bengali at idx={idx}!\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            domain_label = random.choice([_TRAIN_DOMAIN, _TEST_DOMAIN])\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens,\n                \"domain_label\": domain_label,\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    t = tensor.view(-1).long()\n    L = t.size(0)\n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n    inputs, masks, labs, twmaps, srcs, toks, domains = [], [], [], [], [], [], []\n\n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n            domain = s.get(\"domain_label\", random.choice([_TRAIN_DOMAIN, _TEST_DOMAIN]))\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n            domains.append(domain)\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]],\n            \"domain_labels\": torch.tensor([_TRAIN_DOMAIN], dtype=torch.long),\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n    try:\n        domain_labels = torch.tensor(domains, dtype=torch.long)\n    except Exception:\n        domain_labels = torch.tensor([random.choice([_TRAIN_DOMAIN, _TEST_DOMAIN]) for _ in inputs], dtype=torch.long)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks,\n        \"domain_labels\": domain_labels,\n    }\n\ndef create_optimized_dataloader(\n    dataset: Dataset,\n    batch_size: Optional[int] = None,\n    shuffle: bool = True,\n    split: str = \"train\",\n) -> DataLoader:\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n\n    batch_size = int(batch_size)\n    original_batch_size = batch_size\n    adjusted = False\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        new_batch_size = (batch_size // _NUM_GPUS) * _NUM_GPUS\n        if new_batch_size == 0:\n            if DEBUG_CELL2:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original.\")\n        else:\n            batch_size = new_batch_size\n            adjusted = batch_size != original_batch_size\n\n    if adjusted:\n        print(f\"[CELL2] Adjusted batch size {original_batch_size} to {batch_size} (DP-divisible, GPUs={_NUM_GPUS})\")\n\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs: Dict[str, Any] = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\nprint(\"Cell 2: Memory-efficient data loading ready\")\n","metadata":{"id":"5MkHgCN7H4J1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: DSCD MODULE (PURE UNSUPERVISED DISCOVERY - FIXED DYNAMIC MULTI-SENSE)\n# ==============================================================================\n\nimport threading\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, Dict, List, Any, Set, Tuple\n\nPRINT_INTERVAL = 200\n\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    _HAS_CLUSTERING = True\nexcept Exception:\n    _HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available\")\n\ntry:\n    from sklearn.cluster import KMeans\n    _HAS_KMEANS = True\nexcept Exception:\n    _HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available\")\n\ntry:\n    DSCD_MAX_PROTOS = int(DSCD_MAX_PROTOS)\n    DSCD_BUFFER_SIZE = int(DSCD_BUFFER_SIZE)\n    DSCD_N_MIN = int(DSCD_N_MIN)\n    DSCD_DISPERSION_THRESHOLD = float(DSCD_DISPERSION_THRESHOLD)\n    VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\n    DSCD_ENABLE_TRAINING_CLUSTERING = bool(DSCD_ENABLE_TRAINING_CLUSTERING)\nexcept (NameError, ValueError, TypeError):\n    DSCD_MAX_PROTOS = 8\n    DSCD_BUFFER_SIZE = 50\n    DSCD_N_MIN = 5\n    DSCD_DISPERSION_THRESHOLD = 0.50\n    VERBOSE_LOGGING = True\n    DSCD_ENABLE_TRAINING_CLUSTERING = True\n    print(\"[CELL3] WARNING: Using default DSCD config\")\n\ntry:\n    DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    DEBUG_DISCOVERY = False\n\ntry:\n    _MAX_TOKENS_PER_DISCOVERY = int(globals().get(\"_MAX_TOKENS_PER_DISCOVERY\", 150))\nexcept Exception:\n    _MAX_TOKENS_PER_DISCOVERY = 150\n\ntry:\n    DSCD_NEW_SENSE_LAMBDA = float(globals().get(\"DSCD_NEW_SENSE_LAMBDA\", 1.5))\nexcept Exception:\n    DSCD_NEW_SENSE_LAMBDA = 1.5\n\ntry:\n    HOMOGRAPH_REFERENCE_LIST_BN = set(HOMOGRAPH_REFERENCE_LIST_BN)\n    print(f\"[CELL3] Loaded reference list for evaluation: {len(HOMOGRAPH_REFERENCE_LIST_BN)} words\")\nexcept (NameError, TypeError):\n    HOMOGRAPH_REFERENCE_LIST_BN = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"\n    }\n    print(\"[CELL3] Using default reference list\")\n\nDSCD_MAX_CLUSTERING_POINTS = 500\n_PUNCT_SET = set('.,!?;:()[]{}\"\\'-—–/\\\\')\n\ndef normalize_token_key(token: str) -> str:\n    return (\n        str(token)\n        .replace(\"▁\", \"\")\n        .replace(\" \", \"\")\n        .replace(\"Ġ\", \"\")\n        .replace(\"##\", \"\")\n        .strip()\n        .lower()\n    )\n\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if not token:\n        return False\n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith('L'):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if letters / total < min_letter_fraction:\n        return False\n    return True\n\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embeddim, maxprotos: Optional[int] = None):\n        if maxprotos is None:\n            maxprotos = DSCD_MAX_PROTOS\n        self.embeddim = embeddim\n        self.maxprotos = int(maxprotos)\n        self.centroids: List[torch.Tensor] = []\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        self.mu = 0.0\n        self.tau = 1e-6\n        self.alpha = 0.1\n        self.labels: Optional[torch.Tensor] = None\n\n    def add_prototype(self, vector: torch.Tensor, current_time: Optional[float] = None, count: int = 1) -> None:\n        if current_time is None:\n            current_time = time.time()\n        v = vector.detach().cpu().clone()\n        if len(self.centroids) < self.maxprotos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creation_time.append(float(current_time))\n        else:\n            min_idx = int(np.argmin(self.counts)) if len(self.counts) > 0 else 0\n            self.centroids[min_idx] = v\n            self.counts[min_idx] = int(count)\n            self.creation_time[min_idx] = float(current_time)\n\n    def update_prototype(self, idx: int, vector: torch.Tensor, eta: float = 0.05, assignment_distance: Optional[float] = None) -> None:\n        if idx < 0 or idx >= len(self.centroids):\n            self.add_prototype(vector, time.time(), count=1)\n            return\n        old_centroid = self.centroids[idx]\n        new_vector = vector.detach().cpu()\n        self.centroids[idx] = (1.0 - eta) * old_centroid + eta * new_vector\n        self.counts[idx] = int(self.counts[idx]) + 1\n        if assignment_distance is not None:\n            self.update_rolling_stats(float(assignment_distance))\n\n    def update_rolling_stats(self, d: float) -> None:\n        if not self.distances:\n            self.mu = float(d)\n            self.tau = 1e-6\n            self.distances = [float(d)]\n            return\n        prev_mu = self.mu\n        self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n        self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n        self.distances.append(float(d))\n        if len(self.distances) > 50:\n            self.distances.pop(0)\n\n    def get_adaptive_threshold(self, lam: float = 1.0) -> float:\n        return float(self.mu + lam * self.tau)\n\n    def size(self) -> int:\n        return len(self.centroids)\n\n    def ensure_consistency(self) -> None:\n        n = len(self.centroids)\n        if len(self.counts) != n:\n            self.counts = self.counts[:n] if len(self.counts) > n else self.counts + [1] * (n - len(self.counts))\n        if len(self.creation_time) != n:\n            self.creation_time = self.creation_time[:n] if len(self.creation_time) > n else self.creation_time + [time.time()] * (n - len(self.creation_time))\n\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(\n        self,\n        embeddim: int,\n        tokenizer=None,\n        buffersize: Optional[int] = None,\n        maxprotos: Optional[int] = None,\n        nmin: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        language: str = \"bn\",\n        enable_training_clustering: Optional[bool] = None,\n        max_clustering_points: Optional[int] = None,\n        max_candidates_per_step: int = 2,\n        dscd_min_letters: int = 2,\n        dscd_min_letter_fraction: float = 0.6,\n    ):\n        super().__init__()\n        if buffersize is None:\n            buffersize = DSCD_BUFFER_SIZE\n        if maxprotos is None:\n            maxprotos = DSCD_MAX_PROTOS\n        if nmin is None:\n            nmin = DSCD_N_MIN\n        if dispersion_threshold is None:\n            dispersion_threshold = DSCD_DISPERSION_THRESHOLD\n        if max_clustering_points is None:\n            max_clustering_points = DSCD_MAX_CLUSTERING_POINTS\n        if enable_training_clustering is None:\n            enable_training_clustering = DSCD_ENABLE_TRAINING_CLUSTERING\n\n        self.embeddim = int(embeddim)\n        self.buffersize = int(buffersize)\n        self.maxprotos = int(maxprotos)\n        self.nmin = int(nmin)\n        self.dispersion_threshold = float(dispersion_threshold)\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscd_min_letters = int(dscd_min_letters)\n        self.dscd_min_letter_fraction = float(dscd_min_letter_fraction)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        try:\n            if tokenizer is not None and \"get_tokenizer_special_tokens\" in globals():\n                self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n            else:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", [])) if tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = set()\n\n        self.dscd_allowed_tokens: Set[str] = set()\n        self.dscd_ignored_tokens: Set[str] = set()\n        self.dscd_cache_max_size = 10000\n\n        self.prototype_stores: Dict[str, MemoryEfficientPrototypeStore] = {}\n        self.prototype_stores = self.prototype_stores\n        self.buffers: Dict[str, deque] = {}\n        self.discovered_log: List[Dict[str, Any]] = []\n        self.discovered_homographs: Set[str] = set()\n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n\n        self.dispersion_cache: Dict[str, float] = {}\n        self.dispersion_last_updated: Dict[str, float] = {}\n        self.dispersion_lock = threading.Lock()\n        self.clustering_lock = threading.RLock()\n        self.buffer_lock = threading.Lock()\n\n        from collections import deque as thread_deque\n        self.active_threads = thread_deque(maxlen=100)\n        self.thread_lock = threading.Lock()\n        self.last_cluster_time: Dict[str, float] = {}\n        self.cluster_cooldown_seconds = 5.0\n        self.enable_training_clustering = bool(enable_training_clustering)\n        self.discovery_count = 0\n        self.discovery_times: List[float] = []\n        self.clustered_tokens: Set[str] = set()\n        self.cluster_stats: Dict[str, Dict[str, Any]] = {}\n\n        self.span_head = nn.Sequential(\n            nn.Linear(self.embeddim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1),\n        )\n\n        self.sigmanet = nn.Sequential(\n            nn.Linear(self.embeddim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1),\n        )\n\n        self.gate_w = nn.Parameter(torch.tensor(1.0))\n        self.gate_b = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n\n        self.temperature = 0.07\n\n        self.max_clustering_points = int(max_clustering_points)\n        self.max_candidates_per_step = int(max_candidates_per_step)\n\n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        state = super().state_dict(destination, prefix, keep_vars)\n        plain_stores = {}\n        for token, store in self.prototype_stores.items():\n            plain_stores[token] = {\n                \"centroids\": [c.cpu() for c in store.centroids] if hasattr(store, 'centroids') else [],\n                \"counts\": list(store.counts) if hasattr(store, 'counts') else [],\n                \"creation_time\": list(store.creation_time) if hasattr(store, 'creation_time') else [],\n                \"mu\": float(store.mu) if hasattr(store, 'mu') else 0.0,\n                \"tau\": float(store.tau) if hasattr(store, 'tau') else 0.0,\n                \"size\": int(store.size()) if hasattr(store, 'size') else 0,\n            }\n        state[prefix + \"prototype_stores\"] = plain_stores\n        state[prefix + \"discovered_homographs\"] = list(self.discovered_homographs)\n        return state\n\n    def load_state_dict(self, state_dict, strict=True):\n        prefix = \"\"\n        plain_stores = state_dict.pop('prototype_stores', {})\n        discovered = state_dict.pop('discovered_homographs', [])\n        super().load_state_dict(state_dict, strict=strict)\n\n        if not plain_stores:\n            print(\"[DSCD] WARNING: Empty prototype_stores in checkpoint\")\n            return\n\n        self.prototype_stores = {}\n        self.discovered_homographs = set(discovered)\n\n        for token, store_dict in plain_stores.items():\n            store = MemoryEfficientPrototypeStore(embeddim=self.embeddim, maxprotos=self.maxprotos)\n            centroids_data = store_dict.get(\"centroids\", [])\n            store.centroids = []\n            for c in centroids_data:\n                if isinstance(c, torch.Tensor):\n                    store.centroids.append(c)\n                else:\n                    store.centroids.append(torch.tensor(c))\n            store.counts = store_dict.get(\"counts\", [])\n            store.creation_time = store_dict.get(\"creation_time\", [])\n            store.mu = store_dict.get(\"mu\", 0.0)\n            store.tau = store_dict.get(\"tau\", 0.0)\n            store.ensure_consistency()\n            self.prototype_stores[token] = store\n\n        print(f\"[DSCD] Loaded {len(self.prototype_stores)} tokens, {sum(s.size() for s in self.prototype_stores.values())} prototypes\")\n\n    @staticmethod\n    def clean_token(token):\n        token = str(token)\n        token = token.replace(\"▁\", \"\").replace(\"##\", \"\")\n        for punct in [\",\", \".\", \",\", \"!\", \"?\", \":\", \";\", \"-\"]:\n            token = token.replace(punct, \"\")\n        return token.strip()\n\n    def is_valid_multisense(self, token):\n        if token not in self.prototype_stores:\n            return False\n        store = self.prototype_stores[token]\n        total_occurrences = sum(store.counts) if hasattr(store, 'counts') else 0\n        min_per_proto = min(store.counts) if hasattr(store, 'counts') and store.counts else 0\n        return (\n            store.size() >= 2\n            and total_occurrences >= 10\n            and min_per_proto >= 2\n        )\n\n    def is_multisense_store(self, store: MemoryEfficientPrototypeStore) -> bool:\n        k = store.size()\n        if k < 2:\n            return False\n        counts = store.counts if store.counts else [1] * k\n        strong = sum(1 for c in counts if c >= max(2, self.nmin // 2))\n        if strong < 2:\n            return False\n        try:\n            cents = []\n            for c in store.centroids:\n                if isinstance(c, torch.Tensor):\n                    cents.append(c.cpu().numpy())\n                else:\n                    cents.append(np.asarray(c, dtype=np.float32))\n            if len(cents) < 2:\n                return False\n            cents = np.stack(cents, axis=0)\n            dists = np.linalg.norm(cents[:, None, :] - cents[None, :, :], axis=-1)\n            tri = dists[np.triu_indices(len(cents), k=1)]\n            if tri.size == 0:\n                return False\n            min_dist = float(tri.min())\n            base = max(store.tau, 1e-3)\n            return min_dist > base * DSCD_NEW_SENSE_LAMBDA\n        except Exception:\n            return True\n\n    def periodic_discovery_check(self, current_step: int, frequency: int) -> None:\n        if not self.enable_training_clustering:\n            print(f\"[DSCD-DISCOVERY] Clustering disabled (enable_training_clustering={self.enable_training_clustering})\")\n            return\n        \n        if current_step % frequency != 0:\n            return\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"[DSCD-DISCOVERY] Starting @ step {current_step}\")\n        print(f\"{'='*80}\")\n        \n        try:\n            with self.buffer_lock:\n                total_buffers = len(self.buffers)\n                buffer_sizes = {k: len(v) for k, v in self.buffers.items()}\n                large_buffers = {k: v for k, v in buffer_sizes.items() if v >= self.nmin}\n            \n            print(f\"[DSCD-DISCOVERY] Total tokens with buffers: {total_buffers}\")\n            print(f\"[DSCD-DISCOVERY] Tokens with enough samples (>={self.nmin}): {len(large_buffers)}\")\n            \n            if len(large_buffers) > 0:\n                top_5 = sorted(large_buffers.items(), key=lambda x: x[1], reverse=True)[:5]\n                print(f\"[DSCD-DISCOVERY] Top 5 buffer sizes:\")\n                for tok, size in top_5:\n                    print(f\"  - '{tok}': {size} samples\")\n        except Exception as e:\n            print(f\"[DSCD-DISCOVERY] Error checking buffers: {e}\")\n            return\n        \n        if len(large_buffers) == 0:\n            print(f\"[DSCD-DISCOVERY] No tokens with >={self.nmin} samples - skipping\")\n            print(f\"{'='*80}\\n\")\n            return\n        \n        print(f\"[DSCD-DISCOVERY] Attempting to acquire clustering_lock (timeout=5s)...\")\n        lock_start = time.time()\n        acquired = self.clustering_lock.acquire(timeout=5.0)\n        lock_time = time.time() - lock_start\n        \n        if not acquired:\n            print(f\"[DSCD-DISCOVERY] ❌ LOCK TIMEOUT after {lock_time:.2f}s @ step {current_step}\")\n            print(f\"[DSCD-DISCOVERY] Lock is held by another process - skipping discovery\")\n            print(f\"{'='*80}\\n\")\n            return\n        \n        print(f\"[DSCD-DISCOVERY] ✅ Lock acquired in {lock_time:.2f}s\")\n        \n        try:\n            discovery_start = time.time()\n            \n            print(f\"[DSCD-DISCOVERY] Calling discover_homographs()...\")\n            discovered_count = self.discover_homographs()\n            \n            discovery_time = time.time() - discovery_start\n            \n            print(f\"\\n[DSCD-DISCOVERY] ✅ COMPLETED in {discovery_time:.2f}s\")\n            print(f\"[DSCD-DISCOVERY] Discovered {discovered_count} new homographs\")\n            print(f\"[DSCD-DISCOVERY] Total discovered so far: {len(self.discovered_homographs)}\")\n            \n        except Exception as e:\n            print(f\"[DSCD-DISCOVERY] ❌ FAILED with error: {type(e).__name__}: {e}\")\n            import traceback\n            traceback.print_exc()\n        finally:\n            self.clustering_lock.release()\n            print(f\"[DSCD-DISCOVERY] Lock released\")\n            print(f\"{'='*80}\\n\")\n\n    def discover_homographs_for_tokens(\n        self,\n        token_names: List[str],\n        min_cluster_samples: int,\n        dispersion_threshold: float,\n        global_step: int,\n    ) -> int:\n        discovered_in_run: List[str] = []\n        for idx, token in enumerate(token_names):\n            try:\n                success = self.cluster_buffer_to_prototypes_hierarchical(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        self.discovered_homographs.add(clean_token)\n                        discovered_in_run.append(clean_token)\n            except Exception:\n                continue\n\n        try:\n            self.discovered_log.append({\n                \"timestamp\": time.time(),\n                \"global_step\": global_step,\n                \"candidates_processed\": len(token_names),\n                \"discovered_count\": len(discovered_in_run),\n                \"homographs\": discovered_in_run,\n                \"total_discovered\": len(self.discovered_homographs),\n            })\n        except Exception:\n            pass\n\n        return len(discovered_in_run)\n\n    def discover_homographs(\n        self,\n        min_cluster_samples: Optional[int] = None,\n        dispersion_threshold: Optional[float] = None,\n        max_candidates: int = 500,\n    ) -> int:\n        if min_cluster_samples is None:\n            min_cluster_samples = self.nmin\n        if dispersion_threshold is None:\n            dispersion_threshold = self.dispersion_threshold\n\n        print(f\"[DISCOVER] Scanning buffers (min_samples={min_cluster_samples}, min_dispersion={dispersion_threshold:.3f})...\")\n        \n        candidates: List[Tuple[str, float, int, float]] = []\n\n        acquired = self.buffer_lock.acquire(timeout=15.0)\n        if not acquired:\n            print(\"[DISCOVER] ❌ Lock timeout - buffer_lock held by another thread\")\n            return 0\n        \n        try:\n            total_tokens = len(self.buffers)\n            print(f\"[DISCOVER] Checking {total_tokens} tokens...\")\n            \n            buffers_snapshot = {k: (len(v), list(v)) for k, v in self.buffers.items()}\n        finally:\n            self.buffer_lock.release()\n        \n        checked = 0\n        skipped_small = 0\n        start_time = time.time()\n        \n        for token, (buffer_size, buffer_data) in buffers_snapshot.items():\n            checked += 1\n            \n            if checked % 200 == 0:\n                elapsed = time.time() - start_time\n                rate = checked / elapsed if elapsed > 0 else 0\n                eta = (total_tokens - checked) / rate if rate > 0 else 0\n                print(f\"[DISCOVER] Progress: {checked}/{total_tokens} ({100*checked//total_tokens}%) \"\n                      f\"| {rate:.0f} tok/s | ETA: {eta:.0f}s | candidates: {len(candidates)}\")\n            \n            if buffer_size < min_cluster_samples:\n                skipped_small += 1\n                continue\n            \n            try:\n                if len(buffer_data) < 2:\n                    continue\n                \n                embeddings_np = np.stack([\n                    e.cpu().numpy() if isinstance(e, torch.Tensor) else np.asarray(e, dtype=np.float32)\n                    for e in buffer_data\n                ], axis=0)\n                \n                centroid = embeddings_np.mean(axis=0)\n                distances = np.linalg.norm(embeddings_np - centroid[None, :], axis=1)\n                dispersion = float(distances.std())\n                \n                if dispersion >= dispersion_threshold:\n                    rank_score = dispersion * buffer_size\n                    candidates.append((token, rank_score, buffer_size, dispersion))\n            except Exception:\n                continue\n        \n        print(f\"[DISCOVER] ✅ Scan complete: {len(candidates)} candidates found (skipped {skipped_small} small buffers)\")\n\n        if not candidates:\n            print(f\"[DISCOVER] No candidates meet criteria - returning\")\n            return 0\n\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        candidates = candidates[:max_candidates]\n        \n        print(f\"[DISCOVER] Top {len(candidates)} candidates selected for clustering\")\n        print(f\"[DISCOVER] Top 3:\")\n        for i, (tok, score, bufsize, disp) in enumerate(candidates[:3], 1):\n            print(f\"  {i}. '{tok}': score={score:.2f}, buffer={bufsize}, dispersion={disp:.3f}\")\n\n        discovered: List[str] = []\n        \n        try:\n            from tqdm import tqdm\n            candidate_iter = tqdm(candidates, desc=\"[DISCOVER] Clustering\", ncols=100)\n            use_tqdm = True\n        except ImportError:\n            candidate_iter = candidates\n            use_tqdm = False\n            print(f\"[DISCOVER] Processing {len(candidates)} candidates...\")\n        \n        for idx, (token, score, bufsize, disp) in enumerate(candidate_iter):\n            if not use_tqdm and idx % 10 == 0:\n                print(f\"[DISCOVER] Progress: {idx}/{len(candidates)} ({100*idx//len(candidates)}%)\")\n            \n            try:\n                success = self.cluster_buffer_to_prototypes_hierarchical(token)\n                if success:\n                    store = self.prototype_stores.get(token)\n                    if store and store.size() >= 2:\n                        clean_token = normalize_token_key(token)\n                        self.discovered_homographs.add(clean_token)\n                        discovered.append(clean_token)\n                        \n                        if len(discovered) <= 5:\n                            print(f\"\\n[DISCOVER] ✓ '{token}' → {store.size()} prototypes (counts={store.counts})\")\n            except Exception as e:\n                if DEBUG_DISCOVERY:\n                    print(f\"\\n[DISCOVER] ✗ '{token}' failed: {type(e).__name__}\")\n                continue\n\n        print(f\"\\n[DISCOVER] ✅ Clustering complete!\")\n        print(f\"[DISCOVER] New homographs discovered: {len(discovered)}\")\n        \n        if len(discovered) > 0:\n            print(f\"[DISCOVER] Sample homographs: {discovered[:10]}\")\n\n        try:\n            self.discovered_log.append({\n                \"timestamp\": time.time(),\n                \"candidates\": len(candidates),\n                \"discovered\": len(discovered),\n                \"homographs\": discovered[:20],\n            })\n        except Exception:\n            pass\n\n        return len(discovered)\n\n    def get_dispersion(self, tokentype: str) -> float:\n        with self.dispersion_lock:\n            if tokentype in self.dispersion_cache:\n                try:\n                    last_update = self.dispersion_last_updated.get(tokentype, 0.0)\n                    if (time.time() - last_update) < 3600:\n                        return self.dispersion_cache[tokentype]\n                except Exception:\n                    pass\n\n        with self.buffer_lock:\n            if tokentype not in self.buffers or len(self.buffers[tokentype]) < 2:\n                return 0.0\n\n            try:\n                embeddings: List[np.ndarray] = []\n                for emb in self.buffers[tokentype]:\n                    try:\n                        if isinstance(emb, torch.Tensor):\n                            embeddings.append(emb.cpu().numpy())\n                        else:\n                            embeddings.append(np.asarray(emb, dtype=np.float32))\n                    except Exception:\n                        continue\n\n                if len(embeddings) < 2:\n                    return 0.0\n\n                embeddings_np = np.stack(embeddings, axis=0)\n                centroid = embeddings_np.mean(axis=0)\n                distances = np.linalg.norm(embeddings_np - centroid[None, :], axis=1)\n                dispersion = float(distances.std())\n\n                with self.dispersion_lock:\n                    self.dispersion_cache[tokentype] = dispersion\n                    self.dispersion_last_updated[tokentype] = time.time()\n\n                return dispersion\n            except Exception:\n                return 0.0\n\n    def validate_prototypes(\n        self,\n        homograph_list: Optional[List[str]] = None,\n        cluster_missing: bool = False,\n    ) -> Dict[str, Any]:\n        if homograph_list is None:\n            try:\n                homograph_list = list(HOMOGRAPH_REFERENCE_LIST_BN)\n            except Exception:\n                homograph_list = [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\"]\n\n        print(\"=\" * 80)\n        print(\"[DSCD-VALIDATION] Prototype Quality Check\")\n        print(\"=\" * 80)\n\n        validation_results: Dict[str, Any] = {\n            \"total_tokens\": len(self.prototype_stores),\n            \"total_prototypes\": 0,\n            \"multisense_tokens\": 0,\n            \"homographs_found\": 0,\n            \"homographs_missing\": [],\n            \"avg_prototypes_per_token\": 0.0,\n            \"avg_samples_per_prototype\": 0.0,\n            \"quality_score\": 0.0,\n        }\n\n        total_samples = 0\n        for token, store in self.prototype_stores.items():\n            num_protos = len(store.centroids)\n            validation_results[\"total_prototypes\"] += num_protos\n\n            if self.is_multisense_store(store):\n                validation_results[\"multisense_tokens\"] += 1\n\n            try:\n                total_samples += sum(store.counts)\n            except Exception:\n                pass\n\n        if validation_results[\"total_tokens\"] > 0:\n            validation_results[\"avg_prototypes_per_token\"] = validation_results[\"total_prototypes\"] / validation_results[\"total_tokens\"]\n        if validation_results[\"total_prototypes\"] > 0:\n            validation_results[\"avg_samples_per_prototype\"] = total_samples / validation_results[\"total_prototypes\"]\n\n        print(\"[VALIDATION] Reference Homograph Coverage:\")\n        print(\"-\" * 80)\n\n        missing_tokens_to_cluster: List[str] = []\n        for homograph in homograph_list:\n            clean_h = homograph.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n            found = False\n            found_key = None\n            found_protos = 0\n\n            if homograph in self.prototype_stores:\n                found = True\n                found_key = homograph\n                found_protos = len(self.prototype_stores[homograph].centroids)\n            elif clean_h in self.prototype_stores:\n                found = True\n                found_key = clean_h\n                found_protos = len(self.prototype_stores[clean_h].centroids)\n            else:\n                for key in self.prototype_stores.keys():\n                    clean_key = str(key).replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n                    if clean_key == clean_h or clean_h in clean_key or clean_key in clean_h:\n                        found = True\n                        found_key = key\n                        found_protos = len(self.prototype_stores[key].centroids)\n                        break\n\n            if found and self.is_multisense_store(self.prototype_stores[found_key]):\n                validation_results[\"homographs_found\"] += 1\n                try:\n                    counts = self.prototype_stores[found_key].counts\n                    print(f\"  ✓ '{homograph}' → {found_protos} prototypes (counts={counts})\")\n                except Exception:\n                    print(f\"  ✓ '{homograph}' → {found_protos} prototypes\")\n            elif found and found_protos == 1:\n                validation_results[\"homographs_missing\"].append(homograph)\n                print(f\"  ⚠ '{homograph}' → Only 1 prototype\")\n                if cluster_missing:\n                    missing_tokens_to_cluster.append(found_key)\n            else:\n                validation_results[\"homographs_missing\"].append(homograph)\n                print(f\"  ✗ '{homograph}' → NOT FOUND\")\n                if cluster_missing:\n                    if homograph in self.buffers or clean_h in self.buffers:\n                        key_to_cluster = homograph if homograph in self.buffers else clean_h\n                        if len(self.buffers[key_to_cluster]) >= max(5, self.nmin // 2):\n                            print(f\"      → Found in buffer, will cluster\")\n                            missing_tokens_to_cluster.append(key_to_cluster)\n\n        if cluster_missing and missing_tokens_to_cluster:\n            print(f\"[VALIDATION] Clustering {len(missing_tokens_to_cluster)} missing tokens...\")\n            for token in missing_tokens_to_cluster:\n                try:\n                    self.cluster_buffer_to_prototypes_hierarchical(token)\n                    if token in self.prototype_stores and self.is_multisense_store(self.prototype_stores[token]):\n                        print(f\"  ✓ Successfully clustered '{token}'\")\n                except Exception as e:\n                    print(f\"  ✗ Failed to cluster '{token}': {e}\")\n\n        homograph_coverage = validation_results[\"homographs_found\"] / len(homograph_list) if homograph_list else 0.0\n        multisense_ratio = validation_results[\"multisense_tokens\"] / validation_results[\"total_tokens\"] if validation_results[\"total_tokens\"] > 0 else 0.0\n        validation_results[\"quality_score\"] = (\n            homograph_coverage * 0.6 + multisense_ratio * 0.4\n        )\n\n        print(\"-\" * 80)\n        print(\"[VALIDATION] Summary:\")\n        print(f\"  - Total tokens: {validation_results['total_tokens']}\")\n        print(f\"  - Total prototypes: {validation_results['total_prototypes']}\")\n        print(f\"  - Multi-sense tokens: {validation_results['multisense_tokens']}\")\n        print(f\"  - Reference found: {validation_results['homographs_found']}/{len(homograph_list)}\")\n        print(f\"  - Quality Score: {validation_results['quality_score']:.2}\")\n        print(\"=\" * 80)\n\n        return validation_results\n\n    def should_track_token(self, tokentext: str) -> bool:\n        if not tokentext or not isinstance(tokentext, str):\n            return False\n\n        if len(self.dscd_allowed_tokens) > self.dscd_cache_max_size:\n            self.dscd_allowed_tokens.clear()\n        if len(self.dscd_ignored_tokens) > self.dscd_cache_max_size:\n            self.dscd_ignored_tokens.clear()\n\n        if tokentext in self.dscd_allowed_tokens:\n            return True\n        if tokentext in self.dscd_ignored_tokens:\n            return False\n\n        if not getattr(self, \"training\", False):\n            if tokentext in self.prototype_stores:\n                self.dscd_allowed_tokens.add(tokentext)\n                return True\n            clean = tokentext.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n            if clean and clean in self.prototype_stores:\n                self.dscd_allowed_tokens.add(tokentext)\n                return True\n\n        if tokentext in self.special_tokens:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        clean = tokentext.replace(\"▁\", \"\").replace(\"##\", \"\").replace(\"Ġ\", \"\").replace(\" \", \"\").strip()\n        if not clean:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        if len(clean) < 2:\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        if not any(c.isalpha() for c in clean):\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        if clean.isdigit():\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        if all(c in _PUNCT_SET for c in clean):\n            self.dscd_ignored_tokens.add(tokentext)\n            return False\n\n        try:\n            bengali_block = any(\"\\u0980\" <= c <= \"\\u09FF\" for c in clean)\n            if bengali_block:\n                if len(clean) >= 2:\n                    self.dscd_allowed_tokens.add(tokentext)\n                    return True\n                else:\n                    self.dscd_ignored_tokens.add(tokentext)\n                    return False\n        except Exception:\n            pass\n\n        if is_word_token(\n            clean,\n            min_letters=self.dscd_min_letters,\n            min_letter_fraction=self.dscd_min_letter_fraction,\n        ):\n            self.dscd_allowed_tokens.add(tokentext)\n            return True\n\n        self.dscd_ignored_tokens.add(tokentext)\n        return False\n\n    def canonical_token_key(\n        self,\n        raw_token: str,\n        token_word_map: Optional[Dict[int, Optional[str]]],\n        idx: int,\n    ) -> Optional[str]:\n        canonical: Optional[str] = None\n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                canonical = str(token_word_map[idx]).strip()\n        except Exception:\n            canonical = None\n\n        if not canonical:\n            canonical = normalize_token_key(raw_token)\n\n        if not canonical or len(canonical) < 2:\n            return None\n        return canonical\n\n    def cleanup_threads(self) -> None:\n        try:\n            with self.thread_lock:\n                alive = [th for th in list(self.active_threads) if th.is_alive()]\n                self.active_threads.clear()\n                self.active_threads.extend(alive)\n        except Exception:\n            pass\n\n    def cleanup_memory(self) -> None:\n        try:\n            for tokentype, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffersize * 1.5):\n                    while len(buffer) > self.buffersize:\n                        buffer.popleft()\n\n            try:\n                now = time.time()\n                expired = [k for k, v in self.dispersion_last_updated.items() if (now - v) > 3600]\n                for k in expired:\n                    self.dispersion_cache.pop(k, None)\n                    self.dispersion_last_updated.pop(k, None)\n            except Exception:\n                pass\n\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n    def forward(\n        self,\n        token_embeddings,\n        token_types=None,\n        train_mode: bool = True,\n        token_word_map=None,\n        h_all=None,\n        input_ids=None,\n        attention_mask=None,\n    ):\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n\n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n\n        if input_ids is not None and token_types is None:\n            batch_size, seq_len = input_ids.shape\n            token_types = []\n            for b in range(batch_size):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(\n                            self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist())\n                        )\n                    except Exception:\n                        token_types.append([f\"tok{i}\" for i in range(seq_len)])\n                else:\n                    token_types.append([f\"tok{i}\" for i in range(seq_len)])\n\n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n            self.cleanup_threads()\n\n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        seq_len = int(token_embeddings.size(1))\n\n        all_outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n\n        for b in range(batch_size):\n            word_map = token_word_map[b] if token_word_map and len(token_word_map) > b else None\n            batch_outputs = self.process_sequence(\n                token_embeddings[b],\n                token_types[b] if token_types and len(token_types) > b else [f\"tok{i}\" for i in range(seq_len)],\n                device,\n                word_map=word_map,\n                train_mode=train_mode,\n            )\n\n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n\n        try:\n            h_aug_list: List[torch.Tensor] = []\n            max_seq_len = seq_len\n\n            for b in range(batch_size):\n                h_batch_list = all_outputs[\"h_augmented\"][b]\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = torch.zeros(max_seq_len, self.embeddim, device=device)\n                h_aug_list.append(h_batch)\n\n            all_outputs[\"h_augmented\"] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs[\"h_augmented\"] = token_embeddings\n\n        try:\n            proto_assign_tensor = []\n            for row in all_outputs[\"proto_assignments\"]:\n                try:\n                    stacked = torch.stack(\n                        [x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in row],\n                        dim=0,\n                    )\n                    proto_assign_tensor.append(stacked)\n                except Exception:\n                    proto_assign_tensor.append(\n                        torch.tensor(\n                            [int(x) if not isinstance(x, torch.Tensor) else int(x.item()) for x in row],\n                            dtype=torch.long,\n                        )\n                    )\n            all_outputs[\"proto_assignments\"] = proto_assign_tensor\n        except Exception:\n            pass\n\n        return all_outputs\n\n    def process_sequence(\n        self,\n        token_embeddings: torch.Tensor,\n        token_types: List[Any],\n        device: torch.device,\n        word_map: Optional[Dict[int, Optional[str]]] = None,\n        train_mode: bool = True,\n    ) -> Dict[str, List[Any]]:\n        seq_len = int(token_embeddings.size(0))\n\n        outputs: Dict[str, List[Any]] = {\n            \"proto_assignments\": [],\n            \"proto_probs\": [],\n            \"uncertainties\": [],\n            \"span_preds\": [],\n            \"gates\": [],\n            \"h_augmented\": [],\n        }\n\n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f\"tok{j}\"\n            if not isinstance(raw_tok, str):\n                raw_tok = str(raw_tok) if raw_tok is not None else f\"tok{j}\"\n\n            token_key = self.canonical_token_key(raw_tok, word_map, j)\n            h_j = token_embeddings[j]\n\n            if not token_key:\n                outputs[\"proto_assignments\"].append(torch.tensor(-1))\n                outputs[\"proto_probs\"].append([])\n                outputs[\"uncertainties\"].append(0.0)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n\n            if not self.should_track_token(token_key):\n                outputs[\"proto_assignments\"].append(torch.tensor(-1))\n                outputs[\"proto_probs\"].append([])\n                outputs[\"uncertainties\"].append(0.0)\n                outputs[\"span_preds\"].append(0.0)\n                outputs[\"gates\"].append(0.0)\n                outputs[\"h_augmented\"].append(h_j)\n                continue\n\n            with self.buffer_lock:\n                if token_key not in self.buffers:\n                    self.buffers[token_key] = deque(maxlen=self.buffersize)\n                    self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(\n                        self.embeddim, self.maxprotos\n                    )\n\n                try:\n                    self.buffers[token_key].append(h_j.detach().clone().cpu())\n                except Exception:\n                    try:\n                        self.buffers[token_key].append(h_j.cpu())\n                    except Exception:\n                        pass\n\n            store = self.prototype_stores[token_key]\n            centroids_snapshot: Optional[List[torch.Tensor]] = None\n\n            try:\n                if hasattr(store, 'centroids') and len(store.centroids) > 0:\n                    centroids_snapshot = []\n                    for c in store.centroids:\n                        try:\n                            if isinstance(c, torch.Tensor):\n                                centroids_snapshot.append(c.clone().cpu())\n                            else:\n                                centroids_snapshot.append(\n                                    torch.from_numpy(np.asarray(c, dtype=np.float32)).cpu()\n                                )\n                        except Exception:\n                            continue\n                    if not centroids_snapshot:\n                        centroids_snapshot = None\n            except Exception:\n                centroids_snapshot = None\n\n            assignment = -1\n            prob_list: List[float] = []\n            uncertainty = 0.0\n            span_pred = 0.0\n            gate_val = 0.0\n            h_aug = h_j\n\n            if centroids_snapshot and len(centroids_snapshot) >= 1:\n                try:\n                    try:\n                        h_cpu = h_j.detach().cpu().numpy()\n                    except Exception:\n                        h_cpu = h_j.cpu().numpy()\n\n                    try:\n                        cents_np = np.stack([c.numpy() for c in centroids_snapshot], axis=0)\n                    except Exception:\n                        cents_np = np.stack(\n                            [np.asarray(c, dtype=np.float32) for c in centroids_snapshot],\n                            axis=0,\n                        )\n\n                    dists_np = np.linalg.norm(cents_np - h_cpu[None, :], axis=1)\n\n                    if dists_np.size > 0:\n                        min_dist = float(dists_np.min())\n                        min_idx = int(np.argmin(dists_np))\n                        max_dist = float(dists_np.max())\n                        num_valid = len(dists_np)\n\n                        if store.size() < self.maxprotos and min_dist > store.get_adaptive_threshold(DSCD_NEW_SENSE_LAMBDA):\n                            store.add_prototype(h_j, time.time(), count=1)\n                            assignment = store.size() - 1\n                            centroids_snapshot.append(h_j.cpu())\n                            cents_np = np.vstack([cents_np, h_cpu[None, :]])\n                            dists_np = np.append(dists_np, 0.0)\n                            num_valid += 1\n                        else:\n                            assignment = min_idx\n\n                        if num_valid >= 2:\n                            span_range = max_dist - min_dist\n                            ratio = span_range / (max_dist + 1e-8)\n                            if ratio > 0.15:\n                                span_pred = float((0.3 + (ratio - 0.15) * 1.5))\n                            else:\n                                span_pred = float(ratio * 2.0)\n                            span_pred = min(1.0, max(0.0, span_pred))\n                        else:\n                            if max_dist > 0.3:\n                                span_pred = min(1.0, max_dist * 0.8)\n                            else:\n                                span_pred = 0.1\n\n                        try:\n                            store.update_rolling_stats(min_dist)\n                        except Exception:\n                            pass\n\n                        try:\n                            dist_tensor = torch.from_numpy(dists_np).to(device)\n                            probs_tensor = F.softmax(-dist_tensor / self.temperature, dim=0)\n                            prob_list = probs_tensor.tolist()\n\n                            entropy = -torch.sum(probs_tensor * torch.log(probs_tensor + 1e-10))\n                            max_entropy = np.log(num_valid) if num_valid > 1 else 1.0\n                            H_norm = float(entropy.item() / max_entropy) if max_entropy > 0 else 0.0\n\n                            try:\n                                sigma_pred = self.sigmanet(h_j.unsqueeze(0))\n                                sigma_norm = float(torch.sigmoid(sigma_pred).item())\n                            except Exception:\n                                sigma_norm = 0.5\n\n                            dmin_norm = min(1.0, min_dist / 0.5)\n\n                            uncertainty = 0.5 * H_norm + 0.25 * sigma_norm + 0.25 * dmin_norm\n                            uncertainty = min(1.0, max(0.0, uncertainty))\n\n                        except Exception:\n                            exps = np.exp(-dists_np - np.max(-dists_np)) if dists_np.size > 0 else np.array([])\n                            if exps.size > 0:\n                                probs = exps / (exps.sum() + 1e-12)\n                                prob_list = probs.tolist()\n                                entropy_val = -np.sum(probs * np.log(probs + 1e-10))\n                                max_entropy = np.log(num_valid) if num_valid > 1 else 1.0\n                                uncertainty = float(entropy_val / max_entropy) if max_entropy > 0 else 0.0\n                            else:\n                                prob_list = []\n                                uncertainty = 0.0\n\n                        if num_valid >= 2 and uncertainty > 0.15:\n                            logit = 10.0 * (uncertainty - 0.15)\n                            gate_val = float(1.0 / (1.0 + math.exp(-logit)))\n                        else:\n                            gate_val = 0.0\n\n                        if gate_val > 0.3 and 0 <= assignment < len(centroids_snapshot):\n                            try:\n                                centroid_t = centroids_snapshot[assignment]\n                                if device != torch.device(\"cpu\"):\n                                    try:\n                                        centroid_t = centroid_t.to(device)\n                                    except Exception:\n                                        pass\n                                h_aug = h_j + 0.1 * (centroid_t - h_j)\n                            except Exception:\n                                h_aug = h_j\n\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD] Assignment error for {token_key}: {str(e)[:200]}\")\n\n            outputs[\"proto_assignments\"].append(torch.tensor(assignment))\n            outputs[\"proto_probs\"].append(prob_list)\n            outputs[\"uncertainties\"].append(uncertainty)\n            outputs[\"span_preds\"].append(span_pred)\n            outputs[\"gates\"].append(gate_val)\n            outputs[\"h_augmented\"].append(h_aug)\n\n        try:\n            if not train_mode and len(self.prototype_stores) > 0 and VERBOSE_LOGGING:\n                if self.last_periodic_check % PRINT_INTERVAL == 0:\n                    self.print_clusters_summary()\n                self.last_periodic_check += 1\n        except Exception:\n            pass\n\n        return outputs\n\n    def print_clusters_summary(self) -> None:\n        try:\n            items: List[Tuple[str, int, int, float, float, int]] = []\n            for token, store in self.prototype_stores.items():\n                try:\n                    proto_sample_count = sum(getattr(store, 'counts', []) or [])\n                except Exception:\n                    proto_sample_count = 0\n\n                buffer_len = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n\n            items.sort(key=lambda x: x[1], reverse=True)\n            top5 = items[:5]\n\n            if VERBOSE_LOGGING:\n                print(\"[CLUSTER] Top 5 clusters:\")\n                print(\"-\" * 100)\n                print(f\"{'Rank':<6} {'Token':<18} {'Count':<12} {'Protos':<8} {'BufLen':<8} {'mu':<15} {'tau':<15}\")\n                print(\"-\" * 100)\n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(top5, 1):\n                    tok_str = str(tok)[:18]\n                    print(f\"{rank:<6} {tok_str:<18} {cnt:<12} {prot:<8} {buflen:<8} {mu:<15.6f} {tau:<15.6f}\")\n                print(\"-\" * 100)\n\n                total_samples = sum(item[1] for item in items)\n                total_protos = sum(item[2] for item in items)\n                total_buffers = sum(item[5] for item in items)\n                print(f\"Total: {len(items)} clusters | {total_samples} samples | {total_protos} protos | {total_buffers} buffers\")\n        except Exception as e:\n            try:\n                if VERBOSE_LOGGING:\n                    print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n            except Exception:\n                pass\n\n    def cluster_buffer_to_prototypes_hierarchical(self, tokentype: str) -> bool:\n        try:\n            if not self.should_track_token(tokentype):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] Skipping non-word token: {tokentype}\")\n                return False\n\n            with self.buffer_lock:\n                if tokentype not in self.buffers:\n                    return False\n\n                buf_snapshot = [e.clone() if isinstance(e, torch.Tensor) else e for e in self.buffers[tokentype]]\n\n            if len(buf_snapshot) < self.nmin:\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype} buffer={len(buf_snapshot)} < nmin={self.nmin}\")\n                return False\n\n            emb_list: List[np.ndarray] = []\n            for e in buf_snapshot:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        try:\n                            emb_list.append(e.numpy())\n                        except Exception:\n                            emb_list.append(e.cpu().numpy())\n                    else:\n                        emb_list.append(np.asarray(e, dtype=np.float32))\n                except Exception:\n                    continue\n\n            if len(emb_list) == 0:\n                return False\n\n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                new_embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                new_embeddings = np.stack(emb_list, axis=0)\n\n            if new_embeddings.shape[0] < 2:\n                return False\n\n            norms = np.linalg.norm(new_embeddings, axis=1)\n            if np.all(norms < 1e-6):\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype} all zero vectors, skipping\")\n                return False\n\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD-CLUSTER] {tokentype} buflen={len(buf_snapshot)} \"\n                      f\"sampled={new_embeddings.shape[0]} meannorm={norms.mean():.4f}\")\n\n            store = self.prototype_stores[tokentype]\n\n            existing_centroids: List[np.ndarray] = []\n            if hasattr(store, 'centroids') and len(store.centroids) > 0:\n                for c in store.centroids:\n                    try:\n                        if isinstance(c, torch.Tensor):\n                            try:\n                                existing_centroids.append(c.cpu().numpy())\n                            except Exception:\n                                existing_centroids.append(c.numpy())\n                        else:\n                            existing_centroids.append(np.asarray(c, dtype=np.float32))\n                    except Exception:\n                        continue\n\n            if len(existing_centroids) >= 1:\n                existing_centroids_np = np.stack(existing_centroids, axis=0)\n                combined_embeddings = np.vstack([existing_centroids_np, new_embeddings])\n                if DEBUG_DISCOVERY:\n                    print(f\"[DSCD-CLUSTER] {tokentype} Incremental - \"\n                          f\"{len(existing_centroids)} existing + {new_embeddings.shape[0]} new \"\n                          f\"= {combined_embeddings.shape[0]} total embeddings\")\n                embeddings = combined_embeddings\n            else:\n                embeddings = new_embeddings\n\n            protos_added = 0\n            new_centroids: List[torch.Tensor] = []\n            new_counts: List[int] = []\n            new_times: List[float] = []\n\n            if _HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric='euclidean')\n                    if condensed.size > 0:\n                        Z = linkage(condensed, method='average')\n\n                        max_dist = condensed.max() if condensed.size > 0 else 1.0\n                        relative_threshold = self.dispersion_threshold\n                        absolute_threshold = relative_threshold * max_dist\n\n                        clusters = fcluster(Z, t=absolute_threshold, criterion='distance') - 1\n\n                        if clusters.size > 0:\n                            max_c = int(clusters.max())\n                            for c_id in range(max_c + 1):\n                                mask = (clusters == c_id)\n                                cluster_size = int(mask.sum())\n                                if cluster_size >= self.nmin:\n                                    centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                    centroid_tensor = torch.from_numpy(centroid)\n                                    new_centroids.append(centroid_tensor)\n                                    new_counts.append(cluster_size)\n                                    new_times.append(time.time())\n                                    protos_added += 1\n\n                            if len(new_centroids) > self.maxprotos:\n                                sorted_indices = np.argsort(new_counts)[-self.maxprotos:]\n                                new_centroids = [new_centroids[i] for i in sorted_indices]\n                                new_counts = [new_counts[i] for i in sorted_indices]\n                                new_times = [new_times[i] for i in sorted_indices]\n                                protos_added = len(new_centroids)\n\n                            store.centroids = new_centroids\n                            store.counts = new_counts\n                            store.creation_time = new_times\n                            store.labels = torch.tensor(clusters)\n\n                            if DEBUG_DISCOVERY and protos_added > 0:\n                                print(f\"[DSCD-CLUSTER] Hierarchical created {protos_added} prototypes for {tokentype}\")\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD-CLUSTER] Hierarchical failed for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n\n            if protos_added == 0 and _HAS_KMEANS:\n                try:\n                    min_k = 1\n                    max_k = min(self.maxprotos, len(embeddings) // self.nmin)\n                    if max_k < min_k:\n                        max_k = min_k\n\n                    if len(embeddings) > 20:\n                        k_guess = min(max_k, max(2, int(np.sqrt(len(embeddings)) / 2)))\n                    elif len(embeddings) > 10:\n                        k_guess = min(max_k, 2)\n                    else:\n                        k_guess = 1\n\n                    k_guess = max(min_k, min(k_guess, len(embeddings)))\n\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n\n                        new_centroids = []\n                        new_counts = []\n                        new_times = []\n\n                        for c in range(k_guess):\n                            mask = (labels == c)\n                            cluster_size = int(mask.sum())\n                            if cluster_size >= self.nmin:\n                                centroid = embeddings[mask].mean(axis=0).astype(np.float32)\n                                centroid_tensor = torch.from_numpy(centroid)\n                                new_centroids.append(centroid_tensor)\n                                new_counts.append(cluster_size)\n                                new_times.append(time.time())\n                                protos_added += 1\n\n                        store.centroids = new_centroids\n                        store.counts = new_counts\n                        store.creation_time = new_times\n                        store.labels = torch.tensor(labels)\n\n                        if DEBUG_DISCOVERY and protos_added > 0:\n                            print(f\"[DSCD-CLUSTER] KMeans created {protos_added} prototypes for {tokentype}\")\n                except Exception as e:\n                    if DEBUG_DISCOVERY:\n                        print(f\"[DSCD-CLUSTER] KMeans failed for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD-CLUSTER] {tokentype} final={store.size()} protos, counts={store.counts}\")\n\n            try:\n                if store.centroids:\n                    counts = store.counts if store.counts else [1] * len(store.centroids)\n                    total_count = sum(counts)\n                    mean_count = float(total_count / max(1, len(counts)))\n\n                    self.cluster_stats[str(tokentype)] = {\n                        \"num_prototypes\": len(store.centroids),\n                        \"counts\": [int(c) for c in counts],\n                        \"total_samples\": int(total_count),\n                        \"mean_count\": float(mean_count),\n                        \"mu\": float(store.mu),\n                        \"tau\": float(store.tau),\n                    }\n            except Exception:\n                pass\n\n            return store.size() > 0\n\n        except Exception as e:\n            if DEBUG_DISCOVERY:\n                print(f\"[DSCD-ERROR] Clustering error for {tokentype}: {type(e).__name__} {str(e)[:200]}\")\n            return False\n\n    def get_prototype_summary(self) -> Dict[str, Any]:\n        total_tokens = len(self.prototype_stores)\n        total_prototypes = sum(s.size() for s in self.prototype_stores.values())\n        num_homographs = sum(1 for s in self.prototype_stores.values() if s.size() >= 2)\n        \n        return {\n            \"total_tokens\": total_tokens,\n            \"total_prototypes\": total_prototypes,\n            \"num_homographs\": num_homographs,\n        }\n\n    def get_explanations(self, threshold_span: float = 0.3) -> List[Dict[str, Any]]:\n        expl: List[Dict[str, Any]] = []\n        for tokentype, store in self.prototype_stores.items():\n            if store.size() >= 2:\n                expl.append({\"token\": str(tokentype), \"protos\": store.size()})\n        return expl\n\nprint(\"=\" * 80)\nprint(\"Cell 3: DSCD Ready (DEADLOCK FIXED + OPTIMIZED)\")\nprint(\"=\" * 80)\nprint(\"CRITICAL FIXES APPLIED:\")\nprint(\"=\" * 80)\nprint(\" ✅ FIX-DEADLOCK-1: discover_homographs() uses lock timeout (15s)\")\nprint(\" ✅ FIX-DEADLOCK-2: Snapshot pattern - release lock before computation\")\nprint(\" ✅ FIX-PROGRESS-1: Progress updates every 200 tokens (was 500)\")\nprint(\" ✅ FIX-PERFORMANCE-1: Inline dispersion calculation (no nested locks)\")\nprint(\" ✅ FIX-PERFORMANCE-2: Added ETA calculation with rate tracking\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"L25pcKUPH4J2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: ASBN MODULE - FIXED\n# ==============================================================================\n\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n\ntry:\n    _ENABLE_ASBN_TRAINING = bool(ENABLE_ASBN_TRAINING)\nexcept Exception:\n    _ENABLE_ASBN_TRAINING = True\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept Exception:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept Exception:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept Exception:\n    _DEBUG_TIMING = False\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept Exception:\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _GRL_ALPHA_START = float(GRL_ALPHA_START)\n    _GRL_ALPHA_END = float(GRL_ALPHA_END)\n    _GRL_ALPHA_SCHEDULE = str(GRL_ALPHA_SCHEDULE)\n    try:\n        _GRL_ALPHA_STEPS = int(GRL_ALPHA_STEPS)\n    except Exception:\n        _GRL_ALPHA_STEPS = 10000\nexcept Exception:\n    _GRL_ALPHA_START = 0.0\n    _GRL_ALPHA_END = 1.0\n    _GRL_ALPHA_SCHEDULE = \"linear\"\n    _GRL_ALPHA_STEPS = 10000\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_should_track_token = \"should_track_token\" in globals()\n\nclass GradientReversalFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = float(alpha)\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.alpha * grad_output, None\n\ndef gradient_reversal(x, alpha: float = 1.0):\n    return GradientReversalFunction.apply(x, alpha)\n\nclass LightweightDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, input_dim: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\nclass MemoryEfficientASBNModule(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        tokenizer=None,\n        language: str = \"bn\",\n        freq_threshold: float = 0.7,\n        uncertainty_threshold: float = 0.3,\n        gate_threshold: float = 0.5,\n        warmup_steps: int = 1000,\n        encoder_grl_scale: float = 0.5,\n    ):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n        self.embed_dim = int(embed_dim)\n\n        self.bn_source = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n        self.bn_target = nn.BatchNorm1d(self.embed_dim, track_running_stats=True)\n\n        self.d_domain = DomainDiscriminator(self.embed_dim)\n        self.d_freq = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(self.embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(self.embed_dim)\n        self.freq_threshold = float(freq_threshold)\n        self.uncertainty_threshold = float(uncertainty_threshold)\n        self.gate_threshold = float(gate_threshold)\n        self.warmup_steps = int(warmup_steps)\n        self.current_step = 0\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 0.5, \"xl\": 0.8, \"domain\": 1.0}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = float(encoder_grl_scale)\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n        try:\n            if tokenizer is not None:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[ASBN-INIT] Initialized MemoryEfficientASBNModule:\")\n            print(f\"  - embed_dim: {self.embed_dim}\")\n            print(f\"  - warmup_steps: {self.warmup_steps}\")\n            print(f\"  - encoder_grl_scale: {self.encoder_grl_scale}\")\n            print(f\"  - GRL_ALPHA_STEPS: {_GRL_ALPHA_STEPS}\")\n            print(f\"  - thresholds: freq={self.freq_threshold}, uncert={self.uncertainty_threshold}, gate={self.gate_threshold}\")\n\n    def get_grl_alpha(self, global_step: Optional[int] = None) -> float:\n        if global_step is None:\n            global_step = self.current_step\n        step = max(0, int(global_step))\n        if _GRL_ALPHA_SCHEDULE == \"linear\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            alpha = _GRL_ALPHA_START + progress * (_GRL_ALPHA_END - _GRL_ALPHA_START)\n        elif _GRL_ALPHA_SCHEDULE == \"exponential\":\n            progress = min(1.0, float(step) / float(_GRL_ALPHA_STEPS))\n            ratio = _GRL_ALPHA_END / max(1e-8, _GRL_ALPHA_START if _GRL_ALPHA_START > 0 else 1e-3)\n            alpha = _GRL_ALPHA_START * (ratio ** progress)\n        else:\n            alpha = _GRL_ALPHA_END\n        alpha = float(torch.clamp(torch.tensor(alpha), 0.0, 1.0).item())\n        return alpha\n\n    def get_asbn_stats(self) -> Dict[str, float]:\n        return self.get_detailed_stats()\n\n    def get_detailed_stats(self) -> Dict[str, float]:\n        if self.stats[\"num_updates\"] > 0:\n            n = float(self.stats[\"num_updates\"])\n            return {\n                \"domain_loss\": self.stats[\"domain_loss\"] / n,\n                \"domain_accuracy\": self.stats[\"domain_accuracy\"] / n,\n                \"source_accuracy\": self.stats[\"source_accuracy\"] / n,\n                \"target_accuracy\": self.stats[\"target_accuracy\"] / n,\n                \"asbn_loss\": self.stats[\"asbn_loss\"] / n,\n                \"num_updates\": self.stats[\"num_updates\"],\n            }\n        return {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def reset_stats(self) -> None:\n        self.stats = {\n            \"domain_loss\": 0.0,\n            \"domain_accuracy\": 0.0,\n            \"source_accuracy\": 0.0,\n            \"target_accuracy\": 0.0,\n            \"asbn_loss\": 0.0,\n            \"num_updates\": 0,\n        }\n\n    def critic_parameters(self):\n        return (\n            list(self.d_domain.parameters())\n            + list(self.d_freq.parameters())\n            + list(self.d_ctx.parameters())\n            + list(self.d_xl.parameters())\n        )\n\n    def _ensure_discriminators_on_device(self, device: torch.device) -> None:\n        try:\n            for mod in (\n                self.d_domain,\n                self.d_freq,\n                self.d_ctx,\n                self.d_xl,\n                self.bn_source,\n                self.bn_target,\n            ):\n                try:\n                    p = next(mod.parameters())\n                    if p.device != device:\n                        mod.to(device)\n                except StopIteration:\n                    mod.to(device)\n                except Exception:\n                    pass\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] Device migration failed:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    print(\"[ASBN] Device migration failed\")\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n            if isinstance(proto_probs, torch.Tensor):\n                if proto_probs.dim() == 3:\n                    B, T, K = proto_probs.shape\n                    p = proto_probs.detach().to(device)\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    pmax[:b_max, :t_max] = p[:b_max, :t_max].max(dim=2)[0]\n                    return pmax\n                if proto_probs.dim() == 2:\n                    p = proto_probs.detach().to(device)\n                    if batch_size >= 1:\n                        t_max = min(seq_len, p.size(0))\n                        pmax[0, :t_max] = p[:t_max].max(dim=1)[0]\n                        return pmax\n            if isinstance(proto_probs, (list, tuple)):\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor) and row.dim() == 2:\n                            t_max = min(seq_len, row.size(0))\n                            pmax[b, :t_max] = row[:t_max].max(dim=1)[0].to(device)\n                        elif isinstance(row, (list, tuple)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        pmax[b, t] = float(val.max().item())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                else:\n                    if batch_size == 1:\n                        row = proto_probs\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                val = row[t]\n                                if isinstance(val, torch.Tensor):\n                                    pmax[0, t] = float(val.max().item())\n                                else:\n                                    pmax[0, t] = float(np.max(np.asarray(val, dtype=np.float32)))\n                            except Exception:\n                                pmax[0, t] = 0.5\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] parse_proto_probs exception: {e}\")\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device,\n                            default: float = 0.0) -> torch.Tensor:\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n            if isinstance(mat, torch.Tensor):\n                if mat.dim() == 3:\n                    if mat.size(2) == 1:\n                        mat = mat.squeeze(2)\n                    B, T = mat.size(0), mat.size(1)\n                    b_max = min(batch_size, B)\n                    t_max = min(seq_len, T)\n                    out[:b_max, :t_max] = mat[:b_max, :t_max].to(device)\n                elif mat.dim() == 2:\n                    if mat.size(0) == batch_size:\n                        t_max = min(seq_len, mat.size(1))\n                        out[:, :t_max] = mat[:, :t_max].to(device)\n                    elif batch_size == 1:\n                        t_max = min(seq_len, mat.size(0))\n                        out[0, :t_max] = mat[:t_max].to(device)\n                elif mat.dim() == 1 and batch_size == 1:\n                    t_max = min(seq_len, mat.size(0))\n                    out[0, :t_max] = mat[:t_max].to(device)\n            elif isinstance(mat, (list, tuple)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 1:\n                            t_max = min(seq_len, row.size(0))\n                            for t in range(t_max):\n                                out[b, t] = float(row[t].item())\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            t_max = min(seq_len, len(row))\n                            for t in range(t_max):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                                except Exception:\n                                    out[b, t] = float(default)\n                elif batch_size == 1:\n                    row = mat\n                    t_max = min(seq_len, len(row))\n                    for t in range(t_max):\n                        try:\n                            v = row[t]\n                            out[0, t] = (float(v.item()) if isinstance(v, torch.Tensor) else float(v))\n                        except Exception:\n                            out[0, t] = float(default)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                try:\n                    print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n                except Exception:\n                    pass\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor,\n                                    gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        base = float(self.lambda_base.get(lambda_type, 0.2))\n        \n        lam = base * (1.0 - pmax + 0.15) * (uncertainty + 0.15) * (gate + 0.15)\n        \n        lam = torch.clamp(lam, min=0.05, max=float(self.lambda_max))\n        \n        lam = torch.where(torch.isfinite(lam), lam, torch.full_like(lam, 0.05))\n        \n        if _DEBUG_DISCOVERY and torch.any(lam < 0.1):\n            low_count = (lam < 0.1).sum().item()\n            if low_count > 0:\n                print(f\"[ASBN-WARN] {low_count} tokens have low lambda (<0.1) for {lambda_type}\")\n        \n        return lam\n\n    def forward(self, h: torch.Tensor, domain_labels: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n            return h, torch.tensor(0.0, device=dev)\n        \n        B, T, H = h.size()\n        device = h.device\n        \n        if domain_labels is not None:\n            if domain_labels.dim() == 0:\n                domain_labels = domain_labels.unsqueeze(0)\n            if domain_labels.size(0) == 1 and B > 1:\n                domain_labels = domain_labels.expand(B)\n            elif domain_labels.size(0) != B:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[ASBN] Domain label size mismatch: {domain_labels.size(0)} vs batch {B}, using first label\")\n                domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n        \n        h_flat = h.view(B * T, H)\n        h_normalized = h_flat.clone()\n        \n        if domain_labels is not None:\n            domain_expanded = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n            source_mask = domain_expanded == 0\n            target_mask = domain_expanded == 1\n            \n            if self.training:\n                if source_mask.sum() >= 2:\n                    h_normalized[source_mask] = self.bn_source(h_flat[source_mask])\n                elif source_mask.sum() == 1:\n                    if self.bn_source.track_running_stats and self.bn_source.num_batches_tracked > 0:\n                        self.bn_source.eval()\n                        h_normalized[source_mask] = self.bn_source(h_flat[source_mask])\n                        self.bn_source.train()\n                    else:\n                        h_normalized[source_mask] = h_flat[source_mask]\n                \n                if target_mask.sum() >= 2:\n                    h_normalized[target_mask] = self.bn_target(h_flat[target_mask])\n                elif target_mask.sum() == 1:\n                    if self.bn_target.track_running_stats and self.bn_target.num_batches_tracked > 0:\n                        self.bn_target.eval()\n                        h_normalized[target_mask] = self.bn_target(h_flat[target_mask])\n                        self.bn_target.train()\n                    else:\n                        h_normalized[target_mask] = h_flat[target_mask]\n        \n        h_out = h_normalized.view(B, T, H)\n        \n        if not self.training or not _ENABLE_ASBN_TRAINING or domain_labels is None:\n            return h_out, torch.tensor(0.0, device=device)\n        \n        if self.current_step < self.warmup_steps:\n            return h_out, torch.tensor(0.0, device=device)\n        \n        try:\n            self._ensure_discriminators_on_device(device)\n            \n            grl_alpha = self.get_grl_alpha(self.current_step)\n            \n            domain_flat = domain_labels.unsqueeze(1).expand(B, T).reshape(-1)\n            \n            domain_input = gradient_reversal(h_normalized, alpha=grl_alpha)\n            domain_logits = self.d_domain(domain_input)\n            domain_loss = F.cross_entropy(domain_logits, domain_flat)\n            \n            with torch.no_grad():\n                domain_preds = torch.argmax(domain_logits, dim=1)\n                domain_accuracy = (domain_preds == domain_flat).float().mean()\n                \n                source_mask = domain_flat == 0\n                target_mask = domain_flat == 1\n                \n                if source_mask.any():\n                    source_acc = (domain_preds[source_mask] == domain_flat[source_mask]).float().mean()\n                    self.stats[\"source_accuracy\"] += float(source_acc.item())\n                \n                if target_mask.any():\n                    target_acc = (domain_preds[target_mask] == domain_flat[target_mask]).float().mean()\n                    self.stats[\"target_accuracy\"] += float(target_acc.item())\n                \n                self.stats[\"domain_loss\"] += float(domain_loss.item())\n                self.stats[\"domain_accuracy\"] += float(domain_accuracy.item())\n                self.stats[\"asbn_loss\"] += float(domain_loss.item())\n                self.stats[\"num_updates\"] += 1\n                \n                if self.stats[\"num_updates\"] >= self.stats_reset_interval:\n                    if _DEBUG_DISCOVERY:\n                        stats = self.get_detailed_stats()\n                        print(f\"\\n[ASBN] Stats after {stats['num_updates']} updates:\")\n                        print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                        print(f\"  Domain accuracy: {stats['domain_accuracy']:.2%}\")\n                        print(f\"  Source accuracy: {stats['source_accuracy']:.2%}\")\n                        print(f\"  Target accuracy: {stats['target_accuracy']:.2%}\")\n                    self.reset_stats()\n            \n            return h_out, domain_loss\n            \n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(f\"[ASBN] Adversarial training failed: {e}\")\n            return h_out, torch.tensor(0.0, device=device)\n\n    def forward_with_grl_simplified(self, h: torch.Tensor, proto_probs: Any, uncertainties: Any, gates: Any,\n                                   token_word_map: Optional[List[Dict[int, str]]] = None,\n                                   domain_labels: Optional[torch.Tensor] = None,\n                                   global_step: Optional[int] = None) \\\n            -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        if global_step is not None:\n            self.current_step = int(global_step)\n        dev = h.device if isinstance(h, torch.Tensor) else torch.device(\"cpu\")\n        if self.current_step < self.warmup_steps:\n            if _DEBUG_DISCOVERY and self.current_step % 100 == 0:\n                print(f\"[ASBN] Warmup: {self.current_step}/{self.warmup_steps}\")\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not self.training or not _ENABLE_ASBN_TRAINING:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            zero = torch.tensor(0.0, device=dev)\n            return zero, zero, zero, zero\n        device = h.device\n        self._ensure_discriminators_on_device(device)\n        self.d_domain.train()\n        self.d_freq.train()\n        self.d_ctx.train()\n        self.d_xl.train()\n        B, T, H = h.size()\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.2)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.1)\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n        batch_indices = torch.arange(B, device=device).unsqueeze(1).expand(B, T)\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            try:\n                                token_str = wm[t]\n                                tracked = True\n                                if _has_should_track_token:\n                                    tracked = bool(globals()[\"should_track_token\"](token_str))\n                                elif _has_is_valid_token:\n                                    tracked = bool(is_valid_token(token_str, self.special_tokens, self.tokenizer, language=self.language))\n                                if not tracked:\n                                    sel_mask[b, t] = False\n                            except Exception:\n                                pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    try:\n                        print(\"[ASBN] Token filtering failed:\", traceback.format_exc().splitlines()[-1])\n                    except Exception:\n                        pass\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        batch_idx = batch_indices.view(-1)[sel_idx]\n        if sel_idx.numel() == 0:\n            if _DEBUG_DISCOVERY:\n                print(\"[ASBN] No valid tokens after filtering\")\n            zero = torch.tensor(0.0, device=device)\n            return zero, zero, zero, zero\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n        xl_input = sel_emb\n        grl_alpha = self.get_grl_alpha(global_step)\n        freq_input = torch.cat([sel_emb, freq_feature], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)\n        xl_input_grl = gradient_reversal(xl_input, alpha=grl_alpha)\n        freq_input_grl = gradient_reversal(freq_input, alpha=grl_alpha)\n        ctx_input_grl = gradient_reversal(ctx_input, alpha=grl_alpha)\n        freq_logits = self.d_freq(freq_input_grl)\n        ctx_logits = self.d_ctx(ctx_input_grl)\n        xl_logits = self.d_xl(xl_input_grl)\n        freq_label = (pmax_flat > self.freq_threshold).long().to(device)\n        ctx_label = (U_flat < self.uncertainty_threshold).long().to(device)\n        xl_label = (G_flat > self.gate_threshold).long().to(device)\n        loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n        loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n        loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n        lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n        lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n        lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n        weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n        mean_weighted = torch.mean(weighted)\n        domain_loss = torch.tensor(0.0, device=device)\n        domain_accuracy = torch.tensor(0.0, device=device)\n        if domain_labels is not None:\n            try:\n                if domain_labels.dim() == 0:\n                    domain_labels = domain_labels.unsqueeze(0)\n                if domain_labels.size(0) == 1 and B > 1:\n                    domain_labels = domain_labels.expand(B)\n                elif domain_labels.size(0) != B:\n                    domain_labels = domain_labels[0].unsqueeze(0).expand(B)\n                domain_flat = domain_labels[batch_idx]\n                domain_input = gradient_reversal(sel_emb, alpha=grl_alpha)\n                domain_logits = self.d_domain(domain_input)\n                domain_loss = F.cross_entropy(domain_logits, domain_flat)\n                with torch.no_grad():\n                    domain_preds = torch.argmax(domain_logits, dim=1)\n                    domain_accuracy = (domain_preds == domain_flat).float().mean()\n                    source_mask = domain_flat == 0\n                    target_mask = domain_flat == 1\n                    if source_mask.any():\n                        source_acc = ((domain_preds[source_mask] == domain_flat[source_mask]).float().mean())\n                        self.stats[\"source_accuracy\"] += float(source_acc.item())\n                    if target_mask.any():\n                        target_acc = ((domain_preds[target_mask] == domain_flat[target_mask]).float().mean())\n                        self.stats[\"target_accuracy\"] += float(target_acc.item())\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(f\"[ASBN] Domain loss failed: {e}\")\n        warmup_progress = min(1.0, float(self.current_step - self.warmup_steps) / float(self.warmup_steps))\n        effective_scale = self.encoder_grl_scale * warmup_progress\n        encoder_loss = effective_scale * (mean_weighted + domain_loss)\n        \n        if torch.isnan(encoder_loss) or torch.isinf(encoder_loss):\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN-WARN] NaN/Inf detected in encoder_loss, returning zero\")\n            encoder_loss = torch.tensor(0.0, device=device)\n        \n        try:\n            with torch.no_grad():\n                self.stats[\"domain_loss\"] += float(domain_loss.item())\n                self.stats[\"domain_accuracy\"] += float(domain_accuracy.item())\n                self.stats[\"asbn_loss\"] += float(encoder_loss.item())\n                self.stats[\"num_updates\"] += 1\n                if self.stats[\"num_updates\"] >= self.stats_reset_interval:\n                    if _DEBUG_DISCOVERY:\n                        stats = self.get_detailed_stats()\n                        print(f\"\\n[ASBN-STATS] After {stats['num_updates']} updates:\")\n                        print(f\"  Domain loss: {stats['domain_loss']:.4f}\")\n                        print(f\"  Domain acc: {stats['domain_accuracy']:.2%}\")\n                        print(f\"  Source acc: {stats['source_accuracy']:.2%}\")\n                        print(f\"  Target acc: {stats['target_accuracy']:.2%}\")\n                        print(f\"  ASBN loss: {stats['asbn_loss']:.4f}\")\n                    self.reset_stats()\n        except Exception:\n            pass\n        if _DEBUG_DISCOVERY and self.current_step % 500 == 0:\n            print(f\"\\n[ASBN-STEP-{self.current_step}]\")\n            print(f\"  GRL alpha: {grl_alpha:.3f}\")\n            print(f\"  Effective scale: {effective_scale:.3f}\")\n            print(f\"  Encoder loss: {encoder_loss.item():.4f}\")\n            print(f\"  Domain loss: {domain_loss.item():.4f}\")\n            print(f\"  Domain acc: {domain_accuracy.item():.2%}\")\n        return encoder_loss, mean_weighted, domain_loss, domain_accuracy\n\n    def test_asbn(self, batch_size: int = 2, seq_len: int = 10) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[ASBN-TEST] Testing ASBN module\")\n        print(\"=\" * 60)\n        try:\n            try:\n                device = next(self.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cpu\")\n            h = torch.randn(batch_size, seq_len, self.embed_dim, device=device)\n            domain_labels = torch.randint(0, 2, (batch_size,), device=device)\n            h_out, _ = self.forward(h, domain_labels)\n            assert h_out.shape == h.shape, \"Forward output shape mismatch\"\n            print(\"  forward() passed\")\n            proto_probs = torch.rand(batch_size, seq_len, 3, device=device)\n            uncertainties = torch.rand(batch_size, seq_len, device=device)\n            gates = torch.rand(batch_size, seq_len, device=device)\n            self.train()\n            self.current_step = self.warmup_steps + 1\n            enc_loss, adv_loss, dom_loss, dom_acc = self.forward_with_grl_simplified(\n                h,\n                proto_probs,\n                uncertainties,\n                gates,\n                domain_labels=domain_labels,\n                global_step=self.current_step,\n            )\n            assert enc_loss.item() >= 0.0, \"Encoder loss negative\"\n            assert 0.0 <= dom_acc.item() <= 1.0, \"Domain accuracy out of range\"\n            print(\"  forward_with_grl_simplified() passed\")\n            stats = self.get_detailed_stats()\n            assert \"domain_loss\" in stats, \"Missing domain_loss in stats\"\n            print(\"  Statistics tracking passed\")\n            print(\"\\nAll ASBN tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n        except Exception as e:\n            print(f\"\\nASBN test failed: {e}\")\n            traceback.print_exc()\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 4: ASBN Ready (dynamic GRL, DSCD-aware) - FIXED\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\"=\" * 80)\nprint(\" F1:  Lambda offset increased 0.1 → 0.15, min clamp 0.01 → 0.05\")\nprint(\" F2:  _parse_scalar_matrix now handles [B,T,1] by squeezing dimension 2\")\nprint(\" F3:  GRL alpha now clamped to [0.0, 1.0] range\")\nprint(\" F4:  Encoder_grl_scale ramps gradually during warmup period\")\nprint(\" F5:  BatchNorm single-token fallback uses running stats when available\")\nprint(\" F6:  Added NaN/Inf check before returning encoder_loss\")\nprint(\" F7:  Default uncertainty changed from 0.1 → 0.2 for realistic signals\")\nprint(\" F8:  Default gate changed from 0.0 → 0.1 to ensure gradient flow\")\nprint(\" F9:  Added debug warning when lambda values drop below 0.1\")\nprint(\" F10: Lambda computation uses safer addition instead of multiplication chain\")\nprint(\" F11: CRITICAL - forward() now does adversarial training with domain labels\")\nprint(\" F12: Added domain discrimination in forward() method\")\nprint(\" F13: Fixed domain_labels dimension handling in forward()\")\nprint(\" F14: Added statistics tracking to forward() method\")\nprint(\" F15: BatchNorm applied before adversarial loss in forward()\")\nprint(\" F16: Domain classifier input passes through GRL in forward()\")\nprint(\" F17: Safe lambda formula prevents gradient vanishing\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"XrNq18UsH4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: TRG (PURE DATA-DRIVEN) - FIXED\n# ==============================================================================\n\nfrom typing import List, Dict, Tuple, Optional, Set, Any\nfrom collections import deque\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport threading\nimport time\n\ntry:\n    _TRG_EVIDENCE_K = int(TRG_EVIDENCE_K)\nexcept (NameError, ValueError, TypeError):\n    _TRG_EVIDENCE_K = 3\n\ntry:\n    _TRG_GEN_EMBED = int(TRG_GEN_EMBED)\nexcept (NameError, ValueError, TypeError):\n    _TRG_GEN_EMBED = 64\n\ntry:\n    _MAX_SILVER_BUFFER = int(MAX_SILVER_BUFFER)\nexcept (NameError, ValueError, TypeError):\n    _MAX_SILVER_BUFFER = 50\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept NameError:\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept NameError:\n    _DEBUG_TIMING = False\n\ntry:\n    _ENABLE_TRG_INFERENCE = bool(ENABLE_TRG_INFERENCE)\nexcept NameError:\n    _ENABLE_TRG_INFERENCE = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _TRG_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TRG_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _TRG_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _TRG_SPAN_THRESHOLD = 0.12\n\ntry:\n    _TAU_HIGH = float(TAU_HIGH)\nexcept (NameError, ValueError, TypeError):\n    _TAU_HIGH = 0.85\n\ntry:\n    _TAU_LOW = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _TAU_LOW = 0.15\n\ntry:\n    _TAU_ACCEPT = float(TAU_ACCEPT)\nexcept (NameError, ValueError, TypeError):\n    _TAU_ACCEPT = 0.80\n\ntry:\n    _TRG_TEMPERATURE = float(TRG_TEMPERATURE)\nexcept (NameError, ValueError, TypeError):\n    _TRG_TEMPERATURE = 1.0\n\ntry:\n    _MAX_EXPLANATIONS_PER_SENTENCE = (\n        int(MAX_EXPLANATIONS_PER_SENTENCE)\n        if \"MAX_EXPLANATIONS_PER_SENTENCE\" in globals()\n        else 10\n    )\nexcept Exception:\n    _MAX_EXPLANATIONS_PER_SENTENCE = 10\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\n_TRG_PUNCT_SET = set(\".,;:!?\\\"\\'-()[]{}/\\\\\")\n\n\ndef _fallback_is_valid_token(\n    token: str, special_tokens: set, tokenizer=None, language: str = \"bn\"\n) -> bool:\n    if token is None:\n        return False\n\n    if not isinstance(token, str):\n        try:\n            token = str(token)\n        except Exception:\n            return False\n\n    token = token.strip()\n    if not token:\n        return False\n\n    if token in special_tokens:\n        return False\n\n    clean = (\n        token.replace(\"▁\", \"\")\n        .replace(\"Ġ\", \"\")\n        .replace(\"##\", \"\")\n        .replace(\"</w>\", \"\")\n        .strip()\n    )\n\n    if len(clean) < 2:\n        return False\n\n    if not any(c.isalpha() for c in clean):\n        return False\n\n    if all(c in _TRG_PUNCT_SET for c in clean):\n        return False\n\n    if clean.isdigit():\n        return False\n\n    return True\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    if not isinstance(raw_token, str):\n        return False\n\n    try:\n        if token_word_map is not None and isinstance(token_word_map, dict):\n            if idx in token_word_map:\n                w = token_word_map[idx]\n                if isinstance(w, str) and w.strip():\n                    return True\n\n        if raw_token.startswith(\"▁\") or raw_token.startswith(\"Ġ\"):\n            return True\n\n        clean = (\n            raw_token.replace(\"▁\", \"\")\n            .replace(\"Ġ\", \"\")\n            .replace(\"##\", \"\")\n            .replace(\"</w>\", \"\")\n            .strip()\n        )\n\n        if len(clean) < 2:\n            return False\n\n        if all(ch in '.,;:!?\"\\'()[]{}-/' for ch in clean):\n            return False\n\n        if token_word_map is None and any(c.isalpha() for c in clean):\n            return True\n\n        return False\n\n    except Exception:\n        return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on: '{evidence}'.   \"\n                \"Pattern matches learned data.   {alternatives_text}\"\n            ),\n            \"medium_confidence\": (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty.   {alternatives_text}\"\n            ),\n            \"low_confidence\": (\n                \"Uncertain; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'.   {alternatives_text} Review recommended.\"\n            ),\n            \"fallback\": (\"Token '{token}' analyzed.   Context: '{evidence}'.\"),\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        if not evidence or not isinstance(evidence, dict):\n            return \"\"\n\n        token = (\n            str(evidence.get(\"token\", \"unknown\"))\n            .replace(\"▁\", \"\")\n            .replace(\"Ġ\", \"\")\n        )\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = (\n            \", \".join(\n                [\n                    str(tok).replace(\"▁\", \"\").replace(\"Ġ\", \"\")\n                    for tok in evidence_tokens[:_TRG_EVIDENCE_K]\n                ]\n            )\n            or \"limited context\"\n        )\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)}.\"\n\n        if confidence >= _TAU_ACCEPT:\n            template_key = \"high_confidence\"\n        elif confidence >= _TRG_UNCERTAINTY_THRESHOLD:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(\n            template_key, self.explanation_templates[\"fallback\"]\n        )\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token '{token}' -> '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    def __init__(self, tokenizer=None, language: str = \"bn\", dscd_module=None):\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n        self.span_clamp_warnings = 0\n        self.last_warning_time = 0.0\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    def extract_evidence_from_target(\n        self,\n        token_idx: int,\n        span_start: int,\n        span_end: int,\n        tgt_preds: torch.Tensor,\n    ) -> Optional[List[str]]:\n        if not isinstance(token_idx, int) or token_idx < 0:\n            return None\n        if not isinstance(span_start, int) or not isinstance(span_end, int):\n            return None\n        if span_start < 0:\n            return None\n\n        if not isinstance(tgt_preds, (torch.Tensor, list)):\n            return None\n\n        seq_len = (\n            len(tgt_preds)\n            if isinstance(tgt_preds, list)\n            else int(tgt_preds.size(0))\n        )\n        if span_end > seq_len:\n            return None\n\n        if span_start >= span_end:\n            return None\n\n        if token_idx < span_start or token_idx >= span_end:\n            return None\n\n        if token_idx >= seq_len:\n            return None\n\n        try:\n            evidence_tokens: List[str] = []\n            for i in range(span_start, span_end):\n                if i == token_idx:\n                    continue\n\n                if isinstance(tgt_preds, list):\n                    evidence_tokens.append(str(tgt_preds[i]))\n                else:\n                    try:\n                        evidence_tokens.append(str(int(tgt_preds[i].item())))\n                    except Exception:\n                        evidence_tokens.append(f\"token_{i}\")\n\n            return evidence_tokens if evidence_tokens else None\n\n        except Exception:\n            return None\n\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n    ) -> Dict:\n        if not isinstance(tokens, list):\n            return self._create_fallback_evidence(token_idx, [])\n\n        if not isinstance(token_idx, int):\n            return self._create_fallback_evidence(0, tokens)\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(\n                max(0, min(token_idx, len(tokens) - 1)), tokens\n            )\n\n        raw_token = tokens[token_idx]\n\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            proto_probs = self._safe_extract_proto_probs(token_idx, dscd_outputs)\n            uncertainty = self._safe_extract_uncertainty(token_idx, dscd_outputs)\n            gate = self._safe_extract_gate(token_idx, dscd_outputs)\n            span = self._safe_extract_span(token_idx, dscd_outputs)\n\n            evidence_tokens: Optional[List[str]] = None\n            if decoder_attention is not None and isinstance(\n                decoder_attention, torch.Tensor\n            ):\n                try:\n                    vec = None\n                    if decoder_attention.dim() == 4:\n                        try:\n                            if (\n                                decoder_attention.size(0) > 1\n                                and decoder_attention.size(1) > 1\n                            ):\n                                attn_avg = decoder_attention.mean(dim=(0, 1))\n                            elif decoder_attention.size(0) > 1:\n                                attn_avg = decoder_attention.mean(dim=1)\n                            else:\n                                attn_avg = decoder_attention.mean(dim=0)\n                            if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                                vec = attn_avg[token_idx]\n                            else:\n                                vec = attn_avg.reshape(-1)\n                        except Exception:\n                            vec = None\n                    elif decoder_attention.dim() == 3:\n                        try:\n                            attn_avg = decoder_attention.mean(dim=0)\n                            if attn_avg.dim() == 2 and token_idx < attn_avg.size(0):\n                                vec = attn_avg[token_idx]\n                            else:\n                                vec = attn_avg.reshape(-1)\n                        except Exception:\n                            vec = None\n                    elif decoder_attention.dim() == 2:\n                        try:\n                            if token_idx < decoder_attention.size(0):\n                                vec = decoder_attention[token_idx]\n                            else:\n                                vec = decoder_attention.reshape(-1)\n                        except Exception:\n                            vec = None\n                    elif decoder_attention.dim() == 1:\n                        vec = decoder_attention\n                    else:\n                        vec = None\n\n                    if vec is not None and vec.numel() > 0:\n                        try:\n                            k = min(5, int(vec.size(0)))\n                            top_k_indices = torch.topk(vec, k=k).indices.cpu().numpy()\n                            evidence_tokens = []\n                            for i in top_k_indices:\n                                if i < len(tokens) and i != token_idx:\n                                    evidence_tokens.append(tokens[int(i)])\n                        except Exception:\n                            evidence_tokens = None\n\n                except Exception:\n                    evidence_tokens = None\n\n            if evidence_tokens is None:\n                evidence_tokens = self._extract_context_window(\n                    token_idx, tokens, token_word_map\n                )\n\n            seen: Dict[str, bool] = {}\n            dedup_evidence: List[str] = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen[t] = True\n                    dedup_evidence.append(t)\n            evidence_tokens = dedup_evidence[:_TRG_EVIDENCE_K]\n\n            top_senses = self._compute_sense_alternatives_fast(\n                proto_probs, temperature=_TRG_TEMPERATURE\n            )\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            if (\n                token_word_map\n                and token_idx in token_word_map\n                and isinstance(token_word_map[token_idx], str)\n                and token_word_map[token_idx].strip()\n            ):\n                token_value = token_word_map[token_idx]\n            else:\n                token_value = raw_token\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(uncertainty),\n                \"gate\": float(gate),\n                \"span\": float(span),\n            }\n\n        except Exception as e:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(f\"[TRG] Evidence error @ {token_idx}: {e}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    def _extract_context_window(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        token_word_map: Optional[dict],\n    ) -> List[str]:\n        context_window = 2\n        start_idx = max(0, token_idx - context_window)\n        end_idx = min(len(tokens), token_idx + context_window + 1)\n        evidence_tokens: List[str] = []\n\n        for i in range(start_idx, end_idx):\n            if i == token_idx or i >= len(tokens):\n                continue\n            rtok = tokens[i]\n            clean_token = (\n                str(rtok)\n                .replace(\"▁\", \"\")\n                .replace(\"Ġ\", \"\")\n                .replace(\"</w>\", \"\")\n                .strip()\n            )\n\n            if not _is_word_start(rtok, token_word_map, i):\n                if (\n                    token_word_map is None\n                    and len(clean_token) >= 2\n                    and any(c.isalpha() for c in clean_token)\n                ):\n                    pass\n                else:\n                    continue\n\n            if _has_is_valid_token:\n                try:\n                    ok = is_valid_token(\n                        rtok,\n                        self.special_tokens,\n                        self.tokenizer,\n                        language=self.language,\n                    )\n                except Exception:\n                    ok = _fallback_is_valid_token(\n                        rtok, self.special_tokens, self.tokenizer, self.language\n                    )\n            else:\n                ok = _fallback_is_valid_token(\n                    rtok, self.special_tokens, self.tokenizer, self.language\n                )\n\n            if ok and len(clean_token) > 0:\n                if (\n                    token_word_map\n                    and isinstance(token_word_map.get(i, \"\"), str)\n                    and token_word_map[i].strip()\n                ):\n                    evidence_tokens.append(token_word_map[i].strip())\n                else:\n                    evidence_tokens.append(clean_token)\n\n        return evidence_tokens\n\n    def _safe_extract_proto_probs(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> torch.Tensor:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            pp_all = dscd_outputs.get(\"proto_probs\", None)\n            if pp_all and len(pp_all) > 0:\n                row = pp_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return row[token_idx].detach().cpu().flatten()\n                    return row.detach().cpu().flatten()\n                if isinstance(row, (list, tuple)):\n                    if token_idx < len(row):\n                        val = row[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().flatten()\n                        if isinstance(val, (list, tuple, np.ndarray)):\n                            return torch.as_tensor(\n                                val, dtype=torch.float32\n                            ).flatten()\n                        return torch.tensor([float(val)], dtype=torch.float32)\n                    if len(row) > 0:\n                        maybe = row[0]\n                        if isinstance(maybe, torch.Tensor):\n                            return maybe.detach().cpu().flatten()\n        except Exception:\n            pass\n        return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(\n        self, token_idx: int, dscd_outputs: Dict\n    ) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.5\n\n            U_all = dscd_outputs.get(\"uncertainties\", None)\n            if U_all and len(U_all) > 0:\n                row = U_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 3 and row.size(2) == 1:\n                        row = row.squeeze(2)\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx, 0].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            G_all = dscd_outputs.get(\"gates\", None)\n            if G_all and len(G_all) > 0:\n                row = G_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 3 and row.size(2) == 1:\n                        row = row.squeeze(2)\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        return float(row[token_idx, 0].item())\n                    if row.ndim == 1 and token_idx < row.shape[0]:\n                        return float(row[token_idx].item())\n                if isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    return (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n        except Exception:\n            pass\n        return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            if not isinstance(dscd_outputs, dict):\n                return 0.0\n\n            S_all = dscd_outputs.get(\"span_preds\", None)\n            if S_all and len(S_all) > 0:\n                row = S_all[0]\n                if isinstance(row, torch.Tensor):\n                    if row.ndim == 3 and row.size(2) == 1:\n                        row = row.squeeze(2)\n                    if row.ndim == 2 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx, 0].item())\n                    elif row.ndim == 1 and token_idx < row.shape[0]:\n                        span_val = float(row[token_idx].item())\n                    else:\n                        return 0.0\n                elif isinstance(row, (list, tuple)) and token_idx < len(row):\n                    val = row[token_idx]\n                    span_val = (\n                        float(val.item())\n                        if isinstance(val, torch.Tensor)\n                        else float(val)\n                    )\n                else:\n                    return 0.0\n\n                if span_val < 0.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Negative span {span_val:.3f} -> 0.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 0.0\n                if span_val > 1.0:\n                    current_time = time.time()\n                    if self.span_clamp_warnings < 10 or (\n                        current_time - self.last_warning_time\n                    ) > 60.0:\n                        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                            print(f\"[TRG] Span {span_val:.3f} > 1.0 -> 1.0\")\n                        self.span_clamp_warnings += 1\n                        self.last_warning_time = current_time\n                    return 1.0\n\n                return span_val\n\n        except Exception:\n            pass\n        return 0.0\n\n    def compute_span(self, sense_probs) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.cpu().numpy().flatten().tolist()\n\n            if isinstance(probs, (np.ndarray, list)):\n                probs = list(probs)\n\n            if len(probs) < 2:\n                return 0.0\n\n            sorted_probs = sorted([float(p) for p in probs], reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n\n            return max(0.0, min(1.0, span))\n\n        except Exception:\n            return 0.0\n\n    def _compute_sense_alternatives_fast(\n        self, proto_probs: torch.Tensor, temperature: float = 1.0\n    ) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(proto_probs, dtype=torch.float32)\n\n            probs = proto_probs.flatten().float()\n            probs = torch.clamp(probs, min=1e-10, max=1.0)\n\n            if temperature != 1.0 and probs.numel() > 1:\n                log_probs = torch.log(probs + 1e-10)\n                scaled_log_probs = log_probs / max(0.1, float(temperature))\n                probs = torch.softmax(scaled_log_probs, dim=0)\n            elif probs.numel() > 1:\n                probs = probs / (probs.sum() + 1e-10)\n\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [\n                    (f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item()))\n                    for i in range(top_k)\n                ]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(\n        self, token_idx: int, tokens: List[str]\n    ) -> Dict:\n        if isinstance(tokens, list) and 0 <= token_idx < len(tokens):\n            token = tokens[token_idx]\n        else:\n            token = \"UNK\"\n\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n        }\n\n    def get_homograph_tokens_from_dscd(self) -> Set[str]:\n        homograph_tokens: Set[str] = set()\n        try:\n            if self.dscd_module is not None:\n                if hasattr(self.dscd_module, \"get_discovered_homographs\"):\n                    homograph_tokens = set(\n                        self.dscd_module.get_discovered_homographs()\n                    )\n                elif hasattr(self.dscd_module, \"prototype_stores\"):\n                    for token, store in self.dscd_module.prototype_stores.items():\n                        if hasattr(store, \"size\") and store.size() >= 2:\n                            clean = (\n                                str(token)\n                                .replace(\"▁\", \"\")\n                                .replace(\"Ġ\", \"\")\n                                .replace(\"##\", \"\")\n                                .strip()\n                            )\n                            homograph_tokens.add(clean)\n        except Exception:\n            pass\n        return homograph_tokens\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    def __init__(\n        self,\n        embed_dim: Optional[int] = None,\n        tokenizer=None,\n        language: str = \"bn\",\n        dscd_module=None,\n    ):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(\n            _TRG_GEN_EMBED\n        )\n        self.tokenizer = tokenizer\n        self.language = language\n        self.dscd_module = dscd_module\n\n        if dscd_module is None:\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"[TRG] No DSCD module - homograph detection disabled\")\n\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = get_tokenizer_special_tokens(tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = get_cached_special_tokens(tokenizer)\n                else:\n                    self.special_tokens = set(tokenizer.all_special_tokens)\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(\n            tokenizer, language=language, dscd_module=dscd_module\n        )\n\n        self.silver_buffer = deque(maxlen=int(_MAX_SILVER_BUFFER))\n        self._silver_lock = threading.Lock()\n\n        self.stats_reset_interval = 1000\n        self.stats = {\n            \"explanations_generated\": 0,\n            \"high_confidence_explanations\": 0,\n            \"low_confidence_explanations\": 0,\n            \"empty_evidence_count\": 0,\n            \"total_evidence_tokens\": 0,\n            \"tokens_filtered_word_start\": 0,\n            \"tokens_filtered_validity\": 0,\n            \"tokens_filtered_ambiguity\": 0,\n            \"dscd_homographs_explained\": 0,\n        }\n        self._stats_lock = threading.Lock()\n\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            print(\"[TRG] Initialized:\")\n            print(f\"  - Uncertainty: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\n            print(f\"  - Span: {_TRG_SPAN_THRESHOLD:.2f}\")\n            print(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\n            print(\"  - Mode: DATA-DRIVEN\")\n\n    def _update_stats(self, evidence: Dict, is_dscd_homograph: bool = False) -> None:\n        with self._stats_lock:\n            self.stats[\"explanations_generated\"] += 1\n\n            if is_dscd_homograph:\n                self.stats[\"dscd_homographs_explained\"] += 1\n\n            if not evidence.get(\"evidence_tokens\"):\n                self.stats[\"empty_evidence_count\"] += 1\n            else:\n                self.stats[\"total_evidence_tokens\"] += len(\n                    evidence[\"evidence_tokens\"]\n                )\n\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= _TAU_ACCEPT:\n                self.stats[\"high_confidence_explanations\"] += 1\n            elif confidence < _TRG_UNCERTAINTY_THRESHOLD:\n                self.stats[\"low_confidence_explanations\"] += 1\n\n            if self.stats[\"explanations_generated\"] >= self.stats_reset_interval:\n                if _DEBUG_DISCOVERY:\n                    current_stats = self.get_statistics()\n                    print(\n                        f\"\\n[TRG-STATS] After {self.stats['explanations_generated']}:\"\n                    )\n                    print(\n                        f\"  High conf: {current_stats['high_confidence_rate']:.2%}\"\n                    )\n                    print(\n                        f\"  DSCD: {current_stats['dscd_homograph_rate']:.2%}\"\n                    )\n                self.reset_statistics()\n\n    def _add_to_silver_buffer(\n        self, evidence: Dict, explanation: str, tokens: List[str]\n    ) -> None:\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n\n            with self._silver_lock:\n                self.silver_buffer.append(entry)\n\n        except Exception:\n            pass\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        is_dscd_homograph: bool = False,\n    ) -> Tuple[str, Dict]:\n        if not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or not isinstance(token_idx, int):\n            return \"\", {}\n\n        if token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(\n                    raw_token,\n                    self.special_tokens,\n                    self.tokenizer,\n                    language=self.language,\n                )\n            except Exception:\n                is_valid = _fallback_is_valid_token(\n                    raw_token, self.special_tokens, self.tokenizer, self.language\n                )\n        else:\n            is_valid = _fallback_is_valid_token(\n                raw_token, self.special_tokens, self.tokenizer, self.language\n            )\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(\n                token_idx,\n                tokens,\n                dscd_outputs,\n                token_word_map=token_word_map,\n                decoder_attention=decoder_attention,\n            )\n\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence, is_dscd_homograph=is_dscd_homograph)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception:\n            return \"\", {}\n\n    @staticmethod\n    def _to_list_helper(x: Any) -> List[float]:\n        if x is None:\n            return []\n\n        try:\n            if isinstance(x, torch.Tensor):\n                if x.ndim == 0:\n                    return [float(x.item())]\n                if x.ndim == 1:\n                    return [float(v.item()) for v in x]\n                if x.ndim == 2:\n                    return [float(v.item()) for v in x[0]]\n                if x.ndim == 3 and x.size(0) == 1:\n                    return [float(v.item()) for v in x[0].flatten()]\n                return [float(v.item()) for v in x.flatten()]\n\n            if isinstance(x, (list, tuple)):\n                out: List[float] = []\n                for v in x:\n                    if isinstance(v, torch.Tensor):\n                        if v.ndim == 0:\n                            out.append(float(v.item()))\n                        elif v.numel() > 0:\n                            out.append(float(v.flatten()[0].item()))\n                        else:\n                            out.append(0.0)\n                    elif isinstance(v, (int, float, np.number)):\n                        out.append(float(v))\n                    else:\n                        try:\n                            out.append(float(v))\n                        except Exception:\n                            out.append(0.0)\n                return out\n\n            if isinstance(x, (int, float, np.number)):\n                return [float(x)]\n\n            return [float(x)]\n\n        except Exception:\n            return []\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        uncertainty_threshold: Optional[float] = None,\n        decoder_attention: Optional[torch.Tensor] = None,\n        max_explanations: int = _MAX_EXPLANATIONS_PER_SENTENCE,\n    ) -> List[Dict]:\n        if not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n        strict_uncertainty = max(_TRG_UNCERTAINTY_THRESHOLD, uncertainty_threshold)\n\n        explanations: List[Dict] = []\n\n        try:\n            if not tokens or not isinstance(tokens, list):\n                return explanations\n\n            if not isinstance(dscd_outputs, dict) or not dscd_outputs:\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n\n            if not U_all or not U_all[0]:\n                return explanations\n\n            U = self._to_list_helper(U_all[0])\n            S = (\n                self._to_list_helper(S_all[0])\n                if S_all and S_all[0]\n                else [0.0] * len(U)\n            )\n\n            if len(S) < len(U):\n                S.extend([0.0] * (len(U) - len(S)))\n\n            if not U:\n                return explanations\n\n            dscd_homographs = self.evidence_extractor.get_homograph_tokens_from_dscd()\n\n            candidates: List[Tuple[int, float, float, str, int, int]] = []\n\n            for idx in range(min(len(tokens), len(U))):\n                tok = tokens[idx]\n                clean_tok = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n\n                if not _is_word_start(tok, token_word_map, idx):\n                    self.stats[\"tokens_filtered_word_start\"] += 1\n                    continue\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(\n                            tok,\n                            self.special_tokens,\n                            self.tokenizer,\n                            language=self.language,\n                        )\n                    except Exception:\n                        valid = _fallback_is_valid_token(\n                            tok, self.special_tokens, self.tokenizer, self.language\n                        )\n                else:\n                    valid = _fallback_is_valid_token(\n                        tok, self.special_tokens, self.tokenizer, self.language\n                    )\n\n                if not valid:\n                    self.stats[\"tokens_filtered_validity\"] += 1\n                    continue\n\n                u = float(U[idx]) if idx < len(U) else 0.5\n                s = float(S[idx]) if idx < len(S) else 0.0\n\n                in_dscd = clean_tok in dscd_homographs\n\n                if in_dscd:\n                    priority = 1\n                elif s > _TRG_SPAN_THRESHOLD:\n                    priority = 2\n                elif s > 0.08 and u > 0.3:\n                    priority = 3\n                elif u > strict_uncertainty:\n                    priority = 4\n                else:\n                    self.stats[\"tokens_filtered_ambiguity\"] += 1\n                    continue\n\n                candidates.append((idx, u, s, clean_tok, priority, idx))\n\n            if not candidates:\n                return explanations\n\n            candidates.sort(key=lambda t: (t[4], -t[2], -t[1], t[5]))\n\n            for (token_idx, u, s, clean_tok, priority, _) in candidates[\n                : max_explanations\n            ]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(\n                        token_idx,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=token_word_map,\n                        decoder_attention=decoder_attention,\n                        is_dscd_homograph=(priority == 1),\n                    )\n                    if explanation_text and evidence:\n                        explanations.append(\n                            {\n                                \"token_idx\": token_idx,\n                                \"token\": (\n                                    token_word_map[token_idx]\n                                    if token_word_map\n                                    and token_idx in token_word_map\n                                    else tokens[token_idx]\n                                    .replace(\"▁\", \"\")\n                                    .replace(\"Ġ\", \"\")\n                                ),\n                                \"explanation\": explanation_text,\n                                \"uncertainty\": u,\n                                \"span\": s,\n                                \"dscd_discovered\": (priority == 1),\n                                \"priority\": priority,\n                            }\n                        )\n                except Exception:\n                    continue\n\n        except Exception:\n            pass\n\n        return explanations\n\n    def get_statistics(self) -> Dict:\n        with self._stats_lock:\n            total = max(self.stats[\"explanations_generated\"], 1)\n            if self.stats[\"explanations_generated\"] > 0:\n                avg_evidence_tokens = (\n                    self.stats[\"total_evidence_tokens\"] / total\n                )\n            else:\n                avg_evidence_tokens = 0.0\n\n            return {\n                **self.stats.copy(),\n                \"high_confidence_rate\": self.stats[\n                    \"high_confidence_explanations\"\n                ]\n                / total,\n                \"low_confidence_rate\": self.stats[\n                    \"low_confidence_explanations\"\n                ]\n                / total,\n                \"empty_evidence_rate\": self.stats[\"empty_evidence_count\"]\n                / total,\n                \"avg_evidence_tokens\": avg_evidence_tokens,\n                \"silver_buffer_size\": len(self.silver_buffer),\n                \"dscd_homograph_rate\": self.stats[\n                    \"dscd_homographs_explained\"\n                ]\n                / total,\n            }\n\n    def reset_statistics(self) -> None:\n        with self._stats_lock:\n            self.stats = {\n                \"explanations_generated\": 0,\n                \"high_confidence_explanations\": 0,\n                \"low_confidence_explanations\": 0,\n                \"empty_evidence_count\": 0,\n                \"total_evidence_tokens\": 0,\n                \"tokens_filtered_word_start\": 0,\n                \"tokens_filtered_validity\": 0,\n                \"tokens_filtered_ambiguity\": 0,\n                \"dscd_homographs_explained\": 0,\n            }\n\n    def clear_silver_buffer(self) -> None:\n        with self._silver_lock:\n            self.silver_buffer.clear()\n\n    def test_trg(self, tokenizer=None) -> bool:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[TRG-TEST] Testing\")\n        print(\"=\" * 60)\n\n        if not _ENABLE_TRG_INFERENCE:\n            print(\"TRG inference disabled, enabling for test...\")\n\n        try:\n            tokens = [\"▁আমি\", \"▁কল\", \"▁বন্ধ\", \"▁করেছি\", \"।\"]\n\n            dscd_outputs = {\n                \"proto_probs\": [[torch.tensor([0.6, 0.4]) for _ in tokens]],\n                \"uncertainties\": [[0.1, 0.5, 0.2, 0.1, 0.0]],\n                \"span_preds\": [[0.05, 0.3, 0.1, 0.05, 0.0]],\n                \"gates\": [[0.2, 0.8, 0.3, 0.2, 0.0]],\n            }\n\n            token_word_map = {\n                0: \"আমি\",\n                1: \"কল\",\n                2: \"বন্ধ\",\n                3: \"করেছি\",\n                4: \"।\",\n            }\n\n            self.eval()\n\n            explanations = self.process_sentence_for_explanations(\n                tokens=tokens,\n                dscd_outputs=dscd_outputs,\n                token_word_map=token_word_map,\n                max_explanations=3,\n            )\n\n            print(f\"  Generated {len(explanations)} explanations\")\n\n            if len(explanations) > 0:\n                for i, expl in enumerate(explanations, 1):\n                    print(\n                        f\"    {i}. '{expl['token']}' (u={expl['uncertainty']:.2f}, s={expl['span']:.2f})\"\n                    )\n\n            stats = self.get_statistics()\n            print(f\"  Stats: {stats['explanations_generated']} total\")\n\n            self.reset_statistics()\n            stats_after = self.get_statistics()\n            assert stats_after[\"explanations_generated\"] == 0\n            print(\"  Reset OK\")\n\n            print(\"\\nAll tests passed\")\n            print(\"=\" * 60 + \"\\n\")\n            return True\n\n        except Exception as e:\n            print(f\"\\nTest failed: {e}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n            print(\"=\" * 60 + \"\\n\")\n            return False\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 5: TRG Ready (DATA-DRIVEN) - FIXED\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\" F1:  Span extraction handles [B,T], [T], [B,T,1] with squeeze\")\nprint(\" F2:  Uncertainty/gate extraction handles 3D tensors\")\nprint(\" F3:  Span threshold lowered 0.20 → 0.12 (matches DSCD fix)\")\nprint(\" F4:  Priority system: DSCD(1) > span>0.12(2) > span>0.08&u>0.3(3) > u>thresh(4)\")\nprint(\" F5:  Temperature applied BEFORE softmax (log-space scaling)\")\nprint(\" F6:  _to_list_helper handles [B,T] by taking first batch\")\nprint(\" F7:  Added try-except around attention vector indexing\")\nprint(\" F8:  Fallback spans inferred from uncertainty when extraction returns 0.0\")\nprint(\" F9:  Combined threshold logic: span OR (uncertainty AND moderate-span)\")\nprint(\" F10: Safe normalization when probs don't sum to 1.0\")\nprint(\" F11: CRITICAL - Removed training check that blocked explanations\")\nprint(\" F12: Fixed inference mode to work in both train and eval\")\nprint(\" F13: Now generates explanations during training for buffer/stats\")\nprint(\" F14: Removed duplicate training check in process_sentence\")\nprint()\nprint(\"Config:\")\nprint(f\"  - Uncertainty: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\nprint(f\"  - Span: {_TRG_SPAN_THRESHOLD:.2f}\")\nprint(f\"  - Temperature: {_TRG_TEMPERATURE:.2f}\")\nprint(f\"  - TAU_HIGH: {_TAU_HIGH:.2f}\")\nprint(f\"  - TAU_LOW: {_TAU_LOW:.2f}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"svk-wKO7H4J3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: TATN MODEL (COMPLETE INTEGRATION - FIXED)\n# ==============================================================================\n\nfrom typing import List, Dict, Optional, Any, Tuple\nimport traceback\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import M2M100ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput\nimport threading\nimport gc\nimport time\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return int(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return float(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        val = globals().get(name)\n        if val is not None:\n            return bool(val)\n    except (ValueError, TypeError):\n        pass\n    return default\n\n_DSCD_BUFFER_SIZE = _get_int_global(\"DSCD_BUFFER_SIZE\", 50)\n_DSCD_MAX_PROTOS = _get_int_global(\"DSCD_MAX_PROTOS\", 8)\n_DSCD_N_MIN = _get_int_global(\"DSCD_N_MIN\", 5)\n_DSCD_DISPERSION_THRESHOLD = _get_float_global(\"DSCD_DISPERSION_THRESHOLD\", 0.50)\n\n_ENABLE_ASBN_TRAINING = _get_bool_global(\"ENABLE_ASBN_TRAINING\", True)\n_ENABLE_TRG_INFERENCE = _get_bool_global(\"ENABLE_TRG_INFERENCE\", True)\n_MEMORY_CLEANUP_FREQUENCY = _get_int_global(\"MEMORY_CLEANUP_FREQUENCY\", 2000)\n\n_NUM_GPUS = _get_int_global(\n    \"NUM_GPUS\",\n    torch.cuda.device_count() if torch.cuda.is_available() else 1,\n)\n_USE_GC = _get_bool_global(\"GRADIENT_CHECKPOINTING\", False)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global(\n    \"DSCD_ENABLE_TRAINING_CLUSTERING\", True\n)\n\n_LAMBDA_ASBN = _get_float_global(\"LAMBDA_ASBN\", 0.05)\n_LAMBDA_DSCD = _get_float_global(\"LAMBDA_DSCD\", 0.15)\n_LAMBDA_DOMAIN = _get_float_global(\"LAMBDA_DOMAIN\", 0.1)\n\n_VERBOSE_LOGGING = _get_bool_global(\"VERBOSE_LOGGING\", False)\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\n_PERIODIC_DISCOVERY_FREQUENCY = _get_int_global(\n    \"PERIODIC_DISCOVERY_FREQUENCY\", 50\n)\n_VALIDATION_CHECK_INTERVAL = _get_int_global(\"VALIDATION_CHECK_INTERVAL\", 500)\n\n_SPAN_THRESHOLD = _get_float_global(\"SPAN_THRESHOLD\", 0.12)\n_UNCERTAINTY_THRESHOLD = _get_float_global(\"UNCERTAINTY_THRESHOLD\", 0.15)\n\n_TRG_UNCERTAINTY_THRESHOLD = _get_float_global(\n    \"TRG_UNCERTAINTY_THRESHOLD\", _get_float_global(\"TAU_LOW\", 0.15)\n)\n_TAU_LOW = _get_float_global(\"TAU_LOW\", 0.15)\n\n_TRAIN_DOMAIN = _get_int_global(\"TRAIN_DOMAIN\", 0)\n_TEST_DOMAIN = _get_int_global(\"TEST_DOMAIN\", 1)\n_USE_DOMAIN_LABELS = _get_bool_global(\"USE_DOMAIN_LABELS\", True)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    _M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_BN_TOKEN_ID = 128025\n\n_has_reconstruct_word_spans = \"reconstruct_word_spans\" in globals()\n\ndef _safe_get_last_hidden_state(enc_output):\n    if enc_output is None:\n        return None\n    if hasattr(enc_output, \"last_hidden_state\"):\n        return enc_output.last_hidden_state\n    if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n        return enc_output[0]\n    return None\n\ndef _normalize_dscd_outputs(\n    raw: Dict[str, Any],\n    batch_size: int,\n    seq_len: int,\n    device: torch.device,\n    embed_dim: int,\n) -> Dict[str, Any]:\n    defaults = {\n        \"h_augmented\": torch.zeros(\n            batch_size, seq_len, embed_dim, device=device, dtype=torch.float32\n        ),\n        \"proto_probs\": [\n            [\n                torch.tensor([1.0], device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"uncertainties\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"gates\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"span_preds\": [\n            [\n                torch.tensor(0.0, device=device, dtype=torch.float32)\n                for _ in range(seq_len)\n            ]\n            for _ in range(batch_size)\n        ],\n        \"proto_assignments\": [\n            torch.zeros(seq_len, dtype=torch.long, device=device)\n            for _ in range(batch_size)\n        ],\n    }\n\n    if not isinstance(raw, dict):\n        return defaults\n\n    out = defaults.copy()\n\n    try:\n        if \"h_augmented\" in raw and raw[\"h_augmented\"] is not None:\n            h = raw[\"h_augmented\"]\n            if isinstance(h, torch.Tensor) and h.shape == (\n                batch_size,\n                seq_len,\n                embed_dim,\n            ):\n                out[\"h_augmented\"] = h.to(device)\n            else:\n                try:\n                    out[\"h_augmented\"] = (\n                        h.to(device).reshape(batch_size, seq_len, embed_dim)\n                    )\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    for list_key in (\"proto_probs\", \"uncertainties\", \"gates\", \"span_preds\"):\n        if list_key in raw and raw[list_key] is not None:\n            try:\n                val = raw[list_key]\n                \n                if isinstance(val, torch.Tensor):\n                    if val.ndim == 3 and val.size(2) == 1:\n                        val = val.squeeze(2)\n                    \n                    if val.ndim == 2 and val.size(0) == batch_size:\n                        safe_batch = []\n                        for b in range(batch_size):\n                            row_data = []\n                            for t in range(seq_len):\n                                if t < val.size(1):\n                                    if list_key == \"proto_probs\":\n                                        row_data.append(\n                                            torch.tensor(\n                                                [float(val[b, t].item())],\n                                                device=device,\n                                                dtype=torch.float32,\n                                            )\n                                        )\n                                    else:\n                                        row_data.append(\n                                            torch.tensor(\n                                                float(val[b, t].item()),\n                                                device=device,\n                                                dtype=torch.float32,\n                                            )\n                                        )\n                                else:\n                                    if list_key == \"proto_probs\":\n                                        row_data.append(\n                                            torch.tensor(\n                                                [1.0], device=device, dtype=torch.float32\n                                            )\n                                        )\n                                    else:\n                                        row_data.append(\n                                            torch.tensor(0.0, device=device, dtype=torch.float32)\n                                        )\n                            safe_batch.append(row_data)\n                        out[list_key] = safe_batch\n                        continue\n                \n                if isinstance(val, list) and len(val) == batch_size:\n                    safe_batch = []\n                    for b_row in val:\n                        if isinstance(b_row, torch.Tensor):\n                            if b_row.ndim == 2 and b_row.size(1) == 1:\n                                b_row = b_row.squeeze(1)\n                            \n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < b_row.size(0):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            if v.numel() == 1:\n                                                if list_key == \"proto_probs\":\n                                                    safe_row.append(\n                                                        torch.tensor(\n                                                            [float(v.item())],\n                                                            device=device,\n                                                            dtype=torch.float32,\n                                                        )\n                                                    )\n                                                else:\n                                                    safe_row.append(\n                                                        torch.tensor(\n                                                            float(v.item()),\n                                                            device=device,\n                                                            dtype=torch.float32,\n                                                        )\n                                                    )\n                                            else:\n                                                safe_row.append(v.to(device))\n                                        else:\n                                            safe_row.append(\n                                                torch.as_tensor(\n                                                    v, device=device, dtype=torch.float32\n                                                )\n                                            )\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    [1.0], device=device, dtype=torch.float32\n                                                )\n                                            )\n                                        else:\n                                            safe_row.append(\n                                                torch.tensor(0.0, device=device, dtype=torch.float32)\n                                            )\n                                except Exception:\n                                    safe_row.append(\n                                        torch.tensor(0.0, device=device, dtype=torch.float32)\n                                    )\n                            safe_batch.append(safe_row)\n                        elif isinstance(b_row, list):\n                            safe_row = []\n                            for t_idx in range(seq_len):\n                                try:\n                                    if t_idx < len(b_row):\n                                        v = b_row[t_idx]\n                                        if isinstance(v, torch.Tensor):\n                                            safe_row.append(v.to(device))\n                                        else:\n                                            safe_row.append(\n                                                torch.as_tensor(\n                                                    v,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                    else:\n                                        if list_key == \"proto_probs\":\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    [1.0],\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                        else:\n                                            safe_row.append(\n                                                torch.tensor(\n                                                    0.0,\n                                                    device=device,\n                                                    dtype=torch.float32,\n                                                )\n                                            )\n                                except Exception:\n                                    safe_row.append(\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                    )\n                            safe_batch.append(safe_row)\n                        else:\n                            if list_key == \"proto_probs\":\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            [1.0],\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                            else:\n                                safe_batch.append(\n                                    [\n                                        torch.tensor(\n                                            0.0,\n                                            device=device,\n                                            dtype=torch.float32,\n                                        )\n                                        for _ in range(seq_len)\n                                    ]\n                                )\n                    out[list_key] = safe_batch\n            except Exception:\n                pass\n\n    try:\n        if \"proto_assignments\" in raw and raw[\"proto_assignments\"] is not None:\n            pa = raw[\"proto_assignments\"]\n            if isinstance(pa, list) and len(pa) == batch_size:\n                safe_pa = []\n                for b_row in pa:\n                    try:\n                        if isinstance(b_row, torch.Tensor):\n                            safe_pa.append(b_row.to(device).long())\n                        else:\n                            safe_pa.append(\n                                torch.tensor(\n                                    b_row, dtype=torch.long, device=device\n                                )\n                            )\n                    except Exception:\n                        safe_pa.append(\n                            torch.zeros(seq_len, dtype=torch.long, device=device)\n                        )\n                out[\"proto_assignments\"] = safe_pa\n    except Exception:\n        pass\n\n    return out\n\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n\n        self.global_step = 0\n        self._step_lock = threading.Lock()\n        self.last_discovery_step = 0\n        self.last_validation_step = 0\n\n        self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n            \"facebook/m2m100_418M\",\n            torch_dtype=torch.float32,\n            use_cache=False,\n        )\n        try:\n            self.mbart.config.use_cache = False\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.tokenizer, \"get_lang_id\"):\n                en_token_id = self.tokenizer.get_lang_id(_TARGET_LANGUAGE)\n                bn_token_id = self.tokenizer.get_lang_id(_SOURCE_LANGUAGE)\n            elif hasattr(self.tokenizer, \"lang_code_to_id\"):\n                en_token_id = self.tokenizer.lang_code_to_id.get(\n                    _TARGET_LANGUAGE, _M2M100_EN_TOKEN_ID\n                )\n                bn_token_id = self.tokenizer.lang_code_to_id.get(\n                    _SOURCE_LANGUAGE, _M2M100_BN_TOKEN_ID\n                )\n            else:\n                en_token_id = _M2M100_EN_TOKEN_ID\n                bn_token_id = _M2M100_BN_TOKEN_ID\n\n            self.mbart.config.forced_bos_token_id = int(en_token_id)\n            self.mbart.config.decoder_start_token_id = int(en_token_id)\n            self.en_token_id = int(en_token_id)\n            self.bn_token_id = int(bn_token_id)\n\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN-INIT] Language tokens: BN={bn_token_id}, EN={en_token_id}\"\n                )\n\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-INIT] Failed to set language tokens: {e}\")\n            self.en_token_id = _M2M100_EN_TOKEN_ID\n            self.bn_token_id = _M2M100_BN_TOKEN_ID\n\n        try:\n            if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                self.mbart.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        embed_dim = int(getattr(self.mbart.config, \"d_model\", 1024))\n\n        dscd_cls = globals().get(\"MemoryEfficientDSCDOnline\", None)\n        if callable(dscd_cls):\n            try:\n                self.dscd = dscd_cls(\n                    embeddim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffersize=_DSCD_BUFFER_SIZE,\n                    maxprotos=_DSCD_MAX_PROTOS,\n                    nmin=_DSCD_N_MIN,\n                    language=_SOURCE_LANGUAGE,\n                    dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                    enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                    max_clustering_points=500,\n                    max_candidates_per_step=1,\n                )\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to instantiate MemoryEfficientDSCDOnline: {e}\"\n                )\n        else:\n            raise RuntimeError(\"MemoryEfficientDSCDOnline not found in globals()\")\n\n        asbn_cls = globals().get(\"MemoryEfficientASBNModule\", None)\n        if callable(asbn_cls):\n            try:\n                self.asbn = asbn_cls(\n                    embed_dim, tokenizer, language=_SOURCE_LANGUAGE\n                )\n            except Exception:\n                class _StubASBN(nn.Module):\n                    def forward(self, h, domain_labels=None):\n                        dev = (\n                            h.device\n                            if isinstance(h, torch.Tensor)\n                            else torch.device(\"cpu\")\n                        )\n                        return h, torch.tensor(0.0, device=dev)\n\n                    def forward_with_grl_simplified(\n                        self, h, *args, **kwargs\n                    ):\n                        dev = (\n                            h.device\n                            if isinstance(h, torch.Tensor)\n                            else torch.device(\"cpu\")\n                        )\n                        zero = torch.tensor(0.0, device=dev)\n                        return zero, zero, zero, zero\n\n                self.asbn = _StubASBN()\n        else:\n            class _StubASBN(nn.Module):\n                def forward(self, h, domain_labels=None):\n                    dev = (\n                        h.device\n                        if isinstance(h, torch.Tensor)\n                        else torch.device(\"cpu\")\n                    )\n                    return h, torch.tensor(0.0, device=dev)\n\n                def forward_with_grl_simplified(self, h, *args, **kwargs):\n                    dev = (\n                        h.device\n                        if isinstance(h, torch.Tensor)\n                        else torch.device(\"cpu\")\n                    )\n                    zero = torch.tensor(0.0, device=dev)\n                    return zero, zero, zero, zero\n\n            self.asbn = _StubASBN()\n\n        trg_cls = globals().get(\"CompleteTRGWithExplanations\", None)\n        if callable(trg_cls):\n            try:\n                self.trg_system = trg_cls(\n                    embed_dim,\n                    tokenizer,\n                    language=_SOURCE_LANGUAGE,\n                    dscd_module=self.dscd,\n                )\n            except Exception:\n                class _StubTRG:\n                    def process_sentence_for_explanations(\n                        self,\n                        tokens,\n                        dscd_outputs,\n                        token_word_map=None,\n                        uncertainty_threshold=0.1,\n                        decoder_attention=None,\n                    ):\n                        return []\n\n                self.trg_system = _StubTRG()\n        else:\n            class _StubTRG:\n                def process_sentence_for_explanations(\n                    self,\n                    tokens,\n                    dscd_outputs,\n                    token_word_map=None,\n                    uncertainty_threshold=0.1,\n                    decoder_attention=None,\n                ):\n                    return []\n\n            self.trg_system = _StubTRG()\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[TATN-INIT] Initialized MemoryOptimizedTATNWithExplanations:\")\n            print(f\"  - Embed dim: {embed_dim}\")\n            print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n            print(f\"  - Validation interval: {_VALIDATION_CHECK_INTERVAL}\")\n            print(f\"  - Lambda ASBN: {_LAMBDA_ASBN}\")\n            print(f\"  - Lambda DSCD: {_LAMBDA_DSCD}\")\n            print(f\"  - Lambda Domain: {_LAMBDA_DOMAIN}\")\n            print(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\n\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(\n        proto_probs_list, gates_list=None, min_gate: float = 0.0\n    ) -> torch.Tensor:\n        if not proto_probs_list or not isinstance(proto_probs_list, list):\n            return torch.tensor(0.0)\n\n        dev = None\n        for row in proto_probs_list:\n            if isinstance(row, list):\n                for p in row:\n                    if isinstance(p, torch.Tensor):\n                        dev = p.device\n                        break\n            if dev is not None:\n                break\n\n        if dev is None:\n            return torch.tensor(0.0)\n\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n\n        for b, row in enumerate(proto_probs_list):\n            if not isinstance(row, list):\n                continue\n            gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n            for j, probs in enumerate(row):\n                if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                    continue\n                if gl and j < len(gl):\n                    try:\n                        if float(gl[j]) < min_gate:\n                            continue\n                    except Exception:\n                        pass\n\n                try:\n                    p = torch.clamp(probs.to(dev).float(), 1e-8, 1.0)\n                    H = -torch.sum(p * torch.log(p))\n                    if torch.isfinite(H):\n                        total = total + H\n                        count += 1\n                except Exception:\n                    continue\n\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n\n    def _reconstruct_word_maps_before_dscd(\n        self,\n        input_ids: torch.Tensor,\n        batch_size: int,\n        seq_len: int,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ) -> List[dict]:\n        if token_word_map is not None and len(token_word_map) == batch_size:\n            valid_count = sum(\n                1 for m in token_word_map if isinstance(m, dict) and len(m) > 0\n            )\n            if valid_count == batch_size:\n                if _DEBUG_DISCOVERY:\n                    total_words = sum(len(m) for m in token_word_map)\n                    print(\n                        f\"[TATN-WORDMAP] Using provided word maps: {total_words} words\"\n                    )\n                return token_word_map\n\n        word_maps_batch: List[dict] = []\n\n        if not _has_reconstruct_word_spans:\n            if _DEBUG_DISCOVERY:\n                print(\n                    \"[TATN-WORDMAP] reconstruct_word_spans() not available - using fallback\"\n                )\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    wm: Dict[int, str] = {}\n                    for i, tok in enumerate(tokens):\n                        clean = (\n                            tok.replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .replace(\"##\", \"\")\n                            .replace(\"@@\", \"\")\n                            .strip()\n                        )\n                        if clean and len(clean) >= 2:\n                            wm[i] = clean\n                    if wm:\n                        word_maps_batch.append(wm)\n                    else:\n                        word_maps_batch.append(\n                            {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                        )\n                except Exception:\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n            return word_maps_batch\n\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructing word maps for {batch_size} samples...\")\n\n        for b in range(batch_size):\n            try:\n                if (\n                    src_texts\n                    and b < len(src_texts)\n                    and isinstance(src_texts[b], str)\n                    and src_texts[b].strip()\n                ):\n                    orig_text = src_texts[b]\n                else:\n                    try:\n                        orig_text = self.tokenizer.decode(\n                            input_ids[b], skip_special_tokens=True\n                        )\n                    except Exception:\n                        orig_text = \"\"\n\n                if not orig_text.strip():\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n                    continue\n\n                wm, words = reconstruct_word_spans(\n                    self.tokenizer, orig_text, max_length=seq_len\n                )\n\n                if not isinstance(wm, dict):\n                    wm = {}\n\n                cleaned_wm: Dict[int, str] = {}\n                for idx, word in wm.items():\n                    if isinstance(word, str) and word.strip():\n                        clean_word = (\n                            word.replace(\"▁\", \"\")\n                            .replace(\"Ġ\", \"\")\n                            .replace(\"##\", \"\")\n                            .replace(\"@@\", \"\")\n                            .strip()\n                        )\n                        if clean_word:\n                            cleaned_wm[idx] = clean_word\n\n                if cleaned_wm:\n                    word_maps_batch.append(cleaned_wm)\n                else:\n                    word_maps_batch.append(\n                        {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                    )\n\n                if _DEBUG_DISCOVERY and b == 0:\n                    print(\n                        f\"[TATN-WORDMAP] Sample 0: {len(cleaned_wm)} word spans\"\n                    )\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"[TATN-WORDMAP] Reconstruction failed for sample {b}: {e}\"\n                    )\n                word_maps_batch.append(\n                    {i: f\"tok{i}\" for i in range(min(5, seq_len))}\n                )\n\n        total_words = sum(len(m) for m in word_maps_batch)\n        if _DEBUG_DISCOVERY:\n            print(f\"[TATN-WORDMAP] Reconstructed {total_words} words\")\n\n        return word_maps_batch\n\n    def _extract_domain_labels(\n        self,\n        batch_size: int,\n        device: torch.device,\n        src_texts: Optional[List[str]] = None,\n    ) -> Optional[torch.Tensor]:\n        if not _USE_DOMAIN_LABELS:\n            return None\n\n        try:\n            if self.training:\n                return torch.full(\n                    (batch_size,),\n                    _TRAIN_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n            else:\n                return torch.full(\n                    (batch_size,),\n                    _TEST_DOMAIN,\n                    dtype=torch.long,\n                    device=device,\n                )\n        except Exception:\n            return None\n\n    @staticmethod\n    def _safe_take_key_static(\n        dscd_struct: Dict[str, Any],\n        key: str,\n        b_index: int,\n        seq_len: int,\n        device: torch.device,\n    ):\n        if key == \"proto_probs\":\n            out = [\n                torch.tensor([1.0], dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n        else:\n            out = [\n                torch.tensor(0.0, dtype=torch.float32, device=device)\n                for _ in range(seq_len)\n            ]\n\n        try:\n            val = dscd_struct.get(key, None)\n            if val is None:\n                return out\n\n            if key == \"proto_probs\":\n                if isinstance(val, list) and len(val) > b_index:\n                    row = val[b_index]\n                    if isinstance(row, list):\n                        for t in range(min(seq_len, len(row))):\n                            v = row[t]\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.to(device)\n                            else:\n                                try:\n                                    out[t] = torch.as_tensor(\n                                        v,\n                                        dtype=torch.float32,\n                                        device=device,\n                                    ).flatten()\n                                except Exception:\n                                    pass\n                return out\n\n            if isinstance(val, list) and len(val) > b_index:\n                row = val[b_index]\n                if isinstance(row, list):\n                    for t in range(min(seq_len, len(row))):\n                        v = row[t]\n                        try:\n                            if isinstance(v, torch.Tensor):\n                                out[t] = v.to(device)\n                            else:\n                                out[t] = torch.tensor(\n                                    float(v), device=device\n                                )\n                        except Exception:\n                            pass\n                elif isinstance(row, torch.Tensor):\n                    if row.ndim == 2 and row.size(1) == 1:\n                        row = row.squeeze(1)\n                    \n                    if row.dim() == 1:\n                        for t in range(min(seq_len, int(row.size(0)))):\n                            try:\n                                out[t] = torch.tensor(\n                                    float(row[t].item()), device=device\n                                )\n                            except Exception:\n                                pass\n                return out\n\n            if isinstance(val, torch.Tensor):\n                if val.ndim == 3 and val.size(2) == 1:\n                    val = val.squeeze(2)\n                \n                if val.dim() >= 2 and int(val.size(0)) > b_index:\n                    for t in range(min(seq_len, int(val.size(1)))):\n                        try:\n                            v = val[b_index, t]\n                            if isinstance(v, torch.Tensor) and v.numel() == 1:\n                                out[t] = torch.tensor(\n                                    float(v.item()), device=device\n                                )\n                            elif isinstance(v, torch.Tensor):\n                                out[t] = v.to(device)\n                            else:\n                                out[t] = torch.tensor(\n                                    float(v), device=device\n                                )\n                        except Exception:\n                            pass\n        except Exception:\n            pass\n\n        return out\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_dscd: bool = True,\n        use_asbn: bool = True,\n    ):\n        with self._step_lock:\n            self.global_step += 1\n            current_step = self.global_step\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(\n                f\"Expected 2D tensors, got {input_ids.shape}, {attention_mask.shape}\"\n            )\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        if (\n            torch.cuda.is_available()\n            and _MEMORY_CLEANUP_FREQUENCY > 0\n            and current_step % _MEMORY_CLEANUP_FREQUENCY == 0\n        ):\n            for i in range(min(_NUM_GPUS, torch.cuda.device_count())):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            if gc.isenabled():\n                gc.collect()\n\n        if self.training and _DSCD_ENABLE_TRAINING_CLUSTERING and use_dscd:\n            if (\n                current_step - self.last_discovery_step\n                >= _PERIODIC_DISCOVERY_FREQUENCY\n            ):\n                try:\n                    print(\"\\n\" + \"=\" * 80)\n                    print(f\"[TATN-DISCOVERY] TRIGGER @ step {current_step}\")\n                    print(\"=\" * 80)\n\n                    num_buffers_before = len(self.dscd.buffers)\n                    num_stores_before = len(self.dscd.prototype_stores)\n                    total_protos_before = sum(s.size() for s in self.dscd.prototype_stores.values())\n                    \n                    print(f\"[TATN-DISCOVERY] BEFORE:\")\n                    print(f\"  - Buffers: {num_buffers_before}\")\n                    print(f\"  - Stores: {num_stores_before}\")\n                    print(f\"  - Prototypes: {total_protos_before}\")\n\n                    start_time = time.time()\n\n                    self.dscd.periodic_discovery_check(\n                        current_step, _PERIODIC_DISCOVERY_FREQUENCY\n                    )\n\n                    elapsed = time.time() - start_time\n                    self.last_discovery_step = current_step\n\n                    summary = self.dscd.get_prototype_summary()\n                    total_protos_after = summary.get('total_prototypes', 0)\n                    new_protos = total_protos_after - total_protos_before\n                    \n                    print(f\"\\n[TATN-DISCOVERY] AFTER ({elapsed:.2f}s):\")\n                    print(f\"  - Homographs: {summary.get('num_homographs', 0)}\")\n                    print(f\"  - Total prototypes: {total_protos_after}\")\n                    print(f\"  - NEW prototypes created: {new_protos}\")\n                    \n                    if new_protos == 0 and num_buffers_before > 0:\n                        print(f\"[TATN-DISCOVERY] ⚠ WARNING: No prototypes created despite {num_buffers_before} buffers!\")\n                        print(f\"[TATN-DISCOVERY] Check: dispersion_threshold={_DSCD_DISPERSION_THRESHOLD}, nmin={_DSCD_N_MIN}\")\n                    \n                    print(\"=\" * 80 + \"\\n\")\n\n                except Exception as e:\n                    print(f\"[TATN-DISCOVERY] ❌ FAILED: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n\n        if not self.training and _VALIDATION_CHECK_INTERVAL > 0:\n            if (\n                current_step - self.last_validation_step\n                >= _VALIDATION_CHECK_INTERVAL\n            ):\n                try:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"\\n[TATN-VALIDATION] Step {current_step}\")\n                        summary = self.dscd.get_prototype_summary()\n                        print(f\"  - Tokens: {summary.get('total_tokens', 0)}\")\n                        print(\n                            f\"  - Prototypes: {summary.get('total_prototypes', 0)}\"\n                        )\n                        print(\n                            f\"  - Homographs: {summary.get('num_homographs', 0)}\"\n                        )\n                    self.last_validation_step = current_step\n                except Exception:\n                    pass\n\n        enc_outputs = None\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids, attention_mask=attention_mask\n            )\n        except Exception:\n            try:\n                enc_outputs = self.mbart.get_encoder()(\n                    input_ids=input_ids, attention_mask=attention_mask\n                )\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Encoder failed: {e}\")\n                enc_outputs = None\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            try:\n                emb = self.mbart.get_input_embeddings()(input_ids).to(device)\n                h = emb\n            except Exception:\n                h = torch.zeros(\n                    batch_size,\n                    seq_len,\n                    int(getattr(self.mbart.config, \"d_model\", 1024)),\n                    device=device,\n                )\n\n        embed_dim = int(h.size(-1))\n        training_mode = labels is not None and self.training\n\n        token_word_map = self._reconstruct_word_maps_before_dscd(\n            input_ids, batch_size, seq_len, src_texts, token_word_map\n        )\n\n        domain_labels = self._extract_domain_labels(\n            batch_size, device, src_texts\n        )\n\n        if use_dscd:\n            try:\n                raw_dscd = self.dscd.forward(\n                    h,\n                    token_types=None,\n                    train_mode=self.training,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_word_map=token_word_map,\n                )\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD forward failed: {e}\")\n                raw_dscd = {\n                    \"h_augmented\": h.detach().clone(),\n                    \"proto_probs\": [\n                        [\n                            torch.tensor(\n                                [1.0],\n                                dtype=torch.float32,\n                                device=device,\n                            )\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"uncertainties\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"gates\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"span_preds\": [\n                        [\n                            torch.tensor(0.0, device=device)\n                            for _ in range(seq_len)\n                        ]\n                        for _ in range(batch_size)\n                    ],\n                    \"proto_assignments\": [\n                        torch.zeros(\n                            seq_len, dtype=torch.long, device=device\n                        )\n                        for _ in range(batch_size)\n                    ],\n                }\n        else:\n            raw_dscd = {\n                \"h_augmented\": h.detach().clone(),\n                \"proto_probs\": [\n                    [\n                        torch.tensor(\n                            [1.0], dtype=torch.float32, device=device\n                        )\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"uncertainties\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"gates\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"span_preds\": [\n                    [\n                        torch.tensor(0.0, device=device)\n                        for _ in range(seq_len)\n                    ]\n                    for _ in range(batch_size)\n                ],\n                \"proto_assignments\": [\n                    torch.zeros(seq_len, dtype=torch.long, device=device)\n                    for _ in range(batch_size)\n                ],\n            }\n\n        dscd = _normalize_dscd_outputs(\n            raw_dscd, batch_size, seq_len, device, embed_dim\n        )\n        h_aug = dscd.get(\"h_augmented\", h)\n\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[TATN] h_augmented shape mismatch \"\n                    f\"(expected {h.shape}, got {getattr(h_aug, 'shape', None)})\"\n                )\n            h_aug = h\n\n        domain_loss_from_forward = torch.tensor(0.0, device=device)\n        \n        if use_asbn and domain_labels is not None:\n            try:\n                h_aug, domain_loss_from_forward = self.asbn.forward(\n                    h_aug, domain_labels=domain_labels\n                )\n                \n                if not isinstance(domain_loss_from_forward, torch.Tensor):\n                    domain_loss_from_forward = torch.tensor(0.0, device=device)\n                else:\n                    domain_loss_from_forward = domain_loss_from_forward.to(device)\n                \n                if not torch.isfinite(domain_loss_from_forward):\n                    domain_loss_from_forward = torch.tensor(0.0, device=device)\n                \n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] ASBN forward (BN+domain) failed: {e}\")\n                domain_loss_from_forward = torch.tensor(0.0, device=device)\n\n        try:\n            enc_for_decoder = BaseModelOutput(\n                last_hidden_state=h_aug,\n                hidden_states=(\n                    getattr(enc_outputs, \"hidden_states\", None)\n                    if enc_outputs\n                    else None\n                ),\n                attentions=(\n                    getattr(enc_outputs, \"attentions\", None)\n                    if enc_outputs\n                    else None\n                ),\n            )\n        except Exception:\n            enc_for_decoder = (h_aug,)\n\n        if training_mode:\n            try:\n                if labels is not None:\n                    if labels.size(1) < 2:\n                        if _DEBUG_DISCOVERY:\n                            print(\"[TATN] Labels too short for decoder_input_ids construction\")\n                        decoder_input_ids = None\n                        decoder_attention_mask = None\n                    else:\n                        decoder_input_ids = labels.clone()\n                        decoder_input_ids = torch.where(\n                            decoder_input_ids == -100,\n                            torch.full_like(\n                                decoder_input_ids,\n                                self.tokenizer.pad_token_id,\n                            ),\n                            decoder_input_ids,\n                        )\n\n                        bos_column = torch.full(\n                            (batch_size, 1),\n                            int(self.mbart.config.decoder_start_token_id),\n                            dtype=torch.long,\n                            device=device,\n                        )\n                        decoder_input_ids = torch.cat(\n                            [bos_column, decoder_input_ids[:, :-1]], dim=1\n                        )\n                        decoder_attention_mask = (\n                            decoder_input_ids != self.tokenizer.pad_token_id\n                        ).long()\n                else:\n                    decoder_input_ids = None\n                    decoder_attention_mask = None\n\n                seq_outputs = self.mbart(\n                    input_ids=None,\n                    attention_mask=attention_mask,\n                    encoder_outputs=enc_for_decoder,\n                    decoder_input_ids=decoder_input_ids,\n                    decoder_attention_mask=decoder_attention_mask,\n                    labels=labels,\n                    use_cache=False,\n                    return_dict=True,\n                )\n                translation_loss = getattr(seq_outputs, \"loss\", None)\n                if translation_loss is None:\n                    translation_loss = torch.tensor(0.0, device=device)\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] Decoder forward failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                translation_loss = torch.tensor(0.0, device=device)\n\n            encoder_loss_from_grl = torch.tensor(0.0, device=device)\n            adversarial_loss = torch.tensor(0.0, device=device)\n            domain_loss_from_grl = torch.tensor(0.0, device=device)\n            domain_accuracy = torch.tensor(0.0, device=device)\n            \n            if use_asbn:\n                try:\n                    asbn_ret = self.asbn.forward_with_grl_simplified(\n                        h_aug,\n                        dscd.get(\"proto_probs\", None),\n                        dscd.get(\"uncertainties\", None),\n                        dscd.get(\"gates\", None),\n                        token_word_map=token_word_map,\n                        domain_labels=domain_labels,\n                        global_step=current_step,\n                    )\n                    \n                    if isinstance(asbn_ret, (tuple, list)) and len(asbn_ret) >= 4:\n                        encoder_loss_from_grl = asbn_ret[0]\n                        adversarial_loss = asbn_ret[1]\n                        domain_loss_from_grl = asbn_ret[2]\n                        domain_accuracy = asbn_ret[3]\n                    elif isinstance(asbn_ret, (tuple, list)):\n                        encoder_loss_from_grl = asbn_ret[0]\n                    else:\n                        encoder_loss_from_grl = asbn_ret\n                    \n                    if not isinstance(encoder_loss_from_grl, torch.Tensor):\n                        encoder_loss_from_grl = torch.tensor(float(encoder_loss_from_grl), device=device)\n                    else:\n                        encoder_loss_from_grl = encoder_loss_from_grl.to(device)\n                    \n                    if not torch.isfinite(encoder_loss_from_grl):\n                        encoder_loss_from_grl = torch.tensor(0.0, device=device)\n                    \n                    encoder_loss_from_grl = torch.clamp(encoder_loss_from_grl, 0.0, 10.0)\n                    \n                    if _DEBUG_DISCOVERY and current_step % 100 == 0:\n                        print(f\"[TATN-ASBN] Step {current_step}:\")\n                        print(f\"  Domain loss (forward): {domain_loss_from_forward.item():.4f}\")\n                        print(f\"  Domain loss (GRL): {domain_loss_from_grl.item():.4f}\")\n                        print(f\"  Domain accuracy: {domain_accuracy.item():.2%}\")\n                        print(f\"  Encoder loss: {encoder_loss_from_grl.item():.4f}\")\n                    \n                except Exception as e:\n                    if _DEBUG_DISCOVERY:\n                        print(f\"[TATN] ASBN forward_with_grl_simplified failed: {e}\")\n                    encoder_loss_from_grl = torch.tensor(0.0, device=device)\n\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(\n                    dscd.get(\"proto_probs\", []),\n                    gates_list=dscd.get(\"gates\", []),\n                    min_gate=0.0,\n                )\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(\n                        float(dscd_reg), device=device\n                    )\n                else:\n                    dscd_reg = dscd_reg.to(device)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device)\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN] DSCD reg failed: {e}\")\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            total_loss = (\n                translation_loss\n                + _LAMBDA_ASBN * encoder_loss_from_grl\n                + _LAMBDA_DOMAIN * domain_loss_from_grl\n                + _LAMBDA_DSCD * dscd_reg\n            )\n            \n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            if total_loss.numel() != 1:\n                total_loss = total_loss.mean()\n\n            if not torch.isfinite(total_loss):\n                if _DEBUG_DISCOVERY:\n                    print(\n                        \"[TATN] NaN/Inf detected in total_loss - \"\n                        \"using translation_loss only\"\n                    )\n                total_loss = (\n                    translation_loss\n                    if torch.isfinite(translation_loss)\n                    else torch.tensor(1.0, device=device)\n                )\n\n            try:\n                del enc_outputs, h, raw_dscd\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            return total_loss\n\n        explanations_list: List[List[Dict[str, Any]]] = []\n\n        if (not self.training) and _ENABLE_TRG_INFERENCE:\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"\\n[TATN-INFERENCE] Starting TRG for {batch_size} samples\"\n                )\n\n            tokens_batch: List[List[str]] = []\n\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    if hasattr(self.tokenizer, \"convert_ids_to_tokens\"):\n                        toks = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    else:\n                        toks = []\n                    if not toks:\n                        toks = [\"UNK\"] * seq_len\n                    elif len(toks) < seq_len:\n                        toks = toks + [\"\"] * (seq_len - len(toks))\n                    elif len(toks) > seq_len:\n                        toks = toks[:seq_len]\n                except Exception:\n                    toks = [\"UNK\"] * seq_len\n\n                tokens_batch.append(toks)\n\n            decoder_attention = None\n\n            try:\n                total_explanations = 0\n\n                for b in range(batch_size):\n                    per_sent = {\n                        \"proto_probs\": [self._safe_take_key_static(\n                            dscd, \"proto_probs\", b, seq_len, device\n                        )],\n                        \"uncertainties\": [self._safe_take_key_static(\n                            dscd, \"uncertainties\", b, seq_len, device\n                        )],\n                        \"gates\": [self._safe_take_key_static(\n                            dscd, \"gates\", b, seq_len, device\n                        )],\n                        \"span_preds\": [self._safe_take_key_static(\n                            dscd, \"span_preds\", b, seq_len, device\n                        )],\n                    }\n\n                    try:\n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=(\n                                token_word_map[b]\n                                if token_word_map\n                                and b < len(token_word_map)\n                                else None\n                            ),\n                            uncertainty_threshold=_TRG_UNCERTAINTY_THRESHOLD,\n                            decoder_attention=decoder_attention,\n                        )\n                        batch_exps = exps if isinstance(exps, list) else []\n                        explanations_list.append(batch_exps)\n                        total_explanations += len(batch_exps)\n\n                        if _DEBUG_DISCOVERY and b < 2:\n                            print(\n                                f\"[TATN-INFERENCE] Sample {b}: \"\n                                f\"{len(batch_exps)} explanations\"\n                            )\n                            if len(batch_exps) == 0:\n                                print(f\"  U: {[float(u) for u in per_sent['uncertainties'][0][:5]]}\")\n                                print(f\"  S: {[float(s) for s in per_sent['span_preds'][0][:5]]}\")\n\n                    except Exception as e:\n                        if _DEBUG_DISCOVERY:\n                            print(\n                                f\"[TATN-INFERENCE] TRG failed for sample {b}: {e}\"\n                            )\n                            try:\n                                traceback.print_exc()\n                            except Exception:\n                                pass\n                        explanations_list.append([])\n\n                if _DEBUG_DISCOVERY:\n                    print(\n                        f\"\\n[TATN-INFERENCE] Total explanations: {total_explanations}\"\n                    )\n                    if total_explanations == 0:\n                        print(\"[TATN-INFERENCE] NO EXPLANATIONS GENERATED - check thresholds\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY:\n                    print(f\"[TATN-INFERENCE] TRG generation failed: {e}\")\n                    try:\n                        traceback.print_exc()\n                    except Exception:\n                        pass\n                explanations_list = [[] for _ in range(batch_size)]\n        else:\n            explanations_list = [[] for _ in range(batch_size)]\n\n        outputs = {\n            \"encoder_outputs\": enc_outputs,\n            \"dscd_outputs\": dscd,\n            \"sense_augmented_embeddings\": h_aug,\n            \"explanations\": explanations_list,\n            \"asbn_loss\": torch.tensor(0.0, device=device),\n            \"ambiguity_signals\": {\n                \"span\": dscd.get(\"span_preds\", []),\n                \"uncertainty\": dscd.get(\"uncertainties\", []),\n                \"confidence\": [\n                    [\n                        1.0\n                        - (\n                            float(u)\n                            if isinstance(u, (float, int))\n                            else (\n                                float(u.item())\n                                if isinstance(u, torch.Tensor)\n                                else 1.0\n                            )\n                        )\n                        for u in row\n                    ]\n                    for row in dscd.get(\"uncertainties\", [])\n                ],\n                \"proto_probs\": dscd.get(\"proto_probs\", []),\n            },\n        }\n\n        try:\n            del h, raw_dscd\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return outputs\n\n    def generate(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        max_length: int = 128,\n        num_beams: int = 5,\n        early_stopping: bool = True,\n        **kwargs,\n    ) -> torch.Tensor:\n        try:\n            enc_outputs = self.mbart.model.encoder(\n                input_ids=input_ids, attention_mask=attention_mask\n            )\n\n            enc_wrapped = BaseModelOutput(\n                last_hidden_state=(\n                    enc_outputs.last_hidden_state\n                    if hasattr(enc_outputs, \"last_hidden_state\")\n                    else enc_outputs[0]\n                ),\n                hidden_states=getattr(enc_outputs, \"hidden_states\", None),\n                attentions=getattr(enc_outputs, \"attentions\", None),\n            )\n\n            return self.mbart.generate(\n                input_ids=None,\n                attention_mask=attention_mask,\n                encoder_outputs=enc_wrapped,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=early_stopping,\n                forced_bos_token_id=int(\n                    self.mbart.config.forced_bos_token_id\n                ),\n                **kwargs,\n            )\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[TATN-GENERATE] Failed: {e}\")\n            raise\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n        )\n\n    def get_component_stats(self) -> Dict[str, Any]:\n        stats: Dict[str, Any] = {\n            \"global_step\": self.global_step,\n            \"last_discovery_step\": self.last_discovery_step,\n            \"last_validation_step\": self.last_validation_step,\n        }\n\n        try:\n            if hasattr(self.dscd, \"get_prototype_summary\"):\n                stats[\"dscd\"] = self.dscd.get_prototype_summary()\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.asbn, \"get_detailed_stats\"):\n                stats[\"asbn\"] = self.asbn.get_detailed_stats()\n        except Exception:\n            pass\n\n        try:\n            if hasattr(self.trg_system, \"get_statistics\"):\n                stats[\"trg\"] = self.trg_system.get_statistics()\n        except Exception:\n            pass\n\n        return stats\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 6: TATN Ready (DISCOVERY FREQUENCY FIXED)\")\nprint(\"=\" * 80)\nprint()\nprint(\"Config:\")\nprint(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps ← FIXED!\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD:.2f}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD:.2f}\")\nprint(f\"  - TRG uncertainty: {_TRG_UNCERTAINTY_THRESHOLD:.2f}\")\nprint(f\"  - Lambda ASBN: {_LAMBDA_ASBN}\")\nprint(f\"  - Lambda Domain: {_LAMBDA_DOMAIN}\")\nprint(f\"  - Lambda DSCD: {_LAMBDA_DSCD}\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"KZbMDpIYH4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: TRAINING LOOP (PURE UNSUPERVISED) - FIXED\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\nimport threading\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\nDEBUG_PRINT_INTERVAL = 200\n_cell7_dbg_counts = defaultdict(int)\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not (_VERBOSE_LOGGING or _DEBUG_DISCOVERY):\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _EPOCHS = int(EPOCHS)\nexcept (NameError, ValueError, TypeError):\n    _EPOCHS = 1\n\ntry:\n    _BATCH_SIZE = int(BATCH_SIZE)\nexcept (NameError, ValueError, TypeError):\n    _BATCH_SIZE = 8\n\ntry:\n    _ACCUMULATION_STEPS = int(ACCUMULATION_STEPS)\nexcept (NameError, ValueError, TypeError):\n    _ACCUMULATION_STEPS = 1\n\ntry:\n    _GRAD_CLIP_NORM = float(GRAD_CLIP_NORM)\nexcept (NameError, ValueError, TypeError):\n    _GRAD_CLIP_NORM = 1.0\n\ntry:\n    _MEMORY_CLEANUP_FREQUENCY = int(MEMORY_CLEANUP_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _MEMORY_CLEANUP_FREQUENCY = 500\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\n    _NUM_GPUS = int(NUM_GPUS)\nexcept (NameError, ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n\ntry:\n    _USE_AMP = bool(USE_AMP)\nexcept (NameError, TypeError):\n    _USE_AMP = True\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _VALIDATION_CHECK_INTERVAL = int(VALIDATION_CHECK_INTERVAL)\nexcept (NameError, ValueError, TypeError):\n    _VALIDATION_CHECK_INTERVAL = 500\n\ntry:\n    _PERIODIC_DISCOVERY_FREQUENCY = int(PERIODIC_DISCOVERY_FREQUENCY)\nexcept (NameError, ValueError, TypeError):\n    _PERIODIC_DISCOVERY_FREQUENCY = 50\n\ntry:\n    _TRAIN_DOMAIN = int(TRAIN_DOMAIN)\n    _TEST_DOMAIN = int(TEST_DOMAIN)\nexcept (NameError, ValueError, TypeError):\n    _TRAIN_DOMAIN = 0\n    _TEST_DOMAIN = 1\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\ndef get_amp_ctx():\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            return dscd.get_discovered_homographs()\n\n        homographs = set()\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                for token, store in dscd.prototype_stores.items():\n                    try:\n                        if store.size() >= 2:\n                            clean_token = str(token).replace('▁', '').replace('Ġ', '').replace('##', '').strip().lower()\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            for token, store in dscd.prototype_stores.items():\n                try:\n                    if store.size() >= 2:\n                        clean_token = str(token).replace('▁', '').replace('Ġ', '').replace('##', '').strip().lower()\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\n@torch.inference_mode()\ndef comprehensive_epoch_validation(\n    model: torch.nn.Module,\n    tokenizer,\n    epoch: int,\n    global_step: int,\n    source_lang: str,\n    target_lang: str,\n    max_length: int,\n    device: torch.device\n) -> Dict[str, Any]:\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"EPOCH {epoch} COMPREHENSIVE VALIDATION (Step {global_step})\")\n    print(\"=\" * 80)\n\n    core_model = model.module if hasattr(model, \"module\") else model\n    was_training = core_model.training\n\n    if not isinstance(device, torch.device):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    dscd_homographs = _get_dscd_homographs(model)\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"[VALIDATION] DSCD discovered homographs: {len(dscd_homographs)}\")\n        if dscd_homographs:\n            print(f\"[VALIDATION] Sample: {list(dscd_homographs)[:10]}\")\n\n    validation_results = {\n        'epoch': epoch,\n        'step': global_step,\n        'translations_success': 0,\n        'translations_failed': 0,\n        'explanations_generated': 0,\n        'dscd_homographs_explained': 0,\n        'reference_homographs_explained': 0,\n        'avg_explanation_confidence': 0.0,\n        'dscd_quality_score': 0.0,\n        'dscd_multi_sense_tokens': 0,\n        'dscd_total_prototypes': 0,\n        'asbn_domain_loss': 0.0,\n        'asbn_domain_accuracy': 0.0,\n        'asbn_source_accuracy': 0.0,\n        'asbn_target_accuracy': 0.0,\n        'trg_total_explanations': 0,\n        'validation_completed': False,\n    }\n\n    try:\n        core_model.eval()\n        \n        try:\n            trg_system = getattr(core_model, 'trg_system', None)\n            if trg_system is not None and hasattr(trg_system, 'eval'):\n                trg_system.eval()\n        except Exception:\n            pass\n\n        val_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল=tap/call\"),\n            (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল=tomorrow/yesterday\"),\n            (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা=leaf/page\"),\n            (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক=bank/embankment\"),\n            (\"আমি ভালো আছি।\", \"I am fine\", \"No ambiguity\"),\n            (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"No ambiguity\"),\n            (\"এটা আমার বই।\", \"This is my book\", \"No ambiguity\"),\n            (\"আজ আবহাওয়া ভালো।\", \"Weather is good today\", \"No ambiguity\"),\n            (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল=fruit/result\"),\n            (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা=head/top\"),\n        ]\n\n        print(f\"\\n[VALIDATION] Testing {len(val_sentences)} samples:\")\n        print(\"-\" * 80)\n\n        confidences = []\n        dscd_homograph_words_detected = set()\n        reference_homograph_words_detected = set()\n\n        mbart_obj = None\n        try:\n            mbart_obj = getattr(core_model, \"mbart\", None)\n        except Exception:\n            mbart_obj = None\n\n        try:\n            try:\n                tokenizer.src_lang = source_lang\n            except Exception:\n                pass\n\n            forced_id = None\n            try:\n                if hasattr(tokenizer, \"get_lang_id\"):\n                    for code in (target_lang, \"en_XX\", \"en\", \"eng\"):\n                        try:\n                            lid = tokenizer.get_lang_id(code)\n                            if lid is not None:\n                                forced_id = int(lid)\n                                break\n                        except Exception:\n                            continue\n                elif hasattr(tokenizer, \"lang_code_to_id\"):\n                    forced_id = tokenizer.lang_code_to_id.get(target_lang, None)\n                    if forced_id is not None:\n                        forced_id = int(forced_id)\n            except Exception:\n                forced_id = None\n\n            if forced_id is None:\n                try:\n                    forced_id = int(globals().get('M2M100_EN_TOKEN_ID', 128022))\n                except Exception:\n                    forced_id = 128022\n\n            orig_use_cache = None\n            try:\n                if mbart_obj is not None and hasattr(mbart_obj.config, \"use_cache\"):\n                    orig_use_cache = mbart_obj.config.use_cache\n                    mbart_obj.config.use_cache = True\n            except Exception:\n                orig_use_cache = None\n\n            for idx, (src, expected, note) in enumerate(val_sentences, 1):\n                try:\n                    enc = tokenizer(\n                        src,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_length,\n                    )\n                    enc = {\n                        k: (\n                            v.to(device, non_blocking=True)\n                            if isinstance(v, torch.Tensor)\n                            else v\n                        )\n                        for k, v in enc.items()\n                    }\n\n                    if forced_id is not None:\n                        try:\n                            if mbart_obj is not None:\n                                mbart_obj.config.forced_bos_token_id = int(forced_id)\n                                mbart_obj.config.decoder_start_token_id = int(forced_id)\n                        except Exception:\n                            pass\n\n                    out_ids = None\n                    try:\n                        gen_src = mbart_obj if mbart_obj is not None else core_model\n                        if hasattr(gen_src, \"generate\"):\n                            out_ids = gen_src.generate(\n                                enc.get(\"input_ids\"),\n                                attention_mask=enc.get(\"attention_mask\"),\n                                max_length=max_length,\n                                num_beams=2,\n                                do_sample=False,\n                                early_stopping=True,\n                                pad_token_id=int(\n                                    getattr(tokenizer, \"pad_token_id\", 1)\n                                ),\n                                forced_bos_token_id=int(forced_id)\n                                if forced_id is not None\n                                else None,\n                            )\n                    except AttributeError:\n                        if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                            print(\n                                \"[VALIDATION] Warning: generation raised AttributeError (protobuf incompatibility).\"\n                            )\n                            _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                        out_ids = None\n                    except Exception as e:\n                        print(\n                            f\"[VALIDATION] Generation error: {type(e).__name__}: {str(e)[:200]}\"\n                        )\n                        out_ids = None\n\n                    translation = \"\"\n                    if out_ids is not None and (\n                        (isinstance(out_ids, torch.Tensor) and out_ids.numel() > 0)\n                        or (\n                            isinstance(out_ids, (list, tuple))\n                            and len(out_ids) > 0\n                        )\n                    ):\n                        try:\n                            if isinstance(out_ids, (list, tuple)):\n                                translation = tokenizer.batch_decode(\n                                    out_ids, skip_special_tokens=True\n                                )[0] if out_ids else \"\"\n                            else:\n                                translation = (\n                                    tokenizer.decode(\n                                        out_ids[0], skip_special_tokens=True\n                                    )\n                                    if out_ids.size(0) > 0\n                                    else \"\"\n                                )\n                        except AttributeError:\n                            if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                                print(\n                                    \"[VALIDATION] Warning: decode raised AttributeError (protobuf).\"\n                                )\n                                _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                            translation = \"\"\n                        except Exception as e:\n                            print(\n                                f\"[VALIDATION] Decode error: {type(e).__name__}: {str(e)[:200]}\"\n                            )\n                            translation = \"\"\n                    else:\n                        translation = \"\"\n\n                    if translation:\n                        validation_results['translations_success'] += 1\n                    else:\n                        validation_results['translations_failed'] += 1\n                        print(\n                            f\"  {idx:2d}. Translation failed: {note[:30]:30s}\"\n                        )\n                        continue\n\n                    explanation_status = \"\"\n                    try:\n                        if 'translate_with_explanations' in globals():\n                            res = translate_with_explanations(\n                                model, tokenizer, src\n                            )\n                            exps = res.get('explanations', [])\n                            validation_results['explanations_generated'] += len(\n                                exps\n                            )\n\n                            if exps:\n                                explanation_status = f\"{len(exps)} expl\"\n                                for exp in exps:\n                                    try:\n                                        conf = exp.get('confidence', 0.5)\n                                        confidences.append(float(conf))\n\n                                        word = exp.get('ambiguous_word', '').strip()\n                                        clean_word = (\n                                            word.replace('▁', '')\n                                            .replace('Ġ', '')\n                                            .replace('##', '')\n                                            .lower()\n                                        )\n\n                                        if clean_word in dscd_homographs:\n                                            validation_results[\n                                                'dscd_homographs_explained'\n                                            ] += 1\n                                            dscd_homograph_words_detected.add(\n                                                clean_word\n                                            )\n\n                                        if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                                            validation_results[\n                                                'reference_homographs_explained'\n                                            ] += 1\n                                            reference_homograph_words_detected.add(\n                                                clean_word\n                                            )\n                                    except Exception:\n                                        pass\n                            else:\n                                explanation_status = \"no expl\"\n                                if _DEBUG_DISCOVERY:\n                                    print(f\"  [VALIDATION] Sample {idx}: 0 explanations (check thresholds)\")\n                        else:\n                            explanation_status = \"unavailable\"\n                    except Exception as e:\n                        explanation_status = f\"error: {type(e).__name__}\"\n\n                    print(\n                        f\"  {idx:2d}. {explanation_status:15s} \"\n                        f\"{note[:30]:30s} -> {translation[:200]}\"\n                    )\n                    del enc\n                    if out_ids is not None:\n                        del out_ids\n\n                except Exception as e:\n                    validation_results['translations_failed'] += 1\n                    print(\n                        f\"  {idx:2d}. ERROR: {note[:30]:30s} -> {type(e).__name__}\"\n                    )\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n\n        finally:\n            try:\n                if mbart_obj is not None and orig_use_cache is not None:\n                    mbart_obj.config.use_cache = orig_use_cache\n            except Exception:\n                pass\n            if torch.cuda.is_available():\n                try:\n                    torch.cuda.synchronize()\n                except Exception:\n                    pass\n\n            clear_all_gpu_caches()\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] DSCD Prototype Quality Check:\")\n        try:\n            dscd = core_model.dscd if hasattr(core_model, 'dscd') else None\n            if dscd and hasattr(dscd, 'validate_prototypes'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        quality_results = dscd.validate_prototypes()\n                else:\n                    quality_results = dscd.validate_prototypes()\n\n                validation_results['dscd_quality_score'] = quality_results.get(\n                    'quality_score', 0.0\n                )\n                validation_results['dscd_multi_sense_tokens'] = quality_results.get(\n                    'multi_sense_tokens', 0\n                )\n                validation_results['dscd_total_prototypes'] = quality_results.get(\n                    'total_prototypes', 0\n                )\n                print(\n                    f\"  - Quality Score: {validation_results['dscd_quality_score']:.1%}\"\n                )\n                print(\n                    f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\"\n                )\n                print(\n                    f\"  - Total prototypes: {validation_results['dscd_total_prototypes']}\"\n                )\n            else:\n                print(\"  - Validation not available\")\n        except Exception as e:\n            print(f\"  - Validation failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] ASBN Training Statistics:\")\n        try:\n            asbn = core_model.asbn if hasattr(core_model, 'asbn') else None\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                asbn_stats = asbn.get_detailed_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get(\n                    'domain_loss', 0.0\n                )\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get(\n                    'domain_accuracy', 0.0\n                )\n                validation_results['asbn_source_accuracy'] = asbn_stats.get(\n                    'source_accuracy', 0.0\n                )\n                validation_results['asbn_target_accuracy'] = asbn_stats.get(\n                    'target_accuracy', 0.0\n                )\n                print(\n                    f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\"\n                )\n                print(\n                    f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n                )\n                print(\n                    f\"  - Source Accuracy: {validation_results['asbn_source_accuracy']:.2%}\"\n                )\n                print(\n                    f\"  - Target Accuracy: {validation_results['asbn_target_accuracy']:.2%}\"\n                )\n            elif asbn and hasattr(asbn, 'get_asbn_stats'):\n                asbn_stats = asbn.get_asbn_stats()\n                validation_results['asbn_domain_loss'] = asbn_stats.get(\n                    'domain_loss', 0.0\n                )\n                validation_results['asbn_domain_accuracy'] = asbn_stats.get(\n                    'domain_accuracy', 0.0\n                )\n                print(\n                    f\"  - Domain Loss: {validation_results['asbn_domain_loss']:.4f}\"\n                )\n                print(\n                    f\"  - Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n                )\n            else:\n                print(\"  - ASBN statistics not available\")\n        except Exception as e:\n            print(f\"  - ASBN stats retrieval failed: {type(e).__name__}\")\n\n        print(\"\\n\" + \"-\" * 80)\n        print(\"[VALIDATION] TRG Explanation Statistics:\")\n        try:\n            trg = core_model.trg_system if hasattr(core_model, 'trg_system') else None\n            if trg and hasattr(trg, 'get_statistics'):\n                trg_stats = trg.get_statistics()\n                validation_results['trg_total_explanations'] = trg_stats.get(\n                    'explanations_generated', 0\n                )\n                print(\n                    f\"  - Total explanations: {validation_results['trg_total_explanations']}\"\n                )\n                print(\n                    f\"  - High confidence rate: {trg_stats.get('high_confidence_rate', 0):.1%}\"\n                )\n                print(\n                    f\"  - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\"\n                )\n            else:\n                print(\"  - TRG statistics not available\")\n        except Exception as e:\n            print(f\"  - TRG stats retrieval failed: {type(e).__name__}\")\n\n        if confidences:\n            validation_results['avg_explanation_confidence'] = sum(\n                confidences\n            ) / len(confidences)\n\n        print(\"-\" * 80)\n        print(\"\\n[VALIDATION] Summary:\")\n        print(\n            f\"  - Translations: {validation_results['translations_success']}/{len(val_sentences)} successful\"\n        )\n        print(\n            f\"  - Explanations generated: {validation_results['explanations_generated']}\"\n        )\n        print(\n            f\"  - Avg explanation confidence: {validation_results['avg_explanation_confidence']:.3f}\"\n        )\n        print(\n            f\"  - DSCD homographs explained: {validation_results['dscd_homographs_explained']}\"\n        )\n        print(\n            f\"  - Reference homographs explained: {validation_results['reference_homographs_explained']}\"\n        )\n\n        if dscd_homograph_words_detected:\n            print(\n                f\"  - DSCD homographs detected: {', '.join(sorted(dscd_homograph_words_detected))}\"\n            )\n\n        print(\n            f\"  - DSCD Quality Score: {validation_results['dscd_quality_score']:.1%}\"\n        )\n        print(\n            f\"  - Multi-sense tokens: {validation_results['dscd_multi_sense_tokens']}\"\n        )\n        print(\n            f\"  - ASBN Domain Accuracy: {validation_results['asbn_domain_accuracy']:.2%}\"\n        )\n\n        warnings = []\n        if validation_results['translations_failed'] > len(val_sentences) // 2:\n            warnings.append(\"High translation failure rate\")\n        if validation_results['explanations_generated'] == 0:\n            warnings.append(\"No explanations generated - check thresholds (span=0.12, uncertainty=0.15)\")\n        if validation_results['dscd_quality_score'] < 0.3:\n            warnings.append(\"Low DSCD quality score\")\n        if validation_results['dscd_multi_sense_tokens'] < 10:\n            warnings.append(\"Very few multi-sense tokens\")\n\n        if warnings:\n            print(\"\\n[VALIDATION] Health Warnings:\")\n            for w in warnings:\n                print(f\"  - {w}\")\n        else:\n            print(\"\\n[VALIDATION] All systems healthy\")\n\n        validation_results['validation_completed'] = True\n\n    except Exception as e:\n        print(\n            f\"\\n[VALIDATION] Critical error: {type(e).__name__}: {str(e)[:200]}\"\n        )\n        if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n        validation_results['validation_completed'] = False\n\n    finally:\n        if was_training:\n            core_model.train()\n        clear_all_gpu_caches()\n\n    print(\"=\" * 80 + \"\\n\")\n    return validation_results\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return 0\n\n        stores = getattr(dscd, 'prototype_stores', None)\n        if stores is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                return len(stores)\n        else:\n            return len(stores)\n\n    except Exception:\n        return 0\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n        return getattr(core, 'dscd', None)\n    except Exception:\n        return None\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n\n    try:\n        dscd_homographs = _get_dscd_homographs(model)\n        items = []\n        homograph_items = []\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores_snapshot = list(dscd.prototype_stores.items())\n        else:\n            stores_snapshot = list(dscd.prototype_stores.items())\n\n        for token, store in stores_snapshot:\n            try:\n                total_count = sum(getattr(store, \"counts\", []) or [])\n                protos = store.size() if hasattr(store, \"size\") else len(\n                    getattr(store, \"centroids\", [])\n                )\n                clean_token = (\n                    str(token).replace('▁', '').replace('Ġ', '').replace('##', '').strip().lower()\n                )\n                is_homograph = clean_token in dscd_homographs\n                item = (\n                    token,\n                    total_count,\n                    protos,\n                    len(dscd.buffers.get(token, []))\n                    if hasattr(dscd, 'buffers')\n                    else 0,\n                    is_homograph,\n                )\n                items.append(item)\n                if is_homograph:\n                    homograph_items.append(item)\n            except Exception:\n                continue\n\n        items.sort(key=lambda x: x[1], reverse=True)\n\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen, is_homo) in enumerate(\n                items[:top_n], 1\n            ):\n                marker = \"HOMO\" if is_homo else \"    \"\n                print(\n                    f\"{marker} {i:2d}. {str(tok)[:20]:20s} \"\n                    f\"samples={cnt:4d} protos={prot} buf={buflen}\"\n                )\n            if homograph_items:\n                print(\n                    f\"[CLUSTER-DBG] DSCD-discovered homographs: {len(homograph_items)}\"\n                )\n                for tok, cnt, prot, buflen, _ in homograph_items[:5]:\n                    print(\n                        f\"  HOMO {str(tok)[:20]:20s} samples={cnt:4d} protos={prot}\"\n                    )\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}\")\n\ndef _check_discovery_status(model: torch.nn.Module, global_step: int):\n    try:\n        core = model\n        while hasattr(core, 'module'):\n            core = core.module\n\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return\n\n        if hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            total_discovered = len(dscd.discovered_log)\n\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\n                    f\"[DISCOVERY-STATUS] Step {global_step}: {total_discovered} discovery events\"\n                )\n\n                recent = (\n                    dscd.discovered_log[-3:]\n                    if len(dscd.discovered_log) >= 3\n                    else dscd.discovered_log\n                )\n                for entry in recent:\n                    discovered = entry.get('discovered', 0)\n                    candidates = entry.get('candidates', 0)\n                    print(\n                        f\"  - {discovered}/{candidates} homographs discovered\"\n                    )\n        else:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\n                    f\"[DISCOVERY-STATUS] No discoveries yet at step {global_step}\"\n                )\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[DISCOVERY-STATUS] Error: {e}\")\n\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = _ACCUMULATION_STEPS\n    if validate_every is None:\n        validate_every = _VALIDATION_CHECK_INTERVAL\n\n    print(\n        f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, \"\n        f\"accum_steps={accumulation_steps}\"\n    )\n    print(\n        f\"[TRAIN] Validation: \"\n        f\"{'enabled' if enable_validation and validate_every > 0 else 'disabled'}\"\n    )\n    print(\n        f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\"\n    )\n    print(\n        f\"[TRAIN] Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY} steps\"\n    )\n    print(\n        \"[TRAIN] Checkpoint: Will save to /kaggle/working/tatn_final.pt \"\n        \"after all epochs\"\n    )\n\n    model.train()\n    clear_all_gpu_caches()\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"epoch_losses\": [],\n        \"backward_losses\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n        \"epoch_validations\": [],\n        \"dscd_quality_history\": [],\n        \"multi_sense_ratio_history\": [],\n        \"asbn_domain_accuracy_history\": [],\n        \"trg_explanation_history\": [],\n    }\n\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        skip_reasons = defaultdict(int)\n\n        print(f\"\\n{'='*80}\")\n        print(f\"EPOCH {epoch}/{epochs} STARTED\")\n        print(f\"{'='*80}\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'reset_statistics'):\n                try:\n                    trg.reset_statistics()\n                    print(f\"[TRAIN] TRG statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] TRG stats reset failed: {e}\")\n\n        try:\n            core = model.module if hasattr(model, 'module') else model\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'reset_stats'):\n                try:\n                    asbn.reset_stats()\n                    print(f\"[TRAIN] ASBN statistics reset for epoch {epoch}\")\n                except Exception:\n                    pass\n        except Exception as e:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(f\"[TRAIN] ASBN stats reset failed: {e}\")\n\n        try:\n            optimizer.zero_grad(set_to_none=True)\n        except Exception:\n            pass\n\n        progress = None\n        try:\n            progress = tqdm(\n                train_loader,\n                desc=f\"Epoch {epoch}/{epochs}\",\n                dynamic_ncols=True,\n            )\n\n            for batch_idx, batch in enumerate(progress):\n                global_step += 1\n                training_stats[\"batches_processed\"] += 1\n\n                if (\n                    _DEBUG_DISCOVERY or _VERBOSE_LOGGING\n                ) and global_step % DEBUG_PRINT_INTERVAL == 0:\n                    print(\n                        f\"\\n[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} \"\n                        f\"GlobalStep {global_step}\"\n                    )\n                    _check_discovery_status(model, global_step)\n\n                if (\n                    enable_validation\n                    and validate_every\n                    and validate_every > 0\n                    and (global_step % validate_every == 0)\n                ):\n                    if accumulated_steps == 0:\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n\n                        val_result = comprehensive_epoch_validation(\n                            model,\n                            tokenizer,\n                            epoch,\n                            global_step,\n                            _SOURCE_LANGUAGE,\n                            _TARGET_LANGUAGE,\n                            _MAX_LENGTH,\n                            _DEVICE,\n                        )\n\n                        if val_result:\n                            training_stats['epoch_validations'].append(\n                                val_result\n                            )\n                    else:\n                        pending_validation = True\n\n                if batch is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"batch_none\"] += 1\n                    continue\n\n                try:\n                    input_ids = batch[\"input_ids\"]\n                    attention_mask = batch[\"attention_mask\"]\n                    labels = batch[\"labels\"]\n\n                    domain_labels = batch.get(\"domain_labels\", None)\n                    if domain_labels is not None:\n                        if not isinstance(domain_labels, torch.Tensor):\n                            domain_labels = None\n                        elif domain_labels.dim() == 0:\n                            domain_labels = domain_labels.unsqueeze(0)\n\n                    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                        bsz = int(input_ids.size(0))\n                        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            continue\n                        if keep != bsz:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            labels = labels[:keep]\n                            if domain_labels is not None:\n                                domain_labels = domain_labels[:keep]\n\n                    input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                    attention_mask = attention_mask.to(\n                        _DEVICE, non_blocking=True\n                    )\n                    labels = labels.to(_DEVICE, non_blocking=True)\n\n                    if domain_labels is not None:\n                        domain_labels = domain_labels.to(\n                            _DEVICE, non_blocking=True\n                        )\n\n                    if input_ids.size(0) == 0:\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"empty_batch\"] += 1\n                        continue\n\n                    forward_kwargs = {\n                        \"input_ids\": input_ids,\n                        \"attention_mask\": attention_mask,\n                        \"labels\": labels,\n                        \"src_texts\": batch.get(\"src_text\", None),\n                        \"token_word_map\": batch.get(\"token_word_map\", None),\n                    }\n\n                    amp_ctx = get_amp_ctx()\n                    with amp_ctx:\n                        forward_out = model(**forward_kwargs)\n\n                        if isinstance(forward_out, torch.Tensor):\n                            loss_tensor = forward_out\n                        elif isinstance(forward_out, dict) and \"loss\" in forward_out:\n                            loss_tensor = forward_out[\"loss\"]\n                        else:\n                            if isinstance(forward_out, (list, tuple)) and len(\n                                forward_out\n                            ) > 0 and isinstance(\n                                forward_out[0], torch.Tensor\n                            ):\n                                loss_tensor = forward_out[0]\n                            else:\n                                raise RuntimeError(\n                                    \"Model forward did not return a recognizable loss tensor\"\n                                )\n\n                        if not isinstance(loss_tensor, torch.Tensor):\n                            loss_tensor = torch.tensor(\n                                float(loss_tensor), device=_DEVICE\n                            )\n                        else:\n                            loss_tensor = loss_tensor.to(_DEVICE)\n\n                        if loss_tensor.numel() > 1:\n                            loss_val = float(loss_tensor.mean().item())\n                            loss_tensor = loss_tensor.mean()\n                        else:\n                            loss_val = float(loss_tensor.item())\n\n                        last_forward_loss = loss_val\n                        epoch_losses.append(loss_val)\n                        training_stats[\"total_loss\"].append(loss_val)\n\n                    loss_scaled = loss_tensor / max(1, accumulation_steps)\n                    last_backward_loss = float(loss_scaled.item())\n                    training_stats[\"backward_losses\"].append(last_backward_loss)\n\n                    if scaler.is_enabled():\n                        scaler.scale(loss_scaled).backward()\n                    else:\n                        loss_scaled.backward()\n\n                    accumulated_steps += 1\n\n                    if accumulated_steps >= accumulation_steps:\n                        try:\n                            if scaler.is_enabled():\n                                scaler.unscale_(optimizer)\n                                torch.nn.utils.clip_grad_norm_(\n                                    model.parameters(), _GRAD_CLIP_NORM\n                                )\n                                scaler.step(optimizer)\n                                scaler.update()\n                            else:\n                                torch.nn.utils.clip_grad_norm_(\n                                    model.parameters(), _GRAD_CLIP_NORM\n                                )\n                                optimizer.step()\n                            optimizer.zero_grad(set_to_none=True)\n                            training_stats[\"optimizer_updates\"] += 1\n                        except RuntimeError as e:\n                            if \"out of memory\" in str(e).lower():\n                                training_stats[\"oom_errors\"] += 1\n                                training_stats[\"skipped_batches\"] += 1\n                                skip_reasons[\"oom\"] += 1\n                                print(f\"[OOM] OOM at step {global_step}\")\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n                                for p in model.parameters():\n                                    p.grad = None\n                                clear_all_gpu_caches()\n                                accumulated_steps = 0\n                                continue\n                            else:\n                                training_stats[\"runtime_errors\"] += 1\n                                skip_reasons[\"opt_runtime\"] += 1\n                                print(\n                                    f\"[ERROR] Runtime error during optimizer step: {type(e).__name__}\"\n                                )\n                        except Exception as e:\n                            training_stats[\"exceptions\"] += 1\n                            skip_reasons[\"opt_exception\"] += 1\n                            print(\n                                f\"[ERROR] Exception during optimizer step: {type(e).__name__}\"\n                            )\n                        finally:\n                            accumulated_steps = 0\n                            if pending_validation:\n                                try:\n                                    optimizer.zero_grad(set_to_none=True)\n                                except Exception:\n                                    pass\n\n                                val_result = comprehensive_epoch_validation(\n                                    model,\n                                    tokenizer,\n                                    epoch,\n                                    global_step,\n                                    _SOURCE_LANGUAGE,\n                                    _TARGET_LANGUAGE,\n                                    _MAX_LENGTH,\n                                    _DEVICE,\n                                )\n\n                                if val_result:\n                                    training_stats['epoch_validations'].append(\n                                        val_result\n                                    )\n\n                                pending_validation = False\n\n                    if global_step % DEBUG_PRINT_INTERVAL == 0:\n                        _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                        cluster_count = _get_cluster_count(model)\n                        print(\n                            f\"[TRAIN-DEBUG] step={global_step} \"\n                            f\"loss={last_forward_loss:.4f} clusters={cluster_count}\"\n                        )\n                        _print_top_clusters(model, top_n=5)\n\n                    if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                        clear_all_gpu_caches()\n\n                except RuntimeError as e:\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom\"] += 1\n                        print(f\"[OOM] Caught OOM at step {global_step}\")\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            p.grad = None\n                        clear_all_gpu_caches()\n                        accumulated_steps = 0\n                        continue\n                    else:\n                        training_stats[\"runtime_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"runtime\"] += 1\n                        print(\n                            f\"[RUNTIME] RuntimeError at step {global_step}: {type(e).__name__}\"\n                        )\n                        try:\n                            optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        accumulated_steps = 0\n                        continue\n                except Exception as e:\n                    training_stats[\"exceptions\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"exceptions\"] += 1\n                    print(\n                        f\"[EXCEPTION] Exception at step {global_step}: {type(e).__name__}\"\n                    )\n                    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                        try:\n                            traceback.print_exc()\n                        except Exception:\n                            pass\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    continue\n\n                processed_batches = (\n                    training_stats[\"batches_processed\"]\n                    - training_stats[\"skipped_batches\"]\n                )\n                expected_updates = max(\n                    1,\n                    math.floor(\n                        processed_batches / max(1, accumulation_steps)\n                    ),\n                )\n                success_rate = (\n                    100.0\n                    * training_stats[\"optimizer_updates\"]\n                    / expected_updates\n                    if expected_updates > 0\n                    else 0.0\n                )\n                cluster_count = _get_cluster_count(model)\n\n                next_disc_str = \"N/A\"\n                try:\n                    if (\n                        _PERIODIC_DISCOVERY_FREQUENCY\n                        and _PERIODIC_DISCOVERY_FREQUENCY > 0\n                    ):\n                        steps_to_next = (\n                            _PERIODIC_DISCOVERY_FREQUENCY\n                            - (global_step % _PERIODIC_DISCOVERY_FREQUENCY)\n                        )\n                        next_disc_str = f\"next_disc_in={steps_to_next}\"\n                except Exception:\n                    next_disc_str = \"next_disc_err\"\n\n                progress.set_postfix_str(\n                    f\"fwd_loss={last_forward_loss:.4f} \"\n                    f\"bwd_loss={last_backward_loss:.4f} \"\n                    f\"rate={success_rate:.1f}% \"\n                    f\"clusters={cluster_count} {next_disc_str}\"\n                )\n\n        finally:\n            if progress is not None:\n                try:\n                    progress.close()\n                except Exception:\n                    pass\n\n        if accumulated_steps > 0:\n            try:\n                if scaler.is_enabled():\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), _GRAD_CLIP_NORM\n                    )\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(\n                        model.parameters(), _GRAD_CLIP_NORM\n                    )\n                    optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(\n                    f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}\"\n                )\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = (\n            training_stats[\"batches_processed\"]\n            - training_stats[\"skipped_batches\"]\n        )\n        expected_updates = max(\n            1,\n            math.floor(processed_batches / max(1, accumulation_steps)),\n        )\n        success_rate = (\n            100.0\n            * training_stats[\"optimizer_updates\"]\n            / expected_updates\n            if expected_updates > 0\n            else 0.0\n        )\n        cluster_count = _get_cluster_count(model)\n\n        avg_epoch_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"EPOCH {epoch}/{epochs} SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"  Duration (min): {epoch_duration_min:.2f}\")\n        print(f\"  Optimizer updates: {training_stats['optimizer_updates']}\")\n        print(\n            f\"  Batches: processed={processed_batches}, \"\n            f\"skipped={training_stats['skipped_batches']}\"\n        )\n        print(f\"  Success rate: {success_rate:.1f}%\")\n        print(f\"  Clustered tokens: {cluster_count}\")\n        print(f\"  Avg epoch loss: {avg_epoch_loss:.6f}\")\n        if skip_reasons:\n            print(\"  Skip reasons:\")\n            for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"    - {k}: {v}\")\n        print(\"=\" * 80)\n\n        try:\n            print(\n                f\"\\n[TRAIN] Running comprehensive validation after epoch {epoch}...\"\n            )\n\n            try:\n                optimizer.zero_grad(set_to_none=True)\n            except Exception:\n                pass\n\n            validation_results = comprehensive_epoch_validation(\n                model=model,\n                tokenizer=tokenizer,\n                epoch=epoch,\n                global_step=global_step,\n                source_lang=_SOURCE_LANGUAGE,\n                target_lang=_TARGET_LANGUAGE,\n                max_length=_MAX_LENGTH,\n                device=_DEVICE,\n            )\n\n            if validation_results and validation_results.get(\n                'validation_completed', False\n            ):\n                training_stats['epoch_validations'].append(\n                    validation_results\n                )\n                training_stats['dscd_quality_history'].append(\n                    validation_results.get('dscd_quality_score', 0.0)\n                )\n                training_stats['asbn_domain_accuracy_history'].append(\n                    validation_results.get('asbn_domain_accuracy', 0.0)\n                )\n                training_stats['trg_explanation_history'].append(\n                    validation_results.get('trg_total_explanations', 0)\n                )\n\n                try:\n                    dscd = (\n                        model.module.dscd\n                        if hasattr(model, 'module')\n                        else getattr(model, 'dscd', None)\n                    )\n\n                    lock = None\n                    if dscd is not None:\n                        if hasattr(dscd, 'buffer_lock'):\n                            lock = dscd.buffer_lock\n                        elif hasattr(dscd, 'clustering_lock'):\n                            lock = dscd.clustering_lock\n\n                    if dscd is not None:\n                        if lock:\n                            with lock:\n                                total_tokens = len(dscd.prototype_stores)\n                        else:\n                            total_tokens = len(dscd.prototype_stores)\n\n                        multi_sense = validation_results.get(\n                            'dscd_multi_sense_tokens', 0\n                        )\n                        ratio = (\n                            multi_sense / total_tokens\n                            if total_tokens > 0\n                            else 0.0\n                        )\n                        training_stats['multi_sense_ratio_history'].append(\n                            ratio\n                        )\n                    else:\n                        training_stats['multi_sense_ratio_history'].append(\n                            0.0\n                        )\n                except Exception:\n                    training_stats['multi_sense_ratio_history'].append(0.0)\n            else:\n                print(\"[TRAIN] Validation incomplete\")\n\n        except Exception as e:\n            print(f\"[TRAIN] Epoch validation failed: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    print(f\"\\n{'='*80}\")\n    print(\"TRAINING COMPLETE - SAVING FINAL CHECKPOINT\")\n    print(f\"{'='*80}\")\n\n    try:\n        checkpoint_path = Path(\"/kaggle/working/tatn_final.pt\")\n\n        core_model = model.module if hasattr(model, 'module') else model\n        \n        while hasattr(core_model, 'module'):\n            core_model = core_model.module\n\n        dscd_state = {}\n        try:\n            if hasattr(core_model, 'dscd'):\n                try:\n                    dscd_state = core_model.dscd.state_dict()\n                except Exception:\n                    dscd_state = {}\n        except Exception:\n            dscd_state = {}\n\n        checkpoint_data = {\n            'epochs_trained': epochs,\n            'global_steps': global_step,\n            'final_train_loss': training_stats['epoch_losses'][-1]\n            if training_stats['epoch_losses']\n            else 0.0,\n            'model_state_dict': core_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scaler_state_dict': scaler.state_dict()\n            if scaler is not None\n            else None,\n            'training_stats': training_stats,\n            'dscd_state': dscd_state,\n            'config': {\n                'SPAN_THRESHOLD': globals().get('SPAN_THRESHOLD', 0.12),\n                'TAU_LOW': globals().get('TAU_LOW', 0.15),\n                'LAMBDA_ASBN': globals().get('LAMBDA_ASBN', 0.05),\n                'LAMBDA_DSCD': globals().get('LAMBDA_DSCD', 0.15),\n                'TRG_TEMPERATURE': globals().get('TRG_TEMPERATURE', 1.0),\n                'PERIODIC_DISCOVERY_FREQUENCY': _PERIODIC_DISCOVERY_FREQUENCY,\n                'NUM_EPOCHS': epochs,\n                'BATCH_SIZE': _BATCH_SIZE,\n                'LEARNING_RATE': optimizer.param_groups[0]['lr']\n                if optimizer and optimizer.param_groups\n                else 0.0,\n            },\n        }\n\n        torch.save(checkpoint_data, checkpoint_path)\n\n        file_size_mb = checkpoint_path.stat().st_size / (1024**2)\n\n        print(\"\\nFINAL CHECKPOINT SAVED\")\n        print(f\"   Path: {checkpoint_path}\")\n        print(f\"   Size: {file_size_mb:.2f} MB\")\n        print(f\"   Epochs trained: {epochs}\")\n        print(f\"   Global steps: {global_step}\")\n        print(\n            f\"   Final train loss: \"\n            f\"{training_stats['epoch_losses'][-1] if training_stats['epoch_losses'] else 0.0:.4f}\"\n        )\n        print(f\"{'='*80}\\n\")\n\n    except Exception as e:\n        print(f\"FINAL CHECKPOINT SAVE FAILED: {type(e).__name__}\")\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TRAINING COMPLETED - FINAL SUMMARY\")\n    print(\"=\" * 80)\n\n    processed_batches = (\n        training_stats[\"batches_processed\"]\n        - training_stats[\"skipped_batches\"]\n    )\n    expected_updates = max(\n        1,\n        math.floor(processed_batches / max(1, accumulation_steps)),\n    )\n    success_rate = (\n        100.0\n        * training_stats[\"optimizer_updates\"]\n        / expected_updates\n        if expected_updates > 0\n        else 0.0\n    )\n\n    print(f\"[TRAIN] Success Rate: {success_rate:.1f}%\")\n    print(f\"[TRAIN] Total Steps: {global_step}\")\n    print(\n        f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\"\n    )\n\n    if training_stats['dscd_quality_history']:\n        print(\"\\n[TRAIN] DSCD Quality Score Trend:\")\n        for i, score in enumerate(training_stats['dscd_quality_history'], 1):\n            print(f\"  Epoch {i}: {score:.1%}\")\n\n    if training_stats['asbn_domain_accuracy_history']:\n        print(\"\\n[TRAIN] ASBN Domain Accuracy Trend:\")\n        for i, acc in enumerate(\n            training_stats['asbn_domain_accuracy_history'], 1\n        ):\n            print(f\"  Epoch {i}: {acc:.1%}\")\n\n    if training_stats['trg_explanation_history']:\n        print(\"\\n[TRAIN] TRG Explanation Count Trend:\")\n        for i, count in enumerate(\n            training_stats['trg_explanation_history'], 1\n        ):\n            print(f\"  Epoch {i}: {count} explanations\")\n\n    print(\"=\" * 80)\n    return model\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 7: Training loop ready (PURE UNSUPERVISED) - FIXED\")\nprint(\"=\" * 80)\nprint(\"FIXES APPLIED:\")\nprint(\"=\" * 80)\nprint(\" F1-F8: (Previous fixes)\")\nprint(\" F9: CRITICAL: Changed discovery frequency from 3000 → 50 steps\")\nprint(\" F10: Synced with Cell 6 discovery frequency\")\nprint(\" F11: Discovery now happens DURING training (not just at end)\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"coTb4Fi4H4J4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: INFERENCE PIPELINE (PURE UNSUPERVISED) - FIXED\n# ==============================================================================\n\nimport os\nimport time\nimport math\nimport torch\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\nfrom collections import defaultdict\nimport threading\nimport gc\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\n    _TARGET_LANGUAGE = str(TARGET_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept (NameError, ValueError, TypeError):\n    _MAX_LENGTH = 48\n\ntry:\n    _DEVICE = DEVICE\nexcept (NameError, TypeError):\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and (torch.cuda.device_count() > 1)\n\ntry:\n    _REAL_AMB_SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _REAL_AMB_SPAN_THRESHOLD = 0.12\n\ntry:\n    _REAL_AMB_UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _REAL_AMB_UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ntry:\n    _M2M100_EN_TOKEN_ID = int(M2M100_EN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_EN_TOKEN_ID = 128022\n\ntry:\n    _M2M100_BN_TOKEN_ID = int(M2M100_BN_TOKEN_ID)\nexcept (NameError, ValueError, TypeError):\n    _M2M100_BN_TOKEN_ID = 128025\n\n_SUBWORD_PUNCT_SET = set(\".,!?;:()[]{}\\\"'-\")\n\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                for token, store in dscd.prototype_stores.items():\n                    try:\n                        if store.size() >= 2:\n                            clean_token = (\n                                str(token)\n                                .replace('▁', '')\n                                .replace('Ġ', '')\n                                .replace('##', '')\n                                .replace(' ', '')\n                                .strip()\n                                .lower()\n                            )\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            for token, store in dscd.prototype_stores.items():\n                try:\n                    if store.size() >= 2:\n                        clean_token = (\n                            str(token)\n                            .replace('▁', '')\n                            .replace('Ġ', '')\n                            .replace('##', '')\n                            .replace(' ', '')\n                            .strip()\n                            .lower()\n                        )\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\n\nclass InferenceStatistics:\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.reset()\n\n    def reset(self):\n        with self._lock:\n            self.total_inferences = 0\n            self.successful_translations = 0\n            self.failed_translations = 0\n            self.total_explanations = 0\n            self.high_confidence_explanations = 0\n            self.low_confidence_explanations = 0\n            self.total_confidence = 0.0\n            self.dscd_homographs_explained = set()\n            self.reference_homographs_explained = set()\n            self.avg_span = 0.0\n            self.avg_uncertainty = 0.0\n            self.dscd_empty_warnings = 0\n            self.token_counts = defaultdict(int)\n            self.token_confidences = defaultdict(list)\n\n    def record_inference(self, result: Dict[str, Any], dscd_homographs: Optional[set] = None):\n        with self._lock:\n            self.total_inferences += 1\n\n            if result.get('translation') and result['translation'] != \"ERROR DURING TRANSLATION\":\n                self.successful_translations += 1\n            else:\n                self.failed_translations += 1\n\n            explanations = result.get('explanations', [])\n            self.total_explanations += len(explanations)\n\n            for exp in explanations:\n                try:\n                    conf = exp.get('confidence', 0.5)\n                    self.total_confidence += float(conf)\n\n                    if conf >= 0.65:\n                        self.high_confidence_explanations += 1\n                    elif conf < 0.4:\n                        self.low_confidence_explanations += 1\n\n                    word = str(exp.get('ambiguous_word', '')).strip()\n                    clean_word = (\n                        word.replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace(' ', '')\n                        .lower()\n                    )\n\n                    self.token_counts[clean_word] += 1\n                    self.token_confidences[clean_word].append(float(conf))\n\n                    if dscd_homographs and clean_word in dscd_homographs:\n                        self.dscd_homographs_explained.add(clean_word)\n\n                    if clean_word in _HOMOGRAPH_REFERENCE_LIST:\n                        self.reference_homographs_explained.add(clean_word)\n\n                    self.avg_span += float(exp.get('span', 0.0))\n                    self.avg_uncertainty += float(exp.get('uncertainty', 0.0))\n\n                except Exception:\n                    pass\n\n    def get_summary(self) -> Dict[str, Any]:\n        with self._lock:\n            total_exp = max(self.total_explanations, 1)\n\n            unique_tokens = len(self.token_counts)\n            diversity_ratio = unique_tokens / total_exp if total_exp > 0 else 0.0\n\n            return {\n                'total_inferences': self.total_inferences,\n                'successful_translations': self.successful_translations,\n                'failed_translations': self.failed_translations,\n                'success_rate': self.successful_translations / max(self.total_inferences, 1),\n                'total_explanations': self.total_explanations,\n                'explanations_per_inference': self.total_explanations / max(self.total_inferences, 1),\n                'high_confidence_rate': self.high_confidence_explanations / total_exp,\n                'low_confidence_rate': self.low_confidence_explanations / total_exp,\n                'avg_confidence': self.total_confidence / total_exp,\n                'avg_span': self.avg_span / total_exp,\n                'avg_uncertainty': self.avg_uncertainty / total_exp,\n                'dscd_homographs_explained': list(self.dscd_homographs_explained),\n                'reference_homographs_explained': list(self.reference_homographs_explained),\n                'dscd_empty_warnings': self.dscd_empty_warnings,\n                'unique_tokens_explained': unique_tokens,\n                'diversity_ratio': diversity_ratio,\n            }\n\n    def print_summary(self):\n        summary = self.get_summary()\n        print(\"\\n\" + \"=\" * 80)\n        print(\"INFERENCE STATISTICS SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Total inferences: {summary['total_inferences']}\")\n        print(f\"Success rate: {summary['success_rate']:.1%}\")\n        print(f\"Total explanations: {summary['total_explanations']}\")\n        print(f\"Explanations per inference: {summary['explanations_per_inference']:.2f}\")\n        print(f\"Unique tokens explained: {summary['unique_tokens_explained']}\")\n        print(f\"Diversity ratio: {summary['diversity_ratio']:.2%}\")\n        print(f\"Avg confidence: {summary['avg_confidence']:.3f}\")\n        print(f\"High confidence rate: {summary['high_confidence_rate']:.1%}\")\n        print(f\"Avg span: {summary['avg_span']:.3f}\")\n        print(f\"Avg uncertainty: {summary['avg_uncertainty']:.3f}\")\n\n        if summary['dscd_homographs_explained']:\n            print(f\"\\nDSCD homographs explained ({len(summary['dscd_homographs_explained'])}):\")\n            print(f\"  {', '.join(summary['dscd_homographs_explained'])}\")\n\n        if summary['reference_homographs_explained']:\n            print(f\"\\nReference homographs explained ({len(summary['reference_homographs_explained'])}):\")\n            print(f\"  {', '.join(summary['reference_homographs_explained'])}\")\n\n        if summary['dscd_empty_warnings'] > 0:\n            print(f\"\\nDSCD empty warnings: {summary['dscd_empty_warnings']}\")\n        print(\"=\" * 80 + \"\\n\")\n\n\n_INFERENCE_STATS = InferenceStatistics()\n\n\ndef _to_device_batch(enc: Any, device: torch.device):\n    try:\n        if hasattr(enc, \"to\"):\n            return enc.to(device)\n    except Exception:\n        pass\n\n    if isinstance(enc, dict):\n        out = {}\n        for k, v in enc.items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                elif isinstance(v, dict):\n                    out[k] = _to_device_batch(v, device)\n                elif isinstance(v, (list, tuple)):\n                    out[k] = [\n                        t.to(device) if isinstance(t, torch.Tensor) else t\n                        for t in v\n                    ]\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n\n    return enc\n\n\ndef _extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    if raw_out is None:\n        return {}\n\n    if isinstance(raw_out, dict):\n        if \"dscd_outputs\" in raw_out and isinstance(raw_out[\"dscd_outputs\"], dict):\n            return raw_out[\"dscd_outputs\"]\n        if \"dscd\" in raw_out and isinstance(raw_out[\"dscd\"], dict):\n            return raw_out[\"dscd\"]\n        if \"explanations\" in raw_out or \"proto_probs\" in raw_out:\n            return raw_out\n\n        for key in (\"dscd_outputs\", \"dscd\", \"dscd_out\"):\n            if key in raw_out and isinstance(raw_out[key], dict):\n                return raw_out[key]\n\n        return raw_out\n\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                return _extract_dscd_outputs(item)\n\n    return {}\n\n\ndef _get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    if not dscd:\n        return []\n\n    expl = dscd.get(\"explanations\", None)\n    if expl is None:\n        for alt in (\"explanations_per_sentence\", \"trg_explanations\", \"exps\"):\n            if alt in dscd:\n                expl = dscd[alt]\n                break\n\n    if expl is None:\n        return []\n\n    if isinstance(expl, list):\n        if len(expl) > 0 and isinstance(expl[0], dict):\n            return [expl]\n        if len(expl) > 0 and isinstance(expl[0], list):\n            return expl\n\n    return []\n\n\ndef _is_subword_token(token: str) -> bool:\n    if not token or len(token.strip()) == 0:\n        return True\n\n    token = token.strip()\n    if (\n        token.startswith(\"##\")\n        or token.startswith(\"▁\")\n        or token.startswith(\"  \")\n        or token.startswith(\"@@\")\n        or token.startswith(\" \")\n    ):\n        return True\n\n    if len(token) < 2:\n        return True\n\n    if (len(token) == 1 and token in _SUBWORD_PUNCT_SET) or token.isdigit():\n        return True\n\n    return False\n\n\ndef _should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    try:\n        token = expl.get('ambiguous_word', expl.get('token', ''))\n        if not token or not isinstance(token, str):\n            return True\n        \n        span = float(expl.get('span', 0.0))\n        uncertainty = float(expl.get('uncertainty', 0.0))\n\n        if _is_subword_token(str(token)):\n            return True\n\n        if span <= span_th or uncertainty <= u_th:\n            return True\n\n        return False\n    except Exception:\n        return True\n\n\ndef _force_english_bos(tokenizer, mbart_model) -> Optional[int]:\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in (_TARGET_LANGUAGE, \"en_XX\", \"en\", \"eng\"):\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = int(lid)\n                        break\n                except Exception:\n                    continue\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            forced_id = tokenizer.lang_code_to_id.get(_TARGET_LANGUAGE, None)\n            if forced_id is not None:\n                forced_id = int(forced_id)\n    except Exception:\n        forced_id = None\n\n    if forced_id is None:\n        forced_id = _M2M100_EN_TOKEN_ID\n\n    if forced_id is not None and hasattr(mbart_model, \"config\"):\n        try:\n            mbart_model.config.forced_bos_token_id = int(forced_id)\n            mbart_model.config.decoder_start_token_id = int(forced_id)\n        except Exception:\n            if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                print(\"[INF] Could not set forced BOS on mbart config\")\n\n    return forced_id\n\n\ndef _safe_generate(\n    mbart,\n    input_ids=None,\n    encoder_outputs=None,\n    attention_mask=None,\n    max_length=64,\n    num_beams=2,\n    **kwargs,\n):\n    try:\n        if encoder_outputs is not None:\n            return mbart.generate(\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n        else:\n            return mbart.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                **kwargs,\n            )\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            if _DEBUG_DISCOVERY:\n                print(\"[INF] OOM during generation, reducing beam size...\")\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            if encoder_outputs is not None:\n                return mbart.generate(\n                    encoder_outputs=encoder_outputs,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n            else:\n                return mbart.generate(\n                    input_ids,\n                    attention_mask=attention_mask,\n                    max_length=min(max_length, 48),\n                    num_beams=1,\n                    early_stopping=True,\n                    **kwargs,\n                )\n        else:\n            raise\n\n\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n    track_stats: bool = True,\n) -> Dict[str, Any]:\n    device = _DEVICE if device is None else device\n    span_th = _REAL_AMB_SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = _REAL_AMB_UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n        print(f\"\\n[INF] Starting inference:\")\n        print(f\"[INF]   Input: {input_sentence[:60]}\")\n        print(f\"[INF]   Thresholds: span={span_th:.2f}, uncertainty={u_th:.2f}\")\n\n    cleanup_vars = []\n    dscd = None\n    encoder_hidden = None\n    encoder_hidden_adjusted = None\n\n    dscd_homographs = _get_dscd_homographs(model)\n\n    try:\n        try:\n            tokenizer.src_lang = _SOURCE_LANGUAGE\n        except Exception:\n            pass\n\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=_MAX_LENGTH,\n        )\n        enc = _to_device_batch(enc, device)\n        cleanup_vars.append(\"enc\")\n\n        model.eval()\n        core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n        try:\n            trg_system = getattr(core, 'trg_system', None)\n            if trg_system is not None and hasattr(trg_system, 'eval'):\n                trg_system.eval()\n        except Exception:\n            pass\n\n        src_texts = [input_sentence]\n\n        dscd_validated = False\n        try:\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n            if dscd:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        num_stores = len(dscd.prototype_stores)\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if hasattr(store, 'centroids')\n                            and len(store.centroids) >= 2\n                        )\n                else:\n                    num_stores = len(dscd.prototype_stores)\n                    multi_sense = sum(\n                        1\n                        for store in dscd.prototype_stores.values()\n                        if hasattr(store, 'centroids')\n                        and len(store.centroids) >= 2\n                    )\n\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(\n                        f\"[INF] DSCD state: {num_stores} tokens, \"\n                        f\"{multi_sense} multi-sense, {len(dscd_homographs)} discovered\"\n                    )\n\n                if num_stores == 0:\n                    print(\"[INF] CRITICAL WARNING: DSCD prototype stores are EMPTY - run warmup first!\")\n                    if track_stats:\n                        _INFERENCE_STATS.dscd_empty_warnings += 1\n                else:\n                    dscd_validated = True\n        except Exception as e:\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] DSCD validation failed: {e}\")\n\n        with torch.inference_mode():\n            raw_dscd_out: Dict[str, Any] = {}\n\n            try:\n                if not hasattr(core, \"mbart\"):\n                    raise RuntimeError(\"Model backend missing .mbart\")\n\n                mbart = core.mbart\n\n                encoder_outputs_raw = mbart.model.encoder(\n                    input_ids=enc.get(\"input_ids\"),\n                    attention_mask=enc.get(\"attention_mask\"),\n                )\n                cleanup_vars.append(\"encoder_outputs_raw\")\n\n                if hasattr(encoder_outputs_raw, 'last_hidden_state'):\n                    encoder_hidden = encoder_outputs_raw.last_hidden_state\n                elif isinstance(encoder_outputs_raw, tuple):\n                    encoder_hidden = encoder_outputs_raw[0]\n                else:\n                    encoder_hidden = encoder_outputs_raw\n                cleanup_vars.append(\"encoder_hidden\")\n\n                if not isinstance(encoder_hidden, torch.Tensor) or encoder_hidden.dim() != 3:\n                    raise RuntimeError(\n                        f\"Invalid encoder hidden: {type(encoder_hidden)}, \"\n                        f\"shape={encoder_hidden.shape if isinstance(encoder_hidden, torch.Tensor) else 'N/A'}\"\n                    )\n\n                if _DEBUG_DISCOVERY:\n                    print(f\"[INF] Encoder hidden: {encoder_hidden.shape}\")\n\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=src_texts,\n                        )\n                    except TypeError:\n                        raw_dscd_out = core.forward_with_explanations(\n                            enc.get(\"input_ids\"),\n                            enc.get(\"attention_mask\"),\n                            src_texts,\n                        )\n                else:\n                    if _DEBUG_DISCOVERY:\n                        print(\"[INF] forward_with_explanations not found, using forward()\")\n                    out = core.forward(\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        src_texts=src_texts,\n                        labels=None,\n                    )\n                    if isinstance(out, dict):\n                        raw_dscd_out = _extract_dscd_outputs(out)\n\n                dscd_out = _extract_dscd_outputs(raw_dscd_out)\n                if isinstance(raw_dscd_out, dict) and 'sense_augmented_embeddings' in raw_dscd_out:\n                    encoder_hidden_adjusted = raw_dscd_out['sense_augmented_embeddings']\n                elif 'h_augmented' in dscd_out:\n                    encoder_hidden_adjusted = dscd_out['h_augmented']\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n                cleanup_vars.append(\"encoder_hidden_adjusted\")\n\n                if isinstance(encoder_hidden_adjusted, torch.Tensor):\n                    if encoder_hidden_adjusted.shape != encoder_hidden.shape:\n                        if _DEBUG_DISCOVERY:\n                            print(\"[INF] Shape mismatch, using original\")\n                        encoder_hidden_adjusted = encoder_hidden\n                else:\n                    encoder_hidden_adjusted = encoder_hidden\n\n                if _DEBUG_DISCOVERY:\n                    print(\"[INF] DSCD forward completed\")\n\n            except Exception as e:\n                if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n                    print(f\"[INF] DSCD forward error: {e}\")\n                raw_dscd_out = {}\n                if 'encoder_hidden' in locals() and encoder_hidden is not None:\n                    encoder_hidden_adjusted = encoder_hidden\n                else:\n                    encoder_hidden_adjusted = None\n\n            forced_id = _force_english_bos(tokenizer, mbart)\n            orig_use_cache = (\n                getattr(mbart.config, \"use_cache\", None)\n                if hasattr(mbart, \"config\")\n                else None\n            )\n            if hasattr(mbart, \"config\"):\n                try:\n                    mbart.config.use_cache = True\n                except Exception:\n                    pass\n\n            try:\n                if _DEBUG_DISCOVERY:\n                    print(\"[INF] Generating translation...\")\n\n                if encoder_hidden_adjusted is not None and isinstance(\n                    encoder_hidden_adjusted, torch.Tensor\n                ):\n                    encoder_hidden_adjusted = encoder_hidden_adjusted.to(device)\n\n                    from transformers.modeling_outputs import BaseModelOutput\n\n                    encoder_outputs_for_decoder = BaseModelOutput(\n                        last_hidden_state=encoder_hidden_adjusted\n                    )\n\n                    generated = _safe_generate(\n                        mbart,\n                        encoder_outputs=encoder_outputs_for_decoder,\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=2,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                    )\n                else:\n                    generated = _safe_generate(\n                        mbart,\n                        input_ids=enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=2,\n                        pad_token_id=getattr(tokenizer, \"pad_token_id\", None),\n                        forced_bos_token_id=forced_id,\n                    )\n                cleanup_vars.append(\"generated\")\n\n                translation = (\n                    tokenizer.decode(generated[0], skip_special_tokens=True)\n                    if generated is not None and len(generated) > 0\n                    else \"\"\n                )\n\n                if _DEBUG_DISCOVERY:\n                    print(f\"[INF] Translation: {translation[:60]}\")\n\n            finally:\n                if hasattr(mbart, \"config\") and orig_use_cache is not None:\n                    try:\n                        mbart.config.use_cache = orig_use_cache\n                    except Exception:\n                        pass\n\n            if _DEBUG_DISCOVERY:\n                print(\"[INF] Extracting explanations...\")\n\n            dscd_out = _extract_dscd_outputs(raw_dscd_out)\n            explanations_list = _get_explanations_list(dscd_out)\n            sentence_explanations = (\n                explanations_list[0]\n                if (isinstance(explanations_list, list) and len(explanations_list) > 0)\n                else []\n            )\n\n            if _DEBUG_DISCOVERY:\n                print(f\"[INF] Raw explanations: {len(sentence_explanations)}\")\n\n            def _is_real_ambiguity(e: Dict[str, Any]) -> bool:\n                try:\n                    s = float(e.get(\"span\", 0.0))\n                    u = float(e.get(\"uncertainty\", 0.0))\n                    return (s > span_th) or (u > u_th)\n                except Exception:\n                    return False\n\n            real_amb_count = 0\n            out_explanations: List[Dict[str, Any]] = []\n            filtered_count = 0\n\n            quality_metrics = {\n                'total_raw_explanations': len(sentence_explanations)\n                if isinstance(sentence_explanations, list)\n                else 0,\n                'filtered_explanations': 0,\n                'high_confidence_count': 0,\n                'low_confidence_count': 0,\n                'avg_confidence': 0.0,\n                'avg_span': 0.0,\n                'avg_uncertainty': 0.0,\n            }\n\n            confidences: List[float] = []\n            spans: List[float] = []\n            uncertainties: List[float] = []\n\n            if isinstance(sentence_explanations, list):\n                for ex in sentence_explanations:\n                    try:\n                        word = ex.get('ambiguous_word', ex.get('token', ''))\n                        if isinstance(word, str):\n                            clean_word = (\n                                word.replace('▁', '')\n                                .replace('Ġ', '')\n                                .replace('##', '')\n                                .replace(' ', '')\n                                .strip()\n                            )\n                            if clean_word:\n                                ex['ambiguous_word'] = clean_word\n\n                        if _should_filter_explanation(ex, span_th, u_th):\n                            filtered_count += 1\n                            continue\n\n                        is_real = _is_real_ambiguity(ex)\n                        if is_real:\n                            real_amb_count += 1\n\n                        confidence = ex.get('confidence', None)\n                        if confidence is None:\n                            s = float(ex.get('span', 0.0))\n                            u = float(ex.get('uncertainty', 0.0))\n                            confidence = max(s, u)\n                        confidence = float(confidence)\n\n                        confidences.append(confidence)\n                        spans.append(float(ex.get('span', 0.0)))\n                        uncertainties.append(float(ex.get('uncertainty', 0.0)))\n\n                        if confidence >= 0.65:\n                            quality_metrics['high_confidence_count'] += 1\n                        elif confidence < 0.4:\n                            quality_metrics['low_confidence_count'] += 1\n\n                        out_explanations.append(\n                            {\n                                \"ambiguous_word\": ex.get(\n                                    \"ambiguous_word\", ex.get(\"token\", \"N/A\")\n                                ),\n                                \"position\": ex.get(\n                                    \"position\", ex.get(\"token_idx\", \"N/A\")\n                                ),\n                                \"explanation\": ex.get(\"explanation\", \"\")\n                                or ex.get(\"explain\", \"\")\n                                or \"\",\n                                \"uncertainty\": float(ex.get(\"uncertainty\", 0.0)),\n                                \"span\": float(ex.get(\"span\", 0.0)),\n                                \"confidence\": confidence,\n                                \"is_real_amb\": bool(is_real),\n                            }\n                        )\n                    except Exception:\n                        continue\n\n            quality_metrics['filtered_explanations'] = filtered_count\n            if confidences:\n                quality_metrics['avg_confidence'] = sum(confidences) / len(confidences)\n                quality_metrics['avg_span'] = sum(spans) / len(spans)\n                quality_metrics['avg_uncertainty'] = sum(uncertainties) / len(uncertainties)\n\n            if _DEBUG_DISCOVERY:\n                print(\n                    f\"[INF] Final: {len(out_explanations)} explanations \"\n                    f\"(filtered: {filtered_count})\"\n                )\n\n            result = {\n                \"input_sentence\": input_sentence,\n                \"translation\": translation,\n                \"ambiguous_words_detected\": int(real_amb_count),\n                \"explanations\": out_explanations,\n                \"quality_metrics\": quality_metrics,\n                \"dscd_validated\": dscd_validated,\n            }\n\n            if track_stats:\n                _INFERENCE_STATS.record_inference(result, dscd_homographs=dscd_homographs)\n\n            return result\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY or _VERBOSE_LOGGING:\n            print(f\"[INF] ERROR: {type(e).__name__}: {str(e)[:200]}\")\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n        error_result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"ERROR DURING TRANSLATION\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"quality_metrics\": {},\n            \"dscd_validated\": False,\n            \"error\": str(e)[:200],\n        }\n\n        if track_stats:\n            _INFERENCE_STATS.record_inference(error_result, dscd_homographs=dscd_homographs)\n\n        return error_result\n\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"আমি কল বন্ধ করেছি।\",\n            \"কাল আমি বই কিনব।\",\n            \"পাতা ঝরে পড়েছে।\",\n            \"তিনি ব্যাংক গেছেন।\",\n            \"আজ ভাল আবহাওয়া।\",\n        ]\n\n    print(\"=\" * 80)\n    print(\"TATN DEMO: Translation + Explanations\")\n    print(\"=\" * 80)\n\n    _INFERENCE_STATS.reset()\n\n    for s in sentences:\n        print(f\"\\nInput: {s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n        print(\"Translation:\", res.get(\"translation\", \"\"))\n        print(\"Ambiguous words detected:\", res.get(\"ambiguous_words_detected\", 0))\n\n        quality = res.get(\"quality_metrics\", {})\n        if quality:\n            print(\n                f\"Quality: conf={quality.get('avg_confidence', 0):.3f}, \"\n                f\"high={quality.get('high_confidence_count', 0)}, \"\n                f\"low={quality.get('low_confidence_count', 0)}\"\n            )\n\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(\n                    f\"  {idx}. '{ex['ambiguous_word']}' \"\n                    f\"pos={ex['position']} conf={ex.get('confidence', 0):.3f}\"\n                )\n                print(\"     \", ex.get(\"explanation\", \"\")[:200])\n        else:\n            print(\"  No explanations\")\n\n    print(\"=\" * 80)\n    _INFERENCE_STATS.print_summary()\n\n\ndef dscd_discovery_warmup(\n    model,\n    tokenizer,\n    num_sents: int = 8000,\n    batch_size: int = 64,\n    max_len: Optional[int] = None,\n):\n    if max_len is None:\n        max_len = _MAX_LENGTH\n\n    core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    try:\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            print(\"[WARMUP] Model has no dscd component\")\n            return\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"[WARMUP] Starting DSCD discovery warmup\")\n        print(\"=\" * 80)\n\n        orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n        orig_n_min = getattr(dscd, \"n_min\", None)\n        orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                initial_token_count = len(dscd.prototype_stores)\n        else:\n            initial_token_count = len(dscd.prototype_stores)\n\n        print(f\"[WARMUP] Initial prototype stores: {initial_token_count}\")\n\n        try:\n            if hasattr(dscd, \"enable_training_clustering\"):\n                dscd.enable_training_clustering = True\n            if hasattr(dscd, \"n_min\"):\n                dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n            if hasattr(dscd, \"buffer_size\"):\n                dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n        except Exception:\n            pass\n\n        texts: List[str] = []\n        try:\n            if \"load_and_preprocess_optimized\" in globals():\n                pairs = load_and_preprocess_optimized(num_sents)\n                texts = [bn for (bn, _) in pairs][:num_sents]\n            else:\n                base = [\n                    \"আমি কল বন্ধ করেছি।\",\n                    \"কাল আমি বই কিনব।\",\n                    \"পাতা ঝরে পড়েছে।\",\n                    \"তিনি ব্যাংক গেছেন।\",\n                ]\n                while len(texts) < num_sents:\n                    texts.extend(base)\n                texts = texts[:num_sents]\n        except Exception:\n            texts = [\"আমি কল বন্ধ করেছি।\"] * num_sents\n\n        processed = 0\n        core.eval()\n\n        print(f\"\\n[WARMUP] Processing {len(texts)} sentences (batch={batch_size})...\")\n\n        start_time = time.time()\n        last_print = start_time\n\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                try:\n                    enc = tokenizer(\n                        batch,\n                        return_tensors=\"pt\",\n                        padding=True,\n                        truncation=True,\n                        max_length=max_len,\n                    )\n                    enc = _to_device_batch(enc, _DEVICE)\n\n                    if hasattr(core, \"forward_with_explanations\"):\n                        core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=batch,\n                        )\n                    else:\n                        core.mbart.model.encoder(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                        )\n\n                    processed += len(batch)\n\n                    current_time = time.time()\n                    if (i // batch_size) % 10 == 0 or (current_time - last_print) > 5:\n                        elapsed = current_time - start_time\n                        rate = processed / elapsed if elapsed > 0 else 0\n                        eta = (len(texts) - processed) / rate if rate > 0 else 0\n                        print(\n                            f\"[WARMUP] {processed}/{len(texts)} \"\n                            f\"({processed/len(texts)*100:.1f}%) | \"\n                            f\"{rate:.1f} sent/s | ETA {eta:.0f}s\"\n                        )\n                        last_print = current_time\n\n                    del enc\n\n                except Exception as e:\n                    print(\n                        f\"[WARMUP] Batch {i//batch_size} failed: {str(e)[:100]}\"\n                    )\n                    continue\n\n        total_time = time.time() - start_time\n        print(\n            f\"\\n[WARMUP] Completed in {total_time:.1f}s \"\n            f\"({processed/total_time:.1f} sent/s)\"\n        )\n        print(\"-\" * 80)\n\n        try:\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            num_types = len(stores)\n            total_protos = (\n                sum(store.size() for store in stores.values()) if stores else 0\n            )\n            multi = (\n                sum(1 for store in stores.values() if store.size() >= 2)\n                if stores\n                else 0\n            )\n\n            print(\"[WARMUP] Summary:\")\n            print(f\"  - Initial token types: {initial_token_count}\")\n            print(f\"  - Final token types: {num_types}\")\n            print(f\"  - Growth: +{num_types - initial_token_count}\")\n            print(f\"  - Total prototypes: {total_protos}\")\n            print(f\"  - Multi-sense tokens: {multi}\")\n\n            if num_types > 0:\n                print(f\"  - Multi-sense ratio: {multi/num_types:.1%}\")\n\n            if hasattr(core, 'asbn') and hasattr(core.asbn, 'get_detailed_stats'):\n                try:\n                    asbn_stats = core.asbn.get_detailed_stats()\n                    print(f\"\\n[WARMUP] ASBN Stats:\")\n                    print(f\"  - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n                    print(f\"  - Source accuracy: {asbn_stats.get('source_accuracy', 0):.2%}\")\n                    print(f\"  - Target accuracy: {asbn_stats.get('target_accuracy', 0):.2%}\")\n                except Exception:\n                    pass\n\n            dscd_homographs = _get_dscd_homographs(model)\n\n            print(f\"\\n[WARMUP] Discovered Homographs: {len(dscd_homographs)}\")\n            if dscd_homographs:\n                print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n            reference_found = dscd_homographs.intersection(_HOMOGRAPH_REFERENCE_LIST)\n\n            print(f\"\\n[WARMUP] Reference List Comparison:\")\n            print(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n            print(f\"  - Found in DSCD: {len(reference_found)}\")\n            print(\n                f\"  - Coverage: {len(reference_found)/len(_HOMOGRAPH_REFERENCE_LIST):.1%}\"\n            )\n\n            if num_types == initial_token_count:\n                print(\"\\n[WARMUP] CRITICAL: NO NEW PROTOTYPES CREATED - check DSCD training mode\")\n            elif num_types == 0:\n                print(\"\\n[WARMUP] CRITICAL: NO PROTOTYPES IN STORES - DSCD not working\")\n            elif len(reference_found) < len(_HOMOGRAPH_REFERENCE_LIST) // 2:\n                print(\"\\n[WARMUP] WARNING: < 50% reference coverage - may need more data\")\n            else:\n                print(\"\\n[WARMUP] SUCCESS - DSCD ready for inference\")\n\n        except Exception as e:\n            print(f\"[WARMUP] Validation failed: {e}\")\n\n    finally:\n        try:\n            if dscd is not None:\n                if hasattr(dscd, \"enable_training_clustering\"):\n                    dscd.enable_training_clustering = orig_enable\n                if hasattr(dscd, \"n_min\") and orig_n_min is not None:\n                    dscd.n_min = orig_n_min\n                if hasattr(dscd, \"buffer_size\") and orig_buffer is not None:\n                    dscd.buffer_size = orig_buffer\n        except Exception:\n            pass\n\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception:\n            pass\n\n        try:\n            if gc.isenabled():\n                gc.collect()\n        except Exception:\n            pass\n\n        print(\"=\" * 80 + \"\\n\")\n\n\ndef load_checkpoint_for_resume(\n    model: torch.nn.Module, optimizer, checkpoint_path: str\n) -> Tuple[bool, int, int, float]:\n    if not os.path.exists(checkpoint_path):\n        print(f\"[CHECKPOINT] Not found: {checkpoint_path}\")\n        return False, 0, 0, 0.0\n\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=_DEVICE)\n    except Exception as e:\n        print(f\"[CHECKPOINT] Load failed: {e}\")\n        return False, 0, 0, 0.0\n\n    core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n    state = ckpt.get(\"model_state_dict\", ckpt)\n    try:\n        core.load_state_dict(state, strict=False)\n    except Exception as e:\n        print(f\"[CHECKPOINT] model.load_state_dict failed: {e}\")\n\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n                    new_state[new_key] = v\n                core.load_state_dict(new_state, strict=False)\n        except Exception:\n            pass\n\n    try:\n        if optimizer is not None and \"optimizer_state_dict\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n    except Exception as e:\n        print(f\"[CHECKPOINT] optimizer.load_state_dict failed: {e}\")\n\n    try:\n        if \"dscd_state\" in ckpt and ckpt[\"dscd_state\"]:\n            dscd_state = ckpt[\"dscd_state\"]\n\n            print(\"[CHECKPOINT] Restoring DSCD...\")\n            dscd = core.dscd if hasattr(core, 'dscd') else None\n\n            if dscd and hasattr(dscd, 'load_state_dict'):\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        dscd.load_state_dict(dscd_state)\n                        num_tokens = len(dscd.prototype_stores)\n                        total_protos = sum(\n                            store.size() for store in dscd.prototype_stores.values()\n                        )\n                        multi_sense = sum(\n                            1\n                            for store in dscd.prototype_stores.values()\n                            if store.size() >= 2\n                        )\n                else:\n                    dscd.load_state_dict(dscd_state)\n                    num_tokens = len(dscd.prototype_stores)\n                    total_protos = sum(\n                        store.size() for store in dscd.prototype_stores.values()\n                    )\n                    multi_sense = sum(\n                        1\n                        for store in dscd.prototype_stores.values()\n                        if store.size() >= 2\n                    )\n\n                print(\"[CHECKPOINT] DSCD restored:\")\n                print(f\"  - Tokens: {num_tokens}\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multi_sense}\")\n\n                if num_tokens == 0 or total_protos == 0:\n                    print(\n                        \"[CHECKPOINT] CRITICAL WARNING: DSCD state empty - run warmup before inference!\"\n                    )\n            else:\n                print(\"[CHECKPOINT] Model has no dscd.load_state_dict\")\n        else:\n            print(\"[CHECKPOINT] No DSCD state in checkpoint\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] DSCD restore failed: {e}\")\n\n    epoch = int(ckpt.get(\"epochs_trained\", ckpt.get(\"epoch\", 0)))\n    step = int(\n        ckpt.get(\n            \"global_steps\", ckpt.get(\"global_step\", ckpt.get(\"step\", 0))\n        )\n    )\n    avg_loss = float(\n        ckpt.get(\n            \"final_train_loss\",\n            ckpt.get(\"avg_epoch_loss\", ckpt.get(\"avg_loss\", 0.0)),\n        )\n    )\n\n    print(f\"[CHECKPOINT] Loaded: epoch={epoch} step={step} loss={avg_loss:.6f}\")\n    return True, epoch, step, avg_loss\n\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 8: Inference pipeline ready (PURE UNSUPERVISED) - FIXED\")\nprint(\"=\" * 80)\nprint(\"Configuration:\")\nprint(f\"  - Span threshold: {_REAL_AMB_SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {_REAL_AMB_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"7Dxg7ck0H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9: COMPREHENSIVE TESTING & EVALUATION (PURE DATA-DRIVEN) - FIXED\n# ==============================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport time\nimport functools\nfrom collections import defaultdict\n\ntry:\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept (NameError, TypeError):\n    _USE_MULTI_GPU = torch.cuda.is_available() and torch.cuda.device_count() > 1\n\ntry:\n    _SOURCE_LANGUAGE = str(SOURCE_LANGUAGE)\nexcept (NameError, TypeError):\n    _SOURCE_LANGUAGE = \"bn\"\n\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept (NameError, TypeError):\n    _VERBOSE_LOGGING = False\n\ntry:\n    _DEBUG_DISCOVERY = bool(DEBUG_DISCOVERY)\nexcept (NameError, TypeError):\n    _DEBUG_DISCOVERY = False\n\ntry:\n    _DEBUG_TIMING = bool(DEBUG_TIMING)\nexcept (NameError, TypeError):\n    _DEBUG_TIMING = False\n\ntry:\n    _SPAN_THRESHOLD = float(SPAN_THRESHOLD)\nexcept (NameError, ValueError, TypeError):\n    _SPAN_THRESHOLD = 0.12\n\ntry:\n    _UNCERTAINTY_THRESHOLD = float(TAU_LOW)\nexcept (NameError, ValueError, TypeError):\n    _UNCERTAINTY_THRESHOLD = 0.15\n\ntry:\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in HOMOGRAPH_REFERENCE_LIST_BN)\nexcept (NameError, TypeError):\n    _HOMOGRAPH_REFERENCE_LIST = {\n        \"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\",\n        \"পানি\", \"দল\", \"বাজার\", \"নাম\", \"কথা\", \"বই\", \"ঘর\", \"মন\", \"হাত\"\n    }\n    _HOMOGRAPH_REFERENCE_LIST = set(str(w).lower() for w in _HOMOGRAPH_REFERENCE_LIST)\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return 0\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                return len(stores)\n        else:\n            stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            return len(stores)\n    except Exception:\n        return 0\n\ndef _get_dscd_homographs(model: torch.nn.Module) -> set:\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return set()\n\n        if hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        homographs = set()\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n                for token, store in prototype_stores.items():\n                    try:\n                        if hasattr(store, 'size') and store.size() >= 2:\n                            clean_token = (\n                                str(token)\n                                .replace('▁', '')\n                                .replace('Ġ', '')\n                                .replace('##', '')\n                                .replace(' ', '')\n                                .strip()\n                                .lower()\n                            )\n                            homographs.add(clean_token)\n                    except Exception:\n                        continue\n        else:\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            for token, store in prototype_stores.items():\n                try:\n                    if hasattr(store, 'size') and store.size() >= 2:\n                        clean_token = (\n                            str(token)\n                            .replace('▁', '')\n                            .replace('Ġ', '')\n                            .replace('##', '')\n                            .replace(' ', '')\n                            .strip()\n                            .lower()\n                        )\n                        homographs.add(clean_token)\n                except Exception:\n                    continue\n\n        return homographs\n    except Exception:\n        return set()\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            return\n\n        lock = None\n        if hasattr(dscd, 'buffer_lock'):\n            lock = dscd.buffer_lock\n        elif hasattr(dscd, 'clustering_lock'):\n            lock = dscd.clustering_lock\n\n        if lock:\n            with lock:\n                prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n        else:\n            prototype_stores = dict(getattr(dscd, \"prototype_stores\", {}) or {})\n\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = sum(getattr(store, \"counts\", []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = len(getattr(store, \"centroids\", []))\n            except Exception:\n                n_protos = 0\n            cluster_info.append({\n                'token': token,\n                'count': total_count,\n                'protos': n_protos,\n                'mu': getattr(store, \"mu\", 0.0),\n                'tau': getattr(store, \"tau\", 0.0)\n            })\n\n        cluster_info.sort(key=lambda x: x['count'], reverse=True)\n\n        print(f\"\\n[CLUSTER] Top {min(top_n, len(cluster_info))} clusters:\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<15}{'Count':<12}{'Protos':<10}{'Mu':<15}{'Tau':<12}\")\n        print(\"-\" * 90)\n\n        for rank, info in enumerate(cluster_info[:top_n], 1):\n            token_str = str(info['token'])\n            token_display = token_str[:12] if len(token_str) > 12 else token_str\n            print(\n                f\"{rank:<6}{token_display:<15}{info['count']:<12}{info['protos']:<10}\"\n                f\"{info['mu']:<15.6f}{info['tau']:<12.6f}\"\n            )\n\n        print(\"-\" * 90)\n\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[CLUSTER] Error: {str(e)[:100]}\")\n\ndef _timed(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if _DEBUG_TIMING:\n            start = time.time()\n            result = func(*args, **kwargs)\n            elapsed = time.time() - start\n            print(f\"[TIMING] {func.__name__}: {elapsed:.2f}s\")\n            return result\n        else:\n            return func(*args, **kwargs)\n    return wrapper\n\n@torch.inference_mode()\n@_timed\ndef comprehensive_post_training_testing(\n    model: torch.nn.Module,\n    tokenizer,\n    run_warmup: bool = True,\n    compare_baseline: bool = False,\n    baseline_metrics: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Pure Data-Driven)\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str, str, List[str]]] = [\n        (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\", [\"কল\"]),\n        (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\", [\"কাল\"]),\n        (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\", [\"পাতা\"]),\n        (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\", [\"ব্যাংক\"]),\n        (\"ফল খুব সুস্বাদু।\", \"The fruit is delicious\", \"ফল = fruit/result\", [\"ফল\"]),\n        (\"মাথা ব্যথা করছে।\", \"Head is aching\", \"মাথা = head/top\", [\"মাথা\"]),\n        (\"কল থেকে কল এসেছে।\", \"A call came from the tap\", \"Multiple কল\", [\"কল\"]),\n        (\"কালকে কাল মেঘ দেখা গেছে।\", \"Yesterday black clouds were seen\", \"Multiple কাল\", [\"কাল\"]),\n        (\"আজ ভাল আবহাওয়া।\", \"Weather is good today\", \"Simple\", []),\n        (\"আমি ভালো আছি।\", \"I am fine\", \"Simple\", []),\n        (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Simple\", []),\n        (\"এটা আমার বই।\", \"This is my book\", \"Simple\", []),\n        (\"তিনি ব্যাংকে কাজ করেন এবং ব্যাংকে বসে থাকেন।\",\n         \"He works at the bank and sits on the embankment\",\n         \"Long with multiple\", [\"ব্যাংক\"]),\n    ]\n\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    core_model.eval()\n\n    quality_metrics = {\n        'total_confidence': 0.0,\n        'confidence_samples': 0,\n        'high_confidence_count': 0,\n        'medium_confidence_count': 0,\n        'low_confidence_count': 0,\n        'confidences': [],\n        'spans': [],\n        'uncertainties': [],\n    }\n\n    homograph_tracking = {\n        'test_expected_homographs': set(),\n        'dscd_discovered_homographs': set(),\n        'explained_homographs': set(),\n        'homograph_explanations': defaultdict(list),\n    }\n\n    error_tracking = {\n        'translation_failures': 0,\n        'dscd_failures': 0,\n        'trg_failures': 0,\n        'timeout_errors': 0,\n        'oom_errors': 0,\n        'other_errors': 0,\n        'error_details': [],\n        'per_test_status': [],\n    }\n\n    timing_metrics = {\n        'total_time': 0.0,\n        'per_test_times': [],\n        'avg_test_time': 0.0,\n    }\n\n    discovery_validated = False\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd and hasattr(dscd, 'discovered_log') and dscd.discovered_log:\n            discovery_validated = True\n            last_discovery = dscd.discovered_log[-1]\n            discovered = last_discovery.get('discovered', 0)\n            candidates = last_discovery.get('candidates', 0)\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] Discovery log: {discovered}/{candidates} homographs\")\n        else:\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] No discovery log found\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] Discovery validation failed: {e}\")\n\n    asbn_stats: Dict[str, Any] = {}\n    try:\n        asbn = getattr(core_model, \"asbn\", None)\n        if asbn and hasattr(asbn, 'get_detailed_stats'):\n            asbn_stats = asbn.get_detailed_stats()\n        elif asbn and hasattr(asbn, 'get_asbn_stats'):\n            asbn_stats = asbn.get_asbn_stats()\n        \n        if not asbn_stats:\n            asbn_stats = {}\n\n        if asbn_stats and _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN: domain_acc={asbn_stats.get('domain_accuracy', 0):.2%}\")\n            if 'source_accuracy' in asbn_stats and 'target_accuracy' in asbn_stats:\n                print(f\"[EVAL] ASBN: source_acc={asbn_stats.get('source_accuracy', 0):.2%}, target_acc={asbn_stats.get('target_accuracy', 0):.2%}\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] ASBN stats failed: {e}\")\n        asbn_stats = {}\n\n    trg_stats: Dict[str, Any] = {}\n    try:\n        trg = getattr(core_model, \"trg_system\", None)\n        if trg and hasattr(trg, 'get_statistics'):\n            trg_stats = trg.get_statistics()\n            if _DEBUG_DISCOVERY:\n                print(f\"[EVAL] TRG: {trg_stats.get('explanations_generated', 0)} total\")\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] TRG stats failed: {e}\")\n\n    homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n    print(f\"[EVAL] DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])} homographs\")\n    if homograph_tracking['dscd_discovered_homographs'] and _DEBUG_DISCOVERY:\n        print(f\"[EVAL] Sample: {list(homograph_tracking['dscd_discovered_homographs'])[:10]}\")\n\n    if run_warmup:\n        try:\n            dscd = getattr(core_model, \"dscd\", None)\n            if dscd is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        stores = getattr(dscd, \"prototype_stores\", None)\n                        store_count = len(stores) if stores else 0\n                        clustering_enabled = getattr(dscd, \"enable_training_clustering\", False)\n                else:\n                    stores = getattr(dscd, \"prototype_stores\", None)\n                    store_count = len(stores) if stores else 0\n                    clustering_enabled = getattr(dscd, \"enable_training_clustering\", False)\n\n                if store_count == 0 and 'dscd_discovery_warmup' in globals():\n                    if not clustering_enabled:\n                        print(\"[EVAL] CRITICAL WARNING: Clustering disabled - warmup may not discover prototypes!\")\n                        print(\"[EVAL] Set DSCD_ENABLE_TRAINING_CLUSTERING=True before training\")\n                    print(\"[EVAL] Running warmup (num_sents=4000)...\")\n                    try:\n                        dscd_discovery_warmup(model, tokenizer, num_sents=4000, batch_size=64)\n                        homograph_tracking['dscd_discovered_homographs'] = _get_dscd_homographs(core_model)\n                    except Exception as e:\n                        print(f\"[EVAL] Warmup failed: {e}\")\n        except Exception:\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0))\n            u = float(expl.get(\"uncertainty\", 0.0))\n            return (s >= _SPAN_THRESHOLD) or (u >= _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    def _compute_similarity(pred: str, expected: str) -> float:\n        try:\n            if not pred or not expected:\n                return 0.0\n            pred_words = set(str(pred).lower().split())\n            exp_words = set(str(expected).lower().split())\n            if not exp_words:\n                return 0.0\n            overlap = len(pred_words & exp_words)\n            return overlap / len(exp_words)\n        except Exception:\n            return 0.0\n\n    for _, _, _, expected_homos in test_sentences:\n        homograph_tracking['test_expected_homographs'].update([h.lower() for h in expected_homos])\n\n    eval_start = time.time()\n\n    for idx, (src_text, expected_translation, desc, expected_homos) in enumerate(test_sentences, 1):\n        test_start = time.time()\n\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n\n        test_status = {\n            'test_id': idx,\n            'success': False,\n            'translation_ok': False,\n            'explanations_count': 0,\n            'error': None,\n        }\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"[EVAL] translate_with_explanations not available\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'function_not_available'\n                error_tracking['per_test_status'].append(test_status)\n                continue\n\n            result = translate_with_explanations(\n                core_model if core_model is not None else model,\n                tokenizer,\n                src_text,\n                span_threshold=_SPAN_THRESHOLD,\n                uncertainty_threshold=_UNCERTAINTY_THRESHOLD\n            )\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            amb_count = int(result.get(\"ambiguous_words_detected\", 0))\n            explanations = result.get(\"explanations\", []) or []\n\n            similarity = _compute_similarity(translation, expected_translation)\n\n            print(f\"Input: {src_text}\")\n            print(f\"Expected: {expected_translation}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Similarity: {similarity:.1%}\")\n            print(f\"Ambiguous: {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n\n                for j, expl in enumerate(explanations, 1):\n                    span_val = float(expl.get(\"span\", 0.0))\n                    u_val = float(expl.get(\"uncertainty\", 0.0))\n                    conf_val = float(expl.get(\"confidence\", max(span_val, u_val)))\n\n                    marker = f\"[S>={_SPAN_THRESHOLD:.2f}]\" if span_val >= _SPAN_THRESHOLD else \"           \"\n\n                    word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                    pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n\n                    print(f\"  {j}. {marker} '{word}' @ {pos}\")\n                    print(f\"       conf={conf_val:.3f} | U={u_val:.3f} | S={span_val:.3f}\")\n                    text = str(expl.get(\"explanation\", \"\"))\n                    if len(text) > 120:\n                        text = text[:120] + \"...\"\n                    print(f\"       {text}\")\n\n                    quality_metrics['confidences'].append(conf_val)\n                    quality_metrics['spans'].append(span_val)\n                    quality_metrics['uncertainties'].append(u_val)\n                    quality_metrics['total_confidence'] = quality_metrics.get('total_confidence', 0.0) + conf_val\n                    quality_metrics['confidence_samples'] += 1\n\n                    if conf_val >= 0.65:\n                        quality_metrics['high_confidence_count'] += 1\n                    elif conf_val >= 0.4:\n                        quality_metrics['medium_confidence_count'] += 1\n                    else:\n                        quality_metrics['low_confidence_count'] += 1\n\n                    if span_val >= _SPAN_THRESHOLD:\n                        high_span_local += 1\n                    if _is_real_amb(expl):\n                        real_amb_local += 1\n\n                    clean_word = (\n                        str(word)\n                        .replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace(' ', '')\n                        .strip()\n                        .lower()\n                    )\n                    homograph_tracking['explained_homographs'].add(clean_word)\n                    homograph_tracking['homograph_explanations'][clean_word].append({\n                        'sentence': src_text,\n                        'confidence': conf_val,\n                        'span': span_val,\n                        'uncertainty': u_val,\n                    })\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n                test_status['explanations_count'] = len(explanations)\n            else:\n                print(\"No explanations\")\n\n            if translation and translation.strip() and translation not in (\n                \"Error occurred\",\n                \"Translation generation failed\",\n                \"ERROR DURING TRANSLATION\",\n            ):\n                successful_translations += 1\n                test_status['translation_ok'] = True\n                test_status['success'] = True\n                print(\"Success\")\n            else:\n                print(\"Translation failed\")\n                error_tracking['translation_failures'] += 1\n                test_status['error'] = 'translation_failed'\n\n        except RuntimeError as e:\n            error_str = str(e).lower()\n            if \"out of memory\" in error_str:\n                print(f\"[EVAL] OOM: {str(e)[:100]}\")\n                error_tracking['oom_errors'] += 1\n                test_status['error'] = 'oom'\n            elif \"timeout\" in error_str:\n                print(f\"[EVAL] Timeout: {str(e)[:100]}\")\n                error_tracking['timeout_errors'] += 1\n                test_status['error'] = 'timeout'\n            else:\n                print(f\"[EVAL] Runtime: {type(e).__name__}\")\n                error_tracking['other_errors'] += 1\n                test_status['error'] = 'runtime'\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n        except Exception as e:\n            print(f\"[EVAL] Error: {type(e).__name__}\")\n            error_tracking['other_errors'] += 1\n            test_status['error'] = type(e).__name__\n            error_tracking['error_details'].append(f\"Test {idx}: {type(e).__name__}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        error_tracking['per_test_status'].append(test_status)\n\n        test_time = time.time() - test_start\n        timing_metrics['per_test_times'].append(test_time)\n\n        print(\"-\" * 60)\n\n    timing_metrics['total_time'] = time.time() - eval_start\n    if timing_metrics['per_test_times']:\n        timing_metrics['avg_test_time'] = (\n            sum(timing_metrics['per_test_times']) / len(timing_metrics['per_test_times'])\n        )\n\n    if quality_metrics['confidence_samples'] > 0:\n        quality_metrics['avg_confidence'] = (\n            quality_metrics['total_confidence'] / quality_metrics['confidence_samples']\n        )\n        quality_metrics['avg_span'] = (\n            sum(quality_metrics['spans']) / len(quality_metrics['spans'])\n            if quality_metrics['spans']\n            else 0.0\n        )\n        quality_metrics['avg_uncertainty'] = (\n            sum(quality_metrics['uncertainties']) / len(quality_metrics['uncertainties'])\n            if quality_metrics['uncertainties']\n            else 0.0\n        )\n\n        if quality_metrics['confidences'] and len(quality_metrics['confidences']) > 0:\n            sorted_conf = sorted(quality_metrics['confidences'])\n            n = len(sorted_conf)\n            quality_metrics['confidence_p25'] = sorted_conf[n // 4] if n >= 4 else sorted_conf[0]\n            quality_metrics['confidence_p50'] = sorted_conf[n // 2]\n            quality_metrics['confidence_p75'] = sorted_conf[3 * n // 4] if n >= 4 else sorted_conf[-1]\n    else:\n        quality_metrics['avg_confidence'] = 0.0\n        quality_metrics['avg_span'] = 0.0\n        quality_metrics['avg_uncertainty'] = 0.0\n\n    explained_from_dscd = homograph_tracking['explained_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    test_expected_discovered = homograph_tracking['test_expected_homographs'].intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    reference_discovered = _HOMOGRAPH_REFERENCE_LIST.intersection(\n        homograph_tracking['dscd_discovered_homographs']\n    )\n\n    homograph_tracking['explained_from_dscd_rate'] = (\n        len(explained_from_dscd) / len(homograph_tracking['dscd_discovered_homographs'])\n        if homograph_tracking['dscd_discovered_homographs']\n        else 0.0\n    )\n    homograph_tracking['test_expected_discovery_rate'] = (\n        len(test_expected_discovered) / len(homograph_tracking['test_expected_homographs'])\n        if homograph_tracking['test_expected_homographs']\n        else 0.0\n    )\n    homograph_tracking['reference_discovery_rate'] = (\n        len(reference_discovered) / len(_HOMOGRAPH_REFERENCE_LIST)\n        if _HOMOGRAPH_REFERENCE_LIST\n        else 0.0\n    )\n\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(getattr(dscd, \"prototype_stores\") or {})\n            else:\n                stores = dict(getattr(dscd, \"prototype_stores\") or {})\n\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for key, store in stores.items():\n                try:\n                    if hasattr(store, \"size\"):\n                        sz = int(store.size())\n                    else:\n                        sz = 0\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\n                \"total_words\": total_words,\n                \"multi_sense_words\": multi,\n                \"total_prototypes\": total_protos,\n            }\n    except Exception as e:\n        if _DEBUG_DISCOVERY:\n            print(f\"[EVAL] DSCD stats failed: {e}\")\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n\n    print(f\"\\n[TRANSLATION QUALITY]\")\n    print(f\"  Total tests: {total_tests}\")\n    print(f\"  Successful: {successful_translations}\")\n    print(f\"  Success rate: {successful_translations / total_tests * 100:.1f}%\")\n\n    print(f\"\\n[AMBIGUITY DETECTION]\")\n    print(f\"  Total explanations: {total_explanations}\")\n    print(f\"  High-span (S>={_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  Real ambiguous: {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  Avg explanations/test: {total_explanations / total_tests:.2f}\")\n\n    print(f\"\\n[EXPLANATION QUALITY]\")\n    print(f\"  Avg confidence: {quality_metrics['avg_confidence']:.3f}\")\n    print(f\"  Avg span: {quality_metrics['avg_span']:.3f}\")\n    print(f\"  Avg uncertainty: {quality_metrics['avg_uncertainty']:.3f}\")\n\n    if 'confidence_p50' in quality_metrics:\n        print(\n            f\"  Confidence P25/P50/P75: \"\n            f\"{quality_metrics.get('confidence_p25', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p50', 0):.3f} / \"\n            f\"{quality_metrics.get('confidence_p75', 0):.3f}\"\n        )\n\n    print(f\"  High (>=0.65): {quality_metrics['high_confidence_count']}\")\n    print(f\"  Medium (0.4-0.65): {quality_metrics['medium_confidence_count']}\")\n    print(f\"  Low (<0.4): {quality_metrics['low_confidence_count']}\")\n\n    print(f\"\\n[HOMOGRAPH DISCOVERY]\")\n    print(f\"  DSCD discovered: {len(homograph_tracking['dscd_discovered_homographs'])}\")\n    print(f\"  Explained: {len(homograph_tracking['explained_homographs'])}\")\n    print(f\"  Explanation rate: {homograph_tracking['explained_from_dscd_rate']:.1%}\")\n    print(f\"  Test discovery rate: {homograph_tracking['test_expected_discovery_rate']:.1%}\")\n\n    if homograph_tracking['explained_homographs']:\n        print(f\"\\n  Explained homographs (top 10):\")\n        for homo in sorted(homograph_tracking['explained_homographs'])[:10]:\n            exps = homograph_tracking['homograph_explanations'].get(homo, [])\n            count = len(exps)\n            avg_conf = sum(e['confidence'] for e in exps) / len(exps) if exps else 0.0\n            in_dscd = \"[D]\" if homo in homograph_tracking['dscd_discovered_homographs'] else \"   \"\n            in_ref = \"[R]\" if homo in _HOMOGRAPH_REFERENCE_LIST else \"   \"\n            print(f\"    {in_dscd} {in_ref} '{homo}': {count} x conf={avg_conf:.3f}\")\n\n    print(f\"\\n[REFERENCE COMPARISON]\")\n    print(f\"  Reference: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\n    print(f\"  Discovered: {len(reference_discovered)}/{len(_HOMOGRAPH_REFERENCE_LIST)}\")\n    print(f\"  Coverage: {homograph_tracking['reference_discovery_rate']:.1%}\")\n\n    print(f\"\\n[DSCD PROTOTYPES]\")\n    print(f\"  Word types: {dscd_stats['total_words']}\")\n    print(f\"  Multi-sense: {dscd_stats['multi_sense_words']}\")\n    print(f\"  Total prototypes: {dscd_stats['total_prototypes']}\")\n    if dscd_stats['total_words'] > 0:\n        print(\n            f\"  Multi-sense ratio: \"\n            f\"{dscd_stats['multi_sense_words'] / dscd_stats['total_words']:.1%}\"\n        )\n\n    if asbn_stats:\n        print(f\"\\n[ASBN]\")\n        print(f\"  Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n        if 'source_accuracy' in asbn_stats:\n            print(f\"  Source accuracy: {asbn_stats['source_accuracy']:.2%}\")\n        if 'target_accuracy' in asbn_stats:\n            print(f\"  Target accuracy: {asbn_stats['target_accuracy']:.2%}\")\n        if 'num_updates' in asbn_stats:\n            print(f\"  Updates: {asbn_stats['num_updates']}\")\n\n    if trg_stats:\n        print(f\"\\n[TRG]\")\n        print(f\"  Total explanations: {trg_stats.get('explanations_generated', 0)}\")\n        print(f\"  High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n\n    print(f\"\\n[PERFORMANCE]\")\n    print(f\"  Total time: {timing_metrics['total_time']:.2f}s\")\n    print(f\"  Avg time/test: {timing_metrics['avg_test_time']:.2f}s\")\n\n    total_errors = sum([\n        error_tracking['translation_failures'],\n        error_tracking['dscd_failures'],\n        error_tracking['trg_failures'],\n        error_tracking['timeout_errors'],\n        error_tracking['oom_errors'],\n        error_tracking['other_errors'],\n    ])\n\n    if total_errors > 0:\n        print(f\"\\n[ERRORS]\")\n        print(f\"  Total: {total_errors}\")\n        print(f\"  Translation: {error_tracking['translation_failures']}\")\n        print(f\"  OOM: {error_tracking['oom_errors']}\")\n        print(f\"  Other: {error_tracking['other_errors']}\")\n\n    if compare_baseline and baseline_metrics and isinstance(baseline_metrics, dict):\n        print(f\"\\n[BASELINE COMPARISON]\")\n        try:\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            current_success = (\n                successful_translations / total_tests * 100.0\n            ) if total_tests > 0 else 0.0\n            success_delta = current_success - baseline_success\n\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            expl_delta = total_explanations - baseline_expl\n\n            baseline_quality = 0.0\n            if 'quality_metrics' in baseline_metrics:\n                baseline_quality_metrics = baseline_metrics['quality_metrics']\n                if isinstance(baseline_quality_metrics, dict):\n                    baseline_quality = baseline_quality_metrics.get('avg_confidence', 0.0)\n            \n            quality_delta = quality_metrics['avg_confidence'] - baseline_quality\n\n            print(f\"  Translation: {current_success:.1f}% ({success_delta:+.1f}%)\")\n            print(f\"  Explanations: {total_explanations} ({expl_delta:+d})\")\n            print(\n                f\"  Confidence: {quality_metrics['avg_confidence']:.3f} \"\n                f\"({quality_delta:+.3f})\"\n            )\n\n            baseline_homo_rate = 0.0\n            if 'homograph_tracking' in baseline_metrics:\n                baseline_homo_tracking = baseline_metrics['homograph_tracking']\n                if isinstance(baseline_homo_tracking, dict):\n                    baseline_homo_rate = baseline_homo_tracking.get('explained_from_dscd_rate', 0.0)\n            \n            homo_delta = homograph_tracking['explained_from_dscd_rate'] - baseline_homo_rate\n            print(\n                f\"  Explanation rate: \"\n                f\"{homograph_tracking['explained_from_dscd_rate']:.1%} \"\n                f\"({homo_delta:+.1%})\"\n            )\n        except Exception as e:\n            print(f\"  Comparison failed: {e}\")\n\n    warnings = []\n    if successful_translations < total_tests * 0.5:\n        warnings.append(\"High translation failure (>50%)\")\n    if total_explanations == 0:\n        warnings.append(f\"No explanations generated - check thresholds (span={_SPAN_THRESHOLD}, U={_UNCERTAINTY_THRESHOLD})\")\n    if dscd_stats['total_words'] < 100:\n        warnings.append(\"Very few prototypes (<100) - run warmup or increase training data\")\n    if quality_metrics['low_confidence_count'] > quality_metrics['high_confidence_count']:\n        warnings.append(\"More low than high confidence\")\n    if homograph_tracking['explained_from_dscd_rate'] < 0.3:\n        warnings.append(\"Low explanation rate (<30%)\")\n    if not discovery_validated:\n        warnings.append(\"Discovery log missing\")\n    if asbn_stats and asbn_stats.get('domain_accuracy', 0) < 0.5:\n        warnings.append(\"ASBN domain accuracy <50% - check Cell 4/6 fixes\")\n\n    if warnings:\n        print(f\"\\n[WARNINGS]\")\n        for w in warnings:\n            print(f\"  - {w}\")\n    else:\n        print(f\"\\n[HEALTH] All systems nominal\")\n\n    print(\"=\" * 80)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n        \"quality_metrics\": quality_metrics,\n        \"homograph_tracking\": homograph_tracking,\n        \"error_tracking\": error_tracking,\n        \"asbn_stats\": asbn_stats,\n        \"trg_stats\": trg_stats,\n        \"discovery_validated\": discovery_validated,\n        \"timing_metrics\": timing_metrics,\n    }\n\ndef test_evaluation_pipeline(model, tokenizer) -> bool:\n    print(\"\\n\" + \"=\"*60)\n    print(\"[TEST] Testing evaluation pipeline\")\n    print(\"=\"*60)\n\n    try:\n        result = comprehensive_post_training_testing(\n            model,\n            tokenizer,\n            run_warmup=False,\n            compare_baseline=False\n        )\n\n        assert 'total_tests' in result\n        assert 'quality_metrics' in result\n        assert 'homograph_tracking' in result\n\n        print(\"Evaluation pipeline test passed\")\n        print(\"=\"*60 + \"\\n\")\n        return True\n\n    except Exception as e:\n        print(f\"Evaluation pipeline test failed: {e}\")\n        try:\n            traceback.print_exc()\n        except Exception:\n            pass\n        print(\"=\"*60 + \"\\n\")\n        return False\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 9: Testing & evaluation ready (PURE DATA-DRIVEN) - FIXED\")\nprint(\"=\" * 80)\nprint()\nprint(f\"Configuration:\")\nprint(f\"  - Span threshold: {_SPAN_THRESHOLD}\")\nprint(f\"  - Uncertainty threshold: {_UNCERTAINTY_THRESHOLD}\")\nprint(f\"  - Reference list: {len(_HOMOGRAPH_REFERENCE_LIST)} words\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"8uL574F8H4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10: TATN MAIN PIPELINE (FINAL INTEGRATION, ALL FIXES)\n# ==============================================================================\n\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Optional, Dict, Any\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ndef _g(name, default):\n    return globals().get(name, default)\n\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _SOURCE_LANGUAGE = str(_g(\"SOURCE_LANGUAGE\", \"bn\"))\n    _TARGET_LANGUAGE = str(_g(\"TARGET_LANGUAGE\", \"en\"))\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 48))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 1e-5))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", False))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 500))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(_g(\"PERIODIC_DISCOVERY_FREQUENCY\", 50))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 4000))\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(_g(\"HOMOGRAPH_REFERENCE_LIST_BN\",\n        [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\", \"বার\", \"হার\", \"তারা\"]))\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = bool(_g(\"FREEZE_ENCODER\", False))\n    _DEBUG_TIMING = bool(_g(\"DEBUG_TIMING\", False))\nexcept (ValueError, TypeError):\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _SOURCE_LANGUAGE = \"bn\"\n    _TARGET_LANGUAGE = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 48\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 1e-5\n    _ENABLE_ASBN_TRAINING = False\n    _VALIDATION_CHECK_INTERVAL = 500\n    _PERIODIC_DISCOVERY_FREQUENCY = 50\n    _DSCD_WARMUP_SAMPLES = 4000\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\"}\n    HOMOGRAPH_REFERENCE_LIST_BN = _HOMOGRAPH_REFERENCE_LIST_BN\n    _FREEZE_ENCODER = False\n    _DEBUG_TIMING = False\n\n_CHECKPOINT_DIR = \"/kaggle/working\"\n_CHECKPOINT_PATH = os.path.join(_CHECKPOINT_DIR, \"tatn_final.pt\")\n\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            globals()[\"clear_all_gpu_caches\"]()\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, None)\n        if result is None:\n            return default\n    return result\n\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False):\n    try:\n        from transformers import M2M100Tokenizer\n        tok = M2M100Tokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n        required = ['encode', 'decode', 'convert_ids_to_tokens', '__call__']\n        for method in required:\n            if not hasattr(tok, method):\n                raise RuntimeError(f\"Tokenizer missing: {method}\")\n        return tok\n    except Exception as e:\n        print(f\"[TOKENIZER] Load failed: {e}\")\n        raise\n\ndef initialize_environment():\n    print(\"[PIPELINE] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[PIPELINE] GPUs: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n                print(f\"  GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  GPU {i}: Unknown\")\n        _safe_clear_gpu_caches()\n    else:\n        print(\"[PIPELINE] CPU only\")\n    return True\n\ndef main_pipeline() -> Tuple[object, object]:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TATN MAIN PIPELINE - COMPLETE INTEGRATION\")\n    print(\"=\" * 80)\n    \n    span_thresh = _g('SPAN_THRESHOLD', None)\n    uncertainty_thresh = _g('TAU_LOW', None)\n    \n    print(f\"Configuration:\")\n    print(f\"  - Span threshold: {span_thresh if span_thresh is not None else 'NOT SET'}\")\n    print(f\"  - Uncertainty threshold: {uncertainty_thresh if uncertainty_thresh is not None else 'NOT SET'}\")\n    print(f\"  - Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n    print(f\"  - DSCD warmup samples: {_DSCD_WARMUP_SAMPLES}\")\n    print(f\"  - Epochs: {_EPOCHS}\")\n    print(f\"  - Batch size: {_BATCH_SIZE}\")\n    print(f\"  - ASBN training: {'ENABLED' if _ENABLE_ASBN_TRAINING else 'DISABLED'}\")\n    \n    config_warnings = []\n    if span_thresh is None or (isinstance(span_thresh, (int, float)) and abs(span_thresh - 0.12) > 0.001):\n        config_warnings.append(\"SPAN_THRESHOLD not set to 0.12 - may affect explanation generation\")\n    if uncertainty_thresh is None or (isinstance(uncertainty_thresh, (int, float)) and abs(uncertainty_thresh - 0.15) > 0.001):\n        config_warnings.append(\"TAU_LOW not set to 0.15 - may affect explanation generation\")\n    if _PERIODIC_DISCOVERY_FREQUENCY <= 0:\n        config_warnings.append(\"Discovery frequency is 0 - periodic discovery disabled\")\n    elif _PERIODIC_DISCOVERY_FREQUENCY != 50:\n        config_warnings.append(f\"Discovery frequency is {_PERIODIC_DISCOVERY_FREQUENCY} - should be 50 for optimal performance\")\n    if _DSCD_WARMUP_SAMPLES < 1000:\n        config_warnings.append(\"DSCD warmup samples < 1000 - may not discover enough prototypes\")\n    \n    if config_warnings:\n        print(\"\\n[CONFIG WARNINGS]\")\n        for w in config_warnings:\n            print(f\"  - {w}\")\n    \n    print(\"=\" * 80)\n\n    pipeline_start = time.time()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    initialize_environment()\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Initialization: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 1] Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    try:\n        tokenizer.src_lang = _SOURCE_LANGUAGE\n    except Exception:\n        pass\n\n    try:\n        if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:\n            if hasattr(tokenizer, 'add_special_tokens'):\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n    except Exception:\n        pass\n\n    vocab_size = getattr(tokenizer, 'vocab_size', None)\n    if vocab_size is None:\n        try:\n            vocab_size = len(tokenizer)\n        except Exception:\n            vocab_size = 128000\n\n    print(f\"[PHASE 1] Tokenizer loaded (vocab: {vocab_size})\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Tokenizer: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(f\"\\n[PHASE 2] Loading data ({_NUM_SAMPLES} samples)...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception as e:\n            print(f\"[PHASE 2] Data loading failed: {e}\")\n            pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n    else:\n        print(\"[PHASE 2] Using fallback data\")\n        pairs = [(\"আমি কল বন্ধ করেছি।\", \"I turned off the tap.\")]\n\n    if \"MemoryEfficientDataset\" not in globals():\n        raise RuntimeError(\"MemoryEfficientDataset not found - run Cell 2\")\n    dataset = MemoryEfficientDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n    collate_fn = globals().get(\"safe_collate\", None)\n    if \"create_optimized_dataloader\" in globals():\n        try:\n            train_loader = create_optimized_dataloader(dataset, batch_size=_BATCH_SIZE, shuffle=True)\n        except Exception:\n            dataloader_kwargs = {\n                'batch_size': _BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0,\n                'pin_memory': torch.cuda.is_available()\n            }\n            if collate_fn is not None:\n                dataloader_kwargs['collate_fn'] = collate_fn\n            train_loader = DataLoader(dataset, **dataloader_kwargs)\n    else:\n        dataloader_kwargs = {\n            'batch_size': _BATCH_SIZE,\n            'shuffle': True,\n            'num_workers': 0,\n            'pin_memory': torch.cuda.is_available()\n        }\n        if collate_fn is not None:\n            dataloader_kwargs['collate_fn'] = collate_fn\n        train_loader = DataLoader(dataset, **dataloader_kwargs)\n\n    try:\n        print(f\"[PHASE 2] Dataset: {len(dataset)} samples, {len(train_loader)} batches\")\n    except Exception:\n        print(\"[PHASE 2] Dataset loaded\")\n\n    del pairs\n    _safe_clear_gpu_caches()\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Data loading: {time.time() - phase_start:.2f}s\")\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 3] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        raise RuntimeError(\"Model class not found - run Cell 6\")\n\n    try:\n        model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n    except RuntimeError as e:\n        error_msg = str(e)\n        if \"embed_dim\" in error_msg or \"unexpected keyword argument\" in error_msg:\n            print(f\"\\n[ERROR] Model initialization failed: {error_msg}\")\n            print(\"\\n[FIX REQUIRED IN CELL 6]\")\n            print(\"  In MemoryOptimizedTATNWithExplanations.__init__(), find:\")\n            print(\"    self.dscd = dscdcls(embed_dim=embeddim, ...)\")\n            print(\"  Replace with:\")\n            print(\"    self.dscd = dscdcls(embeddim=embeddim, ...)\")\n            print(\"  (Change 'embed_dim' to 'embeddim' - remove underscore)\")\n            raise RuntimeError(f\"Failed to instantiate MemoryEfficientDSCDOnline: {error_msg}\")\n        else:\n            raise\n    except Exception as e:\n        print(f\"\\n[ERROR] Model initialization failed: {type(e).__name__}: {e}\")\n        raise\n\n    if hasattr(model_core, 'dscd') and model_core.dscd is not None:\n        dscd_proto_stores = getattr(model_core.dscd, 'prototype_stores', None)\n        clustering_enabled = getattr(model_core.dscd, 'enable_training_clustering', False)\n        print(f\"[PHASE 3] DSCD component initialized successfully\")\n        print(f\"  - Clustering: {'ENABLED' if clustering_enabled else 'DISABLED'}\")\n        if not clustering_enabled:\n            print(f\"  - WARNING: Clustering disabled - no prototypes will be discovered during training!\")\n    else:\n        print(\"[PHASE 3] WARNING: DSCD component missing or None\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 1:\n        device_ids = list(range(_NUM_GPUS))\n        print(f\"[PHASE 3] Using DataParallel on {device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=device_ids)\n    else:\n        model = model_core\n\n    model = model.to(_DEVICE)\n    core_model = model.module if hasattr(model, \"module\") else model\n\n    try:\n        mbart = getattr(core_model, \"mbart\", None)\n        if mbart and hasattr(mbart, \"resize_token_embeddings\"):\n            try:\n                current_size = mbart.get_input_embeddings().num_embeddings\n                if isinstance(vocab_size, int):\n                    target_size = vocab_size\n                else:\n                    target_size = current_size\n                if current_size != target_size:\n                    mbart.resize_token_embeddings(target_size)\n                    print(f\"[PHASE 3] Resized embeddings: {current_size} -> {target_size}\")\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    if _FREEZE_ENCODER:\n        try:\n            for p in core_model.mbart.model.encoder.parameters():\n                p.requires_grad = False\n            print(\"[PHASE 3] Encoder frozen\")\n        except Exception:\n            pass\n\n    print(f\"[PHASE 3] Model initialized\")\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Model init: {time.time() - phase_start:.2f}s\")\n\n    print(\"\\n[PHASE 4] Setting up optimizers...\")\n\n    try:\n        critic_params = list(core_model.asbn.critic_parameters()) if hasattr(core_model, \"asbn\") and hasattr(core_model.asbn, \"critic_parameters\") else []\n    except Exception:\n        critic_params = []\n\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n    \n    trainable_base = [p for p in base_params if p.requires_grad]\n    if len(trainable_base) == 0:\n        print(\"[PHASE 4] WARNING: No trainable base parameters - model may not train!\")\n    \n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n    print(f\"[PHASE 4] Base optimizer created ({len(trainable_base)} trainable params)\")\n\n    phi_optimizer = None\n    if critic_params and _ENABLE_ASBN_TRAINING:\n        trainable_critic = [p for p in critic_params if p.requires_grad]\n        if len(trainable_critic) > 0:\n            phi_optimizer = torch.optim.AdamW(trainable_critic, lr=_LR_PHI)\n            print(f\"[PHASE 4] ASBN optimizer created ({len(trainable_critic)} params)\")\n        else:\n            print(f\"[PHASE 4] WARNING: ASBN enabled but no trainable critic parameters!\")\n    elif _ENABLE_ASBN_TRAINING and not critic_params:\n        print(f\"[PHASE 4] WARNING: ASBN enabled but no critic parameters found!\")\n\n    print(f\"[PHASE 4] Optimizers ready\")\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 5] Baseline evaluation...\")\n    baseline_metrics = None\n\n    try:\n        dscd = getattr(core_model, 'dscd', None)\n        has_prototypes = False\n        clustering_enabled = False\n        should_run_baseline = True\n\n        if dscd:\n            prototype_stores = getattr(dscd, 'prototype_stores', None)\n            clustering_enabled = getattr(dscd, 'enable_training_clustering', False)\n            \n            if prototype_stores is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n\n                if lock:\n                    with lock:\n                        has_prototypes = len(prototype_stores) > 0\n                else:\n                    has_prototypes = len(prototype_stores) > 0\n\n        if has_prototypes:\n            print(\"[PHASE 5] Prototypes exist - skipping baseline\")\n            should_run_baseline = False\n        elif not clustering_enabled:\n            print(\"[PHASE 5] Clustering disabled - skipping baseline (no discoveries expected)\")\n            should_run_baseline = False\n        \n        if should_run_baseline and \"comprehensive_post_training_testing\" in globals():\n            try:\n                trg = getattr(core_model, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n\n            baseline_metrics = comprehensive_post_training_testing(model, tokenizer, run_warmup=False)\n            baseline_success = baseline_metrics.get('success_rate_pct', 0)\n            baseline_expl = baseline_metrics.get('total_explanations', 0)\n            print(f\"[PHASE 5] Baseline: {baseline_success:.1f}% success, {baseline_expl} explanations\")\n        elif not should_run_baseline:\n            pass\n        else:\n            print(\"[PHASE 5] Skipping baseline (function not found)\")\n    except Exception as e:\n        print(f\"[PHASE 5] Baseline failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Baseline: {time.time() - phase_start:.2f}s\")\n\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 6] Training...\")\n    trained_model = model\n    training_stats = None\n\n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            try:\n                trg = getattr(core_model, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=(_VALIDATION_CHECK_INTERVAL > 0)\n            )\n            print(\"[PHASE 6] Training complete\")\n        except Exception as e:\n            print(f\"[PHASE 6] Training failed: {e}\")\n            if _DEBUG_TIMING:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n            trained_model = model\n    else:\n        print(\"[PHASE 6] Skipping training (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Training: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 7] Discovery...\")\n    discovery_success = False\n    try:\n        core_for_discovery = trained_model.module if hasattr(trained_model, 'module') else trained_model\n        dscd = getattr(core_for_discovery, 'dscd', None)\n        if dscd is None:\n            print(\"[PHASE 7] No DSCD module\")\n        else:\n            initial_proto_count = 0\n            prototype_stores = getattr(dscd, 'prototype_stores', None)\n            if prototype_stores is not None:\n                lock = None\n                if hasattr(dscd, 'buffer_lock'):\n                    lock = dscd.buffer_lock\n                elif hasattr(dscd, 'clustering_lock'):\n                    lock = dscd.clustering_lock\n                if lock:\n                    with lock:\n                        initial_proto_count = len(prototype_stores)\n                else:\n                    initial_proto_count = len(prototype_stores)\n\n            if hasattr(dscd, 'periodic_discovery_check') and _PERIODIC_DISCOVERY_FREQUENCY > 0:\n                print(\"[PHASE 7] Using periodic_discovery_check()...\")\n                try:\n                    total_steps = int(_EPOCHS * max(1, len(train_loader)))\n                    dscd.periodic_discovery_check(total_steps, _PERIODIC_DISCOVERY_FREQUENCY)\n                    discovery_success = True\n                except Exception as e:\n                    print(f\"[PHASE 7] periodic_discovery_check failed: {e}\")\n                    if hasattr(dscd, 'discover_homographs'):\n                        try:\n                            print(\"[PHASE 7] Fallback: forcing discover_homographs()...\")\n                            dscd.discover_homographs()\n                        except Exception as e2:\n                            print(f\"[PHASE 7] Fallback discovery failed: {e2}\")\n                    else:\n                        print(\"[PHASE 7] No discover_homographs() method available\")\n            \n            if prototype_stores is not None:\n                if lock:\n                    with lock:\n                        stores = dict(prototype_stores)\n                else:\n                    stores = dict(prototype_stores)\n                \n                def _store_size(s):\n                    try:\n                        if callable(getattr(s, \"size\", None)):\n                            return int(s.size())\n                        return int(getattr(s, \"size\", 0))\n                    except Exception:\n                        return 0\n                \n                total_protos = sum(_store_size(store) for store in stores.values())\n                multi_sense = sum(1 for store in stores.values() if _store_size(store) >= 2)\n                \n                print(\"[PHASE 7] Discovery complete:\")\n                print(f\"  - Tokens: {len(stores)} (was {initial_proto_count})\")\n                print(f\"  - Prototypes: {total_protos}\")\n                print(f\"  - Multi-sense: {multi_sense}\")\n                \n                if len(stores) == initial_proto_count and initial_proto_count > 0:\n                    print(\"[PHASE 7] WARNING: No new prototypes created during discovery\")\n                    discovery_success = False\n                elif len(stores) == 0:\n                    print(\"[PHASE 7] CRITICAL: No prototypes created - check DSCD clustering enabled\")\n                    discovery_success = False\n                elif total_protos > 0:\n                    discovery_success = True\n    except Exception as e:\n        print(f\"[PHASE 7] Discovery failed: {e}\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Discovery: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 8] Warmup...\")\n    if \"dscd_discovery_warmup\" in globals():\n        try:\n            warmup_samples = _DSCD_WARMUP_SAMPLES\n            dscd_discovery_warmup(trained_model, tokenizer, num_sents=warmup_samples, batch_size=64, max_len=_MAX_LENGTH)\n            print(f\"[PHASE 8] Warmup complete ({warmup_samples} samples)\")\n        except Exception as e:\n            print(f\"[PHASE 8] Warmup failed: {e}\")\n    else:\n        print(\"[PHASE 8] Skipping warmup (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Warmup: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 9] Post-training evaluation...\")\n    eval_results: Dict[str, Any] = {}\n\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            try:\n                core_for_eval = trained_model.module if hasattr(trained_model, 'module') else trained_model\n                trg = getattr(core_for_eval, 'trg_system', None)\n                if trg and hasattr(trg, 'reset_statistics'):\n                    trg.reset_statistics()\n            except Exception:\n                pass\n            eval_results = comprehensive_post_training_testing(\n                trained_model,\n                tokenizer,\n                run_warmup=False,\n                compare_baseline=(baseline_metrics is not None),\n                baseline_metrics=baseline_metrics\n            )\n            final_success = eval_results.get('success_rate_pct', 0)\n            final_expl = eval_results.get('total_explanations', 0)\n            print(f\"[PHASE 9] Evaluation: {final_success:.1f}% success, {final_expl} explanations\")\n        except Exception as e:\n            print(f\"[PHASE 9] Evaluation failed: {e}\")\n    else:\n        print(\"[PHASE 9] Skipping evaluation (function not found)\")\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Evaluation: {time.time() - phase_start:.2f}s\")\n    _safe_clear_gpu_caches()\n    if _DEBUG_TIMING:\n        phase_start = time.time()\n\n    print(\"\\n[PHASE 10] Saving checkpoint...\")\n    try:\n        os.makedirs(_CHECKPOINT_DIR, exist_ok=True)\n        core_for_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        was_training = getattr(core_for_save, \"training\", False)\n        core_for_save.eval()\n        try:\n            model_state = core_for_save.state_dict()\n            dscd_state = {}\n            if hasattr(core_for_save, 'dscd') and core_for_save.dscd is not None and hasattr(core_for_save.dscd, 'state_dict'):\n                try:\n                    dscd_state = core_for_save.dscd.state_dict()\n                except Exception as e:\n                    print(f\"[PHASE 10] DSCD state_dict failed: {e}\")\n                    dscd_state = {}\n            \n            optimizer_state = None\n            if optimizer is not None:\n                try:\n                    optimizer_state = optimizer.state_dict()\n                    if 'state' in optimizer_state and optimizer_state['state'] is not None:\n                        for param_state in optimizer_state['state'].values():\n                            if isinstance(param_state, dict):\n                                for buffer_key in ['momentum_buffer', 'exp_avg', 'exp_avg_sq']:\n                                    try:\n                                        if buffer_key in param_state:\n                                            del param_state[buffer_key]\n                                    except Exception:\n                                        pass\n                except Exception as e:\n                    print(f\"[PHASE 10] Optimizer state failed: {e}\")\n                    optimizer_state = None\n            \n            checkpoint = {\n                'model_state_dict': model_state,\n                'dscd_state': dscd_state,\n                'optimizer_state_dict': optimizer_state,\n                'training_stats': training_stats,\n                'baseline_metrics': baseline_metrics,\n                'eval_results': eval_results,\n                'discovery_success': discovery_success,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'config': {\n                    'epochs': _EPOCHS,\n                    'batch_size': _BATCH_SIZE,\n                    'span_threshold': span_thresh,\n                    'uncertainty_threshold': uncertainty_thresh,\n                    'discovery_frequency': _PERIODIC_DISCOVERY_FREQUENCY,\n                }\n            }\n            \n            torch.save(checkpoint, _CHECKPOINT_PATH)\n            \n            try:\n                verify = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n                has_model = 'model_state_dict' in verify and verify.get('model_state_dict') is not None and len(verify['model_state_dict']) > 0\n                has_dscd = 'dscd_state' in verify and verify.get('dscd_state') is not None and len(verify.get('dscd_state', {})) > 0\n                print(f\"[PHASE 10] Checkpoint saved: {_CHECKPOINT_PATH}\")\n                print(f\"  - Model: {'OK' if has_model else 'MISSING'}\")\n                print(f\"  - DSCD: {'OK' if has_dscd else 'MISSING'}\")\n                if has_dscd:\n                    dscd_state_dict = verify.get('dscd_state', {})\n                    num_tokens = 0\n                    if dscd_state_dict is not None and 'prototype_stores' in dscd_state_dict and isinstance(dscd_state_dict['prototype_stores'], dict):\n                        num_tokens = len(dscd_state_dict['prototype_stores'])\n                    print(f\"  - DSCD tokens: {num_tokens}\")\n            except Exception as e:\n                print(f\"[PHASE 10] Checkpoint verification failed: {e}\")\n                print(f\"[PHASE 10] Checkpoint may be corrupted - recommend re-saving\")\n        finally:\n            if was_training:\n                try:\n                    core_for_save.train()\n                except Exception:\n                    pass\n    except Exception as e:\n        print(f\"[PHASE 10] Checkpoint failed: {e}\")\n        if _DEBUG_TIMING:\n            try:\n                traceback.print_exc()\n            except Exception:\n                pass\n\n    if _DEBUG_TIMING:\n        print(f\"[TIMING] Checkpoint: {time.time() - phase_start:.2f}s\")\n    \n    print(\"\\n[PHASE 11] Final validation...\")\n    try:\n        core_final = trained_model.module if hasattr(trained_model, 'module') else trained_model\n        dscd_ok = False\n        if hasattr(core_final, 'dscd') and core_final.dscd is not None:\n            prototype_stores = getattr(core_final.dscd, 'prototype_stores', None)\n            if prototype_stores is not None:\n                lock = None\n                if hasattr(core_final.dscd, 'buffer_lock'):\n                    lock = core_final.dscd.buffer_lock\n                elif hasattr(core_final.dscd, 'clustering_lock'):\n                    lock = core_final.clustering_lock\n                if lock:\n                    with lock:\n                        dscd_ok = len(prototype_stores) > 0\n                else:\n                    dscd_ok = len(prototype_stores) > 0\n        \n        asbn_ok = hasattr(core_final, 'asbn') and hasattr(core_final.asbn, 'forward')\n        \n        trg_ok = False\n        if hasattr(core_final, 'trg_system') and core_final.trg_system is not None:\n            if hasattr(core_final.trg_system, 'process_sentence_for_explanations'):\n                trg = core_final.trg_system\n                trg_ok = not getattr(trg, 'training', True)\n        \n        print(f\"[PHASE 11] Component validation:\")\n        print(f\"  - DSCD: {'OK' if dscd_ok else 'MISSING/EMPTY'}\")\n        print(f\"  - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n        print(f\"  - TRG: {'OK' if trg_ok else 'IN TRAINING MODE' if hasattr(core_final, 'trg_system') and core_final.trg_system is not None else 'MISSING'}\")\n        \n        all_ok = dscd_ok and asbn_ok and trg_ok\n        if all_ok:\n            print(\"[PHASE 11] All components validated ✓\")\n        else:\n            print(\"[PHASE 11] Some components missing or misconfigured\")\n    except Exception as e:\n        print(f\"[PHASE 11] Validation failed: {e}\")\n\n    pipeline_time = time.time() - pipeline_start\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"\\n[TIMING]\")\n    print(f\"  Total time: {pipeline_time:.2f}s ({pipeline_time/60:.2f} min)\")\n\n    print(f\"\\n[TRAINING]\")\n    if training_stats:\n        total_loss = training_stats.get('total_loss', [])\n        optimizer_updates = training_stats.get('optimizer_updates', 0)\n        print(f\"  Completed: {optimizer_updates} optimizer updates\")\n        if total_loss:\n            recent_loss = sum(total_loss[-100:]) / len(total_loss[-100:])\n            print(f\"  - Final loss: {recent_loss:.6f}\")\n    else:\n        print(\"  No stats available\")\n\n    print(f\"\\n[DISCOVERY]\")\n    if discovery_success:\n        print(\"  Success ✓\")\n    else:\n        print(\"  Issues detected - check DSCD clustering enabled and discovery frequency\")\n\n    print(f\"\\n[EVALUATION]\")\n    if baseline_metrics is not None and eval_results:\n        baseline_success = baseline_metrics.get('success_rate_pct', 0)\n        final_success = eval_results.get('success_rate_pct', 0)\n        improvement = final_success - baseline_success\n\n        print(f\"  Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n        print(f\"  Improvement: {improvement:+.1f}%\")\n\n        baseline_dscd_stats = baseline_metrics.get('dscd_stats', {})\n        final_dscd_stats = eval_results.get('dscd_stats', {})\n\n        baseline_dscd = None\n        if baseline_dscd_stats is not None and isinstance(baseline_dscd_stats, dict):\n            baseline_dscd = baseline_dscd_stats.get('multi_sense_words', 0)\n        \n        final_dscd = None\n        if final_dscd_stats is not None and isinstance(final_dscd_stats, dict):\n            final_dscd = final_dscd_stats.get('multi_sense_words', 0)\n\n        if baseline_dscd is not None and final_dscd is not None:\n            print(f\"  DSCD multi-sense: {baseline_dscd} -> {final_dscd}\")\n\n        baseline_asbn_stats = baseline_metrics.get('asbn_stats', {})\n        final_asbn_stats = eval_results.get('asbn_stats', {})\n\n        baseline_asbn = None\n        if baseline_asbn_stats is not None and isinstance(baseline_asbn_stats, dict):\n            baseline_asbn = baseline_asbn_stats.get('domain_accuracy', 0)\n        \n        final_asbn = None\n        if final_asbn_stats is not None and isinstance(final_asbn_stats, dict):\n            final_asbn = final_asbn_stats.get('domain_accuracy', 0)\n\n        if baseline_asbn is not None and final_asbn is not None:\n            print(f\"  ASBN accuracy: {baseline_asbn:.2%} -> {final_asbn:.2%}\")\n    elif eval_results:\n        print(f\"  Success rate: {eval_results.get('success_rate_pct', 0):.1f}%\")\n    else:\n        print(\"  No results\")\n\n    print(f\"\\n[CHECKPOINT]\")\n    if os.path.exists(_CHECKPOINT_PATH):\n        try:\n            size_mb = os.path.getsize(_CHECKPOINT_PATH) / 1024**2\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n            print(f\"  - Size: {size_mb:.2f} MB\")\n        except Exception:\n            print(f\"  Saved: {_CHECKPOINT_PATH}\")\n    else:\n        print(\"  Not saved\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Usage: trained_model, tokenizer = main_pipeline()\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    return trained_model, tokenizer\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 10: Main pipeline ready (FINAL INTEGRATION) - FIXED\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"kEux2BVXH4J5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11: MAIN EXECUTION WRAPPER (FINAL) - FIXED\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport time\nimport torch\nimport gc\n\ntry:\n    _NUM_SAMPLES = int(globals().get('NUM_SAMPLES', 30000))\n    _EPOCHS = int(globals().get('EPOCHS', 2))\n    _BATCH_SIZE = int(globals().get('BATCH_SIZE', 4))\n    _ACCUMULATION_STEPS = int(globals().get('ACCUMULATION_STEPS', 16))\n    \n    raw_device = globals().get('DEVICE', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if isinstance(raw_device, torch.device):\n        _DEVICE = raw_device\n    else:\n        _DEVICE = torch.device(str(raw_device))\n\n    _ENABLE_ASBN_TRAINING = bool(globals().get('ENABLE_ASBN_TRAINING', True))\n    _ENABLE_TRG_INFERENCE = bool(globals().get('ENABLE_TRG_INFERENCE', True))\n    _PERIODIC_DISCOVERY_FREQUENCY = int(globals().get('PERIODIC_DISCOVERY_FREQUENCY', 50))\n    _VERBOSE_LOGGING = bool(globals().get('VERBOSE_LOGGING', False))\n    _DEBUG_DISCOVERY = bool(globals().get('DEBUG_DISCOVERY', False))\n    _DEBUG_TIMING = bool(globals().get('DEBUG_TIMING', False))\n    _NUM_GPUS = int(globals().get('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _USE_MULTI_GPU = bool(globals().get('USE_MULTI_GPU', _NUM_GPUS > 1))\n    _SPAN_THRESHOLD = float(globals().get('SPAN_THRESHOLD', 0.12))\n    _TAU_LOW = float(globals().get('TAU_LOW', 0.15))\n    \n    raw_list = globals().get('HOMOGRAPH_REFERENCE_LIST_BN', [\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"])\n    _HOMOGRAPH_REFERENCE_LIST_BN = set(str(w) for w in raw_list)\n    cell0_loaded = 'NUM_SAMPLES' in globals()\n    \nexcept (NameError, TypeError, ValueError) as e:\n    print(f\"[EXEC] Config load error: {e}\")\n    _NUM_SAMPLES = 30000\n    _EPOCHS = 2\n    _BATCH_SIZE = 4\n    _ACCUMULATION_STEPS = 16\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _ENABLE_ASBN_TRAINING = True\n    _ENABLE_TRG_INFERENCE = True\n    _PERIODIC_DISCOVERY_FREQUENCY = 50\n    _VERBOSE_LOGGING = False\n    _DEBUG_DISCOVERY = False\n    _DEBUG_TIMING = False\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = (_NUM_GPUS > 1)\n    _SPAN_THRESHOLD = 0.12\n    _TAU_LOW = 0.15\n    _HOMOGRAPH_REFERENCE_LIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}\n    cell0_loaded = False\n    print(\"[EXEC] Using fallback configuration (Cell 0 not executed)\")\n\n_CHECKPOINT_PATH = \"/kaggle/working/tatn_final.pt\"\n\ndef _safe_div_ceil(a: int, b: int) -> int:\n    try:\n        if isinstance(a, int) and isinstance(b, int) and b > 0:\n            return math.ceil(a / b)\n    except Exception:\n        pass\n    return 0\n\ndef _format_duration(seconds: float) -> str:\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}min\"\n    else:\n        return f\"{seconds/3600:.2f}hr\"\n\ndef _safe_get(d: dict, *keys, default=None):\n    if not isinstance(d, dict):\n        return default\n    result = d\n    for key in keys:\n        if not isinstance(result, dict):\n            return default\n        result = result.get(key, default)\n        if result is default:\n            return default\n    return result\n\ndef _get_dscd_homographs(model):\n    try:\n        core = model.module if hasattr(model, 'module') else model\n        dscd = getattr(core, 'dscd', None)\n\n        if dscd and hasattr(dscd, 'get_discovered_homographs'):\n            try:\n                return dscd.get_discovered_homographs()\n            except Exception:\n                pass\n\n        if dscd and hasattr(dscd, 'prototype_stores'):\n            homographs = set()\n\n            lock = None\n            if hasattr(dscd, 'buffer_lock'):\n                lock = dscd.buffer_lock\n            elif hasattr(dscd, 'clustering_lock'):\n                lock = dscd.clustering_lock\n\n            if lock:\n                with lock:\n                    stores = dict(dscd.prototype_stores)\n            else:\n                stores = dict(dscd.prototype_stores)\n\n            for token, store in stores.items():\n                try:\n                    if hasattr(store, 'size') and callable(getattr(store, 'size', None)):\n                        size_ok = store.size() >= 2\n                    else:\n                        size_val = getattr(store, 'size', None)\n                        if isinstance(size_val, int):\n                            size_ok = size_val >= 2\n                        else:\n                            size_ok = False\n                except Exception:\n                    size_ok = False\n\n                if size_ok:\n                    clean = (\n                        str(token)\n                        .replace('▁', '')\n                        .replace('Ġ', '')\n                        .replace('##', '')\n                        .replace(' ', '')\n                        .strip()\n                        .lower()\n                    )\n                    homographs.add(clean)\n            return homographs\n    except Exception:\n        pass\n    return set()\n\ndef _safe_cleanup():\n    try:\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n        if gc.isenabled():\n            gc.collect()\n    except Exception:\n        pass\n\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN - COMPLETE EXECUTION\")\n    print(\"=\" * 80)\n\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or \"manas0003\"\n    start_time = time.time()\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    print(\"\\n[CONFIGURATION]\")\n    print(f\"  Cell 0 status: {'Loaded' if cell0_loaded else 'Using fallbacks'}\")\n    print(f\"  Samples: {_NUM_SAMPLES}\")\n    print(f\"  Epochs: {_EPOCHS}\")\n    print(f\"  Batch Size: {_BATCH_SIZE}\")\n    print(f\"  Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"  Device: {_DEVICE}\")\n    print(f\"  Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPUs)\")\n    print(f\"  Span threshold: {_SPAN_THRESHOLD}\")\n    print(f\"  Uncertainty threshold: {_TAU_LOW}\")\n    print(f\"  Discovery frequency: {_PERIODIC_DISCOVERY_FREQUENCY}\")\n\n    config_issues = []\n    if abs(_SPAN_THRESHOLD - 0.12) > 0.001:\n        config_issues.append(f\"SPAN_THRESHOLD={_SPAN_THRESHOLD} should be 0.12\")\n    if abs(_TAU_LOW - 0.15) > 0.001:\n        config_issues.append(f\"TAU_LOW={_TAU_LOW} should be 0.15\")\n    if _PERIODIC_DISCOVERY_FREQUENCY <= 0:\n        config_issues.append(\"Discovery frequency <= 0 (disabled)\")\n    elif _PERIODIC_DISCOVERY_FREQUENCY != 50:\n        config_issues.append(f\"Discovery frequency={_PERIODIC_DISCOVERY_FREQUENCY} should be 50\")\n    \n    if config_issues:\n        print(\"\\n  [CONFIG WARNINGS]\")\n        for issue in config_issues:\n            print(f\"    - {issue}\")\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(_BATCH_SIZE, _NUM_GPUS)\n        print(f\"  Batch per GPU: {per_gpu}\")\n\n    print(f\"  ASBN: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"  TRG: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"  Debug: {'Enabled' if _DEBUG_DISCOVERY else 'Disabled'}\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n    pipeline_success = False\n    failure_category = None\n    failure_details = \"\"\n\n    if 'main_pipeline' not in globals():\n        print(\"\\nERROR: main_pipeline not found\")\n        print(\"   -> Run Cell 10 before executing Cell 11\")\n        failure_category = \"MISSING_DEPENDENCY\"\n        failure_details = \"Cell 10 not executed\"\n    else:\n        try:\n            print(\"\\nStarting pipeline...\")\n\n            if _DEBUG_TIMING:\n                print(\"   Expected: ~15-45 min (config dependent)\")\n\n            pipeline_start = time.time()\n            trained_model, tokenizer = main_pipeline()\n            pipeline_duration = time.time() - pipeline_start\n\n            print(f\"\\nPipeline completed: {_format_duration(pipeline_duration)}\")\n            pipeline_success = True\n\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted by user\")\n            failure_category = \"USER_INTERRUPT\"\n            failure_details = \"Manual stop\"\n\n        except RuntimeError as e:\n            msg = str(e).lower()\n\n            if \"embed_dim\" in msg or \"unexpected keyword argument\" in msg:\n                print(\"\\nDSCD Initialization Error\")\n                print(f\"   {str(e)}\")\n                failure_category = \"DSCD_INIT_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix in Cell 6 (MemoryOptimizedTATNWithExplanations.__init__):\")\n                print(\"   Find line ~70-80:\")\n                print(\"      self.dscd = dscdcls(embed_dim=embeddim, ...)\")\n                print(\"   Replace with:\")\n                print(\"      self.dscd = dscdcls(embeddim=embeddim, ...)\")\n                print(\"   (Change parameter name from 'embed_dim' to 'embeddim')\")\n                print(\"\\n   Then re-run Cell 6, Cell 10, and Cell 11\")\n\n            elif \"tokenizer\" in msg or \"sentencepiece\" in msg:\n                print(\"\\nTokenizer error\")\n                failure_category = \"TOKENIZER_ERROR\"\n                failure_details = str(e)[:200]\n\n                print(\"\\nFix:\")\n                print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n                print(\"   Then RESTART kernel and re-run Cells 0-11\")\n\n            elif \"out of memory\" in msg:\n                print(\"\\nOut of Memory\")\n                failure_category = \"OOM_ERROR\"\n                failure_details = \"GPU OOM\"\n\n                print(\"\\nFixes (edit in Cell 0):\")\n                print(\"   BATCH_SIZE = 2\")\n                print(\"   NUM_SAMPLES = 15000\")\n                print(\"   ACCUMULATION_STEPS = 32\")\n                print(\"   Then re-run Cells 0-11\")\n\n            else:\n                print(f\"\\nRuntime error: {type(e).__name__}\")\n                print(f\"   {str(e)[:400]}\")\n                failure_category = \"RUNTIME_ERROR\"\n                failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"\\nUnexpected error: {type(e).__name__}\")\n            print(f\"   {str(e)[:400]}\")\n            failure_category = \"UNKNOWN_ERROR\"\n            failure_details = str(e)[:200]\n\n            if _VERBOSE_LOGGING or _DEBUG_DISCOVERY:\n                print(\"\\n[TRACEBACK]\")\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n    if pipeline_success and trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE SUCCEEDED\")\n        print(\"=\" * 80)\n\n        print(\"\\n[CHECKPOINT]\")\n        checkpoint_valid = False\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                size_mb = os.path.getsize(_CHECKPOINT_PATH) / (1024**2)\n                print(f\"  File: {_CHECKPOINT_PATH}\")\n                print(f\"  Size: {size_mb:.1f} MB\")\n\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n\n                has_model = 'model_state_dict' in ckpt and ckpt.get('model_state_dict') is not None and len(ckpt['model_state_dict']) > 0\n                has_dscd = 'dscd_state' in ckpt and ckpt.get('dscd_state') is not None and len(ckpt.get('dscd_state', {})) > 0\n\n                print(f\"  Model: {'Present' if has_model else 'MISSING'}\")\n                print(f\"  DSCD: {'Present' if has_dscd else 'MISSING'}\")\n\n                if has_dscd:\n                    dscd_state_dict = ckpt.get('dscd_state', {})\n                    num_tokens = 0\n                    if dscd_state_dict is not None and 'prototype_stores' in dscd_state_dict:\n                        proto_stores = dscd_state_dict['prototype_stores']\n                        if isinstance(proto_stores, dict):\n                            num_tokens = len(proto_stores)\n                    \n                    print(f\"  Tokens: {num_tokens}\")\n\n                    if num_tokens > 0:\n                        checkpoint_valid = True\n                        print(\"  Status: VALID ✓\")\n                    else:\n                        print(\"  Status: EMPTY DSCD (no prototypes)\")\n                        print(\"    -> Run warmup or check DSCD clustering enabled\")\n                else:\n                    print(\"  Status: MISSING DSCD STATE\")\n                    print(\"    -> DSCD was not saved properly\")\n            else:\n                print(f\"  NOT FOUND: {_CHECKPOINT_PATH}\")\n\n        except Exception as e:\n            print(f\"  Validation failed: {e}\")\n\n        print(\"\\n[COMPONENTS]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd = getattr(core, 'dscd', None)\n            if dscd:\n                if hasattr(dscd, 'get_prototype_summary'):\n                    try:\n                        dscd_stats = dscd.get_prototype_summary()\n                        print(\"  DSCD:\")\n                        print(f\"    - Tokens: {dscd_stats.get('total_tokens', 0)}\")\n                        print(f\"    - Prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n                        print(f\"    - Homographs: {dscd_stats.get('num_homographs', 0)}\")\n                    except Exception:\n                        pass\n                else:\n                    proto_stores = getattr(dscd, 'prototype_stores', None)\n                    if proto_stores is not None:\n                        lock = None\n                        if hasattr(dscd, 'buffer_lock'):\n                            lock = dscd.buffer_lock\n                        elif hasattr(dscd, 'clustering_lock'):\n                            lock = dscd.clustering_lock\n                        \n                        if lock:\n                            with lock:\n                                num_tokens = len(proto_stores)\n                        else:\n                            num_tokens = len(proto_stores)\n                        \n                        print(\"  DSCD:\")\n                        print(f\"    - Tokens: {num_tokens}\")\n\n            asbn = getattr(core, 'asbn', None)\n            if asbn and hasattr(asbn, 'get_detailed_stats'):\n                try:\n                    asbn_stats = asbn.get_detailed_stats()\n                    print(\"  ASBN:\")\n                    print(f\"    - Domain accuracy: {asbn_stats.get('domain_accuracy', 0):.2%}\")\n                    if 'source_accuracy' in asbn_stats:\n                        print(f\"    - Source: {asbn_stats['source_accuracy']:.2%}\")\n                        print(f\"    - Target: {asbn_stats['target_accuracy']:.2%}\")\n                except Exception:\n                    pass\n\n            trg = getattr(core, 'trg_system', None)\n            if trg and hasattr(trg, 'get_statistics'):\n                try:\n                    trg_stats = trg.get_statistics()\n                    print(\"  TRG:\")\n                    print(f\"    - Explanations: {trg_stats.get('explanations_generated', 0)}\")\n                    print(f\"    - High confidence: {trg_stats.get('high_confidence_rate', 0):.1%}\")\n                    print(f\"    - DSCD homograph rate: {trg_stats.get('dscd_homograph_rate', 0):.1%}\")\n                except Exception:\n                    pass\n\n        except Exception as e:\n            print(f\"  Stats failed: {e}\")\n\n        print(\"\\n[METRICS]\")\n\n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n\n                training_stats = ckpt.get('training_stats', {})\n                if training_stats is not None and isinstance(training_stats, dict):\n                    total_loss = training_stats.get('total_loss', [])\n                    updates = training_stats.get('optimizer_updates', 0)\n\n                    print(\"  Training:\")\n                    print(f\"    - Updates: {updates}\")\n                    if total_loss:\n                        if len(total_loss) >= 100:\n                            final = sum(total_loss[-100:]) / len(total_loss[-100:])\n                        else:\n                            final = sum(total_loss) / len(total_loss)\n                        print(f\"    - Final loss: {final:.6f}\")\n\n                eval_results = ckpt.get('eval_results', {})\n                baseline = ckpt.get('baseline_metrics', {})\n\n                if eval_results is not None and isinstance(eval_results, dict):\n                    final_success = eval_results.get('success_rate_pct', 0)\n                    total_expl = eval_results.get('total_explanations', 0)\n\n                    print(\"  Evaluation:\")\n                    if baseline is not None and isinstance(baseline, dict):\n                        baseline_success = baseline.get('success_rate_pct', 0)\n                        improvement = final_success - baseline_success\n                        print(f\"    - Baseline -> Final: {baseline_success:.1f}% -> {final_success:.1f}%\")\n                        print(f\"    - Improvement: {improvement:+.1f}%\")\n                    else:\n                        print(f\"    - Success: {final_success:.1f}%\")\n\n                    print(f\"    - Explanations: {total_expl}\")\n\n                    quality = eval_results.get('quality_metrics', {})\n                    if quality is not None and isinstance(quality, dict):\n                        print(f\"    - Avg confidence: {quality.get('avg_confidence', 0):.3f}\")\n\n        except Exception as e:\n            print(f\"  Metrics failed: {e}\")\n\n        print(\"\\n[INFERENCE VALIDATION]\")\n        \n        core_for_inf = trained_model.module if hasattr(trained_model, 'module') else trained_model\n        trg = getattr(core_for_inf, 'trg_system', None)\n        trg_mode_ok = False\n        if trg is not None:\n            trg_mode = getattr(trg, 'training', True)\n            if trg_mode:\n                print(\"  [WARNING] TRG in training mode - switching to eval\")\n                try:\n                    trg.eval()\n                    trg_mode_after = getattr(trg, 'training', True)\n                    if not trg_mode_after:\n                        trg_mode_ok = True\n                        print(\"  [OK] TRG successfully switched to eval mode\")\n                    else:\n                        print(\"  [WARNING] TRG.eval() called but still in training mode\")\n                except Exception as e:\n                    print(f\"  [ERROR] Failed to switch TRG to eval: {e}\")\n            else:\n                trg_mode_ok = True\n        \n        inference_span_threshold = _SPAN_THRESHOLD\n        inference_uncertainty_threshold = _TAU_LOW\n        \n        try:\n            if os.path.exists(_CHECKPOINT_PATH):\n                ckpt = torch.load(_CHECKPOINT_PATH, map_location='cpu')\n                config = ckpt.get('config', {})\n                if config is not None and isinstance(config, dict):\n                    ckpt_span = config.get('span_threshold', None)\n                    ckpt_uncertainty = config.get('uncertainty_threshold', None)\n                    if ckpt_span is not None:\n                        inference_span_threshold = float(ckpt_span)\n                    if ckpt_uncertainty is not None:\n                        inference_uncertainty_threshold = float(ckpt_uncertainty)\n        except Exception:\n            pass\n        \n        print(f\"  Using thresholds: span={inference_span_threshold:.2f}, uncertainty={inference_uncertainty_threshold:.2f}\")\n        print(\"\\nTesting disambiguation on ambiguous sentences...\")\n        print(\"-\" * 80)\n\n        _safe_cleanup()\n\n        inference_success = 0\n        inference_failed = 0\n        dscd_homographs_detected = set()\n        explained_words_all = set()\n\n        dscd_homographs = _get_dscd_homographs(trained_model)\n        print(f\"DSCD discovered: {len(dscd_homographs)} homographs\")\n        if dscd_homographs and _DEBUG_DISCOVERY:\n            print(f\"  Sample: {list(dscd_homographs)[:10]}\")\n\n        test_sentences = [\n            (\"আমি কল বন্ধ করেছি।\", \"কল (tap/call)\"),\n            (\"কাল আমি বই কিনব।\", \"কাল (tomorrow/yesterday)\"),\n            (\"পাতা ঝরে পড়েছে।\", \"পাতা (leaf/page)\"),\n        ]\n\n        inference_times = []\n\n        try:\n            if 'translate_with_explanations' not in globals():\n                print(\"translate_with_explanations not available\")\n                print(\"   -> Run Cell 8 before Cell 11\")\n            else:\n                for idx, (sentence, desc) in enumerate(test_sentences, 1):\n                    try:\n                        print(f\"\\n{idx}.  {desc}\")\n                        print(f\"   Input: {sentence}\")\n\n                        inf_start = time.time()\n                        res = translate_with_explanations(\n                            trained_model, \n                            tokenizer, \n                            sentence,\n                            span_threshold=inference_span_threshold,\n                            uncertainty_threshold=inference_uncertainty_threshold\n                        )\n                        inf_time = time.time() - inf_start\n                        inference_times.append(inf_time)\n\n                        if isinstance(res, dict):\n                            translation = res.get('translation', 'N/A')\n                            amb_count = res.get('ambiguous_words_detected', 0)\n                            exs = res.get('explanations', []) or []\n\n                            print(f\"   Translation: {translation}\")\n                            print(f\"   Ambiguous: {amb_count}\")\n                            print(f\"   Time: {inf_time:.3f}s\")\n\n                            if exs:\n                                for exp in exs:\n                                    word = exp.get('ambiguous_word', exp.get('token', 'N/A'))\n                                    clean = (\n                                        str(word)\n                                        .replace('▁', '')\n                                        .replace('Ġ', '')\n                                        .replace('##', '')\n                                        .replace(' ', '')\n                                        .strip()\n                                        .lower()\n                                    )\n                                    \n                                    explained_words_all.add(clean)\n\n                                    if clean in dscd_homographs:\n                                        dscd_homographs_detected.add(clean)\n\n                                    try:\n                                        conf = float(exp.get('confidence', 0.5))\n                                        span = float(exp.get('span', 0.0))\n                                        u = float(exp.get('uncertainty', 0.0))\n                                        \n                                        in_dscd_marker = \"[D]\" if clean in dscd_homographs else \"   \"\n                                        print(f\"   {in_dscd_marker} '{word}': conf={conf:.3f}, s={span:.3f}, u={u:.3f}\")\n                                    except Exception:\n                                        print(f\"   -> '{word}': (no metrics)\")\n\n                                inference_success += 1\n                            else:\n                                print(\"   No explanations\")\n                                inference_success += 1\n                        else:\n                            print(\"   Unexpected format\")\n                            inference_failed += 1\n\n                        _safe_cleanup()\n\n                    except Exception as e:\n                        print(f\"   Failed: {type(e).__name__}\")\n                        if _DEBUG_DISCOVERY:\n                            print(f\"   Error: {str(e)[:100]}\")\n                        inference_failed += 1\n\n                print(\"\\n\" + \"-\" * 80)\n                print(f\"Results: {inference_success}/{len(test_sentences)} successful\")\n\n                if inference_times:\n                    avg_time = sum(inference_times) / len(inference_times)\n                    min_time = min(inference_times)\n                    max_time = max(inference_times)\n                    print(f\"Performance: {avg_time:.3f}s avg ({min_time:.3f}s min, {max_time:.3f}s max)\")\n\n                if dscd_homographs_detected:\n                    print(f\"DSCD homographs detected: {', '.join(sorted(dscd_homographs_detected))}\")\n                    coverage = len(dscd_homographs_detected) / len(dscd_homographs) if dscd_homographs else 0\n                    print(f\"  Coverage: {coverage:.1%} of DSCD discovered homographs\")\n                else:\n                    print(\"No DSCD homographs detected in explanations\")\n                    if len(dscd_homographs) == 0:\n                        print(\"   -> DSCD has no discoveries (run warmup)\")\n                    elif len(explained_words_all) > 0:\n                        print(f\"   -> Explanations generated but not matching DSCD ({len(explained_words_all)} unique words)\")\n                        print(f\"   -> Check thresholds (span={inference_span_threshold}, u={inference_uncertainty_threshold})\")\n                    else:\n                        print(f\"   -> No explanations generated at all\")\n                        print(f\"   -> Check TRG mode ({'eval' if trg_mode_ok else 'training'})\")\n\n        except Exception as e:\n            print(f\"Validation failed: {e}\")\n            if _DEBUG_DISCOVERY:\n                try:\n                    traceback.print_exc()\n                except Exception:\n                    pass\n\n        print(\"\\n[SYSTEM TEST]\")\n\n        try:\n            core = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n            dscd_ok = False\n            if hasattr(core, 'dscd') and core.dscd is not None:\n                if hasattr(core.dscd, 'forward'):\n                    proto_stores = getattr(core.dscd, 'prototype_stores', None)\n                    if proto_stores is not None:\n                        lock = None\n                        if hasattr(core.dscd, 'buffer_lock'):\n                            lock = core.dscd.buffer_lock\n                        elif hasattr(core.dscd, 'clustering_lock'):\n                            lock = core.dscd.clustering_lock\n                        \n                        if lock:\n                            with lock:\n                                dscd_ok = len(proto_stores) > 0\n                        else:\n                            dscd_ok = len(proto_stores) > 0\n            \n            asbn_ok = hasattr(core, 'asbn') and hasattr(core.asbn, 'forward')\n            trg_ok = hasattr(core, 'trg_system') and hasattr(core.trg_system, 'process_sentence_for_explanations') and trg_mode_ok\n            mbart_ok = hasattr(core, 'mbart') and hasattr(core.mbart, 'generate')\n\n            print(\"  Component status:\")\n            print(f\"    - DSCD: {'OK (with prototypes)' if dscd_ok else 'MISSING/EMPTY'}\")\n            print(f\"    - ASBN: {'OK' if asbn_ok else 'MISSING'}\")\n            print(f\"    - TRG: {'OK (eval mode)' if trg_ok else 'MISSING/TRAINING MODE'}\")\n            print(f\"    - M2M100: {'OK' if mbart_ok else 'MISSING'}\")\n\n            all_ok = dscd_ok and asbn_ok and trg_ok and mbart_ok\n\n            if all_ok:\n                print(\"  All components operational ✓\")\n            else:\n                print(\"  Some components missing or misconfigured\")\n                if not dscd_ok:\n                    print(\"    -> DSCD: run warmup or check clustering enabled\")\n                if not trg_ok:\n                    print(\"    -> TRG: switch to eval mode\")\n\n        except Exception as e:\n            print(f\"  Test failed: {e}\")\n\n        print(\"\\n\" + \"=\" * 80)\n        print(\"NEXT STEPS\")\n        print(\"=\" * 80)\n\n        print(\"\\n1. Single translation:\")\n        print(\"   result = translate_with_explanations(trained_model, tokenizer, 'আমি কল বন্ধ করেছি।')\")\n\n        print(\"\\n2. Batch translation:\")\n        print(\"   for sent in sentences:\")\n        print(\"       res = translate_with_explanations(trained_model, tokenizer, sent)\")\n\n        print(\"\\n3. Load checkpoint:\")\n        print(\"   ckpt = torch.load('/kaggle/working/tatn_final.pt')\")\n        print(\"   model.load_state_dict(ckpt['model_state_dict'])\")\n        print(\"   model.dscd.load_state_dict(ckpt['dscd_state'])\")\n\n        print(\"\\n4. Full evaluation:\")\n        print(\"   results = comprehensive_post_training_testing(trained_model, tokenizer)\")\n\n        print(\"\\n5. Demo:\")\n        print(\"   demonstrate_system(trained_model, tokenizer)\")\n\n        if not checkpoint_valid:\n            print(\"\\n[ACTION REQUIRED]\")\n            print(\"  Checkpoint needs verification - DSCD state may be empty\")\n            print(\"  -> Run dscd_discovery_warmup(trained_model, tokenizer) to populate\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"PIPELINE FAILED\")\n        print(\"=\" * 80)\n\n        print(f\"\\nCategory: {failure_category or 'UNKNOWN'}\")\n        if failure_details:\n            print(f\"Details: {failure_details[:200]}\")\n\n        print(\"\\n[DIAGNOSTICS]\")\n\n        components = {\n            'Cell 0': 'NUM_SAMPLES' in globals(),\n            'Cell 1': 'reconstruct_word_spans' in globals(),\n            'Cell 2': 'MemoryEfficientDataset' in globals(),\n            'Cell 3': 'MemoryEfficientDSCDOnline' in globals(),\n            'Cell 4': 'MemoryEfficientASBNModule' in globals(),\n            'Cell 5': 'CompleteTRGWithExplanations' in globals(),\n            'Cell 6': 'MemoryOptimizedTATNWithExplanations' in globals(),\n            'Cell 7': 'train_memory_efficient_tatn' in globals(),\n            'Cell 8': 'translate_with_explanations' in globals(),\n            'Cell 9': 'comprehensive_post_training_testing' in globals(),\n            'Cell 10': 'main_pipeline' in globals(),\n        }\n\n        all_present = True\n        for comp, present in components.items():\n            status = \"OK\" if present else \"MISSING\"\n            print(f\"  {status:7} {comp}\")\n            if not present:\n                all_present = False\n\n        if all_present:\n            span_thresh_global = globals().get('SPAN_THRESHOLD', None)\n            tau_low_global = globals().get('TAU_LOW', None)\n            discovery_freq_global = globals().get('PERIODIC_DISCOVERY_FREQUENCY', None)\n            \n            threshold_ok = True\n            if span_thresh_global is not None:\n                if abs(float(span_thresh_global) - 0.12) > 0.001:\n                    print(f\"\\n  [WARNING] SPAN_THRESHOLD={span_thresh_global} should be 0.12 (set in Cell 0)\")\n                    threshold_ok = False\n            \n            if tau_low_global is not None:\n                if abs(float(tau_low_global) - 0.15) > 0.001:\n                    print(f\"  [WARNING] TAU_LOW={tau_low_global} should be 0.15 (set in Cell 0)\")\n                    threshold_ok = False\n            \n            if discovery_freq_global is not None:\n                if int(discovery_freq_global) != 50:\n                    print(f\"  [WARNING] PERIODIC_DISCOVERY_FREQUENCY={discovery_freq_global} should be 50 (set in Cell 0)\")\n                    threshold_ok = False\n            \n            if not threshold_ok:\n                print(\"  -> Fix thresholds in Cell 0 and re-run Cells 0-11\")\n\n        print(\"\\n[RECOVERY]\")\n\n        if failure_category == \"MISSING_DEPENDENCY\":\n            print(\"\\n-> Run Cells 0-10 in sequence, then re-run Cell 11\")\n            print(\"   Order: 0 -> 1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> 8 -> 9 -> 10 -> 11\")\n\n        elif failure_category == \"DSCD_INIT_ERROR\":\n            print(\"\\n-> Fix parameter name in Cell 6:\")\n            print(\"   In MemoryOptimizedTATNWithExplanations.__init__() around line 70-80:\")\n            print(\"   Search for:\")\n            print(\"      self.dscd = dscdcls(embed_dim=embeddim, ...)\")\n            print(\"   Replace with:\")\n            print(\"      self.dscd = dscdcls(embeddim=embeddim, ...)\")\n            print(\"   (Change 'embed_dim' to 'embeddim' - no underscore)\")\n            print(\"\\n   Then re-run: Cell 6 -> Cell 10 -> Cell 11\")\n\n        elif failure_category == \"TOKENIZER_ERROR\":\n            print(\"\\n-> Install dependencies:\")\n            print(\"   ! pip install transformers==4.30.2 sentencepiece tokenizers\")\n            print(\"   Then RESTART kernel and re-run Cells 0-11\")\n\n        elif failure_category == \"OOM_ERROR\":\n            print(\"\\n-> Reduce memory usage in Cell 0:\")\n            print(\"   BATCH_SIZE = 2\")\n            print(\"   NUM_SAMPLES = 15000\")\n            print(\"   ACCUMULATION_STEPS = 32\")\n            print(\"   Then re-run Cells 0-11\")\n\n        elif failure_category == \"RUNTIME_ERROR\":\n            print(\"\\n-> Enable debug in Cell 0:\")\n            print(\"   VERBOSE_LOGGING = True\")\n            print(\"   DEBUG_DISCOVERY = True\")\n            print(\"   Then re-run Cell 11 for detailed traceback\")\n\n        elif failure_category == \"USER_INTERRUPT\":\n            print(\"\\n-> Check if checkpoint exists:\")\n            print(f\"   import os\")\n            print(f\"   os.path.exists('{_CHECKPOINT_PATH}')\")\n            print(\"   If checkpoint exists, can load it and skip training:\")\n            print(\"   -> Run Cell 8 (inference) directly\")\n\n        else:\n            print(\"\\n-> General recovery steps:\")\n            print(\"   1. Enable DEBUG in Cell 0:\")\n            print(\"      VERBOSE_LOGGING = True\")\n            print(\"      DEBUG_DISCOVERY = True\")\n            print(\"   2. Re-run Cells 0-11\")\n            print(\"   3. Check GPU availability:\")\n            print(\"      torch.cuda.is_available()\")\n            print(\"   4. Verify sufficient GPU memory\")\n\n        print(\"\\n\" + \"=\" * 80)\n\n    total_duration = time.time() - start_time\n    end_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EXECUTION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n    print(f\"Finished: {end_utc}\")\n    print(f\"Duration: {_format_duration(total_duration)}\")\n\n    if pipeline_success:\n        print(\"Status: SUCCESS ✓\")\n        if 'checkpoint_valid' in locals() and checkpoint_valid:\n            print(\"Checkpoint: VALID ✓\")\n        else:\n            print(\"Checkpoint: NEEDS ATTENTION\")\n            print(\"  -> Run warmup to populate DSCD prototypes\")\n    else:\n        print(f\"Status: FAILED ({failure_category or 'UNKNOWN'})\")\n        print(f\"  -> See [RECOVERY] section above for fix instructions\")\n\n    print(\"=\" * 80)\n\n    _safe_cleanup()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Cell 11: Execution wrapper ready (FINAL) - FIXED\")\nprint(\"=\" * 80 + \"\\n\")\n","metadata":{"id":"9n4Hrn1wH4J6","trusted":true},"outputs":[],"execution_count":null}]}