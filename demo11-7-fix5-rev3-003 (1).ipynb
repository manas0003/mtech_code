{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13304883,"sourceType":"datasetVersion","datasetId":8433540},{"sourceId":13682320,"sourceType":"datasetVersion","datasetId":8700986}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers sentence-transformers\n!pip install transformers==4.30.2 --no-deps --force-reinstall\n!pip install sentencepiece tokenizers sacremoses\n!pip install scipy scikit-learn\n!pip install --upgrade \"protobuf==3.20.3\"\n# optional:\n!pip install sentence-transformers==2.2.2\n!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:54:20.083772Z","iopub.execute_input":"2025-11-22T14:54:20.084022Z","iopub.status.idle":"2025-11-22T14:55:58.942316Z","shell.execute_reply.started":"2025-11-22T14:54:20.083994Z","shell.execute_reply":"2025-11-22T14:55:58.941570Z"},"id":"W8IIWAEHH4Jy","trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nFound existing installation: sentence-transformers 4.1.0\nUninstalling sentence-transformers-4.1.0:\n  Successfully uninstalled sentence-transformers-4.1.0\nCollecting transformers==4.30.2\n  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\nSuccessfully installed transformers-4.30.2\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.36.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2025.11.3)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.10.5)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy) (2024.2.0)\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\nCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.30.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.15.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2025.11.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->sentence-transformers==2.2.2) (2.4.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->sentence-transformers==2.2.2) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->sentence-transformers==2.2.2) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->sentence-transformers==2.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->sentence-transformers==2.2.2) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->sentence-transformers==2.2.2) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=5ae95efb6f7aa9a87f55d2744fa3309f1d4dccaf6ca0830a42e6fef9d9e0b4ea\n  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\nSuccessfully built sentence-transformers\nInstalling collected packages: tokenizers, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-2.2.2 tokenizers-0.13.3\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:55:58.944390Z","iopub.execute_input":"2025-11-22T14:55:58.944621Z","iopub.status.idle":"2025-11-22T14:56:01.249930Z","shell.execute_reply.started":"2025-11-22T14:55:58.944597Z","shell.execute_reply":"2025-11-22T14:56:01.249137Z"}},"outputs":[{"name":"stdout","text":"4.30.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# CELL 0 (FIXED): ⚡ OPTIMIZED ULTRA-FAST TATN CONFIGURATION (DEBUGGED + FIXED)\n# ==============================================================================\n# What I changed (summary):\n# - Fixed _get_csv_row_count: correct chunk-based counting and robust fallback to line counting.\n# - Hardened safe_tokenize_with_offsets to handle BatchEncoding shapes (tensor / list) reliably.\n# - Safer reading of CSV line-count sample check (text mode with errors='ignore').\n# - Minor defensive guards for pandas/transformers presence in places where they are used.\n# - Small clarifying comments and consistent returns for helper functions.\n#\n# These changes remove the primary logic bugs and make the cell robust in common environments.\n# ==============================================================================\n\nimport os\nimport sys\nimport math\nimport random\nimport re\nimport unicodedata\nimport time\nimport threading\nfrom collections import deque, defaultdict\nfrom typing import List, Dict, Tuple, Optional, Union\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nimport gc\n\n# Add pandas for CSV reading (optional but recommended)\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept Exception:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[WARN] pandas not available; CSV loading/validation will be skipped\")\n\n# Prefer fast tokenizer class if available, but be resilient if not\ntry:\n    # Try to import fast variant first (no model download here)\n    from transformers import M2M100TokenizerFast as M2M100Tokenizer\nexcept Exception:\n    try:\n        from transformers import M2M100Tokenizer  # type: ignore\n    except Exception:\n        M2M100Tokenizer = None\n\n# datasets import is used in other data cells; keep import but avoid heavy ops here\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\n# Reduce noisy warnings; keep tokenizer workers single-threaded for stability\nwarnings.filterwarnings(\"ignore\")\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\nos.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n\n# ==============================================================================\n# HARDWARE / DEVICE DETECTION\n# ==============================================================================\nNUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\nUSE_MULTI_GPU = NUM_GPUS > 1\nCUDA_AVAILABLE = torch.cuda.is_available()\n\n# For general code simplicity prefer \"cuda\" device (lets torch pick device:0)\nif CUDA_AVAILABLE:\n    DEVICE = torch.device(\"cuda\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n\nif USE_MULTI_GPU and CUDA_AVAILABLE:\n    print(f\"[Cell 0] Multi-GPU Mode: {NUM_GPUS} GPUs available (using device={DEVICE})\")\nelse:\n    mode = \"Single GPU Mode\" if CUDA_AVAILABLE else \"CPU Mode\"\n    print(f\"[Cell 0] {mode} (using device={DEVICE})\")\n\nprint(f\"[Cell 0] Device: {DEVICE} (visible GPUs: {NUM_GPUS})\")\n\n# ==============================================================================\n# DATASET CONFIGURATION (LOCAL CSV FILE) - update path to your dataset\n# ==============================================================================\nDATASET_CSV_PATH = \"/kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\"  # ← CHANGE THIS\n\n# Validate dataset path exists (early warning). If not present we keep a small fallback.\nif not os.path.exists(DATASET_CSV_PATH):\n    print(f\"[WARN] Dataset CSV not found at: {DATASET_CSV_PATH}\")\n    print(\"[WARN] Training will use a small fallback dataset (to avoid immediate crash).\")\n    _CSV_AVAILABLE = False\nelse:\n    print(f\"[INFO] Dataset CSV found: {DATASET_CSV_PATH}\")\n    _CSV_AVAILABLE = True\n\n# If pandas is available and CSV exists, try a lightweight validation\ndef _get_csv_row_count(path: str) -> Optional[int]:\n    \"\"\"\n    Return the number of rows in a CSV using pandas chunks (memory-efficient).\n    Fallbacks to a safe text-mode line counting if chunked read fails.\n    If pandas not available or file not present, returns None.\n    \"\"\"\n    if not _HAS_PANDAS or not os.path.exists(path):\n        return None\n    try:\n        # Use chunksize iteration and sum actual chunk lengths\n        count = 0\n        for chunk in pd.read_csv(path, chunksize=100000, usecols=[0], dtype=str):\n            count += len(chunk)\n        return int(count)\n    except Exception:\n        # Fallback: try a robust text-mode line count (handles large files)\n        try:\n            cnt = 0\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                for _ in f:\n                    cnt += 1\n            return int(cnt)\n        except Exception:\n            return None\n\nif _CSV_AVAILABLE and _HAS_PANDAS:\n    try:\n        _test_df = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n        if 'src' not in _test_df.columns or 'tgt' not in _test_df.columns:\n            print(f\"[ERROR] CSV missing required columns 'src' and/or 'tgt'. Found: {list(_test_df.columns)}\")\n        else:\n            print(f\"[INFO] CSV validation passed (columns: {list(_test_df.columns)})\")\n        del _test_df\n    except Exception as e:\n        print(f\"[WARN] Could not validate CSV structure: {e}\")\n\n# ==============================================================================\n# ULTRA-FAST CONFIGURATION (user-tunable)\n# ==============================================================================\n\nBATCH_SIZE = 100              # batch size per step\nNUM_SAMPLES = 50000           # Maximum samples to load from CSV (cap)\nMAX_LENGTH = 48               # Maximum sequence length for tokenization\nLR_NMT = 2e-5                 # Learning rate for main NMT model\nLR_TRG = 1e-5                 # Learning rate for TRG component\nLR_PHI = 1e-5                 # Learning rate for sense disambiguation\nEPOCHS = 2                    # Number of training epochs\nGRAD_CLIP_NORM = 1.0          # Gradient clipping threshold\nUSE_AMP = True                # Automatic Mixed Precision (saves memory)\nPRINT_INTERVAL = 500          # Print training stats every N steps\nSEED = 42                     # Random seed for reproducibility\n\n# ==============================================================================\n# MEMORY / PERFORMANCE SETTINGS\n# ==============================================================================\n\n# NOTE: the default of 16 accumulation steps is intentional; keep as constant\nACCUMULATION_STEPS = max(1, 16)       # must be >= 1\nMC_DROPOUT_PASSES = 0                 # Monte Carlo dropout passes (0 = disabled)\nTRG_EVIDENCE_K = 3                    # Top-K evidence for TRG\nMAX_SILVER_BUFFER = 50                # Maximum silver label buffer size\n\nNUM_WORKERS = max(0, 2)               # DataLoader workers (0 safe fallback)\n# Pin memory only if CUDA is available\nPIN_MEMORY = bool(CUDA_AVAILABLE)\nPREFETCH_FACTOR = 2                   # Number of batches to prefetch per worker\n\n# ==============================================================================\n# DSCD PARAMETERS (balanced defaults; change if you know resource limits)\n# ==============================================================================\n\nDSCD_BUFFER_SIZE = 20\nDSCD_MAX_PROTOS = 8\nDSCD_N_MIN = 5\nDSCD_DISPERSION_THRESHOLD = 0.25\nDSCD_EMBED_DIM = 1024\nDSCD_TEMPERATURE = 0.7\nDSCD_DROPOUT = 0.1\nDSCD_AUGMENT_SCALE = 0.1\nDSCD_ENABLE_TRAINING_CLUSTERING = True\nDSCD_WARMUP_SAMPLES = 8000\n\n# ==============================================================================\n# CONTROL FLAGS\n# ==============================================================================\n\nENABLE_ASBN_TRAINING = True\nENABLE_ASBN_INFERENCE = True\nENABLE_TRG_TRAINING = False\nENABLE_TRG_INFERENCE = True\n\nCLUSTERING_TIMEOUT = 5\nMEMORY_CLEANUP_FREQUENCY = 100\nPERIODIC_DISCOVERY_FREQUENCY = 100\n\nVALIDATION_CHECK_INTERVAL = 200  # 0 = disabled\n\nVERBOSE_LOGGING = False\n\n# ==============================================================================\n# CHECKPOINT SETTINGS\n# ==============================================================================\nCHECKPOINT_DIR = \"./checkpoints\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nCHECKPOINT_INTERVAL = 20000\nSAVE_REPLAY_BUFFER = False\nLOAD_REPLAY_BUFFER = False\nREPLAY_BUFFER_SIZE = 25000\nRESUME_FROM_CHECKPOINT = False\nCHECKPOINT_PATH = \"\"\n\n# ==============================================================================\n# TRG / UNCERTAINTY HYPERPARAMETERS\n# ==============================================================================\nTAU_HIGH = 0.85\nTAU_LOW = 0.4\nTAU_ACCEPT = 0.8\nTRG_MAX_GEN_LEN = 16\nTRG_GEN_EMBED = 64\nTRG_GEN_HID = 64\nSPAN_THRESHOLD = 0.3\n\n# ==============================================================================\n# ASBN PARAMETERS\n# ==============================================================================\nASBN_HIDDEN_DIM = 64\nASBN_LAMBDA = 0.1\nASBN_DROPOUT = 0.1\n\nLAMBDA_ASBN = 0.10\nLAMBDA_DSCD = 0.05\n\n# ==============================================================================\n# LANGUAGE / WATCHLIST\n# ==============================================================================\nBN_LANG = \"bn\"\nEN_LANG = \"en\"\nSOURCE_LANGUAGE = \"bn\"\n\nHOMOGRAPH_WATCHLIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}\nWATCHLIST_ONLY_FOR_TRG = False\n\n# ==============================================================================\n# MEMORY OPTIMIZATION FLAGS\n# ==============================================================================\nGRADIENT_CHECKPOINTING = True\n\n# ==============================================================================\n# UTILITY FUNCTIONS\n# ==============================================================================\n\ndef normalize_bengali(t: str) -> str:\n    \"\"\"Normalize Bengali text using NFKC Unicode normalization.\"\"\"\n    if not t:\n        return \"\"\n    return unicodedata.normalize(\"NFKC\", t).strip()\n\ndef normalize_english(t: str) -> str:\n    \"\"\"Normalize English text: NFKC + lowercase + strip.\"\"\"\n    if not t:\n        return \"\"\n    return unicodedata.normalize(\"NFKC\", t).lower().strip()\n\ndef empty_cuda_cache():\n    \"\"\"Safely empty CUDA cache and run garbage collection.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.empty_cache()\n        except Exception:\n            pass\n\ndef safe_cuda_synchronize():\n    \"\"\"Safely synchronize CUDA operations.\"\"\"\n    if torch.cuda.is_available():\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n\ndef monitor_gpu_usage():\n    \"\"\"Print GPU memory usage for all visible GPUs.\"\"\"\n    if torch.cuda.is_available():\n        visible_gpus = torch.cuda.device_count()\n        for i in range(visible_gpus):\n            try:\n                mem_alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n                print(f\"[GPU] {i}: {mem_alloc:.2f}GB allocated / {mem_reserved:.2f}GB reserved\")\n            except Exception:\n                print(f\"[GPU] {i}: memory stats unavailable\")\n    else:\n        print(\"[GPU] CUDA not available\")\n\n# ==============================================================================\n# TIMEOUT DECORATOR\n# ==============================================================================\n\nclass FunctionTimeoutError(Exception):\n    \"\"\"Custom exception for function timeout.\"\"\"\n    pass\n\ndef with_timeout(seconds):\n    \"\"\"\n    Decorator to enforce timeout on functions.\n    Returns None if function exceeds timeout.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            result = [FunctionTimeoutError(\"Function timed out\")]\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n            thread = threading.Thread(target=target, daemon=True)\n            thread.start()\n            thread.join(timeout=seconds)\n            if thread.is_alive():\n                return None  # Timeout occurred\n            if isinstance(result[0], Exception):\n                if isinstance(result[0], FunctionTimeoutError):\n                    return None\n                # Re-raise original exception for obvious failures\n                raise result[0]\n            return result[0]\n        return wrapper\n    return decorator\n\n# ==============================================================================\n# SPECIAL TOKENS & VALIDATION HELPERS\n# ==============================================================================\n\ndef get_special_tokens(tokenizer) -> set:\n    \"\"\"Extract special tokens from tokenizer safely.\"\"\"\n    try:\n        s = getattr(tokenizer, \"all_special_tokens\", None)\n        if s:\n            return set(s)\n    except Exception:\n        pass\n    # Conservative fallback\n    return {\"<pad>\", \"</s>\", \"<s>\", \"<unk>\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n\n# Lightweight token validity with thread-safe caching\n_token_validation_cache: Dict[Tuple[str, str], bool] = {}\n_cache_lock = threading.Lock()\n_cache_max_size = 10000\n\ndef is_valid_token(token, special_tokens: Optional[set] = None,\n                   tokenizer=None, language: str = \"bn\") -> bool:\n    \"\"\"\n    Check if token is valid for homograph disambiguation.\n    Uses thread-safe caching for performance.\n    \"\"\"\n    token = \"\" if token is None else str(token)\n    cache_key = (token, language)\n\n    # Check cache first\n    with _cache_lock:\n        if cache_key in _token_validation_cache:\n            return _token_validation_cache[cache_key]\n\n    # Clean token (remove common subword markers)\n    clean = token.replace(\"▁\", \"\").replace(\"##\", \"\").strip()\n\n    # Bengali homograph watchlist check (always valid)\n    try:\n        if language == \"bn\" and clean in HOMOGRAPH_WATCHLIST_BN:\n            result = True\n            with _cache_lock:\n                if len(_token_validation_cache) < _cache_max_size:\n                    _token_validation_cache[cache_key] = result\n            return result\n    except Exception:\n        pass\n\n    # Special token check\n    if special_tokens and token in special_tokens:\n        result = False\n    else:\n        # Length check (Bengali needs 2+ chars, English needs 3+)\n        min_len = 2 if language == \"bn\" else 3\n        if len(clean) < min_len:\n            result = False\n        elif not any(c.isalpha() for c in clean):\n            # Must contain at least one alphabetic character\n            result = False\n        else:\n            # Must be at least 60% alphabetic\n            alpha_count = sum(c.isalpha() for c in clean)\n            if alpha_count / max(1, len(clean)) < 0.6:\n                result = False\n            else:\n                result = True\n\n    # Cache result safely\n    with _cache_lock:\n        if len(_token_validation_cache) < _cache_max_size:\n            _token_validation_cache[cache_key] = result\n    return result\n\ndef safe_tokenize_with_offsets(tokenizer, text: str, max_length: int = 512):\n    \"\"\"\n    Safely tokenize text with offset mapping.\n    Returns (tokens, offsets) or (None, None) on failure.\n    Supports tokenizers that follow the HuggingFace call API.\n    \"\"\"\n    try:\n        encoded = tokenizer(\n            text,\n            return_offsets_mapping=True,\n            max_length=max_length,\n            truncation=True,\n            add_special_tokens=False\n        )\n        # extract input_ids robustly\n        input_ids = encoded.get(\"input_ids\", None)\n        # input_ids may be tensor or list-of-lists. Normalize to a plain python list for token conversion.\n        if input_ids is None:\n            # try alternative access\n            if hasattr(encoded, \"data\") and isinstance(encoded.data, dict):\n                input_ids = encoded.data.get(\"input_ids\", None)\n        # Normalize input_ids to list of ints representing first example\n        ids_list = []\n        if isinstance(input_ids, list) and input_ids:\n            # Could be list-of-lists\n            first = input_ids[0]\n            if isinstance(first, list):\n                ids_list = list(first)\n            else:\n                ids_list = list(input_ids)\n        elif hasattr(input_ids, \"tolist\"):\n            try:\n                arr = input_ids.tolist()\n                if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                    ids_list = arr[0]\n                else:\n                    ids_list = arr\n            except Exception:\n                ids_list = []\n        else:\n            ids_list = []\n\n        # offsets extraction\n        offsets = encoded.get(\"offset_mapping\", None)\n        if offsets is None and hasattr(encoded, \"data\") and isinstance(encoded.data, dict):\n            offsets = encoded.data.get(\"offset_mapping\", None)\n\n        # Normalize offsets to list-of-(start,end) for first example\n        offsets_list = []\n        if offsets is not None:\n            if isinstance(offsets, list) and len(offsets) > 0:\n                # offsets might be list of lists: offsets[0] is for first example\n                first = offsets[0] if isinstance(offsets[0], (list, tuple)) else offsets\n                offsets_list = [tuple(o) if isinstance(o, (list, tuple)) and len(o) == 2 else (None, None) for o in first]\n            elif hasattr(offsets, \"tolist\"):\n                try:\n                    arr = offsets.tolist()\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        offsets_list = [tuple(o) if isinstance(o, (list, tuple)) and len(o) == 2 else (None, None) for o in arr[0]]\n                except Exception:\n                    offsets_list = []\n        # convert ids_list -> tokens\n        toks = []\n        if ids_list:\n            try:\n                if hasattr(tokenizer, \"convert_ids_to_tokens\"):\n                    toks = tokenizer.convert_ids_to_tokens(ids_list)\n                else:\n                    # best-effort tokenization fallback\n                    toks = tokenizer.tokenize(text) if hasattr(tokenizer, \"tokenize\") else [str(i) for i in ids_list]\n            except Exception:\n                toks = tokenizer.tokenize(text) if hasattr(tokenizer, \"tokenize\") else [str(i) for i in ids_list]\n        else:\n            # fallback: try tokenizer.tokenize on text\n            try:\n                toks = tokenizer.tokenize(text)\n            except Exception:\n                toks = []\n\n        return toks, offsets_list\n    except Exception:\n        return None, None\n\n# ==============================================================================\n# RANDOM SEEDS & BACKEND TWEAKS\n# ==============================================================================\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    try:\n        torch.cuda.manual_seed_all(SEED)\n    except Exception:\n        pass\n\n# PyTorch performance optimizations (safe guarded)\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    try:\n        torch.set_float32_matmul_precision(\"high\")\n    except Exception:\n        pass\n\n# cuDNN optimizations (benchmark/deterministic balance)\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n\n# ==============================================================================\n# FALLBACK: small synthetic dataset when CSV is missing\n# ==============================================================================\n\nFALLBACK_DATASET = [\n    {\"src\": \"তিনি ব্যাংক গেছেন।\", \"tgt\": \"He went to the bank.\"},\n    {\"src\": \"বার্থ পেয়েছিলাম।\", \"tgt\": \"I received a birthday present.\"},\n    {\"src\": \"সে একটি কল আমাকে দিয়েছে।\", \"tgt\": \"He gave me a call.\"},\n]\n\ndef get_effective_num_samples() -> int:\n    \"\"\"Return the number of samples we will actually attempt to use.\"\"\"\n    if _CSV_AVAILABLE and _HAS_PANDAS:\n        try:\n            # quick probe: read small head to ensure file is readable, then compute count using chunk-based helper\n            _ = pd.read_csv(DATASET_CSV_PATH, nrows=1)\n            cnt = _get_csv_row_count(DATASET_CSV_PATH)\n            if cnt is None:\n                return min(NUM_SAMPLES, len(FALLBACK_DATASET))\n            return min(NUM_SAMPLES, int(cnt))\n        except Exception:\n            return min(NUM_SAMPLES, len(FALLBACK_DATASET))\n    else:\n        return min(NUM_SAMPLES, len(FALLBACK_DATASET))\n\nEFFECTIVE_NUM_SAMPLES = get_effective_num_samples()\n\n# ==============================================================================\n# CONFIGURATION SUMMARY (sanity checks)\n# ==============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"⚡ OPTIMIZED ULTRA-FAST TATN CONFIGURATION (Cell 0 - DEBUGGED)\")\nprint(\"=\"*80)\nprint(f\"User: {os.getenv('KAGGLE_USERNAME', os.getenv('USER', 'manas0003'))}\")\nprint(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC\")\nprint(f\"Multi-GPU: {'ENABLED' if USE_MULTI_GPU else 'DISABLED'} ({NUM_GPUS} GPUs visible)\")\nprint(f\"Dataset source: {'LOCAL CSV' if _CSV_AVAILABLE else 'FALLBACK_EMBEDDED_SMALLSET'}\")\nprint(f\"Dataset path: {DATASET_CSV_PATH}\")\nprint(f\"Dataset samples (cap): {NUM_SAMPLES:,} (effective: {EFFECTIVE_NUM_SAMPLES:,})\")\nprint(f\"Batch Size: {BATCH_SIZE} x {ACCUMULATION_STEPS} grad-accum steps\")\nprint(f\"Effective batch size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\nprint(f\"Max Length: {MAX_LENGTH} tokens\")\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Workers: {NUM_WORKERS}, Prefetch: {PREFETCH_FACTOR}, Pin memory: {PIN_MEMORY}\")\nprint(f\"AMP: {'ENABLED' if USE_AMP else 'DISABLED'}\")\nprint(f\"Validation interval: {VALIDATION_CHECK_INTERVAL} ({'DISABLED' if VALIDATION_CHECK_INTERVAL == 0 else 'ENABLED'})\")\nprint()\nprint(\"DSCD Config:\")\nprint(f\"  Buffer size: {DSCD_BUFFER_SIZE}\")\nprint(f\"  Max prototypes: {DSCD_MAX_PROTOS}\")\nprint(f\"  n_min: {DSCD_N_MIN}\")\nprint(f\"  dispersion threshold: {DSCD_DISPERSION_THRESHOLD}\")\nprint(f\"  embedding dim: {DSCD_EMBED_DIM}\")\nprint(f\"  temperature: {DSCD_TEMPERATURE}\")\nprint(f\"  training clustering: {'ENABLED' if DSCD_ENABLE_TRAINING_CLUSTERING else 'DISABLED (warmup only)'}\")\nprint(f\"  warmup samples: {DSCD_WARMUP_SAMPLES}\")\nprint()\nprint(\"TRG & Uncertainty:\")\nprint(f\"  TAU_LOW: {TAU_LOW}, TAU_HIGH: {TAU_HIGH}, TAU_ACCEPT: {TAU_ACCEPT}\")\nprint(f\"  span threshold: {SPAN_THRESHOLD}\")\nprint(f\"  TRG training: {'ENABLED' if ENABLE_TRG_TRAINING else 'DISABLED'}\")\nprint(f\"  TRG inference: {'ENABLED' if ENABLE_TRG_INFERENCE else 'DISABLED'}\")\nprint()\nprint(\"ASBN / Loss weights:\")\nprint(f\"  ASBN training: {'ENABLED' if ENABLE_ASBN_TRAINING else 'DISABLED'}\")\nprint(f\"  ASBN inference: {'ENABLED' if ENABLE_ASBN_INFERENCE else 'DISABLED'}\")\nprint(f\"  LAMBDA_ASBN: {LAMBDA_ASBN}\")\nprint(f\"  LAMBDA_DSCD: {LAMBDA_DSCD}\")\nprint()\nprint(\"Learning Rates:\")\nprint(f\"  NMT: {LR_NMT}, TRG: {LR_TRG}, PHI: {LR_PHI}\")\nprint(\"=\"*80)\nprint(\"🔧 MEMORY OPTIMIZATIONS APPLIED:\")\nprint(f\"  • Batch size: {BATCH_SIZE}\")\nprint(f\"  • Accumulation steps: {ACCUMULATION_STEPS}\")\nprint(f\"  • DSCD buffer reduced: {DSCD_BUFFER_SIZE}\")\nprint(f\"  • Gradient checkpointing: {'ENABLED' if GRADIENT_CHECKPOINTING else 'DISABLED'}\")\nprint(\"=\"*80)\n\n# Final sanity checks and small auto-corrections\nif not (0.0 <= TAU_LOW <= 1.0):\n    print(\"[WARN] TAU_LOW out of range [0, 1]; resetting to 0.4\")\n    TAU_LOW = 0.4\n\nif not (0.0 <= TAU_HIGH <= 1.0):\n    print(\"[WARN] TAU_HIGH out of range [0, 1]; resetting to 0.85\")\n    TAU_HIGH = 0.85\n\nif TAU_LOW >= TAU_HIGH:\n    print(\"[WARN] TAU_LOW >= TAU_HIGH; resetting to TAU_LOW=0.4, TAU_HIGH=0.85\")\n    TAU_LOW, TAU_HIGH = 0.4, 0.85\n\nif VALIDATION_CHECK_INTERVAL != 0:\n    print(f\"[INFO] Validation enabled every {VALIDATION_CHECK_INTERVAL} steps\")\n\nif not _HAS_PANDAS:\n    print(\"[WARN] pandas not installed. CSV loading will use fallback dataset or will require installing pandas.\")\n\nif _CSV_AVAILABLE and _HAS_PANDAS:\n    # quick sample size validation (avoid pathological configs)\n    try:\n        # robust line counting in text mode to estimate file size\n        nrows = None\n        try:\n            nrows = _get_csv_row_count(DATASET_CSV_PATH)\n        except Exception:\n            nrows = None\n        if nrows is not None and nrows < 10 and EFFECTIVE_NUM_SAMPLES > nrows:\n            print(\"[WARN] CSV seems very small relative to NUM_SAMPLES. Adjust NUM_SAMPLES if needed.\")\n    except Exception:\n        pass\n\nprint(\"✅ Cell 0: Configuration loaded and debugged (OOM prevention + DSCD defaults + CSV fallback).\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-22T14:56:01.251075Z","iopub.execute_input":"2025-11-22T14:56:01.251436Z","iopub.status.idle":"2025-11-22T14:56:05.933881Z","shell.execute_reply.started":"2025-11-22T14:56:01.251415Z","shell.execute_reply":"2025-11-22T14:56:05.933236Z"},"id":"5jMPDi9xH4Jz","trusted":true},"outputs":[{"name":"stdout","text":"[Cell 0] Multi-GPU Mode: 2 GPUs available (using device=cuda)\n[Cell 0] Device: cuda (visible GPUs: 2)\n[INFO] Dataset CSV found: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\n[INFO] CSV validation passed (columns: ['idx', 'src', 'tgt', 'word', 'sense'])\n\n================================================================================\n⚡ OPTIMIZED ULTRA-FAST TATN CONFIGURATION (Cell 0 - DEBUGGED)\n================================================================================\nUser: manas0003\nDate: 2025-11-22 14:56:05 UTC\nMulti-GPU: ENABLED (2 GPUs visible)\nDataset source: LOCAL CSV\nDataset path: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\nDataset samples (cap): 50,000 (effective: 50,000)\nBatch Size: 100 x 16 grad-accum steps\nEffective batch size: 1600\nMax Length: 48 tokens\nEpochs: 2\nWorkers: 2, Prefetch: 2, Pin memory: True\nAMP: ENABLED\nValidation interval: 200 (ENABLED)\n\nDSCD Config:\n  Buffer size: 20\n  Max prototypes: 8\n  n_min: 5\n  dispersion threshold: 0.25\n  embedding dim: 1024\n  temperature: 0.7\n  training clustering: ENABLED\n  warmup samples: 8000\n\nTRG & Uncertainty:\n  TAU_LOW: 0.4, TAU_HIGH: 0.85, TAU_ACCEPT: 0.8\n  span threshold: 0.3\n  TRG training: DISABLED\n  TRG inference: ENABLED\n\nASBN / Loss weights:\n  ASBN training: ENABLED\n  ASBN inference: ENABLED\n  LAMBDA_ASBN: 0.1\n  LAMBDA_DSCD: 0.05\n\nLearning Rates:\n  NMT: 2e-05, TRG: 1e-05, PHI: 1e-05\n================================================================================\n🔧 MEMORY OPTIMIZATIONS APPLIED:\n  • Batch size: 100\n  • Accumulation steps: 16\n  • DSCD buffer reduced: 20\n  • Gradient checkpointing: ENABLED\n================================================================================\n[INFO] Validation enabled every 200 steps\n✅ Cell 0: Configuration loaded and debugged (OOM prevention + DSCD defaults + CSV fallback).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================================================================================\n# CELL 1 - SAFE TOKENIZER UTILITIES (HARDENED)\n# - Robust special-token caching\n# - Deterministic offset normalization (encoded[\"offset_mapping\"] always present)\n# - Fast / slow tokenizer handling improved\n# - Word-span reconstruction fallback order: offsets -> SPM markers -> whitespace\n# ===========================================================================================\n\nimport threading\nfrom typing import Tuple, List, Dict, Optional\nimport numpy as np\nimport torch\n\n# Local defaults to avoid hard dependency on other cells\ntry:\n    SAFE_OFFSET_MAX_LEN = int(MAX_LENGTH)\nexcept NameError:\n    SAFE_OFFSET_MAX_LEN = 48\n\ntry:\n    _SOURCE_LANG = SOURCE_LANGUAGE\nexcept NameError:\n    _SOURCE_LANG = \"bn\"  # default to Bengali if not specified\n\n# Thread-safe cache for special tokens\n_SPECIAL_TOKENS_CACHE: Dict[str, set] = {}\n_SPECIAL_TOKENS_LOCK = threading.Lock()\n\n\ndef _special_token_cache_key(tokenizer) -> str:\n    \"\"\"Build a stable key for caching special token sets for a tokenizer.\"\"\"\n    # tokenizer.name_or_path is preferred; fallback to repr\n    name = getattr(tokenizer, \"name_or_path\", None) or getattr(tokenizer, \"name\", None) or repr(tokenizer)\n    # determine vocab size safely\n    vocab = None\n    if hasattr(tokenizer, \"vocab_size\"):\n        try:\n            vocab = int(getattr(tokenizer, \"vocab_size\"))\n        except Exception:\n            vocab = None\n    elif hasattr(tokenizer, \"get_vocab\") and callable(getattr(tokenizer, \"get_vocab\")):\n        try:\n            vocab = len(tokenizer.get_vocab())\n        except Exception:\n            vocab = None\n    # final key:\n    return f\"{name}__vocab={vocab}\"\n\n\ndef get_tokenizer_special_tokens(tokenizer) -> set:\n    \"\"\"\n    Return a cached set of special tokens for `tokenizer`.\n    The result is conservative (includes common placeholders) and avoids\n    repeated expensive introspection.\n    \"\"\"\n    cache_key = _special_token_cache_key(tokenizer)\n    with _SPECIAL_TOKENS_LOCK:\n        if cache_key in _SPECIAL_TOKENS_CACHE:\n            return _SPECIAL_TOKENS_CACHE[cache_key]\n\n        special_tokens = set()\n        try:\n            # Try common tokenizer attributes in order of availability\n            if hasattr(tokenizer, \"all_special_tokens\"):\n                try:\n                    special_tokens.update(x for x in getattr(tokenizer, \"all_special_tokens\") or [] if x)\n                except Exception:\n                    pass\n            if hasattr(tokenizer, \"additional_special_tokens\"):\n                try:\n                    special_tokens.update(x for x in getattr(tokenizer, \"additional_special_tokens\") or [] if x)\n                except Exception:\n                    pass\n            # single-token attributes\n            for attr in (\"pad_token\", \"unk_token\", \"bos_token\", \"eos_token\", \"cls_token\", \"sep_token\", \"mask_token\"):\n                if hasattr(tokenizer, attr):\n                    try:\n                        tok = getattr(tokenizer, attr)\n                        if tok:\n                            special_tokens.add(tok)\n                    except Exception:\n                        pass\n            # special_tokens_map or extended map\n            try:\n                stm = getattr(tokenizer, \"special_tokens_map\", None) or getattr(tokenizer, \"special_tokens_map_extended\", None)\n                if isinstance(stm, dict):\n                    for v in stm.values():\n                        if isinstance(v, str) and v:\n                            special_tokens.add(v)\n            except Exception:\n                pass\n\n        except Exception:\n            # fallback to safe conservative set\n            special_tokens = set()\n\n        # Add conservative language / placeholder tokens likely useful for m2m100 & friends\n        special_tokens.update({\n            \"bn_IN\", \"en_XX\",\n            \"</s>\", \"<pad>\", \"<s>\", \"<unk>\",\n            \"[PAD]\", \"[EOS]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"\n        })\n\n        _SPECIAL_TOKENS_CACHE[cache_key] = special_tokens\n        return special_tokens\n\n\ndef _normalize_offset_mapping_for_batchencoding(enc):\n    \"\"\"\n    Normalize a BatchEncoding (from HF tokenizer) so that enc['offset_mapping']\n    is set and in Python list-of-(start,end) tuples for the first example in the batch.\n    This function mutates enc in-place and returns it.\n    \"\"\"\n    # prefer the direct key if present (works for fast tokenizers)\n    try:\n        if \"offset_mapping\" in enc and enc[\"offset_mapping\"] is not None:\n            off = enc[\"offset_mapping\"]\n            # Case: tensor (pt) or list-of-lists\n            try:\n                # If pt tensor\n                if hasattr(off, \"tolist\"):\n                    arr = off.tolist()\n                    # arr is typically [[ [s,e], [s,e], ... ]]\n                    if isinstance(arr, list) and len(arr) > 0 and isinstance(arr[0], list):\n                        enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, list) and len(x) == 2 else (None, None) for x in arr[0]]\n                        return enc\n                # If already list-like\n                if isinstance(off, (list, tuple)):\n                    # ensure first-element list -> normalize its elements to tuples\n                    if len(off) > 0 and isinstance(off[0], (list, tuple)):\n                        enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in off[0]]\n                        return enc\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # Last resort: if BatchEncoding exposes .data with offset_mapping, try that\n    try:\n        data = getattr(enc, \"data\", None)\n        if data and isinstance(data, dict) and \"offset_mapping\" in data and data[\"offset_mapping\"] is not None:\n            om = data[\"offset_mapping\"]\n            if isinstance(om, (list, tuple)) and len(om) > 0 and isinstance(om[0], (list, tuple)):\n                enc[\"offset_mapping\"] = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in om[0]]\n                return enc\n    except Exception:\n        pass\n\n    # If we reach here, ensure enc[\"offset_mapping\"] exists and is a list for the first example (sequence length placeholder)\n    try:\n        seq_len = 0\n        if \"input_ids\" in enc:\n            input_ids = enc[\"input_ids\"]\n            # input_ids may be tensor or list\n            if hasattr(input_ids, \"shape\"):\n                seq_len = int(input_ids.shape[-1])\n            elif isinstance(input_ids, (list, tuple)) and len(input_ids) > 0 and isinstance(input_ids[0], (list, tuple)):\n                seq_len = len(input_ids[0])\n        # create placeholder offsets\n        enc[\"offset_mapping\"] = [(None, None)] * seq_len\n    except Exception:\n        enc[\"offset_mapping\"] = []\n\n    return enc\n\n\ndef safe_offsets_tokenize(tokenizer, text: str, max_length: Optional[int] = None,\n                          include_special_tokens: bool = False) -> dict:\n    \"\"\"\n    Tokenize `text` with tokenizer and *guarantee* that the return value has:\n      - 'input_ids' and optionally 'attention_mask' (as returned by HF tokenizer)\n      - 'offset_mapping' key present and normalized to a list of (start,end) tuples\n        for the first example in the batch (or an empty list if unavailable).\n\n    Parameters:\n      tokenizer: HF tokenizer instance (fast or slow)\n      text: input string\n      max_length: token truncation max (defaults to SAFE_OFFSET_MAX_LEN)\n      include_special_tokens: whether to include special tokens in tokenization\n    \"\"\"\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str):\n        text = \"\" if text is None else str(text)\n\n    # Limit characters to avoid pathological inputs\n    char_limit = min(eff_max * 20, 2000)\n    sample_text = text[:char_limit]\n\n    is_fast = getattr(tokenizer, \"is_fast\", False)\n\n    # Prefer the fast path; ensure we ask for offsets and tensor outputs for convenience\n    if is_fast:\n        try:\n            enc = tokenizer(\n                sample_text,\n                return_offsets_mapping=True,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=False,\n                max_length=eff_max,\n                add_special_tokens=include_special_tokens,\n            )\n            enc = _normalize_offset_mapping_for_batchencoding(enc)\n            return enc\n        except Exception:\n            # fallthrough to slow path\n            pass\n\n    # Slow tokenizer path: ask for ids, then build best-effort offsets\n    try:\n        enc = tokenizer(\n            sample_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=False,\n            max_length=eff_max,\n            add_special_tokens=include_special_tokens,\n        )\n    except Exception:\n        # If the tokenizer call fails completely, produce a minimal encoding\n        # that downstream code can still handle.\n        enc = {\"input_ids\": torch.tensor([[tokenizer.pad_token_id if hasattr(tokenizer, \"pad_token_id\") else 0]]),\n               \"attention_mask\": torch.tensor([[1]])}\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n    # Try to compute a fallback offset map by aligning decoded token text to source\n    try:\n        # get sequence of token ids (first example)\n        input_ids = None\n        try:\n            input_ids = enc[\"input_ids\"][0].tolist()\n        except Exception:\n            # try alternative access\n            if hasattr(enc, \"data\") and \"input_ids\" in enc.data:\n                input_ids = enc.data[\"input_ids\"][0]\n        tokens = []\n        if input_ids is not None:\n            try:\n                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n            except Exception:\n                tokens = []\n        # Build offsets by searching token text in source progressively\n        offsets_list = []\n        src = sample_text\n        cur_pos = 0\n        for tok in tokens:\n            # clean subword markers commonly used by SPM/BPE/fast tokenizers\n            token_text = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n            if not token_text:\n                offsets_list.append((None, None))\n                continue\n            # naive search from current position\n            idx = src.find(token_text, cur_pos)\n            if idx == -1:\n                idx = src.lower().find(token_text.lower(), cur_pos)\n            if idx == -1:\n                offsets_list.append((None, None))\n            else:\n                start = int(idx)\n                end = int(idx + len(token_text))\n                offsets_list.append((start, end))\n                cur_pos = end\n        # normalize to same format expected by _normalize_offset_mapping_for_batchencoding\n        enc[\"offset_mapping\"] = offsets_list\n        # ensure normalized (wrap as first-example list)\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n    except Exception:\n        # fallback: ensure offset_mapping exists\n        enc = _normalize_offset_mapping_for_batchencoding(enc)\n        return enc\n\n\ndef reconstruct_word_spans(tokenizer, text: str, max_length: Optional[int] = None) -> Tuple[Dict[int, str], List[str]]:\n    \"\"\"\n    Return:\n      - token_word_map: mapping token_index -> reconstructed word string (best-effort)\n      - words: list[str] of words discovered in order\n\n    Strategy:\n      1) Use tokenizer offsets when available -> group contiguous character spans into words.\n      2) If offsets unavailable or unhelpful, use SPM-style '▁' or 'Ġ' markers to assemble subwords.\n      3) Finally fallback to whitespace-splitting.\n    \"\"\"\n    if max_length is None:\n        max_length = SAFE_OFFSET_MAX_LEN\n    eff_max = int(max_length)\n\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return {}, []\n\n    char_limit = min(eff_max * 20, 2000)\n    text = text[:char_limit]\n    text_len = len(text)\n\n    special_tokens = get_tokenizer_special_tokens(tokenizer)\n\n    try:\n        current_lang = SOURCE_LANGUAGE\n    except NameError:\n        current_lang = _SOURCE_LANG\n\n    # Get normalized encoding (guarantees offset_mapping exists)\n    try:\n        encoded = safe_offsets_tokenize(tokenizer, text, max_length=eff_max, include_special_tokens=False)\n    except Exception:\n        return {}, []\n\n    offsets = encoded.get(\"offset_mapping\", [])\n    # ensure input_ids and tokens exist\n    try:\n        input_ids = encoded[\"input_ids\"][0].tolist()\n    except Exception:\n        input_ids = []\n    try:\n        tokens = tokenizer.convert_ids_to_tokens(input_ids) if input_ids else []\n    except Exception:\n        tokens = []\n\n    # Ensure offsets is a list with len(tokens) (if possible)\n    if isinstance(offsets, list) and len(offsets) > 0 and all(isinstance(x, tuple) for x in offsets):\n        offsets_list = offsets\n    elif isinstance(offsets, list) and len(offsets) > 0 and isinstance(offsets[0], (list, tuple)):\n        offsets_list = [tuple(x) if isinstance(x, (list, tuple)) and len(x) == 2 else (None, None) for x in offsets[0]]\n    else:\n        # not usable\n        offsets_list = [(None, None)] * len(tokens)\n\n    token_word_map: Dict[int, str] = {}\n    words: List[str] = []\n\n    # 1) Use offsets to group contiguous spans into words\n    used_any_offset = any((isinstance(o, tuple) and o[0] is not None and o[1] is not None) for o in offsets_list)\n    if used_any_offset:\n        word_start = None\n        word_end = None\n        word_accum = \"\"\n        for idx, (off, tok) in enumerate(zip(offsets_list, tokens)):\n            try:\n                off_start, off_end = (int(off[0]) if off[0] is not None else None, int(off[1]) if off[1] is not None else None)\n            except Exception:\n                off_start, off_end = None, None\n            if off_start is None or off_end is None:\n                # token with no offsets: close existing word and skip\n                if word_start is not None and word_end is not None:\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                word_start = None\n                word_end = None\n                word_accum = \"\"\n                token_word_map[idx] = \"UNK\"\n                continue\n\n            # optionally skip special tokens\n            if tok in special_tokens:\n                token_word_map[idx] = \"\"\n                continue\n\n            # Start new word if needed\n            if word_start is None:\n                word_start = off_start\n                word_end = off_end\n            else:\n                # If this token begins after the previous end -> new word\n                if off_start > word_end:\n                    # flush previous\n                    try:\n                        wtext = text[word_start:word_end].strip()\n                        if wtext:\n                            words.append(wtext)\n                    except Exception:\n                        pass\n                    word_start = off_start\n                    word_end = off_end\n                else:\n                    word_end = max(word_end, off_end)\n\n            # map token to the current word slice (best-effort)\n            try:\n                current_word = text[word_start:word_end].strip()\n                token_word_map[idx] = current_word if current_word else \"UNK\"\n            except Exception:\n                token_word_map[idx] = \"UNK\"\n\n        # flush last\n        if word_start is not None and word_end is not None:\n            try:\n                wtext = text[word_start:word_end].strip()\n                if wtext:\n                    words.append(wtext)\n            except Exception:\n                pass\n\n        if token_word_map:\n            words = [w for w in words if isinstance(w, str) and w.strip()]\n            return token_word_map, words\n\n    # 2) Fallback to SPM/BPE marker assembly (tokens marked with '▁' or 'Ġ')\n    token_word_map = {}\n    assembled = []\n    current = \"\"\n    running_word = \"\"\n    for i, tok in enumerate(tokens):\n        # skip special tokens\n        if tok in special_tokens:\n            token_word_map[i] = \"\"\n            continue\n        # normalize token text\n        clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n        if not clean:\n            token_word_map[i] = \"\"\n            continue\n        if (tok.startswith(\"▁\") or tok.startswith(\"Ġ\")):\n            # new word\n            if current:\n                assembled.append(current)\n            current = clean\n            running_word = current\n        else:\n            # continuation subword\n            current = current + clean\n            running_word = current\n        token_word_map[i] = running_word if running_word else \"UNK\"\n    if current:\n        assembled.append(current)\n    if token_word_map:\n        words = [w for w in assembled if w and w.strip()]\n        return token_word_map, words\n\n    # 3) Final fallback: whitespace-split the original text and assign tokens approximately\n    try:\n        word_list = [w for w in text.split() if w.strip()]\n        token_word_map = {}\n        if tokens and word_list:\n            widx = 0\n            for i, tok in enumerate(tokens):\n                clean = (tok or \"\").replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n                if not clean:\n                    token_word_map[i] = \"\"\n                    continue\n                token_word_map[i] = word_list[min(widx, len(word_list) - 1)]\n                # Heuristic: if token looks long or contains punctuation advance\n                if len(clean) > len(token_word_map[i]) or clean.endswith((\".\", \",\", \";\", \"।\", \"?\" , \"!\" )):\n                    widx = min(widx + 1, len(word_list) - 1)\n        return token_word_map, word_list\n    except Exception:\n        return {}, []\n\n\n# ===========================================================================================\n# LIGHTWEIGHT SELF-TEST\n# ===========================================================================================\ndef test_tokenizer_utilities_quick(tokenizer=None):\n    \"\"\"\n    If tokenizer is None, this will only sanity-check Python-level logic.\n    If tokenizer is provided (HF tokenizer), it will run a quick encode + reconstruct.\n    \"\"\"\n    sample = \"কাল আমি বাজারে যাব।\"  # Bengali: \"Tomorrow I will go to the market.\"\n    print(\"Running tokenizer-utils quick test...\")\n    try:\n        if tokenizer is None:\n            print(\"No tokenizer provided: basic logic OK.\")\n            return True\n        enc = safe_offsets_tokenize(tokenizer, sample, max_length=32, include_special_tokens=False)\n        print(\"  Encoded input_ids len:\", int(enc[\"input_ids\"].shape[-1]) if \"input_ids\" in enc else \"N/A\")\n        print(\"  Offset mapping (first 10):\", (enc.get(\"offset_mapping\") or [])[:10])\n        token_map, words = reconstruct_word_spans(tokenizer, sample, max_length=32)\n        print(\"  Reconstructed words:\", words)\n        print(\"  Token->word examples:\", {k: token_map[k] for k in list(token_map.keys())[:6]})\n        return True\n    except Exception as e:\n        print(\"Tokenizer utilities quick test failed:\", repr(e))\n        return False\n\n\n# This print is a gentle confirmation that the utilities loaded.\nprint(\"✅ Cell 1 (tokenizer utilities) loaded and hardened.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:05.935091Z","iopub.execute_input":"2025-11-22T14:56:05.935464Z","iopub.status.idle":"2025-11-22T14:56:05.974260Z","shell.execute_reply.started":"2025-11-22T14:56:05.935445Z","shell.execute_reply":"2025-11-22T14:56:05.973456Z"},"id":"WZE9PkHyH4J1","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 1 (tokenizer utilities) loaded and hardened.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: MEMORY-EFFICIENT DATA LOADING (FIXED & HARDENED + CSV SUPPORT)\n# ==============================================================================\n# ✅ FIXED: Replaced Samanantar with local CSV loading\n# ✅ FIXED: Added pandas-based CSV reader with proper column mapping\n# ✅ FIXED: Enhanced error handling and validation\n# - Robust fallbacks when datasets/tokenizer utilities are missing\n# - Safer DP-divisible batching logic (floor to nearest multiple by default)\n# - Worker init rebinds tokenizer safely for multiprocessing workers\n# - Deterministic per-worker seeding\n# - Safe collate that always returns stackable tensors and preserves token_word_map\n# - Defensive behaviors and verbose debug prints controlled by VERBOSE_LOGGING\n# ==============================================================================\nfrom typing import Optional, List, Tuple, Dict, Any\nfrom collections import defaultdict\nimport os\nimport time\nimport random\nimport traceback\nimport re\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, get_worker_info\nfrom tqdm import tqdm\n\n# Pandas import for CSV reading (required for local dataset)\ntry:\n    import pandas as pd\n    _HAS_PANDAS = True\nexcept ImportError:\n    pd = None\n    _HAS_PANDAS = False\n    print(\"[CELL2] WARNING: pandas not available; CSV loading will fail!\")\n\n# Optional import - datasets library (not needed for CSV mode)\ntry:\n    from datasets import load_dataset\n    _HAS_DATASETS = True\nexcept Exception:\n    load_dataset = None\n    _HAS_DATASETS = False\n\n# -------------------------\n# Debug control\n# -------------------------\ntry:\n    _VERBOSE_LOGGING = bool(VERBOSE_LOGGING)\nexcept NameError:\n    _VERBOSE_LOGGING = False\n\nDEBUG_CELL2 = bool(_VERBOSE_LOGGING)\nDEBUG_LIMIT = 10\n_cell2_dbg_counts: Dict[str, int] = defaultdict(int)\n\n\ndef cell2_dbg(key: str, msg: str, limit: int = DEBUG_LIMIT):\n    \"\"\"Debug print with rate limiting.\"\"\"\n    if not DEBUG_CELL2:\n        return\n    _cell2_dbg_counts[key] += 1\n    if _cell2_dbg_counts[key] <= limit:\n        print(f\"[CELL2-DBG] {msg}\")\n\n\n# -------------------------\n# Local fallbacks for globals (explicit, safe)\n# -------------------------\ntry:\n    _NUM_SAMPLES = int(NUM_SAMPLES)\nexcept Exception:\n    _NUM_SAMPLES = 50000\n    print(\"[CELL2] WARNING: NUM_SAMPLES not defined, using default 50000\")\n\ntry:\n    _MAX_LENGTH = int(MAX_LENGTH)\nexcept Exception:\n    _MAX_LENGTH = 48\n    print(\"[CELL2] WARNING: MAX_LENGTH not defined, using default 48\")\n\ntry:\n    _BN_LANG = BN_LANG\n    _EN_LANG = EN_LANG\nexcept NameError:\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n    print(\"[CELL2] WARNING: BN_LANG/EN_LANG not defined, using defaults\")\n\ntry:\n    _NUM_GPUS = int(NUM_GPUS)\n    _USE_MULTI_GPU = bool(USE_MULTI_GPU)\nexcept NameError:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    print(f\"[CELL2] WARNING: GPU config not defined, detected {_NUM_GPUS} GPUs\")\n\ntry:\n    _NUM_WORKERS = int(NUM_WORKERS)\nexcept NameError:\n    _NUM_WORKERS = 0\n    print(\"[CELL2] WARNING: NUM_WORKERS not defined, using 0\")\n\ntry:\n    _PIN_MEMORY = bool(PIN_MEMORY)\nexcept NameError:\n    _PIN_MEMORY = False\n\ntry:\n    _PREFETCH_FACTOR = int(PREFETCH_FACTOR)\nexcept NameError:\n    _PREFETCH_FACTOR = 2\n\ntry:\n    _DATASET_CSV_PATH = str(DATASET_CSV_PATH)\nexcept NameError:\n    _DATASET_CSV_PATH = \"/kaggle/input/bengali-english-homograph/bengali_homograph_sentences.csv\"\n    print(f\"[CELL2] WARNING: DATASET_CSV_PATH not defined, using default: {_DATASET_CSV_PATH}\")\n\n# Check availability of utility functions from Cell 0\n_has_normalize = ('normalize_bengali' in globals()) and ('normalize_english' in globals())\n_has_reconstruct_word_spans = 'reconstruct_word_spans' in globals()\n_has_safe_offsets_tokenize = 'safe_offsets_tokenize' in globals()\n\nif not _has_normalize:\n    print(\"[CELL2] WARNING: normalize_bengali/normalize_english not found; using simple .strip()\")\n\n# -------------------------\n# Utility: detect Bengali text heuristically\n# -------------------------\n_BENGALI_CHAR_RE = re.compile(r'[\\u0980-\\u09FF]')\n\ndef is_bengali_text(s: str) -> bool:\n    \"\"\"Check if text contains Bengali Unicode characters.\"\"\"\n    if not isinstance(s, str) or not s:\n        return False\n    # if any Bengali char present, treat as Bengali\n    return bool(_BENGALI_CHAR_RE.search(s))\n\n\n# -------------------------\n# Worker init: reattach tokenizer and set per-worker seed\n# -------------------------\ndef _dataloader_worker_init_fn(worker_id: int):\n    \"\"\"Initialize DataLoader worker with tokenizer and deterministic seed.\"\"\"\n    worker_info = get_worker_info()\n    dataset = worker_info.dataset if worker_info is not None else None\n    \n    # Try to rebind tokenizer from the main process globals into the worker dataset\n    try:\n        if dataset is not None:\n            tk = globals().get('tokenizer', None)\n            if tk is not None:\n                try:\n                    # attach tokenizer reference only (avoid copying heavy state)\n                    dataset.tokenizer = tk\n                    dataset.is_fast = getattr(tk, \"is_fast\", False)\n                except Exception:\n                    dataset.tokenizer = tk\n                    dataset.is_fast = False\n    except Exception:\n        if DEBUG_CELL2:\n            print(f\"[CELL2-WORKER-INIT] tokenizer rebind failed in worker {worker_id}\")\n            traceback.print_exc()\n    \n    # Set a deterministic-ish per-worker seed to avoid RNG issues\n    try:\n        base = int(os.environ.get(\"PYTHONHASHSEED\", \"0\"))\n        # incorporate worker id and time low bits to change per-worker seed\n        seed = (base ^ (worker_id + 1) ^ int(time.time())) & 0xFFFFFFFF\n        random.seed(seed)\n        np.random.seed(seed % (2**31 - 1))\n        torch.manual_seed(seed % (2**31 - 1))\n    except Exception:\n        pass\n\n\n# -------------------------\n# Data loading and preprocessing (CSV-BASED)\n# -------------------------\ndef load_and_preprocess_optimized(num_samples: Optional[int] = None) -> List[Tuple[str, str]]:\n    \"\"\"\n    Load parallel bn-en pairs from local CSV file.\n    CSV format: idx,src,tgt (where src=English, tgt=Bengali)\n    Returns list of (bn, en) pairs.\n    Falls back to a small hard-coded set if CSV load fails.\n    \"\"\"\n    if num_samples is None:\n        num_samples = _NUM_SAMPLES\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    print(f\"[CELL2] Loading up to {num_samples} samples from local CSV: {_DATASET_CSV_PATH}\")\n    \n    # Validate pandas availability\n    if not _HAS_PANDAS:\n        print(\"[CELL2] ERROR: pandas not available; cannot load CSV!\")\n        print(\"[CELL2] Install with: !pip install pandas\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    # Validate CSV file exists\n    if not os.path.exists(_DATASET_CSV_PATH):\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    try:\n        # Read CSV file\n        print(f\"[CELL2] Reading CSV file...\")\n        df = pd.read_csv(_DATASET_CSV_PATH)\n        \n        # Validate required columns\n        if 'src' not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing 'src' column. Found columns: {list(df.columns)}\")\n            return _get_fallback_dataset()\n        \n        if 'tgt' not in df.columns:\n            print(f\"[CELL2] ERROR: CSV missing 'tgt' column. Found columns: {list(df.columns)}\")\n            return _get_fallback_dataset()\n        \n        # Limit to num_samples\n        df = df.head(num_samples)\n        \n        print(f\"[CELL2] Processing {len(df)} rows from CSV...\")\n        \n        pairs: List[Tuple[str, str]] = []\n        skipped = 0\n        \n        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading dataset\"):\n            try:\n                # src = English, tgt = Bengali\n                en = str(row['src']).strip()\n                bn = str(row['tgt']).strip()\n                \n                # Basic validation\n                if not en or not bn:\n                    skipped += 1\n                    cell2_dbg(\"empty_field\", f\"Empty src/tgt at idx={idx}\")\n                    continue\n                \n                # Check for \"nan\" string from pandas\n                if en.lower() == 'nan' or bn.lower() == 'nan':\n                    skipped += 1\n                    cell2_dbg(\"nan_value\", f\"NaN value at idx={idx}\")\n                    continue\n                \n                # Length check (avoid extremely long sentences)\n                max_words = max(40, _MAX_LENGTH)\n                if len(en.split()) > max_words or len(bn.split()) > max_words:\n                    skipped += 1\n                    cell2_dbg(\"too_long\", f\"Too long at idx={idx}: en={len(en.split())} bn={len(bn.split())} words\")\n                    continue\n                \n                # Normalize if available\n                if _has_normalize:\n                    bn_norm = normalize_bengali(bn)\n                    en_norm = normalize_english(en)\n                else:\n                    bn_norm = bn\n                    en_norm = en.lower()\n                \n                # Ensure normalization didn't create empty strings\n                if not bn_norm or not en_norm:\n                    skipped += 1\n                    cell2_dbg(\"empty_after_norm\", f\"Empty after normalization at idx={idx}\")\n                    continue\n                \n                # Store as (Bengali, English) pair - IMPORTANT ORDER!\n                pairs.append((bn_norm, en_norm))\n                \n            except Exception as e:\n                skipped += 1\n                cell2_dbg(\"row_exception\", f\"Row load exception idx={idx}: {type(e).__name__}: {str(e)[:100]}\")\n                continue\n        \n        print(f\"[CELL2] Loaded {len(pairs)} pairs from CSV, skipped {skipped} rows\")\n        \n        if len(pairs) == 0:\n            print(\"[CELL2] ERROR: No valid pairs loaded from CSV!\")\n            return _get_fallback_dataset()\n        \n        return pairs\n        \n    except FileNotFoundError:\n        print(f\"[CELL2] ERROR: CSV file not found at: {_DATASET_CSV_PATH}\")\n        print(\"[CELL2] Using fallback small dataset for debugging.\")\n        return _get_fallback_dataset()\n    \n    except pd.errors.EmptyDataError:\n        print(f\"[CELL2] ERROR: CSV file is empty: {_DATASET_CSV_PATH}\")\n        return _get_fallback_dataset()\n    \n    except Exception as e:\n        print(f\"[CELL2] ERROR loading CSV: {type(e).__name__}: {str(e)}\")\n        print(f\"[CELL2] Traceback: {traceback.format_exc().splitlines()[-3:]}\")\n        print(\"[CELL2] Using fallback dataset\")\n        return _get_fallback_dataset()\n\n\ndef _get_fallback_dataset() -> List[Tuple[str, str]]:\n    \"\"\"Return a small fallback dataset for debugging/testing.\"\"\"\n    print(\"[CELL2] Using fallback small dataset (5 samples)\")\n    fallback_pairs = [\n        (\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\"),\n        (\"সে আমাকে পরে কল করবে।\", \"he will call me later.\"),\n        (\"আমরা প্রতিদিন তাজা ফল খাই।\", \"we eat fresh fruits every day.\"),\n        (\"তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\", \"his hard work has brought good results.\"),\n        (\"গাছে নতুন পাতাগুলো গজিয়েছে।\", \"new leaves have sprouted on the tree.\")\n    ]\n    if _has_normalize:\n        return [(normalize_bengali(bn), normalize_english(en)) for bn, en in fallback_pairs]\n    else:\n        return [(bn.strip(), en.lower().strip()) for bn, en in fallback_pairs]\n\n\n# -------------------------\n# Dataset Class\n# -------------------------\nclass MemoryEfficientDataset(Dataset):\n    \"\"\"\n    Memory-efficient dataset that returns dicts with:\n      - input_ids, attention_mask: torch.LongTensor [L]\n      - labels: torch.LongTensor [L] with pad->-100\n      - token_word_map: dict token_idx->word\n      - src_text: original source string\n      - tokens: list of token strings\n    The tokenizer attribute is excluded from pickled state so DataLoader workers don't crash.\n    \"\"\"\n\n    def __init__(self, pairs: List[Tuple[str, str]], tokenizer: Any = None, max_length: Optional[int] = None):\n        if max_length is None:\n            max_length = _MAX_LENGTH\n        self.max_length = int(max_length)\n        self.tokenizer = tokenizer\n        try:\n            self._tokenizer_name_or_path = getattr(tokenizer, \"name_or_path\", None)\n        except Exception:\n            self._tokenizer_name_or_path = None\n\n        try:\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False)\n        except Exception:\n            self.is_fast = False\n\n        self.pairs: List[Tuple[str, str]] = []\n        invalid = 0\n        \n        # Validate and filter pairs\n        for i, p in enumerate(pairs):\n            try:\n                if not isinstance(p, (list, tuple)) or len(p) != 2:\n                    invalid += 1\n                    cell2_dbg(\"init_badpair\", f\"Bad pair structure at idx={i}\")\n                    continue\n                \n                src, tgt = p\n                \n                # Type validation\n                if not isinstance(src, str) or not isinstance(tgt, str):\n                    invalid += 1\n                    cell2_dbg(\"init_badtype\", f\"Non-string src/tgt at idx={i}\")\n                    continue\n                \n                # Empty check\n                if not src or not tgt:\n                    invalid += 1\n                    cell2_dbg(\"init_empty\", f\"Empty src/tgt at idx={i}\")\n                    continue\n                \n                # Length sanity check (character level)\n                if len(src) > self.max_length * 20 or len(tgt) > self.max_length * 20:\n                    invalid += 1\n                    cell2_dbg(\"init_long\", f\"Extremely long text at idx={i}\")\n                    continue\n                \n                self.pairs.append((src, tgt))\n                \n            except Exception as e:\n                invalid += 1\n                cell2_dbg(\"init_exc\", f\"Init pair exception idx={i}: {type(e).__name__}\")\n        \n        print(f\"[CELL2] Dataset initialized: {len(self.pairs)} valid pairs, {invalid} invalid pairs filtered\")\n\n        # Get special tokens for filtering\n        try:\n            if 'get_special_tokens' in globals():\n                self.special_tokens = get_special_tokens(self.tokenizer)\n            elif 'get_tokenizer_special_tokens' in globals():\n                self.special_tokens = get_tokenizer_special_tokens(self.tokenizer)\n            else:\n                self.special_tokens = set(getattr(self.tokenizer, \"all_special_tokens\", [])) if self.tokenizer is not None else set()\n        except Exception:\n            self.special_tokens = {_BN_LANG, _EN_LANG, \"</s>\", \"<pad>\", \"<s>\", \"<unk>\"}\n            cell2_dbg(\"special_tokens_fallback\", \"Used explicit fallback special tokens\")\n\n    def __getstate__(self):\n        \"\"\"Prepare state for pickling (exclude tokenizer).\"\"\"\n        state = self.__dict__.copy()\n        # avoid serializing tokenizer into worker processes\n        state['tokenizer'] = None\n        state['_tokenizer_name_or_path'] = getattr(self, \"_tokenizer_name_or_path\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Restore state after unpickling (rebind tokenizer).\"\"\"\n        self.__dict__.update(state)\n        try:\n            # rebind tokenizer from global if available (set by worker_init_fn)\n            self.tokenizer = globals().get('tokenizer', None)\n            self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n        except Exception:\n            self.tokenizer = None\n            self.is_fast = False\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def _encode_src(self, src_text: str):\n        \"\"\"Encode source (Bengali) text.\"\"\"\n        src_text = src_text if isinstance(src_text, str) else str(src_text)\n        \n        try:\n            # Ensure tokenizer is available\n            if self.tokenizer is None:\n                try:\n                    self.tokenizer = globals().get('tokenizer', None)\n                    self.is_fast = getattr(self.tokenizer, \"is_fast\", False) if self.tokenizer is not None else False\n                except Exception:\n                    self.tokenizer = None\n                    self.is_fast = False\n\n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n\n            # Set source language hints if tokenizer supports it\n            try:\n                if hasattr(self.tokenizer, \"src_lang\"):\n                    self.tokenizer.src_lang = _BN_LANG\n            except Exception:\n                pass\n\n            # Prefer safe_offsets_tokenize if available\n            if _has_safe_offsets_tokenize:\n                enc = safe_offsets_tokenize(self.tokenizer, src_text, max_length=self.max_length)\n                try:\n                    input_ids = enc[\"input_ids\"].squeeze(0) if isinstance(enc[\"input_ids\"], torch.Tensor) else torch.tensor(enc[\"input_ids\"][0])\n                except Exception:\n                    input_ids = torch.tensor(enc.get(\"input_ids\", [[1]])[0])\n                \n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids))\n                if isinstance(attention_mask, list):\n                    attention_mask = torch.tensor(attention_mask[0]) if attention_mask else torch.ones_like(input_ids)\n                \n                try:\n                    ids_list = input_ids.tolist() if isinstance(input_ids, torch.Tensor) else list(input_ids)\n                    tokens = self.tokenizer.convert_ids_to_tokens(ids_list)\n                except Exception:\n                    tokens = []\n            else:\n                # Standard tokenization\n                enc = self.tokenizer(\n                    src_text,\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\",\n                    add_special_tokens=False\n                )\n                input_ids = enc[\"input_ids\"].squeeze(0)\n                attention_mask = enc.get(\"attention_mask\", torch.ones_like(input_ids)).squeeze(0)\n                try:\n                    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.tolist())\n                except Exception:\n                    tokens = []\n\n            # Build token-word mapping\n            token_word_map: Dict[int, str] = {}\n            if _has_reconstruct_word_spans:\n                try:\n                    wm, words = reconstruct_word_spans(self.tokenizer, src_text, max_length=self.max_length)\n                    if isinstance(wm, dict):\n                        token_word_map = wm\n                except Exception:\n                    cell2_dbg(\"wm_exc\", f\"reconstruct_word_spans failed: {traceback.format_exc().splitlines()[-1]}\")\n                    token_word_map = {}\n            else:\n                # Fallback: mark tokens starting with SPM markers as word starts\n                try:\n                    for idx, tok in enumerate(tokens):\n                        if isinstance(tok, str) and (tok.startswith(\"▁\") or tok.startswith(\"Ġ\")):\n                            token_word_map[idx] = tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n                except Exception:\n                    token_word_map = {}\n\n            return input_ids, attention_mask, tokens, token_word_map\n            \n        except Exception as e:\n            cell2_dbg(\"encode_src_exc\", f\"Encoding source failed: {type(e).__name__}: {str(e)[:60]}\")\n            # Return safe placeholder\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer else 1\n            input_ids = torch.full((self.max_length,), int(pad_id), dtype=torch.long)\n            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n            return input_ids, attention_mask, [], {}\n\n    def _encode_tgt(self, tgt_text: str):\n        \"\"\"Encode target (English) text.\"\"\"\n        tgt_text = tgt_text if isinstance(tgt_text, str) else str(tgt_text)\n        \n        try:\n            if self.tokenizer is None:\n                self.tokenizer = globals().get('tokenizer', None)\n            \n            if self.tokenizer is None:\n                raise RuntimeError(\"Tokenizer not available\")\n            \n            # Set target language hints where supported\n            try:\n                if hasattr(self.tokenizer, \"tgt_lang\"):\n                    self.tokenizer.tgt_lang = _EN_LANG\n            except Exception:\n                pass\n            \n            dec = self.tokenizer(\n                tgt_text,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n                add_special_tokens=False\n            )\n            labels = dec[\"input_ids\"].squeeze(0)\n            \n            # Replace pad tokens with -100 (ignore index for loss)\n            pad_id = getattr(self.tokenizer, \"pad_token_id\", 1) if self.tokenizer else 1\n            labels[labels == int(pad_id)] = -100\n            \n            return labels\n            \n        except Exception as e:\n            cell2_dbg(\"encode_tgt_exc\", f\"Encoding tgt failed: {type(e).__name__}: {str(e)[:60]}\")\n            return torch.full((self.max_length,), -100, dtype=torch.long)\n\n    def _make_safe_sample(self, reason: str = \"fallback\"):\n        \"\"\"Create a safe fallback sample.\"\"\"\n        try:\n            src = \"আমি\"\n            tgt = \"i\"\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens\n            }\n        except Exception:\n            pad_id = 1\n            return {\n                \"input_ids\": torch.full((self.max_length,), int(pad_id), dtype=torch.long),\n                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long),\n                \"token_word_map\": {},\n                \"src_text\": \"\",\n                \"tokens\": []\n            }\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"Get a single sample by index.\"\"\"\n        try:\n            if idx < 0 or idx >= len(self.pairs):\n                cell2_dbg(\"getitem_oob\", f\"Index out of range idx={idx} len={len(self.pairs)}\")\n                return self._make_safe_sample(\"oob\")\n            \n            src, tgt = self.pairs[idx]\n            \n            if not isinstance(src, str) or not isinstance(tgt, str):\n                cell2_dbg(\"getitem_bad_types\", f\"Bad types at idx={idx}\")\n                return self._make_safe_sample(\"bad_types\")\n\n            input_ids, attention_mask, tokens, token_word_map = self._encode_src(src)\n            labels = self._encode_tgt(tgt)\n\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"token_word_map\": token_word_map,\n                \"src_text\": src,\n                \"tokens\": tokens\n            }\n        except Exception as e:\n            cell2_dbg(\"getitem_exc\", f\"Unhandled __getitem__ exception idx={idx}: {type(e).__name__}\")\n            return self._make_safe_sample(\"unhandled\")\n\n\n# ---------------------------\n# Collation and DataLoader helpers\n# ---------------------------\ndef _infer_pad_id_from_sample(sample: Dict[str, Any], default_pad_id: int = 1) -> int:\n    \"\"\"Infer pad token id from tokenizer.\"\"\"\n    try:\n        tk = globals().get(\"tokenizer\", None)\n        if tk is not None:\n            pad = getattr(tk, \"pad_token_id\", None)\n            if pad is not None:\n                return int(pad)\n    except Exception:\n        cell2_dbg(\"infer_pad_exc\", \"infer pad id failed\")\n    return int(default_pad_id)\n\n\ndef _pad_or_truncate_array(tensor: torch.Tensor, length: int, pad_value: int) -> torch.Tensor:\n    \"\"\"Pad or truncate tensor to exact length.\"\"\"\n    if tensor is None:\n        return torch.full((length,), int(pad_value), dtype=torch.long)\n    \n    t = tensor.view(-1).long()\n    L = t.size(0)\n    \n    if L == length:\n        return t\n    if L < length:\n        pad = torch.full((length - L,), int(pad_value), dtype=t.dtype)\n        return torch.cat([t, pad], dim=0)\n    return t[:length]\n\n\ndef safe_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Robust collate: ensures stackable tensors and safe structure.\n    Pads/truncates all sequences to _MAX_LENGTH deterministically.\n    \"\"\"\n    valid = [b for b in batch if isinstance(b, dict) and \"input_ids\" in b and isinstance(b[\"input_ids\"], torch.Tensor)]\n    \n    if not valid:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]]\n        }\n\n    pad_id = _infer_pad_id_from_sample(valid[0], default_pad_id=1)\n\n    inputs, masks, labs, twmaps, srcs, toks = [], [], [], [], [], []\n    \n    for i, s in enumerate(valid):\n        try:\n            in_ids = s[\"input_ids\"]\n            att = s.get(\"attention_mask\", None)\n            lab = s[\"labels\"]\n\n            if att is None:\n                att = (in_ids != pad_id).long()\n            else:\n                try:\n                    att = att.view(-1).long()\n                except Exception:\n                    att = (in_ids != pad_id).long()\n\n            try:\n                in_ids = in_ids.view(-1)\n            except Exception:\n                in_ids = in_ids.flatten()\n            try:\n                lab = lab.view(-1)\n            except Exception:\n                lab = lab.flatten()\n\n            in_ids = _pad_or_truncate_array(in_ids, _MAX_LENGTH, pad_id)\n            att = _pad_or_truncate_array(att, _MAX_LENGTH, 0)\n            lab = _pad_or_truncate_array(lab, _MAX_LENGTH, -100)\n\n            inputs.append(in_ids)\n            masks.append(att)\n            labs.append(lab)\n            twmaps.append(s.get(\"token_word_map\", {}))\n            srcs.append(s.get(\"src_text\", \"\"))\n            toks.append(s.get(\"tokens\", []))\n        except Exception as e:\n            cell2_dbg(\"collate_item_exc\", f\"Collate item exception idx={i}: {type(e).__name__}\")\n            continue\n\n    if not inputs:\n        pad = _infer_pad_id_from_sample({}, default_pad_id=1)\n        return {\n            \"input_ids\": torch.full((1, _MAX_LENGTH), pad, dtype=torch.long),\n            \"attention_mask\": torch.zeros(1, _MAX_LENGTH, dtype=torch.long),\n            \"labels\": torch.full((1, _MAX_LENGTH), -100, dtype=torch.long),\n            \"token_word_map\": [{}],\n            \"src_text\": [\"\"],\n            \"tokens\": [[]]\n        }\n\n    input_ids = torch.stack(inputs, dim=0)\n    attention_mask = torch.stack(masks, dim=0)\n    labels = torch.stack(labs, dim=0)\n\n    # DP-divisible adjustment: trim downward to nearest multiple to avoid OOM\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        bsz = input_ids.size(0)\n        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n        if keep > 0 and keep < bsz:\n            cell2_dbg(\"dp_trunc\", f\"DP truncate from {bsz} to {keep}\")\n            input_ids = input_ids[:keep]\n            attention_mask = attention_mask[:keep]\n            labels = labels[:keep]\n            twmaps = twmaps[:keep]\n            srcs = srcs[:keep]\n            toks = toks[:keep]\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"token_word_map\": twmaps,\n        \"src_text\": srcs,\n        \"tokens\": toks\n    }\n\n\ndef create_optimized_dataloader(dataset: Dataset, batch_size: Optional[int] = None, shuffle: bool = True) -> DataLoader:\n    \"\"\"\n    Create a DataLoader with safe defaults and deterministic worker init.\n    By default, if _USE_MULTI_GPU the batch_size will be floored to nearest multiple of _NUM_GPUS\n    to avoid oversubscribing GPU memory.\n    \"\"\"\n    if batch_size is None:\n        try:\n            batch_size = int(BATCH_SIZE)\n        except NameError:\n            batch_size = 8\n    batch_size = int(batch_size)\n\n    # Floor to nearest multiple for multi-GPU\n    adjust_upwards = False  # change to True if you prefer increasing to next multiple\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0 and batch_size % _NUM_GPUS != 0:\n        if adjust_upwards:\n            adjusted = ((batch_size + _NUM_GPUS - 1) // _NUM_GPUS) * _NUM_GPUS\n            print(f\"[CELL2] Adjusting batch size {batch_size} → {adjusted} to be DP-divisible (GPUs={_NUM_GPUS})\")\n            batch_size = adjusted\n        else:\n            adjusted = (batch_size // _NUM_GPUS) * _NUM_GPUS\n            if adjusted == 0:\n                print(f\"[CELL2] WARNING: batch_size {batch_size} < num_gpus {_NUM_GPUS}. Keeping original batch_size.\")\n            else:\n                print(f\"[CELL2] Adjusting batch size {batch_size} → {adjusted} (floor to multiple of {_NUM_GPUS}) to avoid OOM.\")\n                batch_size = adjusted\n\n    # Validate num_workers\n    num_workers = _NUM_WORKERS if isinstance(_NUM_WORKERS, int) and _NUM_WORKERS >= 0 else 0\n    try:\n        max_possible = max(0, (os.cpu_count() or 1) - 1)\n        if num_workers > max_possible:\n            num_workers = max_possible\n    except Exception:\n        pass\n\n    loader_kwargs = {\n        \"dataset\": dataset,\n        \"batch_size\": batch_size,\n        \"shuffle\": shuffle,\n        \"num_workers\": num_workers,\n        \"pin_memory\": bool(_PIN_MEMORY and torch.cuda.is_available()),\n        \"collate_fn\": safe_collate,\n        \"drop_last\": False,\n    }\n    \n    # Only set worker_init_fn if using workers\n    if num_workers > 0:\n        loader_kwargs[\"worker_init_fn\"] = _dataloader_worker_init_fn\n        loader_kwargs[\"prefetch_factor\"] = _PREFETCH_FACTOR\n        loader_kwargs[\"persistent_workers\"] = False\n\n    try:\n        dataloader = DataLoader(**loader_kwargs)\n    except Exception as e:\n        print(f\"[CELL2] DataLoader init failed with num_workers={num_workers}: {type(e).__name__}: {str(e)[:200]}\")\n        print(\"[CELL2] Retrying with num_workers=0\")\n        loader_kwargs[\"num_workers\"] = 0\n        loader_kwargs.pop(\"prefetch_factor\", None)\n        loader_kwargs.pop(\"persistent_workers\", None)\n        loader_kwargs.pop(\"worker_init_fn\", None)\n        dataloader = DataLoader(**loader_kwargs)\n\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = batch_size // _NUM_GPUS if _NUM_GPUS > 0 else batch_size\n        print(f\"[CELL2] DataLoader created: total_batch={batch_size}, per_gpu={per_gpu}, workers={loader_kwargs.get('num_workers', 0)}\")\n    else:\n        print(f\"[CELL2] DataLoader created: batch_size={batch_size}, workers={loader_kwargs.get('num_workers', 0)}\")\n\n    return dataloader\n\n\nprint(\"✅ Cell 2: Memory-efficient data loading ready (FIXED: CSV support + hardened error handling)\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:05.975407Z","iopub.execute_input":"2025-11-22T14:56:05.975686Z","iopub.status.idle":"2025-11-22T14:56:06.045279Z","shell.execute_reply.started":"2025-11-22T14:56:05.975669Z","shell.execute_reply":"2025-11-22T14:56:06.044680Z"},"id":"5MkHgCN7H4J1","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 2: Memory-efficient data loading ready (FIXED: CSV support + hardened error handling)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3 (ENHANCED): DSCD WITH HIERARCHICAL CLUSTERING + KMEANS FALLBACK + WORD-KEYS\n# DEBUGGED, HARDENED, and SELF-CONTAINED\n# ==============================================================================\n# Fixes applied (high level):\n# - Robust global config fetching via globals().get with sensible defaults.\n# - Added safe helper get_special_tokens(tokenizer) fallback (avoids reliance on external Cell 0 helper).\n# - Rewrote numpy KMeans seeding & iterations to be robust and vectorized.\n# - Defensive handling of tensor <-> numpy conversions to avoid device/stride errors.\n# - Ensure clustering uses CPU numpy arrays only; centroids stored as CPU tensors.\n# - Added explicit checks and guarding for empty buffers and shapes.\n# - Minor readability / stability improvements and additional logging under VERBOSE_LOGGING.\n# ==============================================================================\n\nimport threading\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gc\nfrom collections import deque\nimport unicodedata\nfrom typing import Optional, List, Tuple\n\n# -------------------------\n# Config / feature detection\n# -------------------------\nPRINT_INTERVAL = int(globals().get(\"PRINT_INTERVAL\", 200))\n\n# SciPy hierarchical clustering (optional)\ntry:\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import pdist\n    HAS_CLUSTERING = True\nexcept Exception:\n    HAS_CLUSTERING = False\n    print(\"[CELL3] WARNING: scipy not available - hierarchical clustering disabled\")\n\n# sklearn KMeans (optional)\ntry:\n    from sklearn.cluster import KMeans\n    HAS_KMEANS = True\nexcept Exception:\n    HAS_KMEANS = False\n    print(\"[CELL3] WARNING: sklearn not available - KMeans fallback disabled\")\n\n# DSCD config with safe defaults\nDSCD_MAX_PROTOS = int(globals().get(\"DSCD_MAX_PROTOS\", 8))\nDSCD_BUFFER_SIZE = int(globals().get(\"DSCD_BUFFER_SIZE\", 20))\nDSCD_N_MIN = int(globals().get(\"DSCD_N_MIN\", 5))\nDSCD_DISPERSION_THRESHOLD = float(globals().get(\"DSCD_DISPERSION_THRESHOLD\", 0.25))\nVERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\nHOMOGRAPH_WATCHLIST_BN = set(globals().get(\"HOMOGRAPH_WATCHLIST_BN\",\n                                         {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\", \"ফল\", \"মাথা\"}))\nDSCD_MAX_CLUSTERING_POINTS = int(globals().get(\"DSCD_MAX_CLUSTERING_POINTS\", 2000))\n\n# small deny-prefix set for combining/vowel marks (avoid clustering noise)\nDSCD_TOKEN_DENY_PREFIXES = set(['্', 'ি', 'ে', 'া', 'ী', 'ু', 'ূ', 'ৗ', '্র', 'ৎ', 'ঁ'])\n\n# -------------------------\n# Helper: safe special tokens extractor (fallbacks)\n# -------------------------\ndef get_special_tokens_safe(tok):\n    \"\"\"\n    Return a set of special tokens from tokenizer in a robust way.\n    \"\"\"\n    if tok is None:\n        return set()\n    try:\n        s = getattr(tok, \"all_special_tokens\", None)\n        if s:\n            return set(s)\n    except Exception:\n        pass\n    try:\n        # HF tokenizer mapping\n        stm = getattr(tok, \"special_tokens_map\", None)\n        if isinstance(stm, dict):\n            vals = [v for v in stm.values() if isinstance(v, str)]\n            return set(vals)\n    except Exception:\n        pass\n    try:\n        # fallback to keys of special_tokens_map_extended or similar\n        stmap = getattr(tok, \"additional_special_tokens\", None)\n        if stmap:\n            return set(stmap)\n    except Exception:\n        pass\n    # last resort: look for attributes that may contain token ids/names\n    out = set()\n    for attr in (\"bos_token\", \"eos_token\", \"pad_token\", \"unk_token\", \"sep_token\", \"cls_token\", \"mask_token\"):\n        try:\n            v = getattr(tok, attr, None)\n            if isinstance(v, str):\n                out.add(v)\n        except Exception:\n            pass\n    return out\n\n# -------------------------\n# Token predicate\n# -------------------------\ndef is_word_token(token: str, min_letters: int = 2, min_letter_fraction: float = 0.6) -> bool:\n    \"\"\"\n    Unicode-aware test whether a token is likely a real word to track.\n    \"\"\"\n    if not token or not isinstance(token, str):\n        return False\n    token = token.strip()\n    if token == \"\":\n        return False\n\n    letters = 0\n    total = 0\n    for ch in token:\n        cat = unicodedata.category(ch)\n        if cat.startswith(\"L\"):\n            letters += 1\n        if not ch.isspace():\n            total += 1\n\n    if total == 0:\n        return False\n    if letters < min_letters:\n        return False\n    if (letters / total) < min_letter_fraction:\n        return False\n    return True\n\n# -------------------------\n# Robust small numpy KMeans fallback\n# -------------------------\ndef _numpy_kmeans(X: np.ndarray, n_clusters: int, n_iter: int = 10, random_state: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Simple, robust KMeans implemented with numpy.\n    - X: (N, D)\n    - returns: labels (N,), centroids (n_clusters, D)\n    This implementation uses random initialization with a KMeans++-like heuristic\n    (choose first centroid randomly, subsequent centroids by distance weighting).\n    \"\"\"\n    X = np.asarray(X, dtype=np.float32)\n    N, D = X.shape\n    if N == 0:\n        return np.zeros((0,), dtype=np.int32), np.zeros((0, D), dtype=np.float32)\n    n_clusters = int(max(1, min(n_clusters, N)))\n    rng = np.random.RandomState(random_state)\n\n    # KMeans++ style initialization\n    centroids = np.empty((n_clusters, D), dtype=np.float32)\n    first_idx = rng.randint(0, N)\n    centroids[0] = X[first_idx]\n    for k in range(1, n_clusters):\n        # compute distance to nearest existing centroid\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :k, :], axis=2)  # (N, k)\n        nearest = dists.min(axis=1)  # (N,)\n        probs = nearest / (nearest.sum() + 1e-12)\n        chosen = rng.choice(N, p=probs)\n        centroids[k] = X[chosen]\n\n    labels = np.zeros(N, dtype=np.int32)\n    for it in range(n_iter):\n        # assign\n        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)  # (N, k)\n        new_labels = dists.argmin(axis=1)\n        # update\n        changed = False\n        for j in range(n_clusters):\n            members = (new_labels == j)\n            if members.sum() == 0:\n                # reinitialize empty centroid\n                centroids[j] = X[rng.randint(0, N)]\n                changed = True\n            else:\n                new_cent = X[members].mean(axis=0)\n                if not np.allclose(new_cent, centroids[j], atol=1e-6):\n                    centroids[j] = new_cent.astype(np.float32)\n                    changed = True\n        labels = new_labels\n        if not changed:\n            break\n    return labels, centroids\n\n# -------------------------\n# Prototype store (CPU)\n# -------------------------\nclass MemoryEfficientPrototypeStore:\n    def __init__(self, embed_dim: int, max_protos: Optional[int] = None):\n        self.embed_dim = int(embed_dim)\n        self.max_protos = int(max_protos) if max_protos is not None else DSCD_MAX_PROTOS\n        self.centroids: List[torch.Tensor] = []   # CPU tensors\n        self.counts: List[int] = []\n        self.creation_time: List[float] = []\n        self.distances: List[float] = []\n        self.mu: float = 0.0\n        self.tau: float = 1e-6\n        self.alpha: float = 0.1\n\n    def add_prototype(self, vector, current_time=None, count=1):\n        if current_time is None:\n            current_time = time.time()\n        try:\n            if isinstance(vector, torch.Tensor):\n                v = vector.detach().cpu().float().clone()\n            else:\n                v = torch.from_numpy(np.asarray(vector, dtype=np.float32)).cpu()\n        except Exception:\n            return\n        if len(self.centroids) < self.max_protos:\n            self.centroids.append(v)\n            self.counts.append(int(count))\n            self.creation_time.append(current_time)\n            return\n        # replace least-supported prototype\n        try:\n            min_idx = int(np.argmin(self.counts)) if self.counts else 0\n        except Exception:\n            min_idx = 0\n        min_idx = max(0, min_idx)\n        if min_idx < len(self.centroids):\n            self.centroids[min_idx] = v\n            self.counts[min_idx] = int(count)\n            self.creation_time[min_idx] = current_time\n        else:\n            # pad lists\n            while len(self.centroids) <= min_idx:\n                self.centroids.append(v.clone())\n                self.counts.append(int(count))\n                self.creation_time.append(current_time)\n\n    def update_prototype(self, idx, vector, eta=0.05, assignment_distance=None):\n        try:\n            if idx < 0 or idx >= len(self.centroids):\n                self.add_prototype(vector, time.time(), count=1)\n                return\n            old = self.centroids[idx]\n            newv = vector.detach().cpu() if isinstance(vector, torch.Tensor) else torch.from_numpy(np.asarray(vector, dtype=np.float32)).cpu()\n            try:\n                self.centroids[idx] = (1.0 - eta) * old + eta * newv\n            except Exception:\n                self.centroids[idx] = newv.clone()\n            try:\n                self.counts[idx] = int(self.counts[idx]) + 1\n            except Exception:\n                while len(self.counts) < len(self.centroids):\n                    self.counts.append(1)\n                self.counts[idx] = int(self.counts[idx]) + 1\n        except Exception:\n            try:\n                self.add_prototype(vector, time.time(), count=1)\n            except Exception:\n                pass\n        if assignment_distance is not None:\n            try:\n                self.update_rolling_stats(float(assignment_distance))\n            except Exception:\n                pass\n\n    def update_rolling_stats(self, d: float):\n        try:\n            if not self.distances:\n                self.mu = float(d)\n                self.tau = 1e-6\n                self.distances = [float(d)]\n                return\n            prev_mu = self.mu\n            self.mu = (1 - self.alpha) * self.mu + self.alpha * float(d)\n            self.tau = (1 - self.alpha) * self.tau + self.alpha * abs(float(d) - prev_mu)\n            self.distances.append(float(d))\n            if len(self.distances) > 50:\n                self.distances.pop(0)\n        except Exception:\n            pass\n\n    def get_adaptive_threshold(self, lam=1.0) -> float:\n        try:\n            return float(self.mu + lam * self.tau)\n        except Exception:\n            return float(self.mu)\n\n    def get_centroids(self, device=torch.device(\"cpu\")) -> Optional[torch.Tensor]:\n        if not self.centroids:\n            return None\n        try:\n            return torch.stack([c.to(device) for c in self.centroids], dim=0)\n        except Exception:\n            try:\n                return torch.stack([c.cpu() for c in self.centroids], dim=0).to(device)\n            except Exception:\n                return None\n\n    def get_valid_centroids(self, device=torch.device(\"cpu\"), min_count=None):\n        if min_count is None:\n            min_count = DSCD_N_MIN\n        idxs = [i for i, ct in enumerate(self.counts) if ct >= int(min_count)]\n        if not idxs:\n            return None, None\n        cents = [self.centroids[i].to(device) for i in idxs]\n        return torch.stack(cents, dim=0), idxs\n\n    def set_centroids_from_arrays(self, array_list, counts=None):\n        try:\n            self.centroids = [torch.from_numpy(np.asarray(a, dtype=np.float32)).cpu() for a in array_list]\n            if counts and len(counts) == len(array_list):\n                self.counts = [int(c) for c in counts]\n            else:\n                self.counts = [1 for _ in array_list]\n            self.creation_time = [time.time()] * len(array_list)\n        except Exception:\n            self.centroids = []\n            self.counts = []\n            self.creation_time = []\n\n    def size(self) -> int:\n        return len(self.centroids)\n\n# -------------------------\n# DSCD Online Module\n# -------------------------\nclass MemoryEfficientDSCDOnline(nn.Module):\n    def __init__(self, embed_dim, tokenizer=None, buffer_size=None, max_protos=None,\n                 n_min=None, dispersion_threshold=None, language='bn',\n                 enable_training_clustering=False, max_clustering_points=None,\n                 max_candidates_per_step=2, dscd_min_letters: int = 2,\n                 dscd_min_letter_fraction: float = 0.6):\n        super().__init__()\n        self.embed_dim = int(embed_dim)\n        self.buffer_size = int(buffer_size) if buffer_size is not None else DSCD_BUFFER_SIZE\n        self.max_protos = int(max_protos) if max_protos is not None else DSCD_MAX_PROTOS\n        self.n_min = int(n_min) if n_min is not None else DSCD_N_MIN\n        self.dispersion_threshold = float(dispersion_threshold) if dispersion_threshold is not None else DSCD_DISPERSION_THRESHOLD\n        self.language = language\n        self.tokenizer = tokenizer\n        self.dscd_min_letters = int(dscd_min_letters)\n        self.dscd_min_letter_fraction = float(dscd_min_letter_fraction)\n\n        # special tokens\n        try:\n            if tokenizer is not None:\n                self.special_tokens = get_special_tokens_safe(tokenizer)\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n        self._dscd_allowed_tokens = set()\n        self._dscd_ignored_tokens = set()\n\n        self.prototype_stores = {}\n        self.buffers = {}\n        self.discovery_log = []\n        self.last_periodic_check = 0\n        self.cleanup_counter = 0\n        self.clustering_lock = threading.Lock()\n\n        self.last_cluster_time = {}\n        self.cluster_cooldown_seconds = 60\n        self.enable_training_clustering = bool(enable_training_clustering)\n\n        # small heads kept for compatibility\n        self.span_head = nn.Sequential(\n            nn.Linear(self.embed_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 1)\n        )\n        self.sigma_net = nn.Sequential(\n            nn.Linear(self.embed_dim, 16),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(16, 1)\n        )\n        self.gate_w = nn.Parameter(torch.tensor(1.0))\n        self.gate_b = nn.Parameter(torch.tensor(0.4))\n        self.gamma = nn.Parameter(torch.tensor(0.3))\n\n        self.max_clustering_points = int(max_clustering_points) if max_clustering_points is not None else DSCD_MAX_CLUSTERING_POINTS\n        self.max_candidates_per_step = int(max_candidates_per_step)\n\n        if VERBOSE_LOGGING:\n            print(f\"[DSCD-INIT] embed_dim={self.embed_dim}, buffer_size={self.buffer_size}, max_protos={self.max_protos}, n_min={self.n_min}\")\n            print(f\"[DSCD-INIT] dispersion_threshold={self.dispersion_threshold}, max_clustering_points={self.max_clustering_points}\")\n\n    # ------------------------\n    def should_track_token(self, token_text: str) -> bool:\n        if not token_text or not isinstance(token_text, str):\n            return False\n        if token_text in self._dscd_allowed_tokens:\n            return True\n        if token_text in self._dscd_ignored_tokens:\n            return False\n\n        try:\n            clean = token_text.replace('▁', '').replace('Ġ', '').replace('##', '').replace('@@', '').strip()\n        except Exception:\n            clean = token_text.strip()\n\n        try:\n            if len(clean) <= 2 and any(clean.startswith(p) for p in DSCD_TOKEN_DENY_PREFIXES):\n                self._dscd_ignored_tokens.add(token_text)\n                return False\n        except Exception:\n            pass\n\n        try:\n            if clean in HOMOGRAPH_WATCHLIST_BN:\n                self._dscd_allowed_tokens.add(token_text)\n                if VERBOSE_LOGGING and len(self._dscd_allowed_tokens) <= 20:\n                    print(f\"[DSCD] ✅ Homograph watchlist token tracked: '{clean}'\")\n                return True\n        except Exception:\n            pass\n\n        if token_text in self.special_tokens:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        if clean == \"\":\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        if len(clean) < 2:\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        if not any(c.isalpha() for c in clean):\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        if clean.isdigit():\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        if all(c in '.,!?;:()[]{}\"\\'-—–/\\\\' for c in clean):\n            self._dscd_ignored_tokens.add(token_text)\n            return False\n\n        try:\n            bengali_block = any('\\u0980' <= c <= '\\u09FF' for c in clean)\n            if bengali_block and len(clean) >= 2:\n                self._dscd_allowed_tokens.add(token_text)\n                return True\n        except Exception:\n            pass\n\n        if is_word_token(clean, min_letters=self.dscd_min_letters, min_letter_fraction=self.dscd_min_letter_fraction):\n            self._dscd_allowed_tokens.add(token_text)\n            return True\n\n        self._dscd_ignored_tokens.add(token_text)\n        return False\n\n    def _canonical_token_key(self, raw_token: str, token_word_map: Optional[dict], idx: int) -> str:\n        canonical = None\n        try:\n            if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map and token_word_map[idx]:\n                canonical = str(token_word_map[idx]).strip()\n        except Exception:\n            canonical = None\n        if not canonical:\n            try:\n                canonical = raw_token.replace('▁', '').replace('Ġ', '').replace('##', '').replace('@@', '').strip()\n            except Exception:\n                canonical = raw_token\n        if not canonical:\n            canonical = raw_token\n        return canonical\n\n    def forward(self, token_embeddings, token_types=None, train_mode=True,\n                token_word_map=None, h_all=None, input_ids=None, attention_mask=None):\n        if token_embeddings is None and h_all is not None:\n            token_embeddings = h_all\n        if token_embeddings is None:\n            raise ValueError(\"MemoryEfficientDSCDOnline.forward requires token_embeddings or h_all\")\n\n        # build token_types if missing\n        if input_ids is not None and token_types is None:\n            try:\n                batch_size, seq_len = input_ids.shape\n            except Exception:\n                batch_size = int(token_embeddings.size(0))\n                seq_len = int(token_embeddings.size(1))\n            token_types = []\n            for b in range(batch_size):\n                if self.tokenizer is not None:\n                    try:\n                        token_types.append(self.tokenizer.convert_ids_to_tokens(input_ids[b].tolist()))\n                    except Exception:\n                        token_types.append([f'tok_{i}' for i in range(seq_len)])\n                else:\n                    token_types.append([f'tok_{i}' for i in range(seq_len)])\n\n        self.cleanup_counter += 1\n        if self.cleanup_counter % 50 == 0:\n            self.cleanup_counter = 0\n            self.cleanup_memory()\n\n        device = token_embeddings.device\n        batch_size = int(token_embeddings.size(0))\n        seq_len = int(token_embeddings.size(1))\n\n        all_outputs = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': []\n        }\n\n        for b in range(batch_size):\n            word_map = token_word_map[b] if token_word_map and len(token_word_map) > b else None\n            tt = token_types[b] if token_types and len(token_types) > b else [f'tok_{i}' for i in range(seq_len)]\n            batch_outputs = self.process_sequence(\n                token_embeddings[b],\n                tt,\n                device,\n                word_map=word_map,\n                train_mode=train_mode\n            )\n            for k in all_outputs:\n                all_outputs[k].append(batch_outputs[k])\n\n        # assemble h_augmented into tensor (batch, seq_len, embed_dim) where possible\n        try:\n            h_aug_list = []\n            max_seq_len = seq_len\n            for b in range(batch_size):\n                h_batch_list = all_outputs['h_augmented'][b]\n                if len(h_batch_list) > 0 and isinstance(h_batch_list[0], torch.Tensor):\n                    h_batch = torch.stack(h_batch_list, dim=0)\n                    if h_batch.size(0) < max_seq_len:\n                        pad = max_seq_len - h_batch.size(0)\n                        h_batch = F.pad(h_batch, (0, 0, 0, pad), value=0)\n                    elif h_batch.size(0) > max_seq_len:\n                        h_batch = h_batch[:max_seq_len]\n                else:\n                    h_batch = torch.zeros(max_seq_len, self.embed_dim, device=device)\n                h_aug_list.append(h_batch)\n            all_outputs['h_augmented'] = torch.stack(h_aug_list, dim=0)\n        except Exception:\n            all_outputs['h_augmented'] = token_embeddings\n\n        return all_outputs\n\n    def process_sequence(self, token_embeddings, token_types, device, word_map=None, train_mode=True):\n        seq_len = int(token_embeddings.size(0))\n        outputs = {\n            'proto_assignments': [],\n            'proto_probs': [],\n            'uncertainties': [],\n            'span_preds': [],\n            'gates': [],\n            'h_augmented': []\n        }\n\n        for j in range(seq_len):\n            raw_tok = token_types[j] if j < len(token_types) else f'tok_{j}'\n            token_key = self._canonical_token_key(raw_tok, word_map, j)\n            h_j = token_embeddings[j]\n\n            if not self.should_track_token(token_key):\n                outputs['proto_assignments'].append(torch.tensor(-1))\n                outputs['proto_probs'].append([])\n                outputs['uncertainties'].append(0.0)\n                outputs['span_preds'].append(0.0)\n                outputs['gates'].append(0.0)\n                outputs['h_augmented'].append(h_j)\n                continue\n\n            if token_key not in self.buffers:\n                self.buffers[token_key] = deque(maxlen=self.buffer_size)\n                self.prototype_stores[token_key] = MemoryEfficientPrototypeStore(self.embed_dim, self.max_protos)\n\n            try:\n                self.buffers[token_key].append(h_j.detach().cpu())\n            except Exception:\n                try:\n                    self.buffers[token_key].append(h_j.cpu())\n                except Exception:\n                    pass\n\n            # background clustering trigger\n            try:\n                if self.enable_training_clustering and len(self.buffers[token_key]) >= max(self.n_min, 4):\n                    now = time.time()\n                    last_t = self.last_cluster_time.get(token_key, 0.0)\n                    if now - last_t > self.cluster_cooldown_seconds:\n                        self.last_cluster_time[token_key] = now\n                        def _bg_cluster(tok=token_key):\n                            try:\n                                with self.clustering_lock:\n                                    self._cluster_buffer_to_prototypes_hierarchical(tok)\n                            except Exception:\n                                if VERBOSE_LOGGING:\n                                    import traceback as _tb\n                                    print(f\"[DSCD] Background clustering error for token '{tok}': {_tb.format_exc().splitlines()[-1]}\")\n                        th = threading.Thread(target=_bg_cluster, daemon=True)\n                        th.start()\n            except Exception:\n                if VERBOSE_LOGGING:\n                    import traceback as _tb\n                    print(f\"[DSCD] Failed to trigger background clustering for token {token_key}: {_tb.format_exc().splitlines()[-1]}\")\n\n            store = self.prototype_stores[token_key]\n\n            # atomic centroid snapshot\n            centroids_snapshot = []\n            with self.clustering_lock:\n                try:\n                    for c in getattr(store, \"centroids\", []):\n                        if isinstance(c, torch.Tensor):\n                            centroids_snapshot.append(c.clone().cpu())\n                        else:\n                            centroids_snapshot.append(torch.from_numpy(np.asarray(c)).cpu())\n                except Exception:\n                    centroids_snapshot = []\n\n            assignment = -1\n            prob_list = []\n            uncertainty = 0.0\n            span_pred = 0.0\n            gate_val = 0.0\n            h_aug = h_j\n\n            if centroids_snapshot and len(centroids_snapshot) >= 1:\n                try:\n                    h_cpu = h_j.detach().cpu().numpy()\n                    cents_np = np.stack([c.numpy() for c in centroids_snapshot], axis=0).astype(np.float32)  # (K, D)\n                    dists_np = np.linalg.norm(cents_np - h_cpu[None, :], axis=1)  # (K,)\n                    if dists_np.size > 0:\n                        assignment = int(np.argmin(dists_np))\n                        min_dist = float(dists_np[assignment])\n                        try:\n                            store.update_rolling_stats(min_dist)\n                        except Exception:\n                            pass\n\n                        # softmax over negative distances -> probabilities\n                        try:\n                            neg = -dists_np\n                            exps = np.exp(neg - np.max(neg))\n                            probs = exps / (exps.sum() + 1e-12)\n                            prob_list = probs.tolist()\n                            uncertainty = 1.0 - float(np.max(probs))\n                        except Exception:\n                            prob_list = []\n                            uncertainty = 0.0\n\n                        try:\n                            span_pred = float(torch.sigmoid(self.span_head(h_j)).item())\n                        except Exception:\n                            try:\n                                span_pred = float(torch.sigmoid(self.span_head(h_j.cpu())).item())\n                            except Exception:\n                                span_pred = 0.0\n\n                        try:\n                            gate_val = float(torch.sigmoid(self.gate_w * torch.norm(h_j) + self.gate_b).item())\n                        except Exception:\n                            gate_val = 0.5\n\n                        if gate_val > 0.3 and 0 <= assignment < len(centroids_snapshot):\n                            centroid_t = centroids_snapshot[assignment].to(device)\n                            try:\n                                h_aug = h_j + 0.1 * (centroid_t - h_j)\n                            except Exception:\n                                h_aug = h_j\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD] Assignment error for '{token_key}': {str(e)[:200]}\")\n\n            outputs['proto_assignments'].append(torch.tensor(assignment))\n            outputs['proto_probs'].append(prob_list)\n            outputs['uncertainties'].append(uncertainty)\n            outputs['span_preds'].append(span_pred)\n            outputs['gates'].append(gate_val)\n            outputs['h_augmented'].append(h_aug)\n\n        if not train_mode and len(self.prototype_stores) > 0 and VERBOSE_LOGGING:\n            if self.last_periodic_check % PRINT_INTERVAL == 0:\n                self._print_clusters_summary()\n            self.last_periodic_check += 1\n\n        return outputs\n\n    def _print_clusters_summary(self):\n        try:\n            items = []\n            for token, store in self.prototype_stores.items():\n                try:\n                    proto_sample_count = sum(getattr(store, 'counts', []) or [])\n                except Exception:\n                    proto_sample_count = 0\n                buffer_len = len(self.buffers.get(token, [])) if token in self.buffers else 0\n                total_count = proto_sample_count if proto_sample_count > 0 else buffer_len\n                protos = store.size()\n                mu = getattr(store, 'mu', 0.0)\n                tau = getattr(store, 'tau', 0.0)\n                items.append((token, total_count, protos, mu, tau, buffer_len))\n            items.sort(key=lambda x: x[1], reverse=True)\n            if VERBOSE_LOGGING:\n                print(\"\\n[CLUSTER] Top 5 clusters (by sample count or buffer size):\")\n                print(\"-\" * 100)\n                print(f\"{'Rank':<6} {'Token':<18} {'Count':<12} {'Protos':<8} {'BufLen':<8} {'μ (mean)':<15} {'τ (dev)':<15}\")\n                print(\"-\" * 100)\n                for rank, (tok, cnt, prot, mu, tau, buflen) in enumerate(items[:5], 1):\n                    tok_str = str(tok)[:18]\n                    print(f\"{rank:<6} {tok_str:<18} {cnt:<12} {prot:<8} {buflen:<8} {mu:<15.6f} {tau:<15.6f}\")\n                print(\"-\" * 100)\n                total_samples = sum(item[1] for item in items)\n                total_protos = sum(item[2] for item in items)\n                total_buffers = sum(item[5] for item in items)\n                print(f\"Total clusters: {len(items)} | Total samples: {total_samples} | Total protos: {total_protos} | Sum buffers: {total_buffers}\\n\")\n        except Exception as e:\n            if VERBOSE_LOGGING:\n                print(f\"[CLUSTER] Error printing summary: {str(e)[:200]}\")\n\n    def cleanup_memory(self):\n        try:\n            for token_type, buffer in list(self.buffers.items()):\n                if len(buffer) > int(self.buffer_size * 1.5):\n                    while len(buffer) > self.buffer_size:\n                        buffer.popleft()\n            try:\n                gc.collect()\n            except Exception:\n                pass\n        except Exception:\n            pass\n\n    def _cluster_buffer_to_prototypes_hierarchical(self, token_type):\n        try:\n            if not self.should_track_token(token_type):\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD-CLUSTER] Skipping clustering for non-word token '{token_type}'\")\n                return False\n            if token_type not in self.buffers:\n                return False\n            buf = self.buffers[token_type]\n            if len(buf) < self.n_min:\n                if VERBOSE_LOGGING:\n                    print(f\"[DSCD-CLUSTER] '{token_type}' buffer size {len(buf)} < n_min {self.n_min}\")\n                return False\n\n            emb_list = []\n            for e in buf:\n                try:\n                    if isinstance(e, torch.Tensor):\n                        emb_list.append(e.numpy())\n                    else:\n                        emb_list.append(np.asarray(e))\n                except Exception:\n                    continue\n            if len(emb_list) == 0:\n                return False\n\n            if len(emb_list) > self.max_clustering_points:\n                idxs = np.random.choice(len(emb_list), size=self.max_clustering_points, replace=False)\n                embeddings = np.stack([emb_list[i] for i in idxs], axis=0)\n            else:\n                embeddings = np.stack(emb_list, axis=0)\n\n            if embeddings.shape[0] < 2:\n                return False\n\n            if VERBOSE_LOGGING:\n                norms = np.linalg.norm(embeddings, axis=1)\n                print(f\"[DSCD-CLUSTER] Token '{token_type}' buffer={len(buf)} sampled={embeddings.shape[0]} mean_norm={norms.mean():.4f} std_norm={norms.std():.4f}\")\n\n            store = self.prototype_stores[token_type]\n            store.centroids = []\n            store.counts = []\n            store.creation_time = []\n\n            protos_added = 0\n\n            # hierarchical clustering (scipy)\n            if HAS_CLUSTERING:\n                try:\n                    condensed = pdist(embeddings, metric='euclidean')\n                    if condensed.size > 0:\n                        k_guess = min(self.max_protos, max(2, len(embeddings) // max(1, self.n_min)))\n                        k_guess = max(1, int(k_guess))\n                        Z = linkage(condensed, method='ward')\n                        clusters = fcluster(Z, t=k_guess, criterion='maxclust') - 1\n                        if clusters.size > 0:\n                            maxc = int(clusters.max())\n                            for cid in range(maxc + 1):\n                                mask = (clusters == cid)\n                                if mask.sum() >= self.n_min:\n                                    centroid = torch.from_numpy(embeddings[mask].mean(axis=0).astype(np.float32))\n                                    store.add_prototype(centroid, time.time(), count=int(mask.sum()))\n                                    protos_added += 1\n                    if VERBOSE_LOGGING and protos_added > 0:\n                        print(f\"[DSCD-CLUSTER] Hierarchical clustering created {protos_added} prototypes for '{token_type}'\")\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD-CLUSTER] Hierarchical clustering failed for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n\n            # sklearn KMeans fallback\n            if protos_added == 0 and HAS_KMEANS:\n                try:\n                    k_guess = min(self.max_protos, max(1, len(embeddings) // max(1, self.n_min)))\n                    k_guess = int(min(k_guess, len(embeddings)))\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        km = KMeans(n_clusters=k_guess, random_state=0, n_init=10).fit(embeddings)\n                        labels = km.labels_\n                        for c in range(k_guess):\n                            mask = (labels == c)\n                            if mask.sum() >= self.n_min:\n                                centroid = torch.from_numpy(embeddings[mask].mean(axis=0).astype(np.float32))\n                                store.add_prototype(centroid, time.time(), count=int(mask.sum()))\n                                protos_added += 1\n                        if VERBOSE_LOGGING and protos_added > 0:\n                            print(f\"[DSCD-CLUSTER] KMeans fallback created {protos_added} prototypes for '{token_type}'\")\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD-CLUSTER] KMeans fallback failed for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n\n            # pure-numpy kmeans fallback\n            if protos_added == 0:\n                try:\n                    k_guess = min(self.max_protos, max(1, len(embeddings) // max(1, self.n_min)))\n                    k_guess = int(min(k_guess, len(embeddings)))\n                    if k_guess >= 1 and len(embeddings) >= k_guess:\n                        labels, cents = _numpy_kmeans(embeddings.astype(np.float32), n_clusters=k_guess, n_iter=10, random_state=0)\n                        for c in range(k_guess):\n                            mask = (labels == c)\n                            if mask.sum() >= self.n_min:\n                                centroid = torch.from_numpy(cents[c].astype(np.float32))\n                                store.add_prototype(centroid, time.time(), count=int(mask.sum()))\n                                protos_added += 1\n                        if VERBOSE_LOGGING and protos_added > 0:\n                            print(f\"[DSCD-CLUSTER] numpy-kmeans created {protos_added} prototypes for '{token_type}'\")\n                except Exception as e:\n                    if VERBOSE_LOGGING:\n                        print(f\"[DSCD-CLUSTER] numpy-kmeans failed for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n\n            if VERBOSE_LOGGING:\n                print(f\"[DSCD-CLUSTER] Token '{token_type}': final_protos={store.size()} counts={store.counts}\")\n\n            return store.size() > 0\n\n        except Exception as e:\n            if VERBOSE_LOGGING:\n                print(f\"[DSCD-ERROR] Clustering error for '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n            return False\n\n    def get_explanations(self, threshold_span=0.3):\n        expl = []\n        for token_type, store in self.prototype_stores.items():\n            try:\n                if store.size() >= 2:\n                    expl.append({'token': str(token_type), 'protos': store.size(), 'counts': list(store.counts)})\n            except Exception:\n                continue\n        return expl\n\n# ==============================================================================\n# VERIFICATION MESSAGE\n# ==============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✅ Cell 3 (ENHANCED): DSCD Ready with Homograph Watchlist Integration (Debugged)\")\nprint(\"=\" * 80)\nprint(\"Key features and fixes:\")\nprint(\" ✅ Robust global config loading & defaults\")\nprint(\" ✅ Deny-prefix set for short combining/vowel marks to reduce noise\")\nprint(\" ✅ Atomic centroid snapshot under clustering_lock (race fix)\")\nprint(\" ✅ Hierarchical clustering (scipy) with sklearn KMeans and numpy-KMeans fallback\")\nprint(\" ✅ CPU-only prototype storage and clustering\")\nprint(\" ✅ Unicode-aware token filtering (Bengali/Latin aware)\")\nprint(\" ✅ Sampling for large buffers to avoid OOMs\")\nprint(\" ✅ Safe guards for missing scipy/sklearn with robust logging\")\nprint(\"=\" * 80 + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:06.046218Z","iopub.execute_input":"2025-11-22T14:56:06.046399Z","iopub.status.idle":"2025-11-22T14:56:07.176282Z","shell.execute_reply.started":"2025-11-22T14:56:06.046386Z","shell.execute_reply":"2025-11-22T14:56:07.175470Z"},"id":"L25pcKUPH4J2","trusted":true},"outputs":[{"name":"stdout","text":"\n================================================================================\n✅ Cell 3 (ENHANCED): DSCD Ready with Homograph Watchlist Integration (Debugged)\n================================================================================\nKey features and fixes:\n ✅ Robust global config loading & defaults\n ✅ Deny-prefix set for short combining/vowel marks to reduce noise\n ✅ Atomic centroid snapshot under clustering_lock (race fix)\n ✅ Hierarchical clustering (scipy) with sklearn KMeans and numpy-KMeans fallback\n ✅ CPU-only prototype storage and clustering\n ✅ Unicode-aware token filtering (Bengali/Latin aware)\n ✅ Sampling for large buffers to avoid OOMs\n ✅ Safe guards for missing scipy/sklearn with robust logging\n================================================================================\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Fixed Bangla normalize_bn_word (vowel-aware suffix stripping)\n# - Avoids removing final consonant when suffix includes vowel signs,\n#   e.g. \"ব্যাংকে\" -> \"ব্যাংক\" (not \"ব্যাং\").\nimport re\nimport unicodedata\nfrom typing import Optional, List\n\n# Original suffix list (kept as provided, sorted longest-first below)\n_BN_COMMON_SUFFIXES = [\n    # (same items as in your original list)\n    \"গুলোরই\", \"গুলোরও\", \"গুলোতেও\", \"গুলিকেও\", \"গুলিকেওই\", \"গুলিকেওও\",\n    \"গুলি থেকেও\", \"গুলিতে থেকেও\", \"গুলোতে থেকেও\", \"গুলিতে\", \"গুলিতেও\",\n    \"গুলোতেও\", \"গুলোকেই\", \"গুলোকেইই\", \"গুলোরে\", \"গুলোগুলোকে\", \"গুলোগুলোর\",\n    \"গুলোটা থেকে\", \"গুলোটা\", \"গুলোটি\", \"গুলোটা থেকে\", \"গুলোরই\", \"গুলোরও\",\n    \"গুলোরে\", \"গুলোকে\", \"গুলিকেও\", \"গুলোরা\", \"গুলোদের\", \"গুলিসহ\",\n    \"দেরকে\", \"দেরই\", \"দেরও\", \"দেরওই\", \"দেরে\", \"দেরা\", \"দেরইও\",\n    \"টারই\", \"টারও\", \"টার\", \"টারটা\", \"টিরই\", \"টিরও\", \"টির\", \"টাও\", \"টাই\",\n    \"টিই\", \"টা\", \"টি\", \"টায়\", \"টাতে\", \"টায়ও\",\n    \"থেকে\", \"থেকেও\", \"থেকেওই\", \"থেকেতে\", \"দিয়ে\", \"দিয়ে\", \"দিয়েও\", \"দিয়েই\",\n    \"দ্বারা\", \"মধ্যে\", \"মধ্যেও\", \"পরে\", \"পরে থেকেই\", \"জন্য\", \"জন্যই\", \"পক্ষে\",\n    \"নিয়ে\", \"নিয়ে\", \"সহ\", \"সহেই\", \"বিন্দুতে\", \"সম্পর্কে\", \"অনুযায়ী\", \"অনুযায়ী\",\n    \"অনুযায়ীত\", \"অনুসারে\", \"মতো\", \"সম্পন্ন\", \"নিমিত্তে\",\n    \"এরই\", \"এরও\", \"এরওই\", \"এর\", \"রই\", \"রাও\", \"রাওই\", \"রইও\", \"রে\", \"রো\", \"র\",\n    \"কে\", \"কেই\", \"কেও\", \"কেওই\", \"কো\", \"কোও\", \"কোওই\", \"তে\", \"তেও\", \"তেই\", \"েতে\",\n    \"তো\", \"তেইও\", \"তে থেকেই\",\n    \"ছিলাম\", \"ছিলে\", \"ছিলেন\", \"ছিল\", \"ছিলো\", \"ছেন\", \"ছেনই\", \"ছে\", \"ছি\", \"বো\", \"বেন\",\n    \"বে\", \"ব\", \"তেছি\", \"তেছে\", \"তেছিল\", \"তেছিলেন\", \"তেছিলাম\", \"আছে\", \"আছিল\",\n    \"হয়ে গেছে\", \"হয়েছে\", \"হয়েছে\", \"হয়েছিল\", \"হয়েছিল\", \"যাচ্ছি\", \"যাচ্ছে\",\n    \"যাচ্ছেন\", \"যাবেন\", \"যাবে\", \"গেছে\", \"আসছে\", \"আসছেন\", \"করেছি\", \"করেছে\",\n    \"করেছেন\", \"করছিল\", \"করছিলাম\", \"করছিলেন\", \"করবে\", \"করবেন\", \"করছে\",\n    \"তে পারে\", \"তে পারি\", \"তে পারেন\", \"তে পারবে\", \"তে পারবো\", \"তে পারত\", \"তে পারতেন\",\n    \"তা\", \"ত্য\", \"ত্ব\", \"ীত্ব\", \"িত্ব\", \"ীক\", \"ীয়\", \"ীয়\", \"ীতা\", \"িতা\", \"কারী\", \"বাদ\",\n    \"বাচক\", \"ময়\", \"ময়\", \"সমূহ\", \"গণ\", \"জাত\", \"যোগ্য\", \"যোগ্যতা\", \"পূর্ণ\", \"পূর্ণতা\",\n    \"বৃত্তি\", \"বোধ\", \"সুলভ\",\n    \"রা\", \"জন\", \"জনরা\", \"জনের\", \"জনকে\", \"লোক\", \"লোকেরা\", \"লোকজন\", \"জনগণ\",\n    \"জী\", \"জি\", \"সাহেব\", \"বাবু\", \"মশাই\", \"দাদা\", \"দিদা\", \"মা\", \"বাবা\", \"মামা\", \"তাই\",\n    \"সরি\", \"মর্ন\",\n    \"ও\", \"ই\", \"ন\", \"না\", \"তো\", \"তা\", \"ইও\", \"ইতেই\", \"দেন\", \"দেই\", \"ফলে\", \"থাকলে\",\n    \"পাও\", \"পাই\", \"পেলে\", \"পেলো\", \"পাওয়া\", \"পেয়েছে\", \"পেয়েছে\",\n    \"ময়\", \"ময়\", \"যে\", \"যার\", \"যাকে\", \"যাতে\", \"যাও\", \"যাচ্ছে\", \"যাবে\", \"পরো\", \"পড়েছে\",\n    \"পড়েছে\", \"দিয়ো\", \"দেওয়া\", \"দেওয়া\", \"নেই\", \"থাকি\", \"থাকেন\", \"থাকছে\",\n    \"অপররূপ\", \"অপরপ্রত\", \"অপর\", \"অতি\", \"অতিশয়\", \"অতিশয়\", \"অনুপ্রবেশ\", \"অনুপ্রেরণা\",\n    \"অনু\", \"অন\", \"উপ\", \"উপ-প\", \"উপোস\", \"প্রতি\", \"পুনর\", \"পুনঃ\", \"পুন\", \"স্ব\",\n    \"সম\", \"দ্বি\", \"ত্রি\", \"ত্রৈ\", \"অব\", \"বহু\", \"উদ\", \"অন্তর\", \"অন্তঃ\", \"পর\",\n    \"বিরু\", \"বিন\", \"আত্ম\", \"আত্ম-\", \"নির\", \"নিহিত\", \"অতি-তৃষ্ণা\", \"সু\", \"স্বর\", \"স্বধী\",\n    \"পশ্চাত\", \"পূর্ব\", \"পূর্ব-\" , \"অপ্র\", \"প্রতি-\", \"বিনা\", \"সং\", \"সং-\",\n    \"অ\", \"পুনর\", \"পুনঃ\", \"অব\", \"উপ\", \"প্র\", \"না\", \"নি\", \"অতি\", \"উৎ\", \"উৎপ\", \"উদ্\",\n    \"পরি\", \"সম্ব\", \"সমর্থ\", \"স্বন\", \"সু-\",\n    # safety: very short items last (avoid over-stripping)\n    \"েই\", \"ইই\", \"ই\", \"ওই\", \"ও\", \"্র\", \"্ষ\", \"্ত\", \"্ক\",\n]\n\n# longest-first sort\n_BN_COMMON_SUFFIXES_SORTED: List[str] = sorted(_BN_COMMON_SUFFIXES, key=lambda s: len(s), reverse=True)\n\n# regex to remove punctuation\n_RE_PUNCT = re.compile(r\"[^\\w\\u0980-\\u09FF\\-]+\", flags=re.UNICODE)\n\n# Bengali vowel signs (common combining marks)\n_VOWEL_SIGNS = {\n    \"\\u09BE\",  # া\n    \"\\u09BF\",  # ি\n    \"\\u09C0\",  # ী\n    \"\\u09C1\",  # ু\n    \"\\u09C2\",  # ূ\n    \"\\u09C3\",  # ৃ\n    \"\\u09C7\",  # ে  <-- important for \"কে\"\n    \"\\u09C8\",  # ৈ\n    \"\\u09CB\",  # ো\n    \"\\u09CC\",  # ৌ\n    \"\\u0982\",  # ং\n    \"\\u0983\",  # ঃ\n}\n\n# zero-width cleanup\n_ZW_RE = re.compile(r\"[\\u200b\\u200c\\u200d]+\")\n\ndef _ends_with_vowel_sign(s: str) -> bool:\n    return len(s) > 0 and s[-1] in _VOWEL_SIGNS\n\ndef normalize_bn_word(raw: Optional[str]) -> str:\n    \"\"\"\n    Normalizes Bengali token and does vowel-aware suffix stripping.\n    - NFC unicode normalization\n    - removes common token markers (subword, BPE)\n    - removes punctuation\n    - iterates over sorted suffixes: if suffix matches and suffix ends with vowel sign\n      only remove trailing vowel sign(s) from the word (preserve consonant),\n      otherwise remove whole suffix (longest-first).\n    - requires resulting stem to be of minimal sensible length (>=2)\n    \"\"\"\n    if raw is None:\n        return \"\"\n    s = str(raw).strip()\n    if not s:\n        return \"\"\n\n    # Normalize unicode form (use NFC for stable composed form)\n    s = unicodedata.normalize(\"NFC\", s)\n\n    # remove subword markers\n    for mk in (\"▁\", \"##\", \"Ġ\", \"@@\"):\n        s = s.replace(mk, \"\")\n\n    # remove zero-width joiners/markers\n    s = _ZW_RE.sub(\"\", s)\n\n    # strip surrounding ascii punctuation\n    s = s.strip(\" \\t\\n\\r.,;:!?\\\"'()[]{}—–-\")\n\n    # remove internal punctuation (non-Bengali letter/number/dash)\n    s = _RE_PUNCT.sub(\"\", s)\n\n    # iterate over suffix list (longest-first)\n    for suf in _BN_COMMON_SUFFIXES_SORTED:\n        try:\n            if not suf:\n                continue\n            if s.endswith(suf) and (len(s) - len(suf) >= 2):\n                # If the suffix ends with a vowel sign (e.g., \"কে\" ends with \"ে\"),\n                # prefer removing only the trailing vowel sign(s) from the word\n                if any(ch in _VOWEL_SIGNS for ch in suf):\n                    # strip trailing vowel signs from the word (but keep consonant)\n                    while _ends_with_vowel_sign(s) and len(s) > 1:\n                        s = s[:-1]\n                    s = s.strip()\n                else:\n                    # safe remove whole suffix (as before)\n                    s = s[: -len(suf)].strip()\n                break\n        except Exception:\n            continue\n\n    # final normalization\n    s = unicodedata.normalize(\"NFC\", s).strip()\n    return s\n\n# expose globally if run inside a notebook/script\nglobals()['normalize_bn_word'] = normalize_bn_word\n\n# quick smoke test\nif __name__ == \"__main__\":\n    tests = [\"ব্যাংকে\", \"ব্যাংকের\", \"ব্যাংক\", \"ব্যাংকক\", \"কলগুলোতে\", \"কলগুলোকে\"]\n    for t in tests:\n        print(f\"{t} -> {normalize_bn_word(t)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:56:07.178589Z","iopub.execute_input":"2025-11-22T14:56:07.178999Z","iopub.status.idle":"2025-11-22T14:56:07.196598Z","shell.execute_reply.started":"2025-11-22T14:56:07.178980Z","shell.execute_reply":"2025-11-22T14:56:07.195765Z"}},"outputs":[{"name":"stdout","text":"ব্যাংকে -> ব্যাংক\nব্যাংকের -> ব্যাংকে\nব্যাংক -> ব্যাংক\nব্যাংকক -> ব্যাংকক\nকলগুলোতে -> কলগুলোত\nকলগুলোকে -> কলগুলোক\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# Cell 4 (patched): ASBN module — device-safe, memory-friendly, defensive\n# - Thoroughly hardened and line-by-line defensive fixes applied.\n# - Exposes a toggle ASBN_MONITOR_IN_EVAL to allow monitoring in eval mode if desired.\n# - Robust parsing of DSCD outputs (many possible shapes).\n# - Functional frozen forward for discriminator parameters (GRL-style encoder loss).\n# - Safe device movement for discriminator submodules.\n# ==============================================================================\nimport traceback\nfrom typing import Any, List, Tuple, Optional, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Local safe globals (use globals().get to avoid NameError)\n_MAX_LENGTH = int(globals().get(\"MAX_LENGTH\", 48))\n_ENABLE_ASBN_TRAINING = bool(globals().get(\"ENABLE_ASBN_TRAINING\", True))\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n_SOURCE_LANGUAGE = str(globals().get(\"SOURCE_LANGUAGE\", \"bn\"))\n_ASBN_MONITOR_IN_EVAL = bool(globals().get(\"ASBN_MONITOR_IN_EVAL\", False))  # New: allow monitoring even when module.eval()\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_special_tokens = \"get_special_tokens\" in globals()\n\n# Utility: safe device selector\ndef _device_of(x: Any) -> torch.device:\n    if isinstance(x, torch.Tensor):\n        return x.device\n    # default device\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass LightweightDiscriminator(nn.Module):\n    \"\"\"Small discriminator head used by ASBN (kept intentionally tiny).\"\"\"\n\n    def __init__(self, input_dim: int):\n        super().__init__()\n        # two-layer MLP\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, 2)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.classifier(x)\n\n\nclass MemoryEfficientASBNModule(nn.Module):\n    \"\"\"\n    ASBN module (robust/hardened).\n    - forward_discriminators_simplified: monitoring pass (no grad).\n    - forward_with_grl_simplified: computes encoder loss with frozen discriminator params.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, tokenizer=None, language: str = \"bn\"):\n        super().__init__()\n        self.language = language\n        self.tokenizer = tokenizer\n\n        # discriminators\n        self.d_freq = LightweightDiscriminator(embed_dim + 2)\n        self.d_ctx = LightweightDiscriminator(embed_dim + 2)\n        self.d_xl = LightweightDiscriminator(embed_dim)\n\n        # scaling knobs\n        self.lambda_base = {\"freq\": 1.0, \"ctx\": 0.5, \"xl\": 0.8}\n        self.lambda_max = 2.0\n        self.encoder_grl_scale = float(globals().get(\"ASBN_ENCODER_GRL_SCALE\", 0.1))\n\n        # Cache special tokens robustly\n        try:\n            if tokenizer is not None and _has_get_special_tokens:\n                self.special_tokens = globals()[\"get_special_tokens\"](tokenizer)\n            elif tokenizer is not None:\n                self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            else:\n                self.special_tokens = set()\n        except Exception:\n            self.special_tokens = set()\n\n    def critic_parameters(self):\n        return list(self.d_freq.parameters()) + list(self.d_ctx.parameters()) + list(self.d_xl.parameters())\n\n    # -----------------------\n    # helpers\n    # -----------------------\n    def _ensure_discriminators_on_device(self, device: torch.device):\n        \"\"\"\n        Best-effort: move discriminator modules to device. Do not raise.\n        \"\"\"\n        try:\n            for mod in (self.d_freq, self.d_ctx, self.d_xl):\n                try:\n                    p = next(mod.parameters(), None)\n                    if p is not None and p.device != device:\n                        mod.to(device)\n                except Exception:\n                    try:\n                        mod.to(device)\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(\"[ASBN] warning moving discriminator to device failed\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] _ensure_discriminators_on_device failed:\", traceback.format_exc().splitlines()[-1])\n\n    def _parse_proto_probs_matrix(self, proto_probs: Any, batch_size: int, seq_len: int, device: torch.device) -> torch.Tensor:\n        \"\"\"\n        Normalize proto_probs into pmax tensor [B, T] containing max prototype prob per token.\n        Accepts torch.Tensor (B,T,K | T,K | T | K), lists of lists, numpy arrays, etc.\n        \"\"\"\n        pmax = torch.full((batch_size, seq_len), 0.5, dtype=torch.float32, device=device)\n        try:\n            if proto_probs is None:\n                return pmax\n\n            # case: torch tensor\n            if isinstance(proto_probs, torch.Tensor):\n                p = proto_probs.detach().to(device)\n                if p.dim() == 3:\n                    B, T, K = p.shape\n                    vals = p.max(dim=2)[0]\n                    pmax[:min(batch_size, B), :min(seq_len, T)] = vals[:batch_size, :seq_len]\n                    return pmax\n                elif p.dim() == 2:\n                    # treat rows as per-token vectors for a single batch\n                    if p.size(0) <= seq_len and batch_size >= 1:\n                        vals = p.max(dim=1)[0]\n                        pmax[0, :min(seq_len, vals.size(0))] = vals[:seq_len]\n                        return pmax\n                    else:\n                        vals = p.max(dim=1)[0]\n                        pmax[0, :min(seq_len, vals.size(0))] = vals[:seq_len]\n                        return pmax\n                elif p.dim() == 1:\n                    pmax[0, :min(seq_len, p.size(0))] = p[:seq_len]\n                    return pmax\n\n            # list/tuple handling\n            if isinstance(proto_probs, (list, tuple)):\n                # if matches batch length\n                if len(proto_probs) == batch_size:\n                    for b in range(batch_size):\n                        row = proto_probs[b]\n                        if isinstance(row, torch.Tensor):\n                            if row.dim() == 2:\n                                vals = row.max(dim=1)[0].detach().to(device)\n                                pmax[b, :min(seq_len, vals.size(0))] = vals[:seq_len]\n                            elif row.dim() == 1:\n                                vals = row.detach().to(device)\n                                pmax[b, :min(seq_len, vals.size(0))] = vals[:seq_len]\n                        elif isinstance(row, (list, tuple, np.ndarray)):\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    val = row[t]\n                                    if isinstance(val, torch.Tensor):\n                                        arr = val.detach().cpu().numpy()\n                                        if arr.size:\n                                            pmax[b, t] = float(arr.max())\n                                    else:\n                                        arr = np.asarray(val, dtype=np.float32)\n                                        if arr.size:\n                                            pmax[b, t] = float(np.max(arr))\n                                except Exception:\n                                    pmax[b, t] = 0.5\n                    return pmax\n                else:\n                    # maybe single batch list-of-per-token\n                    if batch_size == 1:\n                        for t in range(min(seq_len, len(proto_probs))):\n                            try:\n                                val = proto_probs[t]\n                                if isinstance(val, torch.Tensor):\n                                    arr = val.detach().cpu().numpy()\n                                    pmax[0, t] = float(np.max(arr)) if arr.size else 0.5\n                                else:\n                                    arr = np.asarray(val, dtype=np.float32)\n                                    pmax[0, t] = float(np.max(arr)) if arr.size else 0.5\n                            except Exception:\n                                pmax[0, t] = 0.5\n                        return pmax\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] parse_proto_probs exception:\", traceback.format_exc().splitlines()[-1])\n        return pmax\n\n    def _parse_scalar_matrix(self, mat: Any, batch_size: int, seq_len: int, device: torch.device, default: float = 0.0) -> torch.Tensor:\n        \"\"\"\n        Normalize scalar-like structures into [B, T] tensor (supports torch.Tensor, list, tuple, numpy).\n        \"\"\"\n        out = torch.full((batch_size, seq_len), float(default), dtype=torch.float32, device=device)\n        try:\n            if mat is None:\n                return out\n\n            if isinstance(mat, torch.Tensor):\n                m = mat.detach().to(device)\n                if m.dim() == 3:\n                    out[:min(batch_size, m.size(0)), :min(seq_len, m.size(1))] = m[:, :seq_len, 0]\n                elif m.dim() == 2:\n                    if m.size(0) == batch_size:\n                        out[:, :min(seq_len, m.size(1))] = m[:, :seq_len]\n                    elif batch_size == 1:\n                        out[0, :min(seq_len, m.size(0))] = m[:seq_len]\n                elif m.dim() == 1:\n                    if batch_size == 1:\n                        out[0, :min(seq_len, m.size(0))] = m[:seq_len]\n                return out\n\n            if isinstance(mat, (list, tuple, np.ndarray)):\n                if len(mat) == batch_size:\n                    for b in range(batch_size):\n                        row = mat[b]\n                        if isinstance(row, torch.Tensor):\n                            r = row.detach().to(device)\n                            for t in range(min(seq_len, r.size(0))):\n                                out[b, t] = float(r[t].item())\n                        else:\n                            for t in range(min(seq_len, len(row))):\n                                try:\n                                    v = row[t]\n                                    out[b, t] = float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n                                except Exception:\n                                    out[b, t] = float(default)\n                    return out\n                else:\n                    # single-batch sequence\n                    if batch_size == 1:\n                        row = mat\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                v = row[t]\n                                out[0, t] = float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n                            except Exception:\n                                out[0, t] = float(default)\n                        return out\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] parse_scalar_matrix exception:\", traceback.format_exc().splitlines()[-1])\n        return out\n\n    def compute_lambda_scaled_tensor(self, pmax: torch.Tensor, uncertainty: torch.Tensor, gate: torch.Tensor, lambda_type: str) -> torch.Tensor:\n        \"\"\"\n        lam = base * pmax * (1 - uncertainty) * gate, clipped to [0, lambda_max]\n        \"\"\"\n        base = float(self.lambda_base.get(lambda_type, 0.2))\n        lam = base * pmax * (1.0 - uncertainty) * gate\n        lam = torch.clamp(lam, 0.0, float(self.lambda_max))\n        lam = torch.where(torch.isfinite(lam), lam, torch.zeros_like(lam))\n        return lam\n\n    # -----------------------\n    # Monitor (no grad) - safe even if discriminators on CPU\n    # -----------------------\n    def forward_discriminators_simplified(\n        self,\n        h: Optional[torch.Tensor],\n        proto_probs: Any,\n        uncertainties: Any,\n        gates: Any,\n        token_word_map: Optional[List[Dict[int, str]]] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Monitoring pass under torch.no_grad(). Returns scalar Tensor on same device as h.\n        Controlled by module training or ASBN_MONITOR_IN_EVAL flag.\n        \"\"\"\n        device = _device_of(h)\n        zero = torch.tensor(0.0, device=device)\n\n        # Monitor only when training by default unless user forces monitoring in eval\n        if (not self.training) and (not _ASBN_MONITOR_IN_EVAL):\n            return zero\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            return zero\n\n        B, T, H = h.size()\n\n        # Ensure discriminators are available on device (best-effort)\n        try:\n            self._ensure_discriminators_on_device(device)\n        except Exception:\n            pass\n\n        pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n        U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n        G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n\n        sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n\n        # Filter using token_word_map and optional is_valid_token helper\n        if token_word_map:\n            try:\n                for b in range(min(B, len(token_word_map))):\n                    wm = token_word_map[b] or {}\n                    for t in range(T):\n                        if t in wm:\n                            if _has_is_valid_token:\n                                try:\n                                    if not is_valid_token(wm[t], self.special_tokens, self.tokenizer, language=self.language):\n                                        sel_mask[b, t] = False\n                                except Exception:\n                                    sel_mask[b, t] = False\n                            else:\n                                w = str(wm[t])\n                                if len(w.strip()) < 2:\n                                    sel_mask[b, t] = False\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[ASBN] token_word_map filter failed:\", traceback.format_exc().splitlines()[-1])\n\n        sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n        if sel_idx.numel() == 0:\n            return zero\n\n        # Gather features\n        h_flat = h.view(B * T, H)\n        sel_emb = h_flat[sel_idx]\n        pmax_flat = pmax_mat.view(-1)[sel_idx]\n        U_flat = U_mat.view(-1)[sel_idx]\n        G_flat = G_mat.view(-1)[sel_idx]\n\n        seq_len_feature = float(T) / max(int(_MAX_LENGTH), 1)\n        ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1)\n        freq_feature = torch.stack([pmax_flat, U_flat], dim=1)\n\n        freq_input = torch.cat([sel_emb, freq_feature.to(device)], dim=1)\n        ctx_input = torch.cat([sel_emb, ctx_feature.to(device)], dim=1)\n        xl_input = sel_emb\n\n        try:\n            with torch.no_grad():\n                self._ensure_discriminators_on_device(device)\n                freq_logits = self.d_freq(freq_input)\n                ctx_logits = self.d_ctx(ctx_input)\n                xl_logits = self.d_xl(xl_input)\n\n                # pseudo labels\n                freq_label = (pmax_flat > 0.7).long().to(device)\n                ctx_label = (U_flat < 0.3).long().to(device)\n                xl_label = (G_flat > 0.5).long().to(device)\n\n                loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n                loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n                loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n\n                lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n                lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n                lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n                weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n                avg_loss = torch.mean(weighted) if weighted.numel() > 0 else torch.tensor(0.0, device=device)\n            return avg_loss\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] Monitor forward failed (device/param issue):\", traceback.format_exc().splitlines()[-1])\n            return zero\n\n    # -----------------------\n    # Encoder GRL using detached/cloned params (functional forward)\n    # -----------------------\n    def forward_with_grl_simplified(\n        self,\n        h: Optional[torch.Tensor],\n        proto_probs: Any,\n        uncertainties: Any,\n        gates: Any,\n        token_word_map: Optional[List[Dict[int, str]]] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Returns (encoder_loss, disc_monitor_loss, zero, zero)\n        encoder_loss is suitable for backprop into encoder representations h.\n        \"\"\"\n        device = _device_of(h)\n        zero = torch.tensor(0.0, device=device)\n\n        if (not self.training) or (not _ENABLE_ASBN_TRAINING):\n            return zero, zero, zero, zero\n\n        if not isinstance(h, torch.Tensor) or h.dim() != 3:\n            return zero, zero, zero, zero\n\n        # monitor (no_grad)\n        try:\n            with torch.no_grad():\n                disc_monitor_loss = self.forward_discriminators_simplified(h, proto_probs, uncertainties, gates, token_word_map)\n                if not isinstance(disc_monitor_loss, torch.Tensor):\n                    disc_monitor_loss = torch.tensor(float(disc_monitor_loss), device=device)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] forward_discriminators_simplified (monitor) failed:\", traceback.format_exc().splitlines()[-1])\n            disc_monitor_loss = torch.tensor(0.0, device=device)\n\n        # compute encoder loss (functional forward)\n        try:\n            B, T, H = h.size()\n            pmax_mat = self._parse_proto_probs_matrix(proto_probs, B, T, device)\n            U_mat = self._parse_scalar_matrix(uncertainties, B, T, device, default=0.1)\n            G_mat = self._parse_scalar_matrix(gates, B, T, device, default=0.0)\n\n            sel_mask = torch.ones((B, T), dtype=torch.bool, device=device)\n            if token_word_map:\n                try:\n                    for b in range(min(B, len(token_word_map))):\n                        wm = token_word_map[b] or {}\n                        for t in range(T):\n                            if t in wm:\n                                if _has_is_valid_token:\n                                    try:\n                                        if not is_valid_token(wm[t], self.special_tokens, self.tokenizer, language=self.language):\n                                            sel_mask[b, t] = False\n                                    except Exception:\n                                        sel_mask[b, t] = False\n                                else:\n                                    w = str(wm[t])\n                                    if len(w.strip()) < 2:\n                                        sel_mask[b, t] = False\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(\"[ASBN] token_word_map filter (GRL) failed:\", traceback.format_exc().splitlines()[-1])\n\n            sel_idx = sel_mask.view(-1).nonzero(as_tuple=False).squeeze(1)\n            if sel_idx.numel() == 0:\n                encoder_loss = torch.tensor(0.0, device=device, requires_grad=True)\n            else:\n                h_flat = h.view(B * T, H)\n                sel_emb = h_flat[sel_idx]           # [N, H]\n                pmax_flat = pmax_mat.view(-1)[sel_idx]\n                U_flat = U_mat.view(-1)[sel_idx]\n                G_flat = G_mat.view(-1)[sel_idx]\n\n                max_len = max(int(_MAX_LENGTH), 1)\n                seq_len_feature = float(T) / float(max_len)\n                freq_feature = torch.stack([pmax_flat, U_flat], dim=1).to(device)\n                ctx_feature = torch.stack([G_flat, torch.full_like(G_flat, seq_len_feature)], dim=1).to(device)\n\n                freq_input = torch.cat([sel_emb, freq_feature], dim=1)     # [N, Df]\n                ctx_input = torch.cat([sel_emb, ctx_feature], dim=1)       # [N, Dc]\n                xl_input = sel_emb                                         # [N, H]\n\n                # extract frozen params (as leaf tensors)\n                def get_frozen_params(module: nn.Module, device: torch.device):\n                    try:\n                        # primary path relies on module.classifier structure used in LightweightDiscriminator\n                        l0 = module.classifier[0]\n                        l1 = module.classifier[3]\n                        w0 = l0.weight.detach().clone().to(device)\n                        b0 = l0.bias.detach().clone().to(device) if l0.bias is not None else None\n                        w1 = l1.weight.detach().clone().to(device)\n                        b1 = l1.bias.detach().clone().to(device) if l1.bias is not None else None\n                        for t in (w0, b0, w1, b1):\n                            if t is not None:\n                                t.requires_grad = False\n                        return (w0, b0, w1, b1)\n                    except Exception:\n                        params = list(module.parameters())\n                        if len(params) >= 4:\n                            try:\n                                w0 = params[0].detach().clone().to(device)\n                                b0 = params[1].detach().clone().to(device)\n                                w1 = params[2].detach().clone().to(device)\n                                b1 = params[3].detach().clone().to(device)\n                                for t in (w0, b0, w1, b1):\n                                    if t is not None:\n                                        t.requires_grad = False\n                                return (w0, b0, w1, b1)\n                            except Exception:\n                                pass\n                        raise RuntimeError(\"Failed to extract frozen params from discriminator module\")\n\n                frozen_freq = get_frozen_params(self.d_freq, device)\n                frozen_ctx = get_frozen_params(self.d_ctx, device)\n                frozen_xl = get_frozen_params(self.d_xl, device)\n\n                def functional_classifier_forward(x: torch.Tensor, frozen_params, dropout_p: float = 0.1):\n                    w0, b0, w1, b1 = frozen_params\n                    y = F.linear(x, w0, b0)\n                    y = F.relu(y)\n                    # dropout in functional form (training False here)\n                    y = F.dropout(y, p=dropout_p, training=False)\n                    y = F.linear(y, w1, b1)\n                    return y\n\n                freq_logits = functional_classifier_forward(freq_input, frozen_freq, dropout_p=0.1)\n                ctx_logits = functional_classifier_forward(ctx_input, frozen_ctx, dropout_p=0.1)\n                xl_logits = functional_classifier_forward(xl_input, frozen_xl, dropout_p=0.1)\n\n                freq_label = (pmax_flat > 0.7).long().to(device)\n                ctx_label = (U_flat < 0.3).long().to(device)\n                xl_label = (G_flat > 0.5).long().to(device)\n\n                loss_freq = F.cross_entropy(freq_logits, freq_label, reduction=\"none\")\n                loss_ctx = F.cross_entropy(ctx_logits, ctx_label, reduction=\"none\")\n                loss_xl = F.cross_entropy(xl_logits, xl_label, reduction=\"none\")\n\n                lam_freq = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"freq\")\n                lam_ctx = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"ctx\")\n                lam_xl = self.compute_lambda_scaled_tensor(pmax_flat, U_flat, G_flat, \"xl\")\n\n                weighted = lam_freq * loss_freq + lam_ctx * loss_ctx + lam_xl * loss_xl\n                mean_weighted = torch.mean(weighted) if weighted.numel() > 0 else torch.tensor(0.0, device=device)\n                encoder_loss = -float(self.encoder_grl_scale) * mean_weighted\n                encoder_loss = encoder_loss.to(device)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[ASBN] GRL computation failed:\", traceback.format_exc().splitlines()[-1])\n            encoder_loss = torch.tensor(0.0, device=device, requires_grad=True)\n\n        return encoder_loss, disc_monitor_loss, torch.tensor(0.0, device=device), torch.tensor(0.0, device=device)\n\n\nprint(\"✅ Cell 4 (patched final, device-safe): ASBN module ready (functional frozen-forward + discriminator device safety)\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:07.197591Z","iopub.execute_input":"2025-11-22T14:56:07.197885Z","iopub.status.idle":"2025-11-22T14:56:07.252249Z","shell.execute_reply.started":"2025-11-22T14:56:07.197861Z","shell.execute_reply":"2025-11-22T14:56:07.251445Z"},"id":"XrNq18UsH4J3","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 4 (patched final, device-safe): ASBN module ready (functional frozen-forward + discriminator device safety)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5 (patched): TRG EXPLANATION SYSTEM (INFERENCE-ONLY + MULTI-GPU OPTIMIZED)\n# - Thorough, line-by-line hardening\n# - Aggregates subword pieces using normalized token_word_map (norm) when available\n# - Avoids early-return on self.training so eval-mode toggling handled externally\n# - Robust proto_probs / scalar parsing, safe paddings and averaging for aggregation\n# ==============================================================================\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom collections import deque\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# Robust config defaults\n_TRG_EVIDENCE_K = int(globals().get(\"TRG_EVIDENCE_K\", 3))\n_TRG_GEN_EMBED = int(globals().get(\"TRG_GEN_EMBED\", 64))\n_MAX_SILVER_BUFFER = int(globals().get(\"MAX_SILVER_BUFFER\", 50))\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n_ENABLE_TRG_INFERENCE = bool(globals().get(\"ENABLE_TRG_INFERENCE\", True))\n_SOURCE_LANGUAGE = str(globals().get(\"SOURCE_LANGUAGE\", \"bn\"))\n_TRG_UNCERTAINTY_THRESHOLD = float(globals().get(\"TAU_LOW\", 0.40))\n\n_has_is_valid_token = \"is_valid_token\" in globals()\n_has_get_tokenizer_special_tokens = \"get_tokenizer_special_tokens\" in globals()\n_has_get_cached_special_tokens = \"get_cached_special_tokens\" in globals()\n\n# Optional normalizer (may be provided by bn_normalizer cell)\n_normalize_fn = globals().get(\"normalize_bn_word\", None)\n\n\ndef _is_word_start(raw_token: str, token_word_map: Optional[dict], idx: int) -> bool:\n    \"\"\"\n    Robust word-start detection (SPM/BPE markers or reconstructed word in token_word_map).\n    \"\"\"\n    try:\n        if token_word_map and isinstance(token_word_map, dict) and idx in token_word_map:\n            w = token_word_map[idx]\n            return isinstance(w, str) and len(w.strip()) > 0\n\n        if isinstance(raw_token, str):\n            if raw_token.startswith(\"▁\") or raw_token.startswith(\"Ġ\"):\n                return True\n            clean = raw_token.replace(\"▁\", \"\").replace(\"Ġ\", \"\").strip()\n            if len(clean) >= 2 and not all(ch in '.,;:!?\"\\'()[]{}-/' for ch in clean):\n                return True\n    except Exception:\n        pass\n    return False\n\n\nclass ComprehensiveTRGExplanationTemplate:\n    \"\"\"Templates to render explanation strings.\"\"\"\n\n    def __init__(self):\n        self.explanation_templates = {\n            \"high_confidence\": (\n                \"Chose '{sense}' with high confidence ({confidence:.1%}) based on contextual evidence: '{evidence}'. \"\n                \"This matches the learned pattern. {alternatives_text}\"\n            ),\n            \"medium_confidence\": (\n                \"Selected '{sense}' with moderate confidence ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. Some uncertainty remains. {alternatives_text}\"\n            ),\n            \"low_confidence\": (\n                \"Uncertain between senses; chose '{sense}' ({confidence:.1%}). \"\n                \"Evidence: '{evidence}'. {alternatives_text} Review recommended.\"\n            ),\n            \"fallback\": (\n                \"Token '{token}' processed with standard analysis. Context: '{evidence}'.\"\n            ),\n        }\n\n    def generate_explanation(self, evidence: Dict) -> str:\n        token = str(evidence.get(\"token\", \"unknown\")).replace(\"▁\", \"\")\n        sense_info = evidence.get(\"chosen_sense\", (\"unknown\", 0.5))\n\n        if isinstance(sense_info, (tuple, list)) and len(sense_info) >= 2:\n            sense_name, confidence = str(sense_info[0]), float(sense_info[1])\n        else:\n            sense_name, confidence = \"unknown\", 0.5\n\n        evidence_tokens = evidence.get(\"evidence_tokens\", [])\n        evidence_str = \", \".join([str(tok).replace(\"▁\", \"\") for tok in evidence_tokens[:_TRG_EVIDENCE_K]]) or \"limited context\"\n\n        alternatives = evidence.get(\"alternatives\", [])\n        alternatives_text = \"\"\n        if isinstance(alternatives, list) and len(alternatives) > 0:\n            alt_parts = []\n            for alt in alternatives[:2]:\n                if isinstance(alt, (tuple, list)) and len(alt) >= 2:\n                    alt_name, alt_conf = str(alt[0]), float(alt[1])\n                    alt_parts.append(f\"'{alt_name}' ({alt_conf:.1%})\")\n            if alt_parts:\n                alternatives_text = f\"Alternatives: {', '.join(alt_parts)} considered.\"\n\n        if confidence >= 0.65:\n            template_key = \"high_confidence\"\n        elif confidence >= 0.4:\n            template_key = \"medium_confidence\"\n        else:\n            template_key = \"low_confidence\"\n\n        template = self.explanation_templates.get(template_key, self.explanation_templates[\"fallback\"])\n\n        try:\n            return template.format(\n                sense=sense_name,\n                confidence=confidence,\n                evidence=evidence_str,\n                alternatives_text=alternatives_text,\n                token=token,\n            )\n        except Exception:\n            return f\"Token '{token}' disambiguated as '{sense_name}' ({confidence:.1%}).\"\n\n\nclass MemoryEfficientTRGExtractor:\n    \"\"\"Extracts evidence around a token for explanation rendering and handles aggregation.\"\"\"\n\n    def __init__(self, tokenizer=None, language=\"bn\"):\n        self.tokenizer = tokenizer\n        self.language = language\n\n        # special tokens retrieval robustly\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = globals()[\"get_tokenizer_special_tokens\"](tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = globals()[\"get_cached_special_tokens\"](tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n    # -------------------------\n    # HIGH-LEVEL: evidence extraction with aggregation across subword pieces\n    # -------------------------\n    def extract_evidence_efficiently(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n    ) -> Dict:\n        \"\"\"Extract evidence safely and aggregate subword pieces using normalized token_word_map when available.\"\"\"\n        if not isinstance(tokens, list) or token_idx < 0 or token_idx >= len(tokens):\n            return self._create_fallback_evidence(token_idx, tokens or [])\n\n        raw_token = tokens[token_idx]\n\n        # Basic validity check\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = raw_token not in self.special_tokens and len(str(raw_token)) >= 2\n        else:\n            is_valid = raw_token not in self.special_tokens and len(str(raw_token)) >= 2\n\n        if not is_valid:\n            return self._create_fallback_evidence(token_idx, tokens)\n\n        try:\n            # Determine indices to aggregate: prefer token_word_map['norm'] equality if available\n            agg_indices = [token_idx]\n            try:\n                if token_word_map and isinstance(token_word_map, dict):\n                    # support two common formats:\n                    # 1) token_word_map is mapping idx -> reconstructed word\n                    # 2) token_word_map is dict with 'orig' and 'norm' maps\n                    norm_map = token_word_map.get(\"norm\") if isinstance(token_word_map.get(\"norm\", None), dict) else None\n                    orig_map = token_word_map.get(\"orig\") if isinstance(token_word_map.get(\"orig\", None), dict) else token_word_map if isinstance(token_word_map, dict) else None\n\n                    if norm_map and token_idx in norm_map and norm_map[token_idx]:\n                        main_norm = norm_map[token_idx]\n                        for k, v in norm_map.items():\n                            try:\n                                if k != token_idx and v == main_norm:\n                                    agg_indices.append(k)\n                            except Exception:\n                                continue\n                    elif orig_map and token_idx in orig_map and orig_map[token_idx]:\n                        # try normalized comparator if normalization fn available\n                        try:\n                            if _normalize_fn:\n                                main_norm = _normalize_fn(orig_map[token_idx])\n                                for k, v in orig_map.items():\n                                    try:\n                                        if k != token_idx and isinstance(v, str) and _normalize_fn(v) == main_norm:\n                                            agg_indices.append(k)\n                                    except Exception:\n                                        continue\n                            else:\n                                # fallback: aggregate adjacent subword pieces heuristically\n                                if token_idx - 1 >= 0 and not str(tokens[token_idx - 1]).startswith('▁'):\n                                    agg_indices.insert(0, token_idx - 1)\n                                if token_idx + 1 < len(tokens) and not str(tokens[token_idx + 1]).startswith('▁'):\n                                    agg_indices.append(token_idx + 1)\n                        except Exception:\n                            pass\n                    else:\n                        # heuristic: include neighbors that are continuation pieces (do not start with word-start marker)\n                        if token_idx - 1 >= 0 and not str(tokens[token_idx - 1]).startswith('▁') and not str(tokens[token_idx - 1]).startswith('Ġ'):\n                            agg_indices.insert(0, token_idx - 1)\n                        if token_idx + 1 < len(tokens) and not str(tokens[token_idx + 1]).startswith('▁') and not str(tokens[token_idx + 1]).startswith('Ġ'):\n                            agg_indices.append(token_idx + 1)\n            except Exception:\n                # leave agg_indices as [token_idx]\n                agg_indices = [token_idx]\n\n            # Collect proto_probs, uncertainties, gates, spans for aggregated indices\n            proto_tensors = []\n            uncerts = []\n            gates = []\n            spans = []\n            for i in agg_indices:\n                p = self._safe_extract_proto_probs(i, dscd_outputs)  # torch tensor\n                if not isinstance(p, torch.Tensor):\n                    try:\n                        p = torch.as_tensor(np.asarray(p, dtype=np.float32), dtype=torch.float32)\n                    except Exception:\n                        p = torch.tensor([1.0], dtype=torch.float32)\n                proto_tensors.append(p.flatten())\n\n                u = self._safe_extract_uncertainty(i, dscd_outputs)\n                g = self._safe_extract_gate(i, dscd_outputs)\n                s = self._safe_extract_span(i, dscd_outputs)\n                try:\n                    uncerts.append(float(u))\n                except Exception:\n                    uncerts.append(0.5)\n                try:\n                    gates.append(float(g))\n                except Exception:\n                    gates.append(0.0)\n                try:\n                    spans.append(float(s))\n                except Exception:\n                    spans.append(0.0)\n\n            # Pad proto vectors to same length and average\n            maxk = max([int(p.numel()) for p in proto_tensors]) if proto_tensors else 1\n            padded = []\n            for p in proto_tensors:\n                if int(p.numel()) < maxk:\n                    p2 = torch.zeros(maxk, dtype=torch.float32)\n                    p2[:p.numel()] = p\n                    padded.append(p2)\n                else:\n                    padded.append(p[:maxk])\n            stacked = torch.stack(padded, dim=0)\n            agg_proto = torch.mean(stacked, dim=0)\n            # normalize if sum > 0\n            try:\n                ssum = float(torch.sum(agg_proto).item())\n                if ssum > 0:\n                    agg_proto = agg_proto / (ssum + 1e-12)\n            except Exception:\n                pass\n\n            agg_uncert = float(sum(uncerts) / len(uncerts)) if uncerts else 0.5\n            agg_gate = float(sum(gates) / len(gates)) if gates else 0.0\n            agg_span = float(sum(spans) / len(spans)) if spans else 0.0\n\n            # Build evidence tokens (context window)\n            context_window = 2\n            start_idx = max(0, token_idx - context_window)\n            end_idx = min(len(tokens), token_idx + context_window + 1)\n            evidence_tokens = []\n            for i in range(start_idx, end_idx):\n                if i == token_idx or i >= len(tokens):\n                    continue\n                rtok = tokens[i]\n                clean_token = str(rtok).replace(\"▁\", \"\").replace(\"</w>\", \"\").strip()\n                # require word-start or token_word_map entry\n                if not _is_word_start(rtok, token_word_map, i):\n                    if token_word_map is None and len(clean_token) >= 2:\n                        pass\n                    else:\n                        continue\n                # validity\n                if _has_is_valid_token:\n                    try:\n                        ok = is_valid_token(rtok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        ok = rtok not in self.special_tokens and len(clean_token) > 1\n                else:\n                    ok = rtok not in self.special_tokens and len(clean_token) > 1\n                if not ok:\n                    continue\n                # prefer normalized/orig mapping when available\n                chosen = None\n                if token_word_map and isinstance(token_word_map, dict):\n                    # try norm -> orig\n                    try:\n                        if isinstance(token_word_map.get(\"norm\", None), dict) and i in token_word_map[\"norm\"]:\n                            chosen = token_word_map[\"norm\"][i]\n                        elif isinstance(token_word_map.get(\"orig\", None), dict) and i in token_word_map[\"orig\"]:\n                            chosen = token_word_map[\"orig\"][i]\n                        elif i in token_word_map:\n                            chosen = token_word_map[i]\n                    except Exception:\n                        chosen = None\n                if chosen and isinstance(chosen, str) and chosen.strip():\n                    evidence_tokens.append(chosen.strip())\n                else:\n                    evidence_tokens.append(clean_token)\n\n            # dedupe & trim\n            seen = set()\n            dedup = []\n            for t in evidence_tokens:\n                if t not in seen:\n                    seen.add(t)\n                    dedup.append(t)\n            evidence_tokens = dedup[:_TRG_EVIDENCE_K]\n\n            # compute top senses\n            top_senses = self._compute_sense_alternatives_fast(agg_proto)\n            chosen_sense = top_senses[0] if len(top_senses) > 0 else (\"unknown\", 0.5)\n            alternatives = top_senses[1:3] if len(top_senses) > 1 else []\n\n            # token value prefer normalized form if available\n            token_value = raw_token\n            try:\n                if token_word_map and isinstance(token_word_map, dict):\n                    if isinstance(token_word_map.get(\"norm\", None), dict) and token_idx in token_word_map[\"norm\"]:\n                        token_value = token_word_map[\"norm\"][token_idx]\n                    elif isinstance(token_word_map.get(\"orig\", None), dict) and token_idx in token_word_map[\"orig\"]:\n                        token_value = token_word_map[\"orig\"][token_idx]\n                    elif token_idx in token_word_map:\n                        token_value = token_word_map[token_idx]\n                # fall back to normalize_fn if available\n                if (_normalize_fn is not None) and (isinstance(token_value, str) and token_value.strip()):\n                    token_value = _normalize_fn(token_value)\n            except Exception:\n                pass\n\n            return {\n                \"token\": token_value,\n                \"token_idx\": token_idx,\n                \"evidence_tokens\": evidence_tokens,\n                \"chosen_sense\": chosen_sense,\n                \"alternatives\": alternatives,\n                \"uncertainty\": float(agg_uncert),\n                \"gate\": float(agg_gate),\n                \"span\": float(agg_span),\n                \"proto_probs\": agg_proto,\n            }\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(f\"[TRG] evidence extraction error at token {token_idx}: {_tb.format_exc().splitlines()[-1]}\")\n            return self._create_fallback_evidence(token_idx, tokens)\n\n    # -------------------------\n    # SAFE EXTRACTORS (robust shapes)\n    # -------------------------\n    def _safe_extract_proto_probs(self, token_idx: int, dscd_outputs: Dict) -> torch.Tensor:\n        \"\"\"\n        Robust extraction of prototype probabilities for a single token.\n        Returns 1D torch.Tensor.\n        \"\"\"\n        try:\n            pp_all = dscd_outputs.get(\"proto_probs\", None) if isinstance(dscd_outputs, dict) else None\n            if pp_all is None:\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            # Torch tensor cases\n            if isinstance(pp_all, torch.Tensor):\n                p = pp_all.detach().cpu()\n                if p.dim() == 3:\n                    # B, T, K -> choose batch 0 if present\n                    B, T, K = p.shape\n                    if token_idx < T:\n                        return p[0, token_idx, :].float()\n                    else:\n                        return p[0].max(dim=1)[0].float()\n                elif p.dim() == 2:\n                    # T,K or B,T (ambiguous). Heuristics:\n                    if p.shape[0] > 1 and token_idx < p.size(0):\n                        return p[token_idx, :].float()\n                    elif token_idx < p.size(0):\n                        return p[token_idx, :].float()\n                    else:\n                        return p.max(dim=1)[0].float()\n                elif p.dim() == 1:\n                    return p.float()\n                else:\n                    return torch.tensor([1.0], dtype=torch.float32)\n\n            # numpy\n            if isinstance(pp_all, np.ndarray):\n                arr = pp_all\n                if arr.ndim == 3:\n                    if token_idx < arr.shape[1]:\n                        return torch.from_numpy(arr[0, token_idx, :].astype(np.float32))\n                elif arr.ndim == 2:\n                    if token_idx < arr.shape[0]:\n                        return torch.from_numpy(arr[token_idx].astype(np.float32))\n                elif arr.ndim == 1:\n                    return torch.from_numpy(arr.astype(np.float32))\n                return torch.tensor([1.0], dtype=torch.float32)\n\n            # list/tuple: many shapes possible\n            if isinstance(pp_all, (list, tuple)):\n                # If it's a batch list\n                if len(pp_all) > 0 and isinstance(pp_all[0], (list, tuple, np.ndarray, torch.Tensor)):\n                    first = pp_all[0]\n                    if isinstance(first, torch.Tensor):\n                        row = first.detach().cpu()\n                        if row.dim() == 2 and token_idx < row.size(0):\n                            return row[token_idx, :].float()\n                        elif row.dim() == 1:\n                            return row.float()\n                    elif isinstance(first, np.ndarray):\n                        if first.ndim >= 1 and token_idx < first.shape[0]:\n                            return torch.from_numpy(first[token_idx].astype(np.float32))\n                    elif isinstance(first, (list, tuple)):\n                        # assume per-token arrays in first\n                        if token_idx < len(first):\n                            val = first[token_idx]\n                            return torch.as_tensor(np.asarray(val, dtype=np.float32), dtype=torch.float32)\n                    # fallback: if outer list length matches token count, use that\n                    if token_idx < len(pp_all):\n                        val = pp_all[token_idx]\n                        if isinstance(val, torch.Tensor):\n                            return val.detach().cpu().float()\n                        else:\n                            return torch.as_tensor(np.asarray(val, dtype=np.float32), dtype=torch.float32)\n                # otherwise try flattening first element\n                if token_idx < len(pp_all):\n                    val = pp_all[token_idx]\n                    if isinstance(val, torch.Tensor):\n                        return val.detach().cpu().float()\n                    else:\n                        return torch.as_tensor(np.asarray(val, dtype=np.float32), dtype=torch.float32)\n\n            # unknown -> fallback\n            return torch.tensor([1.0], dtype=torch.float32)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(\"[TRG] _safe_extract_proto_probs failed:\", _tb.format_exc().splitlines()[-1])\n            return torch.tensor([1.0], dtype=torch.float32)\n\n    def _safe_extract_uncertainty(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            U_all = dscd_outputs.get(\"uncertainties\", None) if isinstance(dscd_outputs, dict) else None\n            if U_all is None:\n                return 0.5\n            # tensor\n            if isinstance(U_all, torch.Tensor):\n                u = U_all.detach().cpu()\n                if u.dim() == 2:\n                    if token_idx < u.size(1):\n                        return float(u[0, token_idx].item())\n                    elif token_idx < u.size(0):\n                        return float(u[token_idx].item())\n                elif u.dim() == 1:\n                    if token_idx < u.size(0):\n                        return float(u[token_idx].item())\n                return 0.5\n            if isinstance(U_all, np.ndarray):\n                if U_all.ndim >= 1 and token_idx < U_all.shape[0]:\n                    return float(U_all[token_idx])\n                return 0.5\n            if isinstance(U_all, (list, tuple)):\n                # try batch->row\n                first = U_all[0] if len(U_all) > 0 else None\n                if isinstance(first, (list, tuple, np.ndarray, torch.Tensor)):\n                    row = first\n                    if isinstance(row, torch.Tensor):\n                        if row.dim() >= 1 and token_idx < row.size(0):\n                            return float(row[token_idx].item())\n                    elif isinstance(row, (list, tuple, np.ndarray)):\n                        if token_idx < len(row):\n                            return float(row[token_idx])\n                if token_idx < len(U_all):\n                    v = U_all[token_idx]\n                    return float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n            return 0.5\n        except Exception:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(\"[TRG] _safe_extract_uncertainty failed:\", _tb.format_exc().splitlines()[-1])\n            return 0.5\n\n    def _safe_extract_gate(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            G_all = dscd_outputs.get(\"gates\", None) if isinstance(dscd_outputs, dict) else None\n            if G_all is None:\n                return 0.0\n            if isinstance(G_all, torch.Tensor):\n                g = G_all.detach().cpu()\n                if g.dim() == 2:\n                    if token_idx < g.size(1):\n                        return float(g[0, token_idx].item())\n                    elif token_idx < g.size(0):\n                        return float(g[token_idx].item())\n                elif g.dim() == 1:\n                    if token_idx < g.size(0):\n                        return float(g[token_idx].item())\n                return 0.0\n            if isinstance(G_all, np.ndarray):\n                if G_all.ndim >= 1 and token_idx < G_all.shape[0]:\n                    return float(G_all[token_idx])\n                return 0.0\n            if isinstance(G_all, (list, tuple)):\n                first = G_all[0] if len(G_all) > 0 else None\n                if isinstance(first, (list, tuple, np.ndarray, torch.Tensor)):\n                    row = first\n                    if isinstance(row, torch.Tensor):\n                        if row.dim() >= 1 and token_idx < row.size(0):\n                            return float(row[token_idx].item())\n                    elif isinstance(row, (list, tuple, np.ndarray)):\n                        if token_idx < len(row):\n                            return float(row[token_idx])\n                if token_idx < len(G_all):\n                    v = G_all[token_idx]\n                    return float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n            return 0.0\n        except Exception:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(\"[TRG] _safe_extract_gate failed:\", _tb.format_exc().splitlines()[-1])\n            return 0.0\n\n    def _safe_extract_span(self, token_idx: int, dscd_outputs: Dict) -> float:\n        try:\n            S_all = dscd_outputs.get(\"span_preds\", None) if isinstance(dscd_outputs, dict) else None\n            if S_all is None:\n                return 0.0\n            if isinstance(S_all, torch.Tensor):\n                s = S_all.detach().cpu()\n                if s.dim() == 2:\n                    if token_idx < s.size(1):\n                        return float(s[0, token_idx].item())\n                    elif token_idx < s.size(0):\n                        return float(s[token_idx].item())\n                elif s.dim() == 1 and token_idx < s.size(0):\n                    return float(s[token_idx].item())\n                return 0.0\n            if isinstance(S_all, np.ndarray):\n                if S_all.ndim >= 1 and token_idx < S_all.shape[0]:\n                    return float(S_all[token_idx])\n                return 0.0\n            if isinstance(S_all, (list, tuple)):\n                first = S_all[0] if len(S_all) > 0 else None\n                if isinstance(first, torch.Tensor):\n                    row = first\n                    if row.dim() >= 1 and token_idx < row.size(0):\n                        return float(row[token_idx].item())\n                elif isinstance(first, (list, tuple, np.ndarray)):\n                    row = first\n                    if token_idx < len(row):\n                        return float(row[token_idx])\n                if token_idx < len(S_all):\n                    v = S_all[token_idx]\n                    return float(v.item()) if isinstance(v, torch.Tensor) else float(v)\n            return 0.0\n        except Exception:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(\"[TRG] _safe_extract_span failed:\", _tb.format_exc().splitlines()[-1])\n            return 0.0\n\n    # -------------------------\n    # SENSE / UTIL\n    # -------------------------\n    def compute_span(self, sense_probs: Any) -> float:\n        try:\n            if isinstance(sense_probs, dict):\n                probs = list(sense_probs.values())\n            else:\n                probs = sense_probs\n\n            if isinstance(probs, torch.Tensor):\n                probs = probs.detach().cpu().numpy().flatten().tolist()\n            elif isinstance(probs, np.ndarray):\n                probs = probs.flatten().tolist()\n            elif isinstance(probs, (list, tuple)):\n                probs = list(probs)\n            else:\n                return 0.0\n\n            if len(probs) < 2:\n                return 0.0\n            sorted_probs = sorted([float(x) for x in probs], reverse=True)\n            span = float(sorted_probs[0]) - float(sorted_probs[1])\n            return max(0.0, span)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[TRG] compute_span error\")\n            return 0.0\n\n    def _compute_sense_alternatives_fast(self, proto_probs: torch.Tensor) -> List[Tuple[str, float]]:\n        try:\n            if not isinstance(proto_probs, torch.Tensor):\n                proto_probs = torch.as_tensor(np.asarray(proto_probs, dtype=np.float32))\n            probs = proto_probs.flatten()\n            if probs.numel() > 1:\n                probs_sorted, indices = torch.sort(probs, descending=True)\n                top_k = min(3, int(indices.numel()))\n                return [(f\"sense_{int(indices[i].item())}\", float(probs_sorted[i].item())) for i in range(top_k)]\n            else:\n                return [(\"sense_0\", float(probs[0].item()))]\n        except Exception:\n            return [(\"unknown\", 0.5)]\n\n    def _create_fallback_evidence(self, token_idx: int, tokens: List[str]) -> Dict:\n        token = tokens[token_idx] if isinstance(tokens, list) and 0 <= token_idx < len(tokens) else \"UNK\"\n        return {\n            \"token\": token,\n            \"token_idx\": token_idx,\n            \"evidence_tokens\": [],\n            \"chosen_sense\": (\"unknown\", 0.5),\n            \"alternatives\": [],\n            \"uncertainty\": 0.5,\n            \"gate\": 0.0,\n            \"span\": 0.0,\n            \"proto_probs\": torch.tensor([1.0], dtype=torch.float32),\n        }\n\n\nclass CompleteTRGWithExplanations(nn.Module):\n    \"\"\"\n    Inference-only disambiguation and explanation component.\n    \"\"\"\n\n    def __init__(self, embed_dim: Optional[int] = None, tokenizer=None, language: str = \"bn\"):\n        super().__init__()\n        self.embed_dim = int(embed_dim) if embed_dim is not None else int(_TRG_GEN_EMBED)\n        self.tokenizer = tokenizer\n        self.language = language\n\n        # Cache special tokens robustly\n        if tokenizer is not None:\n            try:\n                if _has_get_tokenizer_special_tokens:\n                    self.special_tokens = globals()[\"get_tokenizer_special_tokens\"](tokenizer)\n                elif _has_get_cached_special_tokens:\n                    self.special_tokens = globals()[\"get_cached_special_tokens\"](tokenizer)\n                else:\n                    self.special_tokens = set(getattr(tokenizer, \"all_special_tokens\", []))\n            except Exception:\n                self.special_tokens = set()\n        else:\n            self.special_tokens = set()\n\n        self.template_system = ComprehensiveTRGExplanationTemplate()\n        self.evidence_extractor = MemoryEfficientTRGExtractor(tokenizer, language=language)\n\n        self.silver_buffer = deque(maxlen=int(_MAX_SILVER_BUFFER))\n        self.stats = {\n            \"explanations_generated\": 0,\n            \"high_confidence_explanations\": 0,\n            \"low_confidence_explanations\": 0,\n        }\n\n        if _VERBOSE_LOGGING:\n            print(\"[TRG] system initialized (inference-only, multi-GPU compatible)\")\n\n    def generate_explanation_for_token(\n        self,\n        token_idx: int,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n    ) -> Tuple[str, Dict]:\n        \"\"\"Generate an explanation string and its evidence for a single token.\"\"\"\n        # Feature flag must be enabled\n        if not _ENABLE_TRG_INFERENCE:\n            return \"\", {}\n\n        if not isinstance(tokens, list) or token_idx < 0 or token_idx >= len(tokens):\n            return \"\", {}\n\n        raw_token = tokens[token_idx]\n        if _has_is_valid_token:\n            try:\n                is_valid = is_valid_token(raw_token, self.special_tokens, self.tokenizer, language=self.language)\n            except Exception:\n                is_valid = raw_token not in self.special_tokens and len(str(raw_token)) >= 2\n        else:\n            is_valid = raw_token not in self.special_tokens and len(str(raw_token)) >= 2\n\n        if not is_valid:\n            return \"\", {}\n\n        try:\n            evidence = self.evidence_extractor.extract_evidence_efficiently(token_idx, tokens, dscd_outputs, token_word_map=token_word_map)\n            if not evidence:\n                return \"\", {}\n            explanation_text = self.template_system.generate_explanation(evidence)\n            self._update_stats(evidence)\n            self._add_to_silver_buffer(evidence, explanation_text, tokens)\n            return explanation_text, evidence\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(f\"[TRG] generate_explanation error at token {token_idx}: {_tb.format_exc().splitlines()[-1]}\")\n            return \"\", {}\n\n    def process_sentence_for_explanations(\n        self,\n        tokens: List[str],\n        dscd_outputs: Dict,\n        token_word_map: Optional[dict] = None,\n        uncertainty_threshold: Optional[float] = None,\n        top_k: int = 3,\n    ) -> List[Dict]:\n        \"\"\"Select up to top_k tokens and generate explanations for them.\"\"\"\n        if not _ENABLE_TRG_INFERENCE:\n            return []\n\n        if uncertainty_threshold is None:\n            uncertainty_threshold = float(_TRG_UNCERTAINTY_THRESHOLD)\n\n        strict_uncertainty = max(0.40, uncertainty_threshold)\n\n        explanations: List[Dict] = []\n        try:\n            if not tokens or not isinstance(dscd_outputs, dict):\n                return explanations\n\n            U_all = dscd_outputs.get(\"uncertainties\", [])\n            S_all = dscd_outputs.get(\"span_preds\", [])\n\n            # Normalize U and S to python lists safely\n            def _to_list_safe(x):\n                if isinstance(x, torch.Tensor):\n                    x = x.detach().cpu()\n                    if x.dim() == 2:\n                        return [float(v) for v in x[0].tolist()]\n                    elif x.dim() == 1:\n                        return [float(v) for v in x.tolist()]\n                    else:\n                        return []\n                if isinstance(x, (list, tuple, np.ndarray)):\n                    out = []\n                    for v in x:\n                        if isinstance(v, torch.Tensor):\n                            out.append(float(v.item()))\n                        else:\n                            try:\n                                out.append(float(v))\n                            except Exception:\n                                out.append(0.0)\n                    return out\n                return []\n\n            U = _to_list_safe(U_all[0]) if (isinstance(U_all, (list, tuple)) and len(U_all) > 0 and (isinstance(U_all[0], (list, tuple, torch.Tensor, np.ndarray)))) else _to_list_safe(U_all)\n            S = _to_list_safe(S_all[0]) if (isinstance(S_all, (list, tuple)) and len(S_all) > 0 and (isinstance(S_all[0], (list, tuple, torch.Tensor, np.ndarray)))) else _to_list_safe(S_all)\n            if not U:\n                return explanations\n\n            # Collect candidates\n            candidates: List[Tuple[int, float, float]] = []\n            for idx in range(min(len(tokens), len(U))):\n                tok = tokens[idx]\n\n                if not _is_word_start(tok, token_word_map, idx):\n                    if token_word_map is None:\n                        if not isinstance(tok, str) or len(tok.replace(\"▁\", \"\").replace(\"Ġ\", \"\")) < 2:\n                            continue\n                    else:\n                        continue\n\n                if _has_is_valid_token:\n                    try:\n                        valid = is_valid_token(tok, self.special_tokens, self.tokenizer, language=self.language)\n                    except Exception:\n                        valid = tok not in self.special_tokens and len(str(tok)) >= 2\n                else:\n                    valid = tok not in self.special_tokens and len(str(tok)) >= 2\n                if not valid:\n                    continue\n\n                u = float(U[idx]) if idx < len(U) else 0.5\n                s = float(S[idx]) if idx < len(S) else 0.0\n\n                probs = self.evidence_extractor._safe_extract_proto_probs(idx, dscd_outputs)\n                has_multi_sense = isinstance(probs, torch.Tensor) and probs.numel() >= 2\n\n                if not (has_multi_sense or (s > 0.3) or (u > strict_uncertainty)):\n                    continue\n\n                candidates.append((idx, u, s))\n\n            if not candidates:\n                return explanations\n\n            # Prioritize and select\n            span_first = [c for c in candidates if c[2] > 0.3]\n            span_first.sort(key=lambda t: (t[2], t[1]), reverse=True)\n\n            uncertain = [c for c in candidates if c[1] > strict_uncertainty]\n            uncertain.sort(key=lambda t: t[1], reverse=True)\n\n            selected = []\n            selected.extend(span_first)\n            for t in uncertain:\n                if t not in selected:\n                    selected.append(t)\n                if len(selected) >= top_k:\n                    break\n\n            if not selected and candidates:\n                candidates.sort(key=lambda t: (t[2], t[1]), reverse=True)\n                selected = candidates[:max(1, top_k)]\n\n            for (token_idx, u, s) in selected[:top_k]:\n                try:\n                    explanation_text, evidence = self.generate_explanation_for_token(token_idx, tokens, dscd_outputs, token_word_map=token_word_map)\n                    if explanation_text and evidence:\n                        token_label = token_word_map[token_idx] if (token_word_map and isinstance(token_word_map, dict) and token_idx in token_word_map) else tokens[token_idx].replace(\"▁\", \"\")\n                        explanations.append({\n                            \"token_idx\": token_idx,\n                            \"token\": token_label,\n                            \"explanation\": explanation_text,\n                            \"uncertainty\": u,\n                            \"span\": s,\n                        })\n                except Exception as e:\n                    if _VERBOSE_LOGGING:\n                        import traceback as _tb\n                        print(f\"[TRG] explanation generation failure @ idx {token_idx}: {_tb.format_exc().splitlines()[-1]}\")\n                    continue\n\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                import traceback as _tb\n                print(f\"[TRG] sentence processing error: {_tb.format_exc().splitlines()[-1]}\")\n\n        return explanations\n\n    def _update_stats(self, evidence: Dict):\n        try:\n            self.stats[\"explanations_generated\"] += 1\n            confidence = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                try:\n                    confidence = float(chosen[1])\n                except Exception:\n                    confidence = 0.5\n\n            if confidence >= 0.65:\n                self.stats[\"high_confidence_explanations\"] += 1\n            elif confidence < 0.4:\n                self.stats[\"low_confidence_explanations\"] += 1\n        except Exception:\n            pass\n\n    def _add_to_silver_buffer(self, evidence: Dict, explanation: str, tokens: List[str]):\n        try:\n            conf = 0.5\n            chosen = evidence.get(\"chosen_sense\")\n            if isinstance(chosen, (tuple, list)) and len(chosen) >= 2:\n                conf = float(chosen[1])\n            entry = {\n                \"token\": str(evidence.get(\"token\", \"UNK\"))[:20],\n                \"explanation\": str(explanation)[:150],\n                \"confidence\": conf,\n            }\n            self.silver_buffer.append(entry)\n        except Exception:\n            pass\n\n    def get_statistics(self) -> Dict:\n        total = max(self.stats.get(\"explanations_generated\", 0), 1)\n        return {\n            **self.stats,\n            \"high_confidence_rate\": self.stats.get(\"high_confidence_explanations\", 0) / total,\n            \"low_confidence_rate\": self.stats.get(\"low_confidence_explanations\", 0) / total,\n            \"silver_buffer_size\": len(self.silver_buffer),\n        }\n\n\nprint(\"✅ Cell 5 (patched): TRG explanation system ready (robust aggregation & safe proto handling)\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:07.253423Z","iopub.execute_input":"2025-11-22T14:56:07.253647Z","iopub.status.idle":"2025-11-22T14:56:07.497421Z","shell.execute_reply.started":"2025-11-22T14:56:07.253632Z","shell.execute_reply":"2025-11-22T14:56:07.496768Z"},"id":"svk-wKO7H4J3","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 5 (patched): TRG explanation system ready (robust aggregation & safe proto handling)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6 (fixed): ⚡ OPTIMIZED TATN MODEL WITH GRADIENT CHECKPOINTING (M2M100 418M)\n# - Thorough line-by-line hardening and practical fixes\n# - Integrates bn_normalizer usage for normalized token_word_map\n# - Ensures TRG instance is placed into eval() mode by default for inference\n# - Makes DSCD/ASBN/TRG instantiation robust and non-fatal\n# - Device-safe handling of all intermediate tensors and shapes\n# ==============================================================================\nfrom typing import List, Dict, Optional, Any\nimport traceback\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Attempt to import transformers model class (best-effort).\ntry:\n    from transformers import M2M100ForConditionalGeneration\n    from transformers.modeling_outputs import BaseModelOutput\n    _HAS_TRANSFORMERS = True\nexcept Exception:\n    M2M100ForConditionalGeneration = None\n    BaseModelOutput = None\n    _HAS_TRANSFORMERS = False\n\n# -----------------------------------------------------------------------------\n# Defensive global fallback helpers\n# -----------------------------------------------------------------------------\ndef _get_int_global(name: str, default: int) -> int:\n    try:\n        v = globals().get(name, default)\n        return int(v) if v is not None else default\n    except Exception:\n        return default\n\ndef _get_float_global(name: str, default: float) -> float:\n    try:\n        v = globals().get(name, default)\n        return float(v) if v is not None else default\n    except Exception:\n        return default\n\ndef _get_bool_global(name: str, default: bool) -> bool:\n    try:\n        v = globals().get(name, default)\n        return bool(v)\n    except Exception:\n        return default\n\n# Read globals safely (Cell 0 may not have run)\n_DSCD_BUFFER_SIZE = _get_int_global('DSCD_BUFFER_SIZE', 20)\n_DSCD_MAX_PROTOS = _get_int_global('DSCD_MAX_PROTOS', 8)\n_DSCD_N_MIN = _get_int_global('DSCD_N_MIN', 5)\n_DSCD_DISPERSION_THRESHOLD = _get_float_global('DSCD_DISPERSION_THRESHOLD', 0.25)\n_SOURCE_LANGUAGE = globals().get('SOURCE_LANGUAGE', 'bn')\n_ENABLE_ASBN_TRAINING = _get_bool_global('ENABLE_ASBN_TRAINING', True)\n_ENABLE_TRG_INFERENCE = _get_bool_global('ENABLE_TRG_INFERENCE', True)\n_MEMORY_CLEANUP_FREQUENCY = _get_int_global('MEMORY_CLEANUP_FREQUENCY', 100)\n_NUM_GPUS = _get_int_global('NUM_GPUS', torch.cuda.device_count() if torch.cuda.is_available() else 0)\n_USE_GC = _get_bool_global('GRADIENT_CHECKPOINTING', False)\n_DSCD_ENABLE_TRAINING_CLUSTERING = _get_bool_global('DSCD_ENABLE_TRAINING_CLUSTERING', False)\n_LAMBDA_ASBN = _get_float_global('LAMBDA_ASBN', 0.10)\n_LAMBDA_DSCD = _get_float_global('LAMBDA_DSCD', 0.05)\n_VERBOSE_LOGGING = _get_bool_global('VERBOSE_LOGGING', False)\n\n_has_reconstruct_word_spans = 'reconstruct_word_spans' in globals()\n_normalize_fn = globals().get(\"normalize_bn_word\", None)  # bn_normalizer cell (may be present)\n\n# -----------------------------------------------------------------------------\n# Safe helper to obtain last hidden state from various HF encoder outputs\n# -----------------------------------------------------------------------------\ndef _safe_get_last_hidden_state(enc_output: Any) -> Optional[torch.Tensor]:\n    try:\n        if enc_output is None:\n            return None\n        # HuggingFace BaseModelOutput\n        if hasattr(enc_output, 'last_hidden_state'):\n            return enc_output.last_hidden_state\n        # tuple/list like (last_hidden_state, ...)\n        if isinstance(enc_output, (list, tuple)) and len(enc_output) > 0:\n            cand = enc_output[0]\n            if isinstance(cand, torch.Tensor):\n                return cand\n        # dict-like\n        if isinstance(enc_output, dict) and 'last_hidden_state' in enc_output:\n            return enc_output['last_hidden_state']\n    except Exception:\n        if _VERBOSE_LOGGING:\n            print(\"[TATN] _safe_get_last_hidden_state error:\", traceback.format_exc().splitlines()[-1])\n    return None\n\n# -----------------------------------------------------------------------------\n# Normalize DSCD outputs into canonical, CPU/device-consistent structures\n# -----------------------------------------------------------------------------\ndef _normalize_dscd_outputs(raw: Dict[str, Any],\n                            batch_size: int,\n                            seq_len: int,\n                            device: torch.device,\n                            embed_dim: int) -> Dict[str, Any]:\n    \"\"\"\n    Defensive normalization of DSCD raw outputs into canonical forms:\n      - proto_probs: List[List[Tensor]] indexed [B][T] (each entry 1D tensor)\n      - uncertainties/gates/span_preds: List[List[Tensor]] [B][T] (scalars as 0-d/1-d tensors)\n      - proto_assignments: List[Tensor] length B each [T] (long)\n      - h_augmented: Tensor [B, T, H] or zeros fallback\n    This function never raises; logs only when VERBOSE_LOGGING=True.\n    \"\"\"\n    def _log(msg: str):\n        if _VERBOSE_LOGGING:\n            print(\"[DSCD-NORM]\", msg)\n\n    # defaults\n    proto_probs = [[torch.tensor([1.0], dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    uncertainties = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    gates = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    span_preds = [[torch.tensor(0.0, dtype=torch.float32, device=device) for _ in range(seq_len)] for _ in range(batch_size)]\n    proto_assignments = [torch.zeros(seq_len, dtype=torch.long, device=device) for _ in range(batch_size)]\n    h_aug = None\n\n    try:\n        if not isinstance(raw, dict):\n            _log(\"raw DSCD output not a dict; using defaults\")\n            raw = {} if raw is None else dict(raw)\n\n        # --- h_augmented\n        h_raw = raw.get('h_augmented', None)\n        if isinstance(h_raw, torch.Tensor):\n            try:\n                if h_raw.dim() == 3 and int(h_raw.size(0)) == batch_size and int(h_raw.size(1)) == seq_len:\n                    h_aug = h_raw.to(device)\n                else:\n                    # coerce as much as possible\n                    tmp = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=h_raw.dtype)\n                    max_b = min(batch_size, int(h_raw.size(0)))\n                    for b in range(max_b):\n                        row = h_raw[b]\n                        if isinstance(row, torch.Tensor) and row.dim() >= 2:\n                            L = min(seq_len, int(row.size(0)))\n                            tmp[b, :L] = row[:L].to(device)\n                    h_aug = tmp\n            except Exception:\n                _log(\"h_aug coercion from tensor failed; fallback to None\")\n                h_aug = None\n        elif isinstance(h_raw, (list, tuple, np.ndarray)):\n            try:\n                stacked = []\n                for b in range(min(batch_size, len(h_raw))):\n                    row = h_raw[b]\n                    if isinstance(row, torch.Tensor):\n                        stacked.append(row.to(device))\n                    else:\n                        stacked.append(torch.as_tensor(row, device=device))\n                if stacked:\n                    tensor = torch.stack(stacked, dim=0)\n                    if tensor.dim() == 3:\n                        tmp = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=tensor.dtype)\n                        for b in range(min(batch_size, tensor.size(0))):\n                            L = min(seq_len, int(tensor.size(1)))\n                            tmp[b, :L] = tensor[b, :L]\n                        h_aug = tmp\n            except Exception:\n                _log(\"h_aug list coercion failed; fallback to None\")\n                h_aug = None\n\n        # --- proto_probs\n        try:\n            pp = raw.get('proto_probs', None)\n            if pp is not None:\n                def _to_tensor(v):\n                    try:\n                        if isinstance(v, torch.Tensor):\n                            return v.detach().cpu().float()\n                        else:\n                            a = np.asarray(v, dtype=np.float32)\n                            return torch.from_numpy(a).cpu().float()\n                    except Exception:\n                        return torch.tensor([1.0], dtype=torch.float32)\n\n                if isinstance(pp, torch.Tensor):\n                    p = pp.detach().cpu()\n                    if p.dim() == 3:\n                        B, T, K = p.shape\n                        for b in range(min(batch_size, int(B))):\n                            for t in range(min(seq_len, int(T))):\n                                proto_probs[b][t] = _to_tensor(p[b, t])\n                    elif p.dim() == 2:\n                        # either [B, T] or [T, K]\n                        if int(p.size(0)) == batch_size:\n                            for b in range(batch_size):\n                                for t in range(min(seq_len, int(p.size(1)))):\n                                    proto_probs[b][t] = _to_tensor(p[b, t])\n                        elif batch_size == 1:\n                            for t in range(min(seq_len, int(p.size(0)))):\n                                proto_probs[0][t] = _to_tensor(p[t])\n                    elif p.dim() == 1:\n                        for t in range(min(seq_len, int(p.size(0)))):\n                            proto_probs[0][t] = _to_tensor(p[t])\n                elif isinstance(pp, (list, tuple)):\n                    if len(pp) == batch_size:\n                        for b in range(batch_size):\n                            row = pp[b]\n                            if isinstance(row, torch.Tensor):\n                                r = row.detach().cpu()\n                                if r.dim() == 2:\n                                    for t in range(min(seq_len, int(r.size(0)))):\n                                        proto_probs[b][t] = _to_tensor(r[t])\n                                elif r.dim() == 1:\n                                    for t in range(min(seq_len, int(r.size(0)))):\n                                        proto_probs[b][t] = _to_tensor(r[t])\n                            else:\n                                for t in range(min(seq_len, len(row))):\n                                    proto_probs[b][t] = _to_tensor(row[t])\n                    elif batch_size == 1:\n                        for t in range(min(seq_len, len(pp))):\n                            proto_probs[0][t] = _to_tensor(pp[t])\n                    else:\n                        for t in range(min(seq_len, len(pp))):\n                            proto_probs[0][t] = _to_tensor(pp[t])\n        except Exception as e:\n            _log(f\"proto_probs parsing failed: {e}\")\n\n        # --- uncertainties/gates/span_preds normalization helper\n        def _normalize_scalar_matrix(key: str, target):\n            try:\n                val = raw.get(key, None)\n                if val is None:\n                    return\n                if isinstance(val, torch.Tensor):\n                    m = val.detach().cpu()\n                    if m.dim() == 3 and int(m.size(0)) == batch_size:\n                        for b in range(batch_size):\n                            for t in range(min(seq_len, int(m.size(1)))):\n                                target[b][t] = torch.tensor(float(m[b, t].item()), device=device)\n                    elif m.dim() == 2:\n                        if int(m.size(0)) == batch_size:\n                            for b in range(batch_size):\n                                for t in range(min(seq_len, int(m.size(1)))):\n                                    target[b][t] = torch.tensor(float(m[b, t].item()), device=device)\n                        elif batch_size == 1:\n                            for t in range(min(seq_len, int(m.size(0)))):\n                                target[0][t] = torch.tensor(float(m[t].item()), device=device)\n                    elif m.dim() == 1 and batch_size == 1:\n                        for t in range(min(seq_len, int(m.size(0)))):\n                            target[0][t] = torch.tensor(float(m[t].item()), device=device)\n                elif isinstance(val, (list, tuple, np.ndarray)):\n                    if len(val) == batch_size:\n                        for b in range(batch_size):\n                            row = val[b]\n                            if isinstance(row, torch.Tensor):\n                                r = row.detach().cpu()\n                                for t in range(min(seq_len, int(r.size(0)))):\n                                    target[b][t] = torch.tensor(float(r[t].item()), device=device)\n                            else:\n                                for t in range(min(seq_len, len(row))):\n                                    try:\n                                        target[b][t] = torch.tensor(float(row[t]), device=device)\n                                    except Exception:\n                                        pass\n                    elif batch_size == 1:\n                        row = val\n                        for t in range(min(seq_len, len(row))):\n                            try:\n                                target[0][t] = torch.tensor(float(row[t]), device=device)\n                            except Exception:\n                                pass\n            except Exception as e:\n                _log(f\"{key} normalization failed: {e}\")\n\n        _normalize_scalar_matrix('uncertainties', uncertainties)\n        _normalize_scalar_matrix('gates', gates)\n        _normalize_scalar_matrix('span_preds', span_preds)\n\n        # --- proto_assignments normalization\n        try:\n            pa = raw.get('proto_assignments', None)\n            if pa is not None:\n                if isinstance(pa, list) and len(pa) == batch_size:\n                    for b in range(batch_size):\n                        row = pa[b]\n                        try:\n                            if isinstance(row, torch.Tensor):\n                                arr = row.detach().cpu().long().view(-1)\n                                if arr.numel() < seq_len:\n                                    pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                                    proto_assignments[b] = torch.cat([arr.to(device), pad], dim=0)\n                                else:\n                                    proto_assignments[b] = arr[:seq_len].to(device)\n                            else:\n                                arr = torch.as_tensor(row, dtype=torch.long, device=device).view(-1)\n                                if arr.numel() < seq_len:\n                                    pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                                    proto_assignments[b] = torch.cat([arr, pad], dim=0)\n                                else:\n                                    proto_assignments[b] = arr[:seq_len]\n                        except Exception:\n                            proto_assignments[b] = torch.zeros(seq_len, dtype=torch.long, device=device)\n                elif isinstance(pa, torch.Tensor):\n                    p = pa.detach().cpu().long()\n                    if p.dim() == 2 and int(p.size(0)) == batch_size:\n                        for b in range(batch_size):\n                            arr = p[b].view(-1)\n                            if arr.numel() < seq_len:\n                                pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                                proto_assignments[b] = torch.cat([arr.to(device), pad], dim=0)\n                            else:\n                                proto_assignments[b] = arr[:seq_len].to(device)\n                    elif p.dim() == 1 and batch_size == 1:\n                        arr = p.view(-1)\n                        if arr.numel() < seq_len:\n                            pad = torch.zeros(seq_len - arr.numel(), dtype=torch.long, device=device)\n                            proto_assignments[0] = torch.cat([arr.to(device), pad], dim=0)\n                        else:\n                            proto_assignments[0] = arr[:seq_len].to(device)\n        except Exception as e:\n            _log(f\"proto_assignments parse failed: {e}\")\n\n    except Exception as outer:\n        _log(f\"overall normalization failure: {outer}\")\n\n    # final fallback for h_aug\n    if h_aug is None:\n        h_aug = torch.zeros(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32)\n\n    return {\n        'proto_probs': proto_probs,\n        'uncertainties': uncertainties,\n        'gates': gates,\n        'span_preds': span_preds,\n        'proto_assignments': proto_assignments,\n        'h_augmented': h_aug\n    }\n\n# -----------------------------------------------------------------------------\n# Main model wrapper\n# -----------------------------------------------------------------------------\nclass MemoryOptimizedTATNWithExplanations(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.global_step = 0\n\n        # Attempt to load backbone model only if transformers available\n        self.mbart = None\n        if _HAS_TRANSFORMERS and M2M100ForConditionalGeneration is not None:\n            try:\n                if os.environ.get(\"SKIP_MODEL_LOAD\", \"0\") != \"1\":\n                    self.mbart = M2M100ForConditionalGeneration.from_pretrained(\n                        \"facebook/m2m100_418M\",\n                        torch_dtype=torch.float32,\n                        use_cache=False\n                    )\n                    try:\n                        self.mbart.config.use_cache = False\n                    except Exception:\n                        pass\n                    try:\n                        if _USE_GC and hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n                            self.mbart.gradient_checkpointing_enable()\n                    except Exception:\n                        pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] Could not load M2M100 model:\", traceback.format_exc().splitlines()[-1])\n                self.mbart = None\n        else:\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] transformers or model class missing; running without backbone\")\n\n        # embed_dim fallback\n        embed_dim = 512\n        try:\n            if self.mbart is not None:\n                embed_dim = int(getattr(self.mbart.config, \"d_model\", embed_dim))\n        except Exception:\n            pass\n\n        # Initialize DSCD (class must be defined in Cell 3)\n        DSC_CLASS = globals().get('MemoryEfficientDSCDOnline', None)\n        if callable(DSC_CLASS):\n            try:\n                self.dscd = DSC_CLASS(\n                    embed_dim=embed_dim,\n                    tokenizer=tokenizer,\n                    buffer_size=_DSCD_BUFFER_SIZE,\n                    max_protos=_DSCD_MAX_PROTOS,\n                    n_min=_DSCD_N_MIN,\n                    language=_SOURCE_LANGUAGE,\n                    dispersion_threshold=_DSCD_DISPERSION_THRESHOLD,\n                    enable_training_clustering=_DSCD_ENABLE_TRAINING_CLUSTERING,\n                    max_clustering_points=500,\n                    max_candidates_per_step=1\n                )\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] DSCD instantiation failed:\", traceback.format_exc().splitlines()[-1])\n                self.dscd = None\n        else:\n            self.dscd = None\n\n        # ASBN: instantiate if available (Cell 4)\n        ASBN_CLASS = globals().get('MemoryEfficientASBNModule', None)\n        if callable(ASBN_CLASS):\n            try:\n                self.asbn = ASBN_CLASS(embed_dim, tokenizer, language=_SOURCE_LANGUAGE)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] ASBN instantiation failed:\", traceback.format_exc().splitlines()[-1])\n                self.asbn = None\n        else:\n            self.asbn = None\n\n        # TRG system: instantiate if available (Cell 5)\n        TRG_CLASS = globals().get('CompleteTRGWithExplanations', None)\n        if callable(TRG_CLASS):\n            try:\n                self.trg_system = TRG_CLASS(embed_dim, tokenizer, language=_SOURCE_LANGUAGE)\n                # ENSURE TRG is in eval mode for inference so it doesn't early-return\n                try:\n                    self.trg_system.eval()\n                except Exception:\n                    try:\n                        self.trg_system.training = False\n                    except Exception:\n                        pass\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] TRG instantiation failed:\", traceback.format_exc().splitlines()[-1])\n                self.trg_system = None\n        else:\n            self.trg_system = None\n\n    # entropy regularizer helper\n    @staticmethod\n    def _entropy_reg_from_proto_probs_static(proto_probs_list, gates_list=None, min_gate=0.0):\n        dev = None\n        try:\n            if isinstance(proto_probs_list, list):\n                for row in proto_probs_list:\n                    if isinstance(row, list):\n                        for p in row:\n                            if isinstance(p, torch.Tensor):\n                                dev = p.device\n                                break\n                    if dev is not None:\n                        break\n        except Exception:\n            pass\n        if dev is None:\n            dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        total = torch.tensor(0.0, device=dev)\n        count = 0\n        try:\n            for b, row in enumerate(proto_probs_list or []):\n                if not isinstance(row, list):\n                    continue\n                gl = gates_list[b] if (gates_list and b < len(gates_list)) else None\n                for j, probs in enumerate(row):\n                    try:\n                        if not isinstance(probs, torch.Tensor) or probs.numel() == 0:\n                            continue\n                        if gl and j < len(gl):\n                            if float(gl[j]) < min_gate:\n                                continue\n                        p = torch.clamp(probs.to(dev), 1e-8, 1.0)\n                        H = -torch.sum(p * torch.log(p))\n                        total = total + H\n                        count += 1\n                    except Exception:\n                        continue\n        except Exception:\n            pass\n        if count == 0:\n            return torch.tensor(0.0, device=dev)\n        return total / count\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n        labels: Optional[torch.Tensor] = None,\n    ):\n        self.global_step += 1\n\n        if input_ids is None or attention_mask is None:\n            raise ValueError(\"input_ids and attention_mask cannot be None\")\n        if input_ids.dim() != 2 or attention_mask.dim() != 2:\n            raise ValueError(f\"Expected 2D tensors for input_ids/attention_mask, got {input_ids.shape}, {attention_mask.shape}\")\n\n        batch_size, seq_len = int(input_ids.size(0)), int(input_ids.size(1))\n        device = input_ids.device\n\n        # periodic GPU cleanup\n        if torch.cuda.is_available() and (self.global_step % max(1, _MEMORY_CLEANUP_FREQUENCY) == 0):\n            try:\n                torch.cuda.empty_cache()\n            except Exception:\n                pass\n\n        # Encoder forward\n        enc_outputs = None\n        try:\n            if self.mbart is not None:\n                # Prefer calling model.encoder or get_encoder if available\n                try:\n                    if hasattr(self.mbart, \"get_encoder\"):\n                        enc = self.mbart.get_encoder()\n                        enc_outputs = enc(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n                    elif hasattr(self.mbart, \"model\") and hasattr(self.mbart.model, \"encoder\"):\n                        enc_outputs = self.mbart.model.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n                    else:\n                        # as a last resort, run the full model and take its encoder output (costly)\n                        full = self.mbart(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n                        enc_outputs = getattr(full, \"encoder_last_hidden_state\", None) or _safe_get_last_hidden_state(full)\n                except Exception:\n                    enc_outputs = None\n            else:\n                enc_outputs = None\n        except Exception:\n            enc_outputs = None\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] Encoder forward failed:\", traceback.format_exc().splitlines()[-1])\n\n        h = _safe_get_last_hidden_state(enc_outputs)\n        if h is None:\n            try:\n                if self.mbart is not None and hasattr(self.mbart, \"get_input_embeddings\"):\n                    emb = self.mbart.get_input_embeddings()(input_ids).to(device)\n                    h = emb\n                else:\n                    h = torch.zeros(batch_size, seq_len, 512, device=device)\n            except Exception:\n                h = torch.zeros(batch_size, seq_len, 512, device=device)\n\n        embed_dim = int(h.size(-1))\n\n        training_mode = (labels is not None and self.training)\n\n        if token_word_map is None:\n            token_word_map = [{} for _ in range(batch_size)]\n\n        # DSCD forward\n        raw_dscd = {}\n        try:\n            if self.dscd is not None:\n                raw_dscd = self.dscd.forward(h, token_types=None, train_mode=self.training,\n                                             input_ids=input_ids, attention_mask=attention_mask,\n                                             token_word_map=token_word_map)\n            else:\n                raw_dscd = {}\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] DSCD forward failed; using fallback:\", traceback.format_exc().splitlines()[-1])\n            raw_dscd = {}\n\n        # Normalize DSCD outputs into canonical structure\n        dscd = _normalize_dscd_outputs(raw_dscd, batch_size, seq_len, device, embed_dim)\n        h_aug = dscd.get('h_augmented', h)\n        if not isinstance(h_aug, torch.Tensor) or h_aug.shape != h.shape:\n            h_aug = h\n\n        # embedding-based fallback for spans if DSCD did not provide meaningful spans\n        try:\n            span_missing = True\n            for b in range(batch_size):\n                row = dscd['span_preds'][b]\n                if any(float(x) > 1e-6 for x in row):\n                    span_missing = False\n                    break\n            if span_missing:\n                norms = torch.norm(h_aug, dim=-1)\n                for b in range(batch_size):\n                    n = norms[b]\n                    if n.numel() == 0 or torch.all(n == 0):\n                        continue\n                    mn = float(n.min().item())\n                    mx = float(n.max().item())\n                    rng = mx - mn + 1e-8\n                    scaled = (n - mn) / rng\n                    for t in range(min(seq_len, scaled.size(0))):\n                        try:\n                            dscd['span_preds'][b][t] = torch.tensor(float(scaled[t].item()), device=device)\n                        except Exception:\n                            pass\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] Applied embedding-norm fallback for span_preds.\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[TATN] Span fallback error:\", traceback.format_exc().splitlines()[-1])\n\n        # TRAINING path: produce scalar loss\n        if training_mode:\n            try:\n                enc_for_decoder = BaseModelOutput(last_hidden_state=h_aug) if BaseModelOutput is not None else (h_aug,)\n            except Exception:\n                enc_for_decoder = (h_aug,)\n\n            translation_loss = torch.tensor(0.0, device=device)\n            try:\n                if self.mbart is not None:\n                    seq_outputs = self.mbart(encoder_outputs=enc_for_decoder,\n                                             attention_mask=attention_mask,\n                                             labels=labels,\n                                             use_cache=False,\n                                             return_dict=True)\n                    translation_loss = getattr(seq_outputs, 'loss', torch.tensor(0.0, device=device))\n                else:\n                    translation_loss = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] Decoder forward failed during training:\", traceback.format_exc().splitlines()[-1])\n                translation_loss = torch.tensor(0.0, device=device)\n\n            try:\n                if self.asbn is not None:\n                    asbn_ret = self.asbn.forward_with_grl_simplified(h_aug, dscd.get('proto_probs', None),\n                                                                    dscd.get('uncertainties', None),\n                                                                    dscd.get('gates', None),\n                                                                    token_word_map=token_word_map)\n                    asbn_loss = asbn_ret[0] if isinstance(asbn_ret, (tuple, list)) else asbn_ret\n                    if not isinstance(asbn_loss, torch.Tensor):\n                        asbn_loss = torch.tensor(float(asbn_loss), device=device)\n                    else:\n                        asbn_loss = asbn_loss.to(device)\n                    if not torch.isfinite(asbn_loss):\n                        asbn_loss = torch.tensor(0.0, device=device)\n                else:\n                    asbn_loss = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] ASBN forward failed:\", traceback.format_exc().splitlines()[-1])\n                asbn_loss = torch.tensor(0.0, device=device)\n\n            try:\n                dscd_reg = self._entropy_reg_from_proto_probs_static(dscd.get('proto_probs', []),\n                                                                     gates_list=dscd.get('gates', []),\n                                                                     min_gate=0.0)\n                if not isinstance(dscd_reg, torch.Tensor):\n                    dscd_reg = torch.tensor(float(dscd_reg), device=device)\n                else:\n                    dscd_reg = dscd_reg.to(device)\n                if not torch.isfinite(dscd_reg):\n                    dscd_reg = torch.tensor(0.0, device=device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] DSCD reg computation failed:\", traceback.format_exc().splitlines()[-1])\n                dscd_reg = torch.tensor(0.0, device=device)\n\n            total_loss = translation_loss + _LAMBDA_ASBN * asbn_loss + _LAMBDA_DSCD * dscd_reg\n            if not isinstance(total_loss, torch.Tensor):\n                total_loss = torch.tensor(float(total_loss), device=device)\n            try:\n                if total_loss.numel() != 1:\n                    total_loss = total_loss.mean()\n            except Exception:\n                total_loss = torch.tensor(float(total_loss), device=device)\n            return total_loss\n\n        # INFERENCE path: produce explanations (no loss)\n        explanations = {i: [] for i in range(batch_size)}\n        if (not self.training) and _ENABLE_TRG_INFERENCE and self.trg_system is not None:\n            tokens_batch: List[List[str]] = []\n            word_maps_batch: List[dict] = []\n\n            # build tokens and word maps\n            for b in range(batch_size):\n                try:\n                    ids_b = input_ids[b].detach().cpu().tolist()\n                    if hasattr(self.tokenizer, 'convert_ids_to_tokens'):\n                        toks = self.tokenizer.convert_ids_to_tokens(ids_b)\n                    else:\n                        try:\n                            decoded = self.tokenizer.decode(ids_b, skip_special_tokens=True)\n                            toks = decoded.split()[:seq_len]\n                        except Exception:\n                            toks = [''] * seq_len\n                    if len(toks) < seq_len:\n                        toks = toks + [''] * (seq_len - len(toks))\n                    else:\n                        toks = toks[:seq_len]\n                except Exception:\n                    toks = [''] * seq_len\n                tokens_batch.append(toks)\n\n                # reconstruct word spans and create normalized map if available\n                if _has_reconstruct_word_spans:\n                    try:\n                        if src_texts and b < len(src_texts) and isinstance(src_texts[b], str) and src_texts[b].strip():\n                            orig_text = src_texts[b]\n                        else:\n                            try:\n                                orig_text = self.tokenizer.decode(input_ids[b], skip_special_tokens=True)\n                            except Exception:\n                                orig_text = \"\"\n                        wm, _ = reconstruct_word_spans(self.tokenizer, orig_text, max_length=seq_len)\n                        if not isinstance(wm, dict):\n                            wm = {}\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(\"[TATN] reconstruct_word_spans failed:\", traceback.format_exc().splitlines()[-1])\n                        wm = {}\n                else:\n                    wm = {}\n\n                # build normalized word map using normalize_bn_word if available\n                norm_map = {}\n                try:\n                    if isinstance(wm, dict) and _normalize_fn:\n                        for k, v in wm.items():\n                            try:\n                                norm_map[k] = _normalize_fn(v) if isinstance(v, str) and v else v\n                            except Exception:\n                                norm_map[k] = v\n                    else:\n                        # if wm not dict or no normalize fn, try making a trivial map\n                        if isinstance(wm, dict):\n                            norm_map = wm.copy()\n                except Exception:\n                    norm_map = wm.copy() if isinstance(wm, dict) else {}\n\n                word_maps_batch.append({\"orig\": wm, \"norm\": norm_map})\n\n            # helper: safe extractor that returns per-token lists (lists of tensors/scalars)\n            def _safe_take_key(dscd_struct, key, b_index):\n                out = []\n                try:\n                    val = dscd_struct.get(key, None)\n                    if val is None:\n                        # default: scalar zeros or proto tensor fallback\n                        if key == 'proto_probs':\n                            return [torch.tensor([1.0], device=device) for _ in range(seq_len)]\n                        else:\n                            return [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n\n                    # handle list-of-batches\n                    if isinstance(val, list) and len(val) == batch_size:\n                        row = val[b_index]\n                        if isinstance(row, list):\n                            for t in range(min(seq_len, len(row))):\n                                v = row[t]\n                                if isinstance(v, torch.Tensor):\n                                    out.append(v.to(device))\n                                else:\n                                    try:\n                                        out.append(torch.tensor(float(v), device=device))\n                                    except Exception:\n                                        out.append(torch.tensor(0.0, device=device))\n                            # pad\n                            while len(out) < seq_len:\n                                if key == 'proto_probs':\n                                    out.append(torch.tensor([1.0], device=device))\n                                else:\n                                    out.append(torch.tensor(0.0, device=device))\n                            return out\n                        elif isinstance(row, torch.Tensor):\n                            r = row.detach().cpu()\n                            if r.dim() == 1:\n                                for t in range(min(seq_len, int(r.size(0)))):\n                                    out.append(torch.tensor(float(r[t].item()), device=device))\n                            elif r.dim() == 2:\n                                for t in range(min(seq_len, int(r.size(0)))):\n                                    out.append(r[t].to(device))\n                            while len(out) < seq_len:\n                                if key == 'proto_probs':\n                                    out.append(torch.tensor([1.0], device=device))\n                                else:\n                                    out.append(torch.tensor(0.0, device=device))\n                            return out\n\n                    # handle tensor layout [B, T, ...]\n                    if isinstance(val, torch.Tensor):\n                        v = val.detach().cpu()\n                        if v.dim() >= 2 and int(v.size(0)) == batch_size:\n                            for t in range(min(seq_len, int(v.size(1)))):\n                                if v.dim() == 3:\n                                    out.append(v[b_index, t].to(device))\n                                else:\n                                    out.append(torch.tensor(float(v[b_index, t].item()), device=device))\n                            while len(out) < seq_len:\n                                if key == 'proto_probs':\n                                    out.append(torch.tensor([1.0], device=device))\n                                else:\n                                    out.append(torch.tensor(0.0, device=device))\n                            return out\n                        elif v.dim() == 1 and batch_size == 1:\n                            for t in range(min(seq_len, int(v.size(0)))):\n                                out.append(torch.tensor(float(v[t].item()), device=device))\n                            while len(out) < seq_len:\n                                out.append(torch.tensor(0.0, device=device))\n                            return out\n\n                    # numpy arrays or list-of-per-token\n                    if isinstance(val, (list, tuple, np.ndarray)):\n                        if isinstance(val, np.ndarray) and val.ndim >= 2 and val.shape[0] == batch_size:\n                            for t in range(min(seq_len, int(val.shape[1]))):\n                                try:\n                                    out.append(torch.tensor(float(val[b_index, t]), device=device))\n                                except Exception:\n                                    out.append(torch.tensor(0.0, device=device))\n                            while len(out) < seq_len:\n                                out.append(torch.tensor(0.0, device=device))\n                            return out\n                        # treat as per-token sequence for single batch\n                        if len(val) >= seq_len:\n                            for t in range(min(seq_len, len(val))):\n                                vt = val[t]\n                                if isinstance(vt, torch.Tensor):\n                                    out.append(vt.detach().to(device))\n                                else:\n                                    try:\n                                        out.append(torch.tensor(float(vt), device=device))\n                                    except Exception:\n                                        out.append(torch.tensor(0.0, device=device))\n                            return out\n\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(\"[TATN] _safe_take_key error:\", traceback.format_exc().splitlines()[-1])\n                # fallback fill\n                if key == 'proto_probs':\n                    return [torch.tensor([1.0], device=device) for _ in range(seq_len)]\n                return [torch.tensor(0.0, device=device) for _ in range(seq_len)]\n\n            # call TRG system per sentence safely\n            try:\n                for b in range(batch_size):\n                    per_sent = {\n                        'proto_probs': _safe_take_key(dscd, 'proto_probs', b),\n                        'uncertainties': _safe_take_key(dscd, 'uncertainties', b),\n                        'gates': _safe_take_key(dscd, 'gates', b),\n                        'span_preds': _safe_take_key(dscd, 'span_preds', b),\n                    }\n                    try:\n                        # TRG expects dscd_outputs-like dict for a single sentence; pass token_word_map[b] (with 'norm' available)\n                        exps = self.trg_system.process_sentence_for_explanations(\n                            tokens_batch[b],\n                            per_sent,\n                            token_word_map=word_maps_batch[b],\n                            uncertainty_threshold=float(globals().get('TAU_LOW', 0.4)),\n                            top_k=3\n                        )\n                        explanations[b] = exps if isinstance(exps, list) else []\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(f\"[TATN] TRG explanation generation failed for idx={b}:\", traceback.format_exc().splitlines()[-1])\n                        explanations[b] = []\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[TATN] TRG generation failed overall:\", traceback.format_exc().splitlines()[-1])\n                explanations = {i: [] for i in range(batch_size)}\n\n        outputs = {\n            'encoder_outputs': enc_outputs,\n            'dscd_outputs': dscd,\n            'sense_augmented_embeddings': h_aug,\n            'explanations': [explanations.get(i, []) for i in range(batch_size)],\n            'asbn_loss': torch.tensor(0.0, device=device),\n        }\n        return outputs\n\n    def forward_with_explanations(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        src_texts: Optional[List[str]] = None,\n        token_word_map: Optional[List[dict]] = None,\n    ):\n        return self.forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            src_texts=src_texts,\n            token_word_map=token_word_map,\n            labels=None,\n        )\n\n# -----------------------------------------------------------------------------\n# Verification prints (concise)\n# -----------------------------------------------------------------------------\nprint(\"=\" * 80)\nprint(\"✅ Cell 6: TATN model wrapper ready (DEBUGGED & HARDENED)\")\nprint(\"=\" * 80)\nprint(f\"✓ transformers available: {_HAS_TRANSFORMERS}\")\nprint(f\"✓ Gradient checkpointing enabled (config): {_USE_GC}\")\nprint(f\"✓ DSCD training clustering: {'ENABLED' if _DSCD_ENABLE_TRAINING_CLUSTERING else 'DISABLED'}\")\nprint(f\"✓ DSCD buffer: {_DSCD_BUFFER_SIZE}, n_min: {_DSCD_N_MIN}, disp_th: {_DSCD_DISPERSION_THRESHOLD}\")\nprint(\"=\" * 80)","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:07.498757Z","iopub.execute_input":"2025-11-22T14:56:07.499049Z","iopub.status.idle":"2025-11-22T14:56:08.374494Z","shell.execute_reply.started":"2025-11-22T14:56:07.499031Z","shell.execute_reply":"2025-11-22T14:56:08.373713Z"},"id":"KZbMDpIYH4J4","trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\n✅ Cell 6: TATN model wrapper ready (DEBUGGED & HARDENED)\n================================================================================\n✓ transformers available: True\n✓ Gradient checkpointing enabled (config): True\n✓ DSCD training clustering: ENABLED\n✓ DSCD buffer: 20, n_min: 5, disp_th: 0.25\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7 (FIXED): TRAINING LOOP (DP + AMP + AccUM + Progress + DEBUG + CLUSTER TRACKING)\n# Fully debugged, hardened, and compatible with multiple torch versions.\n# ==============================================================================\n# Fixes applied (summary):\n#  - Removed use of private GradScaler API scaler._maybe_opt_step(None) and handled optimizer=None safely.\n#  - Added robust helper scaler_enabled(...) to support older/newer torch versions.\n#  - Added support for ModelOutput-like objects (access .loss attribute) in forward outputs.\n#  - Replaced fragile calls to scaler.is_enabled() with scaler_enabled(scaler) wrapper.\n#  - Graceful epoch-flush when optimizer is None (no exception raised).\n#  - Wrapped scaler.unscale_/step/update calls in try/except for compatibility and safety.\n#  - Ensured we do not call scaler-specific APIs when scaler is disabled.\n#  - Minor logging and defensive guards (avoid uninitialized vars, better OOM handling).\n# ==============================================================================\nimport os\nimport time\nimport math\nimport gc\nimport traceback\nfrom datetime import datetime\nfrom collections import defaultdict, deque\nfrom typing import Optional, Dict, Any, List, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast as cuda_amp_autocast\nfrom tqdm import tqdm\nfrom contextlib import nullcontext\n\n# ---------------- Debug control ----------------\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n\nDEBUG_PRINT_INTERVAL = int(globals().get(\"DEBUG_PRINT_INTERVAL\", 200))\n_cell7_dbg_counts = defaultdict(int)\n\n\ndef cell7_dbg(key: str, msg: str, limit: int = 10):\n    if not _VERBOSE_LOGGING:\n        return\n    _cell7_dbg_counts[key] += 1\n    if _cell7_dbg_counts[key] <= limit:\n        print(f\"[CELL7-DBG] {msg}\")\n\n\n# ---------------- Fallback globals ----------------\n_DEVICE = globals().get(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n_EPOCHS = int(globals().get(\"EPOCHS\", 1))\n_BATCH_SIZE = int(globals().get(\"BATCH_SIZE\", 8))\n_ACCUMULATION_STEPS = int(globals().get(\"ACCUMULATION_STEPS\", 1))\n_GRAD_CLIP_NORM = float(globals().get(\"GRAD_CLIP_NORM\", 1.0))\n_MEMORY_CLEANUP_FREQUENCY = int(globals().get(\"MEMORY_CLEANUP_FREQUENCY\", 100))\n_USE_MULTI_GPU = bool(globals().get(\"USE_MULTI_GPU\", torch.cuda.device_count() > 1))\n_NUM_GPUS = int(globals().get(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n_USE_AMP = bool(globals().get(\"USE_AMP\", True))\n_BN_LANG = str(globals().get(\"BN_LANG\", \"bn\"))\n_EN_LANG = str(globals().get(\"EN_LANG\", \"en\"))\n_MAX_LENGTH = int(globals().get(\"MAX_LENGTH\", 48))\nVALIDATION_CHECK_INTERVAL = int(globals().get(\"VALIDATION_CHECK_INTERVAL\", 0))\n\n# ---------------- Helpers ----------------\ndef clear_all_gpu_caches():\n    gc.collect()\n    if not torch.cuda.is_available():\n        return\n    try:\n        for i in range(torch.cuda.device_count()):\n            with torch.cuda.device(i):\n                try:\n                    torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n\ndef get_amp_ctx():\n    \"\"\"\n    Return a context manager for mixed-precision if enabled and available.\n    Otherwise return a nullcontext.\n    \"\"\"\n    if not _USE_AMP or not torch.cuda.is_available():\n        return nullcontext()\n    try:\n        return cuda_amp_autocast()\n    except Exception:\n        return nullcontext()\n\n\ndef save_checkpoint(model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer], training_stats: Dict[str, Any],\n                    epoch: int, global_step: int, epoch_losses: List[float], ckpt_dir: str = \"checkpoints\"):\n    os.makedirs(ckpt_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    fname = f\"tatn_e{epoch}_s{global_step}_{timestamp}.pt\"\n    path = os.path.join(ckpt_dir, fname)\n    core_model = model.module if hasattr(model, \"module\") else model\n    ckpt = {\n        \"epoch\": epoch,\n        \"global_step\": global_step,\n        \"model_state_dict\": core_model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict() if optimizer is not None else None,\n        \"training_stats\": training_stats,\n        \"avg_epoch_loss\": float(np.mean(epoch_losses)) if epoch_losses else 0.0,\n    }\n    try:\n        torch.save(ckpt, path)\n        print(f\"[CHECKPOINT] Saved {fname} avg_loss={ckpt['avg_epoch_loss']:.6f}\")\n    except Exception as e:\n        print(f\"[CHECKPOINT] Save failed: {type(e).__name__}: {str(e)[:200]}\")\n\n\n# ---------------- Validation (hardened) ----------------\n_PROTOBUF_COMPAT_ERROR_SHOWN = globals().get(\"_PROTOBUF_COMPAT_ERROR_SHOWN\", False)\n\n\n@torch.inference_mode()\ndef quick_validation_check(model: torch.nn.Module, tokenizer, step: int, bn_lang: str, en_lang: str, max_length: int, device: torch.device):\n    \"\"\"\n    Run a few simple translations to sanity-check the model.\n    Robust to protobuf/getprototype errors.\n    \"\"\"\n    global _PROTOBUF_COMPAT_ERROR_SHOWN\n    core_model = model.module if hasattr(model, \"module\") else model\n    gen_target = getattr(core_model, \"mbart\", core_model)\n    was_training = core_model.training\n    core_model.eval()\n\n    samples = [\n        \"আমি কল বন্ধ করেছি।\",\n        \"কাল আমি বই কিনব।\",\n        \"পাতা ঝরে পড়েছে।\",\n        \"আমি ভালো আছি।\",\n        \"আজ আবহাওয়া ভালো।\",\n    ]\n    print(\"\\n\" + \"=\" * 70)\n    print(f\"[VALIDATION] Quick validation at step {step}\")\n    print(\"=\" * 70)\n    try:\n        try:\n            tokenizer.src_lang = bn_lang\n        except Exception:\n            pass\n\n        forced_id = None\n        try:\n            if hasattr(tokenizer, \"get_lang_id\"):\n                for code in (en_lang, \"en_XX\", \"en\", \"eng\"):\n                    try:\n                        lid = tokenizer.get_lang_id(code)\n                        if lid is not None:\n                            forced_id = lid\n                            break\n                    except Exception:\n                        continue\n            elif hasattr(tokenizer, \"lang_code_to_id\"):\n                forced_id = tokenizer.lang_code_to_id.get(en_lang, None)\n        except Exception:\n            forced_id = None\n\n        mbart_obj = getattr(core_model, \"mbart\", None)\n        orig_use_cache = None\n        try:\n            if mbart_obj is not None and hasattr(mbart_obj.config, \"use_cache\"):\n                orig_use_cache = mbart_obj.config.use_cache\n                mbart_obj.config.use_cache = True\n        except Exception:\n            orig_use_cache = None\n\n        for i, src in enumerate(samples, 1):\n            try:\n                enc = tokenizer(src, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n                enc = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in enc.items()}\n                if forced_id is not None:\n                    try:\n                        if mbart_obj is not None:\n                            mbart_obj.config.forced_bos_token_id = int(forced_id)\n                            mbart_obj.config.decoder_start_token_id = int(forced_id)\n                    except Exception:\n                        pass\n                out_ids = None\n                try:\n                    gen_src = getattr(core_model, \"mbart\", None) or core_model\n                    if hasattr(gen_src, \"generate\"):\n                        out_ids = gen_src.generate(\n                            enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            max_length=max_length,\n                            num_beams=2,\n                            do_sample=False,\n                            early_stopping=True,\n                            pad_token_id=int(getattr(tokenizer, \"pad_token_id\", 1)),\n                            forced_bos_token_id=int(forced_id) if forced_id is not None else None\n                        )\n                except AttributeError as ae:\n                    if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                        print(\"[VALIDATION] Warning: generation raised AttributeError (often protobuf incompatibility).\")\n                        print(\"  Suggestion: pip install 'protobuf==3.20.3' and restart the kernel.\")\n                        _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                    out_ids = None\n                except Exception as e:\n                    print(f\"[VALIDATION] Generation error: {type(e).__name__}: {str(e)[:200]}\")\n                    out_ids = None\n\n                if out_ids is not None:\n                    try:\n                        if isinstance(out_ids, (list, tuple)):\n                            pred = tokenizer.batch_decode(out_ids, skip_special_tokens=True)[0]\n                        else:\n                            # ensure out_ids is tensor\n                            if isinstance(out_ids, torch.Tensor):\n                                pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n                            else:\n                                pred = str(out_ids)\n                    except AttributeError:\n                        if not _PROTOBUF_COMPAT_ERROR_SHOWN:\n                            print(\"[VALIDATION] Warning: decode raised AttributeError (protobuf). Pin protobuf and restart.\")\n                            _PROTOBUF_COMPAT_ERROR_SHOWN = True\n                        pred = \"\"\n                    except Exception as e:\n                        print(f\"[VALIDATION] Decode error: {type(e).__name__}: {str(e)[:200]}\")\n                        pred = \"\"\n                else:\n                    pred = \"\"\n                print(f\"{i}. {src} -> {pred}\")\n            except Exception as e:\n                print(f\"{i}. Validation error: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n    finally:\n        try:\n            if mbart_obj is not None and orig_use_cache is not None:\n                mbart_obj.config.use_cache = orig_use_cache\n        except Exception:\n            pass\n        if torch.cuda.is_available():\n            try:\n                torch.cuda.synchronize()\n            except Exception:\n                pass\n        clear_all_gpu_caches()\n        if was_training:\n            core_model.train()\n    print(\"=\" * 70)\n\n\ndef _print_gpu_mem(prefix: str = \"\"):\n    if not torch.cuda.is_available():\n        return\n    try:\n        lines = [f\"{prefix} GPU mem (GB):\"]\n        for i in range(torch.cuda.device_count()):\n            try:\n                alloc = torch.cuda.memory_allocated(i) / (1024**3)\n                resv = torch.cuda.memory_reserved(i) / (1024**3)\n                lines.append(f\"  GPU {i}: alloc={alloc:.2f} resv={resv:.2f}\")\n            except Exception:\n                lines.append(f\"  GPU {i}: mem query failed\")\n        print(\"\\n\".join(lines))\n    except Exception:\n        pass\n\n\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else model.dscd\n        return len(getattr(dscd, \"prototype_stores\", {}) or {})\n    except Exception:\n        return 0\n\n\ndef _get_dscd_safe(model: torch.nn.Module):\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        return getattr(core, \"dscd\", None)\n    except Exception:\n        return None\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        if _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] No DSCD instance attached to model.\")\n        return\n    try:\n        items = []\n        for token, store in dscd.prototype_stores.items():\n            total_count = sum(getattr(store, \"counts\", []) or [])\n            protos = store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n            items.append((token, total_count, protos, len(dscd.buffers.get(token, []))))\n        items.sort(key=lambda x: x[1], reverse=True)\n        if _VERBOSE_LOGGING:\n            print(\"[CLUSTER-DBG] Top clusters:\")\n            for i, (tok, cnt, prot, buflen) in enumerate(items[:top_n], 1):\n                print(f\"  {i:2d}. {str(tok)[:20]:20s} samples={cnt:4d} protos={prot} buf={buflen}\")\n    except Exception as e:\n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_top_clusters error: {type(e).__name__}: {str(e)[:200]}\")\n\n\ndef _print_cluster_stats(model: torch.nn.Module):\n    dscd = _get_dscd_safe(model)\n    if dscd is None:\n        return\n    try:\n        total_tokens = len(dscd.prototype_stores)\n        total_protos = 0\n        total_samples = 0\n        total_buffers = 0\n        for token, store in dscd.prototype_stores.items():\n            total_protos += store.size() if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []))\n            total_samples += sum(getattr(store, \"counts\", []) or [])\n            total_buffers += len(dscd.buffers.get(token, []))\n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] tokens_with_stores={total_tokens} total_prototypes={total_protos} total_samples={total_samples} total_buffered_embeddings={total_buffers}\")\n    except Exception as e:\n        if _VERBOSE_LOGGING:\n            print(f\"[CLUSTER-DBG] _print_cluster_stats error: {type(e).__name__}: {str(e)[:200]}\")\n\n\n# ---------------- batch unpacking helper ----------------\ndef _unpack_batch(batch: Any) -> Dict[str, Any]:\n    \"\"\"\n    Accept common batch formats:\n      - dict with keys\n      - tuple/list: (input_ids, attention_mask, labels, src_texts?, token_word_map?)\n    Return dict with keys 'input_ids','attention_mask','labels','src_text','token_word_map'\n    \"\"\"\n    if batch is None:\n        return {}\n    if isinstance(batch, dict):\n        return dict(batch)\n    if isinstance(batch, (list, tuple)):\n        out = {}\n        # heuristics: first two are input_ids, attention_mask\n        try:\n            if len(batch) >= 2:\n                out['input_ids'] = batch[0]\n                out['attention_mask'] = batch[1]\n            if len(batch) >= 3:\n                out['labels'] = batch[2]\n            if len(batch) >= 4:\n                out['src_text'] = batch[3]\n            if len(batch) >= 5:\n                out['token_word_map'] = batch[4]\n        except Exception:\n            pass\n        return out\n    # fallback: can't unpack\n    return {}\n\n\n# Helper to check scaler availability robustly across torch versions\ndef scaler_enabled(scaler: Optional[GradScaler]) -> bool:\n    if scaler is None:\n        return False\n    try:\n        # GradScaler in newer torch versions has is_enabled()\n        return bool(getattr(scaler, \"is_enabled\", lambda: False)())\n    except Exception:\n        # Backwards compat: use enabled attribute or assume True if instance\n        return getattr(scaler, \"enabled\", False) if hasattr(scaler, \"enabled\") else True\n\n\n# ---------------- Main training loop ----------------\ndef train_memory_efficient_tatn(\n    model: torch.nn.Module,\n    tokenizer,\n    train_loader: torch.utils.data.DataLoader,\n    optimizer: Optional[torch.optim.Optimizer],\n    phi_optimizer: Optional[torch.optim.Optimizer] = None,\n    epochs: Optional[int] = None,\n    accumulation_steps: Optional[int] = None,\n    validate_every: Optional[int] = None,\n    enable_validation: bool = True\n) -> torch.nn.Module:\n    if epochs is None:\n        epochs = _EPOCHS\n    if accumulation_steps is None:\n        accumulation_steps = max(1, _ACCUMULATION_STEPS)\n    if validate_every is None:\n        validate_every = VALIDATION_CHECK_INTERVAL\n\n    print(f\"[TRAIN] Starting training: epochs={epochs}, batch={_BATCH_SIZE}, accum_steps={accumulation_steps}\")\n    print(f\"[TRAIN] Validation: {'enabled' if enable_validation and validate_every > 0 else 'disabled'}\")\n    print(f\"[TRAIN] DP enabled: {_USE_MULTI_GPU}, GPUs: {_NUM_GPUS}, Device: {_DEVICE}\")\n\n    model.train()\n    clear_all_gpu_caches()\n\n    # GradScaler enabled only if AMP requested and CUDA present\n    scaler = GradScaler(enabled=(_USE_AMP and torch.cuda.is_available()))\n\n    global_step = 0\n    accumulated_steps = 0\n    pending_validation = False\n\n    training_stats: Dict[str, Any] = {\n        \"total_loss\": [],\n        \"batches_processed\": 0,\n        \"optimizer_updates\": 0,\n        \"skipped_batches\": 0,\n        \"oom_errors\": 0,\n        \"runtime_errors\": 0,\n        \"exceptions\": 0,\n    }\n\n    skip_reasons = defaultdict(int)\n    last_forward_loss = 0.0\n    last_backward_loss = 0.0\n\n    for epoch in range(1, epochs + 1):\n        epoch_start = time.time()\n        epoch_losses: List[float] = []\n        try:\n            if optimizer is not None:\n                try:\n                    optimizer.zero_grad(set_to_none=True)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n        progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", ncols=180, dynamic_ncols=False)\n\n        for batch_idx, batch in enumerate(progress):\n            global_step += 1\n            training_stats[\"batches_processed\"] += 1\n\n            if _VERBOSE_LOGGING and global_step % DEBUG_PRINT_INTERVAL == 0:\n                print(f\"[TRAIN-DEBUG] Epoch {epoch} Batch {batch_idx} GlobalStep {global_step}\")\n\n            # Validation scheduling\n            if enable_validation and validate_every and validate_every > 0 and (global_step % validate_every == 0):\n                if accumulated_steps == 0:\n                    try:\n                        quick_validation_check(model, tokenizer, global_step, _BN_LANG, _EN_LANG, _MAX_LENGTH, _DEVICE)\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(\"[TRAIN] quick_validation_check failed:\", traceback.format_exc().splitlines()[-1])\n                else:\n                    pending_validation = True\n\n            # Validate batch\n            if batch is None:\n                training_stats[\"skipped_batches\"] += 1\n                skip_reasons[\"batch_none\"] += 1\n                cell7_dbg(\"batch_none\", f\"Batch is None at idx={batch_idx}\")\n                continue\n\n            try:\n                # Unpack batch robustly\n                bdict = _unpack_batch(batch)\n                input_ids = bdict.get(\"input_ids\", None)\n                attention_mask = bdict.get(\"attention_mask\", None)\n                labels = bdict.get(\"labels\", None)\n\n                # If key tensors missing, skip\n                if input_ids is None or attention_mask is None:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"missing_tensors\"] += 1\n                    cell7_dbg(\"missing_tensors\", f\"Missing tensors in batch idx={batch_idx}\")\n                    continue\n\n                # ensure tensors are on correct dtype/device\n                try:\n                    if isinstance(input_ids, torch.Tensor):\n                        input_ids = input_ids.to(_DEVICE, non_blocking=True)\n                        if input_ids.dtype not in (torch.long, torch.int64):\n                            input_ids = input_ids.long()\n                    if isinstance(attention_mask, torch.Tensor):\n                        attention_mask = attention_mask.to(_DEVICE, non_blocking=True)\n                    if labels is not None and isinstance(labels, torch.Tensor):\n                        labels = labels.to(_DEVICE, non_blocking=True)\n                except Exception:\n                    # fallback: move using .to with try/except\n                    try:\n                        input_ids = input_ids.to(_DEVICE)\n                    except Exception:\n                        pass\n                    try:\n                        attention_mask = attention_mask.to(_DEVICE)\n                    except Exception:\n                        pass\n                    try:\n                        if labels is not None and isinstance(labels, torch.Tensor):\n                            labels = labels.to(_DEVICE)\n                    except Exception:\n                        pass\n\n                # DP-divisible truncation safety (should already be handled by collate)\n                if _USE_MULTI_GPU and _NUM_GPUS > 0:\n                    try:\n                        bsz = int(input_ids.size(0))\n                        keep = (bsz // _NUM_GPUS) * _NUM_GPUS\n                        if keep == 0:\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"dp_keep_zero\"] += 1\n                            cell7_dbg(\"dp_keep_zero\", f\"DP keep==0 bsz={bsz}, gpus={_NUM_GPUS}\")\n                            continue\n                        if keep != bsz:\n                            input_ids = input_ids[:keep]\n                            attention_mask = attention_mask[:keep]\n                            if labels is not None:\n                                labels = labels[:keep]\n                    except Exception:\n                        # If we can't determine size, skip to be safe\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"dp_size_error\"] += 1\n                        continue\n\n                if isinstance(input_ids, torch.Tensor) and input_ids.size(0) == 0:\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"empty_batch\"] += 1\n                    continue\n\n                # Optional debugging: token_word_map presence in the batch (non-tensor)\n                if _VERBOSE_LOGGING and 'token_word_map' in bdict:\n                    try:\n                        sample_map = bdict['token_word_map'][:2]\n                        cell7_dbg(\"tokmap_sample\", f\"token_word_map sample lens: {[len(x) if x else 0 for x in sample_map]}\", limit=3)\n                    except Exception:\n                        pass\n\n                forward_kwargs = {\n                    \"input_ids\": input_ids,\n                    \"attention_mask\": attention_mask,\n                    \"labels\": labels,\n                    \"src_texts\": bdict.get(\"src_text\", None),\n                    \"token_word_map\": bdict.get(\"token_word_map\", None),\n                }\n\n                amp_ctx = get_amp_ctx()\n                with amp_ctx:\n                    forward_out = model(**forward_kwargs)\n\n                    # Determine loss tensor\n                    loss_tensor = None\n                    # handle HF ModelOutput-like (with .loss attribute)\n                    try:\n                        if hasattr(forward_out, \"loss\"):\n                            loss_tensor = getattr(forward_out, \"loss\")\n                    except Exception:\n                        pass\n\n                    if loss_tensor is None:\n                        if isinstance(forward_out, torch.Tensor):\n                            loss_tensor = forward_out\n                        elif isinstance(forward_out, dict):\n                            # common keys to check\n                            possible_loss_keys = [\"loss\", \"total_loss\", \"translation_loss\"]\n                            for k in possible_loss_keys:\n                                if k in forward_out:\n                                    loss_tensor = forward_out[k]\n                                    break\n                            # if no explicit loss, model may have returned scalar in fieldless dict\n                            if loss_tensor is None:\n                                # try to find any tensor value that is scalar-like\n                                for v in forward_out.values():\n                                    if isinstance(v, torch.Tensor) and v.numel() == 1:\n                                        loss_tensor = v\n                                        break\n                        elif isinstance(forward_out, (list, tuple)) and len(forward_out) > 0:\n                            if isinstance(forward_out[0], torch.Tensor):\n                                loss_tensor = forward_out[0]\n\n                    if loss_tensor is None:\n                        # As a last resort try converting numeric outputs\n                        try:\n                            if isinstance(forward_out, (int, float, np.floating, np.integer)):\n                                loss_tensor = torch.tensor(float(forward_out), device=_DEVICE)\n                        except Exception:\n                            pass\n\n                    if loss_tensor is None:\n                        raise RuntimeError(\"Model forward did not return a recognizable loss tensor\")\n\n                    # Ensure scalar and on device\n                    if not isinstance(loss_tensor, torch.Tensor):\n                        loss_tensor = torch.tensor(float(loss_tensor), device=_DEVICE)\n                    else:\n                        try:\n                            loss_tensor = loss_tensor.to(_DEVICE)\n                        except Exception:\n                            pass\n\n                    if loss_tensor.numel() > 1:\n                        loss_val = float(loss_tensor.mean().item())\n                        loss_tensor = loss_tensor.mean()\n                    else:\n                        loss_val = float(loss_tensor.item())\n\n                    last_forward_loss = loss_val\n                    epoch_losses.append(loss_val)\n                    training_stats[\"total_loss\"].append(loss_val)\n\n                # backward + accumulation\n                loss_scaled = loss_tensor / max(1, accumulation_steps)\n                try:\n                    last_backward_loss = float(loss_scaled.item())\n                except Exception:\n                    try:\n                        last_backward_loss = float(loss_scaled.detach().cpu().item()) if isinstance(loss_scaled, torch.Tensor) else float(loss_scaled)\n                    except Exception:\n                        last_backward_loss = 0.0\n\n                # Backward: use scaler only if enabled\n                try:\n                    if scaler_enabled(scaler):\n                        scaler.scale(loss_scaled).backward()\n                    else:\n                        loss_scaled.backward()\n                except RuntimeError as e:\n                    # immediate OOM during backward\n                    if \"out of memory\" in str(e).lower():\n                        training_stats[\"oom_errors\"] += 1\n                        training_stats[\"skipped_batches\"] += 1\n                        skip_reasons[\"oom_backward\"] += 1\n                        print(f\"[OOM] OOM during backward at step {global_step}: {str(e)[:200]}\")\n                        try:\n                            if optimizer is not None:\n                                optimizer.zero_grad(set_to_none=True)\n                        except Exception:\n                            pass\n                        for p in model.parameters():\n                            if p is not None:\n                                p.grad = None\n                        clear_all_gpu_caches()\n                        accumulated_steps = 0\n                        continue\n                    else:\n                        raise\n\n                accumulated_steps += 1\n\n                # optimizer step\n                if accumulated_steps >= accumulation_steps:\n                    try:\n                        if optimizer is None:\n                            # Nothing to step; just zero grads and move on\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"no_optimizer\"] += 1\n                            try:\n                                model.zero_grad(set_to_none=True)\n                            except Exception:\n                                for p in model.parameters():\n                                    if p.grad is not None:\n                                        p.grad = None\n                        else:\n                            # Unscale first if using scaler, then clip\n                            if scaler_enabled(scaler):\n                                try:\n                                    scaler.unscale_(optimizer)\n                                except Exception:\n                                    # unscale_ might not exist in some versions; ignore and proceed\n                                    pass\n                            # gradient clip (guard against generator/params that might be empty)\n                            try:\n                                torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                            except Exception:\n                                pass\n                            # step\n                            if scaler_enabled(scaler):\n                                try:\n                                    scaler.step(optimizer)\n                                    scaler.update()\n                                except Exception as e:\n                                    # If scaler.step failed, try a plain step (best-effort)\n                                    try:\n                                        optimizer.step()\n                                    except Exception:\n                                        raise\n                            else:\n                                optimizer.step()\n                            # zero grads\n                            try:\n                                optimizer.zero_grad(set_to_none=True)\n                            except Exception:\n                                for p in model.parameters():\n                                    if p.grad is not None:\n                                        p.grad.detach_()\n                                        p.grad.zero_()\n                            training_stats[\"optimizer_updates\"] += 1\n                    except RuntimeError as e:\n                        if \"out of memory\" in str(e).lower():\n                            training_stats[\"oom_errors\"] += 1\n                            training_stats[\"skipped_batches\"] += 1\n                            skip_reasons[\"oom\"] += 1\n                            print(f\"[OOM] OOM at step {global_step}: {str(e)[:200]}\")\n                            try:\n                                if optimizer is not None:\n                                    optimizer.zero_grad(set_to_none=True)\n                            except Exception:\n                                pass\n                            for p in model.parameters():\n                                p.grad = None\n                            clear_all_gpu_caches()\n                            accumulated_steps = 0\n                            continue\n                        else:\n                            training_stats[\"runtime_errors\"] += 1\n                            skip_reasons[\"opt_runtime\"] += 1\n                            print(f\"[ERROR] Runtime error during optimizer step: {type(e).__name__}: {str(e)[:200]}\")\n                    except Exception as e:\n                        training_stats[\"exceptions\"] += 1\n                        skip_reasons[\"opt_exception\"] += 1\n                        print(f\"[ERROR] Exception during optimizer step: {type(e).__name__}: {str(e)[:200]}\")\n                    finally:\n                        accumulated_steps = 0\n                        if pending_validation:\n                            try:\n                                quick_validation_check(model, tokenizer, global_step, _BN_LANG, _EN_LANG, _MAX_LENGTH, _DEVICE)\n                            except Exception:\n                                if _VERBOSE_LOGGING:\n                                    print(\"[TRAIN] deferred quick_validation_check failed:\", traceback.format_exc().splitlines()[-1])\n                            pending_validation = False\n\n                # periodic housekeeping & logs\n                if global_step % DEBUG_PRINT_INTERVAL == 0:\n                    _print_gpu_mem(\"[TRAIN-DEBUG]\")\n                    try:\n                        cluster_count = _get_cluster_count(model)\n                    except Exception:\n                        cluster_count = 0\n                    print(f\"[TRAIN-DEBUG] step={global_step} loss={last_forward_loss:.4f} opt_updates={training_stats['optimizer_updates']} clusters={cluster_count}\")\n                    _print_top_clusters(model, top_n=5)\n                    _print_cluster_stats(model)\n\n                if global_step % _MEMORY_CLEANUP_FREQUENCY == 0:\n                    clear_all_gpu_caches()\n\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    training_stats[\"oom_errors\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"oom\"] += 1\n                    print(f\"[OOM] Caught OOM at step {global_step}: {str(e)[:200]}\")\n                    try:\n                        if optimizer is not None:\n                            optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    for p in model.parameters():\n                        p.grad = None\n                    clear_all_gpu_caches()\n                    accumulated_steps = 0\n                    continue\n                else:\n                    training_stats[\"runtime_errors\"] += 1\n                    training_stats[\"skipped_batches\"] += 1\n                    skip_reasons[\"runtime\"] += 1\n                    print(f\"[RUNTIME] RuntimeError at step {global_step}: {type(e).__name__}: {str(e)[:200]}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    try:\n                        if optimizer is not None:\n                            optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        pass\n                    accumulated_steps = 0\n                    continue\n            except Exception as e:\n                training_stats[\"exceptions\"] += 1\n                training_stats[\"skipped_batches\"] += 1\n                skip_reasons[\"exceptions\"] += 1\n                print(f\"[EXCEPTION] Exception at step {global_step}: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                try:\n                    if optimizer is not None:\n                        optimizer.zero_grad(set_to_none=True)\n                except Exception:\n                    pass\n                accumulated_steps = 0\n                continue\n\n            # update progress bar postfix\n            processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n            expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n            success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n            cluster_count = _get_cluster_count(model)\n            try:\n                progress.set_postfix_str(\n                    f\"fwd_loss={last_forward_loss:.4f} bwd_loss={last_backward_loss:.6f} rate={success_rate:.1f}% proc={processed_batches} skip={training_stats['skipped_batches']} clusters={cluster_count}\"\n                )\n            except Exception:\n                # ignore progress bar update errors\n                pass\n\n        # end epoch: flush remaining grads if any\n        if accumulated_steps > 0:\n            try:\n                if optimizer is None:\n                    # Cannot flush without optimizer: just zero gradients and log\n                    try:\n                        model.zero_grad(set_to_none=True)\n                    except Exception:\n                        for p in model.parameters():\n                            if p.grad is not None:\n                                p.grad = None\n                    print(\"[EPOCH-FLUSH] Skipped flush because optimizer is None.\")\n                else:\n                    if scaler_enabled(scaler):\n                        try:\n                            scaler.unscale_(optimizer)\n                        except Exception:\n                            pass\n                        try:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                        except Exception:\n                            pass\n                        try:\n                            scaler.step(optimizer)\n                            scaler.update()\n                        except Exception:\n                            try:\n                                optimizer.step()\n                            except Exception:\n                                raise\n                    else:\n                        try:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), _GRAD_CLIP_NORM)\n                        except Exception:\n                            pass\n                        optimizer.step()\n                    try:\n                        optimizer.zero_grad(set_to_none=True)\n                    except Exception:\n                        for p in model.parameters():\n                            if p.grad is not None:\n                                p.grad.detach_()\n                                p.grad.zero_()\n                    training_stats[\"optimizer_updates\"] += 1\n            except Exception as e:\n                print(f\"[EPOCH-FLUSH] Exception on epoch flush: {type(e).__name__}: {str(e)[:200]}\")\n            finally:\n                accumulated_steps = 0\n\n        epoch_duration_min = (time.time() - epoch_start) / 60.0\n        processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n        expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n        success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n        cluster_count = _get_cluster_count(model)\n\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"Epoch {epoch} summary:\")\n        print(f\"  duration (min): {epoch_duration_min:.2f}\")\n        print(f\"  optimizer updates: {training_stats['optimizer_updates']}\")\n        print(f\"  batches processed: {training_stats['batches_processed']} (processed={processed_batches}, skipped={training_stats['skipped_batches']})\")\n        print(f\"  success rate (updates/expected): {success_rate:.1f}%\")\n        print(f\"  clustered token types: {cluster_count}\")\n        if training_stats[\"total_loss\"]:\n            print(f\"  avg forward loss: {float(np.mean(training_stats['total_loss'])):.6f}\")\n        if skip_reasons:\n            print(\"  skip reasons:\")\n            for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]):\n                print(f\"    - {k}: {v}\")\n        print(\"=\" * 80)\n\n        # save checkpoint at epoch end\n        try:\n            save_checkpoint(model, optimizer, training_stats, epoch, global_step, epoch_losses)\n        except Exception as e:\n            print(f\"[CHECKPOINT] Save at epoch end failed: {type(e).__name__}: {str(e)[:200]}\")\n\n    print(\"\\n[TRAIN] Training completed\")\n    processed_batches = training_stats[\"batches_processed\"] - training_stats[\"skipped_batches\"]\n    expected_updates = max(1, math.floor(processed_batches / max(1, accumulation_steps)))\n    success_rate = 100.0 * training_stats[\"optimizer_updates\"] / expected_updates if expected_updates > 0 else 0.0\n    print(f\"[TRAIN] Success Rate (updates/expected): {success_rate:.1f}%\")\n    print(f\"[TRAIN] Batches processed={processed_batches} skipped={training_stats['skipped_batches']}\")\n    print(f\"[TRAIN] Clustered Token Types: {_get_cluster_count(model)}\")\n    return model\n\n\nprint(\"\\n✅ Cell 7: Training loop ready (patched & hardened)\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:08.375556Z","iopub.execute_input":"2025-11-22T14:56:08.375942Z","iopub.status.idle":"2025-11-22T14:56:08.447378Z","shell.execute_reply.started":"2025-11-22T14:56:08.375925Z","shell.execute_reply":"2025-11-22T14:56:08.446679Z"},"id":"coTb4Fi4H4J4","trusted":true},"outputs":[{"name":"stdout","text":"\n✅ Cell 7: Training loop ready (patched & hardened)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8 (patched): INFERENCE + PIPELINE (HARDENED + FIXED)\n# ==============================================================================\n# Fixes applied (line-by-line hardening & behavioral fixes):\n#  - Robust global fallback reads via globals().get(...) to avoid NameError\n#  - BatchEncoding.to(device) guarded and per-tensor fallback to ensure tensors moved safely\n#  - Corrected subword detection: tokens that start with '▁' are WORD STARTS (not subwords)\n#    (previous logic incorrectly treated '▁' as subword prefix)\n#  - Stronger filtering logic: remove tokens that are punctuation/short fragments,\n#    strip SentencePiece markers before length tests, and optionally normalize via normalize_bn_word\n#  - Safer mbart.generate invocation with progressive fallbacks:\n#      * reduced max_length and beams on OOM\n#      * per-sentence smaller-enc fallback\n#      * reliable restore of mbart.config.use_cache\n#  - Decoding handles tensor shapes (1D/2D) and list/tuple results\n#  - _extract_dscd_outputs made more permissive and defensive: looks for nested dicts and list-contained dicts\n#  - _get_explanations_list normalizes many shapes into list-of-lists\n#  - All exception paths respect VERBOSE_LOGGING; stack traces only when verbose\n#  - Prefer normalize_bn_word (if provided by bn_normalizer cell) to canonicalize tokens used for filtering/display\n#  - Ensure model.eval() is set for generation; keep TRG in eval externally (model wrapper should have set it)\n#  - Return structure consistent and stable even on internal errors\n# ==============================================================================\nimport os\nimport time\nimport math\nimport traceback\nfrom typing import List, Dict, Any, Tuple, Optional\n\nimport numpy as np\nimport torch\n\n# Local fallbacks (read from Cell 0 if available)\n_BN_LANG = globals().get(\"BN_LANG\", \"bn\")\n_EN_LANG = globals().get(\"EN_LANG\", \"en\")\n_MAX_LENGTH = int(globals().get(\"MAX_LENGTH\", 48))\n_DEVICE = globals().get(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n_USE_MULTI_GPU = bool(globals().get(\"USE_MULTI_GPU\", torch.cuda.is_available() and torch.cuda.device_count() > 1))\n\n# Real ambiguity thresholds (defaults safe)\n_REAL_AMB_SPAN_THRESHOLD = float(globals().get(\"SPAN_THRESHOLD\", 0.3))\n_REAL_AMB_UNCERTAINTY_THRESHOLD = float(globals().get(\"TAU_LOW\", 0.4))\n\n# Optional canonicalizer from bn_normalizer\n_normalize_fn = globals().get(\"normalize_bn_word\", None)\n\n# ------------------------------------------------------------------------------\n# Helpers\n# ------------------------------------------------------------------------------\ndef _to_device_batch(enc: Any, device: torch.device):\n    \"\"\"\n    Move tokenizer output to device. Prefer BatchEncoding.to(device) if present.\n    Otherwise, move any tensor values in the dict to device.\n    Returns a dict-like object with tensor values on the requested device.\n    \"\"\"\n    try:\n        # HF BatchEncoding has .to(device)\n        if hasattr(enc, \"to\") and callable(getattr(enc, \"to\")):\n            try:\n                return enc.to(device)\n            except Exception:\n                if _VERBOSE_LOGGING:\n                    print(\"[CELL8] BatchEncoding.to() raised; falling back to per-tensor move\")\n    except Exception:\n        pass\n\n    # fallback: assume mapping of key -> tensor\n    out = {}\n    try:\n        for k, v in dict(enc).items():\n            try:\n                if isinstance(v, torch.Tensor):\n                    out[k] = v.to(device)\n                else:\n                    out[k] = v\n            except Exception:\n                out[k] = v\n        return out\n    except Exception:\n        # last-resort: return input unchanged\n        if _VERBOSE_LOGGING:\n            print(\"[CELL8] _to_device_batch fallback failed; returning original enc\")\n        return enc\n\n\ndef _extract_dscd_outputs(raw_out: Any) -> Dict[str, Any]:\n    \"\"\"\n    Accept many possible model forward outputs and return a dict that contains DSCD/TRG outputs.\n    Heuristics:\n      - If dict and contains common DSCD keys -> return (or nested dict value)\n      - If list/tuple, search for a dict element that looks like DSCD outputs\n    \"\"\"\n    if raw_out is None:\n        return {}\n\n    # If it's already a dict with DSCD-like keys, prefer that\n    if isinstance(raw_out, dict):\n        # common nested keys\n        for key in (\"dscd_outputs\", \"dscd\", \"dscd_out\", \"dscd_outputs_cpu\"):\n            v = raw_out.get(key, None)\n            if isinstance(v, dict):\n                return v\n        # if the dict itself contains proto_probs/explanations\n        if any(k in raw_out for k in (\"proto_probs\", \"explanations\", \"span_preds\", \"uncertainties\", \"trg_explanations\")):\n            return raw_out\n        # sometimes inside 'outputs' / 'result' fields\n        for key in (\"outputs\", \"result\", \"result_dict\"):\n            v = raw_out.get(key, None)\n            if isinstance(v, dict) and any(k in v for k in (\"proto_probs\", \"explanations\", \"span_preds\", \"uncertainties\")):\n                return v\n        # best-effort: return raw_out (caller will handle missing keys)\n        return raw_out\n\n    # If list/tuple, search for a dict inside\n    if isinstance(raw_out, (list, tuple)):\n        for item in raw_out:\n            if isinstance(item, dict):\n                sub = _extract_dscd_outputs(item)\n                if sub:\n                    return sub\n    # otherwise unknown shape\n    return {}\n\n\ndef _get_explanations_list(dscd: Dict[str, Any]) -> List[List[Dict[str, Any]]]:\n    \"\"\"\n    Normalize various 'explanations' layouts into a list-of-lists where each outer entry\n    corresponds to a sentence.\n    Accepts:\n      - explanations: [ {..}, {..} ]  -> wrapped -> [ [..] ]\n      - explanations: [ [ {..}, ... ], [ ... ] ] -> returned as-is\n      - explanations: dict keyed by sentence idx -> converted to ordered list if possible\n    \"\"\"\n    if not dscd:\n        return []\n    expl = None\n    for k in (\"explanations\", \"trg_explanations\", \"explanations_per_sentence\", \"exps\", \"explanations_list\"):\n        if k in dscd:\n            expl = dscd[k]\n            break\n    if expl is None:\n        return []\n\n    # list-of-lists -> pass through\n    if isinstance(expl, list):\n        if len(expl) == 0:\n            return []\n        if isinstance(expl[0], list):\n            return expl\n        # list-of-dicts -> treat as single sentence\n        if isinstance(expl[0], dict):\n            return [expl]\n    # if dict keyed by sentence index\n    if isinstance(expl, dict):\n        try:\n            # try numeric keys first\n            numeric_keys = sorted((int(k) for k in expl.keys() if str(k).isdigit()))\n            if numeric_keys:\n                out = []\n                for nk in numeric_keys:\n                    v = expl.get(str(nk), expl.get(nk))\n                    if isinstance(v, list):\n                        out.append(v)\n                    elif isinstance(v, dict):\n                        out.append([v])\n                if out:\n                    return out\n        except Exception:\n            pass\n    return []\n\n\ndef _is_subword_token(token: Optional[str]) -> bool:\n    \"\"\"\n    Heuristic for detecting subword tokens/fragments to filter.\n    Important: SentencePiece uses '▁' to mark word-start. Tokens that START with '▁'\n    are word-beginnings and should NOT be treated as subword fragments.\n    Treat '##' and '@@' as continuation markers (subword fragments).\n    \"\"\"\n    if token is None:\n        return True\n    t = str(token).strip()\n    if t == \"\":\n        return True\n    # continuation markers (BPE style)\n    if t.startswith(\"##\") or t.startswith(\"@@\"):\n        return True\n    # SentencePiece word start marker -> NOT subword\n    if t.startswith(\"▁\"):\n        return False\n    # short fragments (after stripping leading markers)\n    clean = t.lstrip(\"▁\").lstrip(\"Ġ\").replace(\"</w>\", \"\").strip()\n    if len(clean) < 2:\n        return True\n    # punctuation-only or digit-only\n    if all(ch in '.,!?;:()[]{}\"\\'-—–/\\\\' for ch in clean):\n        return True\n    if clean.isdigit():\n        return True\n    return False\n\n\ndef _should_filter_explanation(expl: Dict[str, Any], span_th: float, u_th: float) -> bool:\n    \"\"\"\n    Return True if an explanation should be filtered out because it is low-quality.\n    Filter if:\n      - token is subword/empty/punctuation\n      - BOTH span <= span_th and uncertainty <= u_th (i.e., not enough signal)\n    \"\"\"\n    try:\n        token_raw = expl.get(\"token\", \"\") or expl.get(\"ambiguous_word\", \"\") or expl.get(\"token_value\", \"\")\n        # prefer token field that exists; sanitize\n        token = str(token_raw)\n        # remove SPM markers for length check\n        token_clean = token.lstrip(\"▁\").lstrip(\"Ġ\").replace(\"</w>\", \"\").strip()\n        # canonicalize if normalizer available (helps group inflected forms)\n        if _normalize_fn and token_clean:\n            try:\n                token_clean = _normalize_fn(token_clean)\n            except Exception:\n                pass\n        # filter tiny/punct tokens\n        if not token_clean or len(token_clean) < 2 or all(ch in '.,!?;:()[]{}\"\\'-—–/\\\\' for ch in token_clean):\n            return True\n\n        span = float(expl.get(\"span\", 0.0) or 0.0)\n        uncertainty = float(expl.get(\"uncertainty\", 0.0) or 0.0)\n\n        # If both metrics are below thresholds, filter out\n        if span <= span_th and uncertainty <= u_th:\n            return True\n        return False\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        return True\n\n\ndef _force_english_bos(tokenizer, mbart_model) -> Optional[int]:\n    \"\"\"\n    Try to determine English forced BOS id for tokenizer and set it in mbart_model.config.\n    Return the forced_id or None.\n    \"\"\"\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            forced_id = tokenizer.get_lang_id(_EN_LANG)\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            forced_id = tokenizer.lang_code_to_id.get(_EN_LANG, None)\n    except Exception:\n        forced_id = None\n\n    if forced_id is not None and hasattr(mbart_model, \"config\"):\n        try:\n            mbart_model.config.forced_bos_token_id = int(forced_id)\n            mbart_model.config.decoder_start_token_id = int(forced_id)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL8] Could not set forced_bos_token_id on mbart config\")\n    return forced_id\n\n\n# ------------------------------------------------------------------------------\n# translate_with_explanations\n# ------------------------------------------------------------------------------\ndef translate_with_explanations(\n    model,\n    tokenizer,\n    input_sentence: str,\n    device: Optional[torch.device] = None,\n    span_threshold: Optional[float] = None,\n    uncertainty_threshold: Optional[float] = None,\n) -> Dict[str, Any]:\n    device = _DEVICE if device is None else device\n    span_th = _REAL_AMB_SPAN_THRESHOLD if span_threshold is None else float(span_threshold)\n    u_th = _REAL_AMB_UNCERTAINTY_THRESHOLD if uncertainty_threshold is None else float(uncertainty_threshold)\n\n    try:\n        # prepare encoding\n        try:\n            # Some tokenizers use src_lang attribute (M2M100)\n            if hasattr(tokenizer, \"src_lang\"):\n                try:\n                    setattr(tokenizer, \"src_lang\", _BN_LANG)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n        enc = tokenizer(\n            input_sentence,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=_MAX_LENGTH\n        )\n        enc = _to_device_batch(enc, device)\n\n        # ensure model in eval\n        model.eval()\n        core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n\n        # Attempt to get DSCD/TRG outputs via forward_with_explanations or forward\n        raw_dscd_out = {}\n        try:\n            with torch.inference_mode():\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        raw_dscd_out = core.forward_with_explanations(\n                            input_ids=enc.get(\"input_ids\"),\n                            attention_mask=enc.get(\"attention_mask\"),\n                            src_texts=[input_sentence]\n                        )\n                    except TypeError:\n                        # fallback positional argument order\n                        raw_dscd_out = core.forward_with_explanations(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), [input_sentence])\n                else:\n                    # try generic forward and extract DSCD outputs\n                    try:\n                        out = core.forward(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"), src_texts=[input_sentence], labels=None)\n                    except TypeError:\n                        out = core.forward(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), [input_sentence], None)\n                    raw_dscd_out = _extract_dscd_outputs(out)\n        except Exception as e:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL8] DSCD/TRG forward error:\", str(e))\n                traceback.print_exc()\n            raw_dscd_out = {}\n\n        # Prepare mbart.generate (if available)\n        translation = \"\"\n        mbart_obj = getattr(core, \"mbart\", None)\n        if mbart_obj is None:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL8] core.mb is missing .mbart -> skipping generation\")\n            translation = \"\"\n        else:\n            forced_id = _force_english_bos(tokenizer, mbart_obj)\n            orig_use_cache = None\n            try:\n                if hasattr(mbart_obj, \"config\"):\n                    orig_use_cache = getattr(mbart_obj.config, \"use_cache\", None)\n                    mbart_obj.config.use_cache = True\n            except Exception:\n                orig_use_cache = None\n\n            generated = None\n            try:\n                try:\n                    # primary generation call\n                    pad_id = getattr(tokenizer, \"pad_token_id\", None) or getattr(tokenizer, \"eos_token_id\", None) or 1\n                    generated = mbart_obj.generate(\n                        enc.get(\"input_ids\"),\n                        attention_mask=enc.get(\"attention_mask\"),\n                        max_length=min(_MAX_LENGTH, 64),\n                        num_beams=2,\n                        early_stopping=True,\n                        pad_token_id=int(pad_id),\n                        forced_bos_token_id=forced_id if forced_id is not None else getattr(mbart_obj.config, \"forced_bos_token_id\", None),\n                    )\n                except RuntimeError as gen_err:\n                    # handle OOM by trying more conservative generation options\n                    if \"out of memory\" in str(gen_err).lower():\n                        if torch.cuda.is_available():\n                            try:\n                                torch.cuda.empty_cache()\n                            except Exception:\n                                pass\n                        try:\n                            small_enc = tokenizer(input_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=min(_MAX_LENGTH, 48))\n                            small_enc = _to_device_batch(small_enc, device)\n                            pad_id = getattr(tokenizer, \"pad_token_id\", None) or getattr(tokenizer, \"eos_token_id\", None) or 1\n                            generated = mbart_obj.generate(\n                                small_enc.get(\"input_ids\"),\n                                attention_mask=small_enc.get(\"attention_mask\"),\n                                max_length=min(_MAX_LENGTH, 48),\n                                num_beams=1,\n                                early_stopping=True,\n                                pad_token_id=int(pad_id),\n                                forced_bos_token_id=forced_id if forced_id is not None else getattr(mbart_obj.config, \"forced_bos_token_id\", None),\n                            )\n                        except Exception as e2:\n                            if _VERBOSE_LOGGING:\n                                print(\"[CELL8] fallback generation also failed:\", str(e2))\n                                traceback.print_exc()\n                            generated = None\n                    else:\n                        # other runtime error -> re-raise to outer handler\n                        raise\n            finally:\n                # restore original cache setting\n                try:\n                    if hasattr(mbart_obj, \"config\") and orig_use_cache is not None:\n                        mbart_obj.config.use_cache = orig_use_cache\n                except Exception:\n                    pass\n\n            # decode translation safely\n            if generated is not None:\n                try:\n                    if isinstance(generated, (list, tuple)):\n                        translation = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n                    elif isinstance(generated, torch.Tensor):\n                        # generated may be shape (1, L) or (N, L)\n                        if generated.dim() == 2:\n                            translation = tokenizer.decode(generated[0], skip_special_tokens=True)\n                        else:\n                            translation = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n                    else:\n                        translation = str(generated)\n                except Exception:\n                    try:\n                        translation = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            print(\"[CELL8] decode failed for generated; returning empty translation\")\n                        translation = \"\"\n            else:\n                translation = \"\"\n\n        # Process DSCD/TRG explanations\n        dscd_out = _extract_dscd_outputs(raw_dscd_out)\n        explanations_list = _get_explanations_list(dscd_out)\n        sentence_explanations = explanations_list[0] if (isinstance(explanations_list, list) and len(explanations_list) > 0) else []\n\n        real_amb_count = 0\n        out_explanations: List[Dict[str, Any]] = []\n        if isinstance(sentence_explanations, list):\n            for ex in sentence_explanations:\n                try:\n                    if _should_filter_explanation(ex, span_th, u_th):\n                        continue\n                    s_val = float(ex.get(\"span\", 0.0) or 0.0)\n                    u_val = float(ex.get(\"uncertainty\", 0.0) or 0.0)\n                    is_real = (s_val > span_th) or (u_val > u_th)\n                    if is_real:\n                        real_amb_count += 1\n                    # canonical ambiguous token for output: try several keys and clean markers\n                    raw_tok = ex.get(\"token\") or ex.get(\"ambiguous_word\") or ex.get(\"token_value\") or \"\"\n                    tok_str = str(raw_tok)\n                    tok_clean = tok_str.lstrip(\"▁\").lstrip(\"Ġ\").replace(\"</w>\", \"\").strip()\n                    if _normalize_fn and tok_clean:\n                        try:\n                            tok_clean = _normalize_fn(tok_clean)\n                        except Exception:\n                            pass\n\n                    out_explanations.append({\n                        \"ambiguous_word\": tok_clean,\n                        \"position\": ex.get(\"token_idx\", ex.get(\"position\", \"N/A\")),\n                        \"explanation\": ex.get(\"explanation\", \"\") or ex.get(\"explain\", \"\") or ex.get(\"text\", \"\") or \"\",\n                        \"uncertainty\": float(u_val),\n                        \"span\": float(s_val),\n                        \"is_real_amb\": bool(is_real),\n                    })\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    continue\n\n        result = {\n            \"input_sentence\": input_sentence,\n            \"translation\": translation,\n            \"ambiguous_words_detected\": int(real_amb_count),\n            \"explanations\": out_explanations,\n        }\n        return result\n\n    except Exception as e:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        return {\n            \"input_sentence\": input_sentence,\n            \"translation\": \"\",\n            \"ambiguous_words_detected\": 0,\n            \"explanations\": [],\n            \"error\": str(e)[:200],\n        }\n\n\n# ------------------------------------------------------------------------------\n# demonstrate_system: small runner that prints nicely\n# ------------------------------------------------------------------------------\ndef demonstrate_system(model, tokenizer, sentences: Optional[List[str]] = None):\n    if sentences is None:\n        sentences = [\n            \"আমি কল বন্ধ করেছি।\",\n            \"কাল আমি বই কিনব।\",\n            \"পাতা ঝরে পড়েছে।\",\n            \"তিনি ব্যাংক গেছেন।\",\n            \"আজ ভাল আবহাওয়া।\",\n        ]\n    print(\"=\" * 80)\n    print(\"TATN DEMO: translating and listing DSCD/TRG explanations\")\n    print(\"=\" * 80)\n    for s in sentences:\n        print(f\"\\nInput: {s}\")\n        res = translate_with_explanations(model, tokenizer, s)\n        print(\"Translation:\", res.get(\"translation\", \"\"))\n        print(\"Ambiguous words detected (real):\", res.get(\"ambiguous_words_detected\", 0))\n        if res.get(\"explanations\"):\n            for idx, ex in enumerate(res[\"explanations\"], 1):\n                print(f\"  {idx}. word='{ex['ambiguous_word']}' pos={ex['position']} span={ex['span']:.3f} U={ex['uncertainty']:.3f} real={ex['is_real_amb']}\")\n                print(\"     \", (ex.get(\"explanation\") or \"\")[:200])\n        else:\n            print(\"  No explanations\")\n    print(\"=\" * 80)\n\n\n# ------------------------------------------------------------------------------\n# dscd_discovery_warmup: warm-up helper (kept for convenience)\n# ------------------------------------------------------------------------------\ndef dscd_discovery_warmup(model, tokenizer, num_sents: int = 8000, batch_size: int = 64, max_len: Optional[int] = None):\n    if max_len is None:\n        max_len = _MAX_LENGTH\n\n    core = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    dscd = getattr(core, \"dscd\", None)\n    if dscd is None:\n        print(\"[WARMUP] No DSCD attached to model; skipping.\")\n        return\n\n    print(\"[WARMUP] Starting DSCD discovery warmup...\")\n    orig_enable = getattr(dscd, \"enable_training_clustering\", False)\n    orig_n_min = getattr(dscd, \"n_min\", None)\n    orig_buffer = getattr(dscd, \"buffer_size\", None)\n\n    try:\n        dscd.enable_training_clustering = True\n        dscd.n_min = max(3, int(getattr(dscd, \"n_min\", 5)))\n        dscd.buffer_size = max(200, int(getattr(dscd, \"buffer_size\", 300)))\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    texts = []\n    try:\n        if \"load_and_preprocess_optimized\" in globals():\n            pairs = load_and_preprocess_optimized(num_sents)\n            texts = [bn for (bn, _) in pairs][:num_sents]\n        else:\n            base = [\"আমি কল বন্ধ করেছি।\", \"কাল আমি বই কিনব।\", \"পাতা ঝরে পড়েছে।\", \"তিনি ব্যাংক গেছেন।\"]\n            while len(texts) < num_sents:\n                texts.extend(base)\n            texts = texts[:num_sents]\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        texts = [\"আমি কল বন্ধ করেছি।\"] * num_sents\n\n    processed = 0\n    core.eval()\n    with torch.inference_mode():\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            try:\n                enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n                enc = _to_device_batch(enc, _DEVICE)\n                if hasattr(core, \"forward_with_explanations\"):\n                    try:\n                        core.forward_with_explanations(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"), src_texts=batch)\n                    except TypeError:\n                        core.forward_with_explanations(enc.get(\"input_ids\"), enc.get(\"attention_mask\"), batch)\n                else:\n                    try:\n                        if hasattr(core, \"mbart\") and hasattr(core.mbart.model, \"encoder\"):\n                            core.mbart.model.encoder(input_ids=enc.get(\"input_ids\"), attention_mask=enc.get(\"attention_mask\"))\n                    except Exception:\n                        pass\n                processed += len(batch)\n                if _VERBOSE_LOGGING and ((i // batch_size) % 10 == 0):\n                    print(f\"[WARMUP] processed {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%)\")\n            except Exception as e:\n                if _VERBOSE_LOGGING:\n                    print(\"[WARMUP] batch failed:\", str(e))\n                    traceback.print_exc()\n                continue\n\n    try:\n        stores = getattr(dscd, \"prototype_stores\", {}) or {}\n        num_types = len(stores)\n        total_protos = sum(store.size() for store in stores.values()) if stores else 0\n        multi = sum(1 for store in stores.values() if store.size() >= 2) if stores else 0\n        print(f\"[WARMUP] Prototype discovery: word_types={num_types}, total_protos={total_protos}, multi_sense={multi}\")\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n    finally:\n        try:\n            dscd.enable_training_clustering = orig_enable\n            if orig_n_min is not None:\n                dscd.n_min = orig_n_min\n            if orig_buffer is not None:\n                dscd.buffer_size = orig_buffer\n            print(\"[WARMUP] Restored DSCD configuration\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n\n# End of Cell 8\nprint(\"✅ Cell 8: Inference pipeline & warmup helpers loaded (patched and hardened)\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:08.448315Z","iopub.execute_input":"2025-11-22T14:56:08.448607Z","iopub.status.idle":"2025-11-22T14:56:08.498656Z","shell.execute_reply.started":"2025-11-22T14:56:08.448590Z","shell.execute_reply":"2025-11-22T14:56:08.498102Z"},"id":"7Dxg7ck0H4J5","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 8: Inference pipeline & warmup helpers loaded (patched and hardened)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==============================================================================\n# CELL 9 (patched): COMPREHENSIVE TESTING & EVALUATION (MULTI-GPU OPTIMIZED)\n# DEBUGGED, HARDENED, and MORE DEFENSIVE\n# ==============================================================================\n# Fixes applied (line-by-line highlights):\n#  - Robust global lookups via globals().get(...) with safe defaults.\n#  - Defensive handling when model is DataParallel / wrapped / None.\n#  - Guarded access to DSCD internals (prototype_stores) and safe numeric conversions.\n#  - Resilient use of translate_with_explanations (handles missing function gracefully).\n#  - Added normalization (normalize_bn_word) for printed tokens when available.\n#  - Clearer cluster-stat printing and protections for empty/no-prototype cases.\n#  - Wrapped all per-test calls with try/except so one failing test doesn't abort evaluation.\n#  - Ensured all numeric computations guard division-by-zero and invalid types.\n#  - Useful verbose logging controlled by VERBOSE_LOGGING; debug traces only when enabled.\n# ==============================================================================\nfrom typing import Dict, List, Tuple, Optional, Any\nimport torch\nimport traceback\nimport math\n\n# Robust reads from globals (Cell 0)\n_USE_MULTI_GPU = bool(globals().get(\"USE_MULTI_GPU\", torch.cuda.is_available() and torch.cuda.device_count() > 1))\n_BN_LANG = str(globals().get(\"BN_LANG\", \"bn\"))\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n\n# thresholds fallback consistent with earlier cells\n_SPAN_THRESHOLD = float(globals().get(\"SPAN_THRESHOLD\", 0.3))\n_UNCERTAINTY_THRESHOLD = float(globals().get(\"TAU_LOW\", 0.4))\n\n# optional normalizer\n_normalize_fn = globals().get(\"normalize_bn_word\", None)\n\n\n# ---------\n# Cluster analysis helpers (defensive)\n# ---------\ndef _get_cluster_count(model: torch.nn.Module) -> int:\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else getattr(model, \"dscd\", None)\n        stores = getattr(dscd, \"prototype_stores\", None) if dscd is not None else None\n        if not stores:\n            return 0\n        return len(stores)\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        return 0\n\n\ndef _print_top_clusters(model: torch.nn.Module, top_n: int = 5):\n    \"\"\"\n    Print top N clusters by sample count (homographs discovered by DSCD).\n    Defensive against missing attributes and types.\n    \"\"\"\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else getattr(model, \"dscd\", None)\n        prototype_stores = getattr(dscd, \"prototype_stores\", None) or {}\n        if not prototype_stores:\n            print(\"[CLUSTER] No clusters found yet\")\n            return\n\n        cluster_info = []\n        for token, store in prototype_stores.items():\n            try:\n                total_count = int(sum(getattr(store, \"counts\", []) or []))\n            except Exception:\n                total_count = 0\n            try:\n                n_protos = int(store.size()) if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []) or [])\n            except Exception:\n                n_protos = 0\n            mu = float(getattr(store, \"mu\", 0.0) or 0.0)\n            tau = float(getattr(store, \"tau\", 0.0) or 0.0)\n            cluster_info.append({\n                \"token\": token,\n                \"count\": total_count,\n                \"protos\": n_protos,\n                \"mu\": mu,\n                \"tau\": tau\n            })\n\n        cluster_info.sort(key=lambda x: x[\"count\"], reverse=True)\n\n        display_n = min(top_n, len(cluster_info))\n        print(f\"\\n[CLUSTER] Top {display_n} clusters (by sample count):\")\n        print(\"-\" * 90)\n        print(f\"{'Rank':<6}{'Token':<18}{'Count':<12}{'Protos':<10}{'μ (mean)':<15}{'τ (dev)':<12}\")\n        print(\"-\" * 90)\n        for rank, info in enumerate(cluster_info[:display_n], 1):\n            tstr = str(info[\"token\"])\n            token_display = (tstr[:15] + \"..\") if len(tstr) > 17 else tstr\n            print(f\"{rank:<6}{token_display:<18}{info['count']:<12}{info['protos']:<10}{info['mu']:<15.6f}{info['tau']:<12.6f}\")\n        print(\"-\" * 90)\n        total_samples = sum(c[\"count\"] for c in cluster_info)\n        print(f\"Total clusters: {len(cluster_info)} | Total samples in clusters: {total_samples}\")\n    except Exception as e:\n        print(f\"[CLUSTER] Error: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n\ndef _print_cluster_stats(model: torch.nn.Module):\n    \"\"\"\n    Aggregate cluster statistics: totals and simple distribution values.\n    \"\"\"\n    try:\n        dscd = model.module.dscd if hasattr(model, \"module\") else getattr(model, \"dscd\", None)\n        prototype_stores = getattr(dscd, \"prototype_stores\", None) or {}\n        if not prototype_stores:\n            if _VERBOSE_LOGGING:\n                print(\"[CLUSTER-STATS] No prototype stores.\")\n            return\n\n        total_clusters = len(prototype_stores)\n        total_samples = 0\n        total_protos = 0\n        cluster_counts = []\n        for token, store in prototype_stores.items():\n            try:\n                cnt = int(sum(getattr(store, \"counts\", []) or []))\n            except Exception:\n                cnt = 0\n            protos = int(store.size()) if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []) or [])\n            total_samples += cnt\n            total_protos += protos\n            cluster_counts.append(cnt)\n\n        avg_samples = (total_samples / total_clusters) if total_clusters > 0 else 0.0\n        avg_protos = (total_protos / total_clusters) if total_clusters > 0 else 0.0\n        max_samples = max(cluster_counts) if cluster_counts else 0\n        min_samples = min(cluster_counts) if cluster_counts else 0\n\n        print(\"\\n[CLUSTER-STATS] Cluster Statistics:\")\n        print(f\"  • Total clusters: {total_clusters}\")\n        print(f\"  • Total samples: {total_samples}\")\n        print(f\"  • Total prototypes: {total_protos}\")\n        print(f\"  • Avg samples/cluster: {avg_samples:.1f}\")\n        print(f\"  • Avg protos/cluster: {avg_protos:.1f}\")\n        print(f\"  • Max samples/cluster: {max_samples}\")\n        print(f\"  • Min samples/cluster: {min_samples}\")\n    except Exception as e:\n        print(f\"[CLUSTER-STATS] Error: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n\n# ----------------------------\n# Evaluation routine\n# ----------------------------\n@torch.inference_mode()\ndef comprehensive_post_training_testing(model: torch.nn.Module, tokenizer) -> Dict[str, Any]:\n    \"\"\"\n    Compact comprehensive evaluation:\n      - Translate curated Bengali sentences\n      - Count detected ambiguous tokens (real ambiguity: span>_SPAN_THRESHOLD or uncertainty>_UNCERTAINTY_THRESHOLD)\n      - Print explanations and DSCD prototype stats\n      - Optionally run small DSCD warmup if no prototypes and helper exists\n    Returns aggregated metrics dict.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPREHENSIVE POST-TRAINING EVALUATION (Cell 9)\")\n    print(\"=\" * 80)\n\n    test_sentences: List[Tuple[str, str]] = [\n        (\"আমি কল বন্ধ করেছি।\", \"কল = tap / call\"),\n        (\"কাল আমি বই কিনব।\", \"কাল = tomorrow / yesterday\"),\n        (\"পাতা ঝরে পড়েছে।\", \"পাতা = leaf / page\"),\n        (\"তিনি ব্যাংক গেছেন।\", \"ব্যাংক = bank / embankment\"),\n        (\"আজ ভাল আবহাওয়া।\", \"Simple sentence (no ambiguity expected)\"),\n    ]\n\n    # prefer underlying core if DataParallel wrapping was used\n    core_model = model.module if (_USE_MULTI_GPU and hasattr(model, \"module\")) else model\n    try:\n        core_model.eval()\n    except Exception:\n        pass\n\n    # If DSCD has no prototypes and warmup helper exists, run a shorter warmup (best-effort)\n    try:\n        dscd = getattr(core_model, \"dscd\", None)\n        stores = getattr(dscd, \"prototype_stores\", None) if dscd is not None else None\n        # only run warmup if no prototypes at all and warmup helper available\n        if (not stores or len(stores) == 0) and \"dscd_discovery_warmup\" in globals():\n            try:\n                print(\"[EVAL] No DSCD prototypes found. Running moderate warmup (num_sents=2000)...\")\n                # run a modest warmup to seed prototypes (user can skip if heavy)\n                dscd_discovery_warmup(core_model, tokenizer, num_sents=2000, batch_size=64)\n            except Exception as e:\n                print(f\"[EVAL] DSCD warmup failed/skipped: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n    except Exception:\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    # Metrics\n    total_tests = len(test_sentences)\n    successful_translations = 0\n    total_explanations = 0\n    total_high_span = 0\n    total_real_ambiguous = 0\n\n    print(f\"\\n[EVAL] Running {total_tests} tests...\")\n    print(\"-\" * 80)\n\n    # Configure tokenizer if it supports src_lang attribute\n    try:\n        tokenizer.src_lang = _BN_LANG\n    except Exception:\n        pass\n\n    def _is_real_amb(expl: Dict[str, Any]) -> bool:\n        try:\n            s = float(expl.get(\"span\", 0.0) or 0.0)\n            u = float(expl.get(\"uncertainty\", 0.0) or 0.0)\n            return (s > _SPAN_THRESHOLD) or (u > _UNCERTAINTY_THRESHOLD)\n        except Exception:\n            return False\n\n    # iterate tests\n    for idx, (src_text, desc) in enumerate(test_sentences, 1):\n        print(f\"\\nTest {idx}/{total_tests}: {desc}\")\n        print(\"=\" * 60)\n        try:\n            if \"translate_with_explanations\" not in globals():\n                print(\"[EVAL] translate_with_explanations not available; skipping this test.\")\n                continue\n\n            try:\n                # call the inference wrapper - pass core_model (best-effort)\n                result = translate_with_explanations(core_model if core_model is not None else model, tokenizer, src_text)\n            except Exception as e:\n                print(f\"[EVAL] translate_with_explanations raised: {type(e).__name__}: {str(e)[:200]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                result = {\"translation\": \"\", \"ambiguous_words_detected\": 0, \"explanations\": []}\n\n            translation = str(result.get(\"translation\", \"\") or \"\")\n            try:\n                amb_count = int(result.get(\"ambiguous_words_detected\", 0) or 0)\n            except Exception:\n                amb_count = 0\n            explanations = result.get(\"explanations\", []) or []\n\n            print(f\"Input: {src_text}\")\n            print(f\"Translation: {translation}\")\n            print(f\"Ambiguous Words (real, counted): {amb_count}\")\n\n            if explanations:\n                print(\"\\nExplanations:\")\n                high_span_local = 0\n                real_amb_local = 0\n                for j, expl in enumerate(explanations, 1):\n                    try:\n                        span_val = float(expl.get(\"span\", 0.0) or 0.0)\n                        u_val = float(expl.get(\"uncertainty\", 0.0) or 0.0)\n                        marker = \"[SPAN>0.3]\" if span_val > _SPAN_THRESHOLD else \"           \"\n                        raw_word = expl.get(\"ambiguous_word\", expl.get(\"token\", \"N/A\"))\n                        word = str(raw_word or \"N/A\")\n                        # normalize for display if possible\n                        try:\n                            if _normalize_fn and isinstance(word, str) and word.strip():\n                                word = _normalize_fn(word)\n                        except Exception:\n                            pass\n                        pos = expl.get(\"position\", expl.get(\"token_idx\", \"N/A\"))\n                        print(f\"  {j}. {marker} '{word}' @ pos {pos}\")\n                        print(f\"       U={u_val:.3f} | S={span_val:.3f}\")\n                        text = str(expl.get(\"explanation\", \"\") or \"\")\n                        if len(text) > 120:\n                            text = text[:120] + \"...\"\n                        print(f\"       {text}\")\n                        if span_val > _SPAN_THRESHOLD:\n                            high_span_local += 1\n                        if _is_real_amb(expl):\n                            real_amb_local += 1\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        continue\n\n                total_explanations += len(explanations)\n                total_high_span += high_span_local\n                total_real_ambiguous += real_amb_local\n            else:\n                print(\"No explanations produced (likely high-confidence translation)\")\n\n            # Consider translation successful if non-empty and not an error sentinel\n            try:\n                bad_sentinels = {\"\", \"Error occurred\", \"Translation generation failed\", \"ERROR DURING TRANSLATION\"}\n                if translation and translation.strip() and translation not in bad_sentinels:\n                    successful_translations += 1\n                    print(\"Translation successful\")\n                else:\n                    print(\"Translation failed or empty\")\n            except Exception:\n                print(\"Translation check encountered an error; counted as failure\")\n\n        except Exception as e:\n            print(f\"[EVAL] Test {idx} failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            continue\n\n        print(\"-\" * 60)\n\n    # DSCD statistics (best-effort)\n    try:\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n        dscd = getattr(core_model, \"dscd\", None)\n        if dscd is not None and hasattr(dscd, \"prototype_stores\"):\n            stores = getattr(dscd, \"prototype_stores\") or {}\n            total_words = 0\n            multi = 0\n            total_protos = 0\n            for key, store in stores.items():\n                try:\n                    sz = int(store.size()) if hasattr(store, \"size\") else len(getattr(store, \"centroids\", []) or [])\n                except Exception:\n                    sz = 0\n                total_words += 1\n                total_protos += sz\n                if sz >= 2:\n                    multi += 1\n            dscd_stats = {\"total_words\": total_words, \"multi_sense_words\": multi, \"total_prototypes\": total_protos}\n        else:\n            dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n    except Exception as e:\n        print(f\"[EVAL] Could not retrieve DSCD stats: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n        dscd_stats = {\"total_words\": 0, \"multi_sense_words\": 0, \"total_prototypes\": 0}\n\n    # Summary\n    print(\"\\n\" + \"=\" * 80)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\" * 80)\n    print(f\"Total tests: {total_tests}\")\n    print(f\"Successful translations: {successful_translations}\")\n    success_rate = (successful_translations / total_tests * 100.0) if total_tests > 0 else 0.0\n    print(f\"Success rate: {success_rate:.1f}%\")\n    print(\"\")\n    print(\"Ambiguity detection:\")\n    print(f\"  - Total explanations produced: {total_explanations}\")\n    print(f\"  - High-span (S>{_SPAN_THRESHOLD}): {total_high_span}\")\n    print(f\"  - Real ambiguous (S>{_SPAN_THRESHOLD} or U>{_UNCERTAINTY_THRESHOLD}): {total_real_ambiguous}\")\n    if total_tests > 0:\n        print(f\"  - Avg explanations/test: {total_explanations / total_tests:.2f}\")\n        print(f\"  - Avg real ambiguous/test: {total_real_ambiguous / total_tests:.2f}\")\n    print(\"\")\n    print(\"DSCD Prototype Discovery:\")\n    print(f\"  - Word types tracked: {dscd_stats.get('total_words', 0)}\")\n    print(f\"  - Multi-sense words (>=2 protos): {dscd_stats.get('multi_sense_words', 0)}\")\n    print(f\"  - Total prototypes: {dscd_stats.get('total_prototypes', 0)}\")\n    if dscd_stats.get(\"total_words\", 0) > 0:\n        avg_protos_word = dscd_stats.get(\"total_prototypes\", 0) / max(1, dscd_stats.get(\"total_words\", 1))\n        print(f\"  - Avg prototypes/word: {avg_protos_word:.2f}\")\n    print(\"=\" * 80)\n\n    return {\n        \"total_tests\": total_tests,\n        \"successful_translations\": successful_translations,\n        \"success_rate_pct\": success_rate,\n        \"total_explanations\": total_explanations,\n        \"total_high_span\": total_high_span,\n        \"total_real_ambiguous\": total_real_ambiguous,\n        \"dscd_stats\": dscd_stats,\n    }\n\n\nprint(\"✅ Cell 9: Comprehensive testing & evaluation ready (debugged + hardened).\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:08.499421Z","iopub.execute_input":"2025-11-22T14:56:08.499593Z","iopub.status.idle":"2025-11-22T14:56:08.534006Z","shell.execute_reply.started":"2025-11-22T14:56:08.499580Z","shell.execute_reply":"2025-11-22T14:56:08.533360Z"},"id":"8uL574F8H4J5","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 9: Comprehensive testing & evaluation ready (debugged + hardened).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==============================================================================\n# CELL 10 (patched): TATN MAIN PIPELINE (DISCOVERY FIXES + HOMOGRAPH VERIFICATION)\n# Debugged, hardened, and self-contained replacement of original Cell 10.\n# ==============================================================================\n# Key behavior:\n#  - Robust, defensive global reads and fallbacks\n#  - Safe tokenizer loading with whitespace fallback if transformers missing\n#  - Minimal dataset fallback when data loading utilities are absent\n#  - Robust DataLoader construction and safe collate fallback\n#  - Conservative, non-forcing clustering limits and clear diagnostics\n#  - Normalized homograph verification (canonical forms) and helpful warnings\n#  - Safe saving of model state\n# ==============================================================================\n\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Optional, Iterable, List\n\nimport gc\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nimport unicodedata\n\n# -------------------------\n# Safe defaults (if Cell 0 not executed)\n# -------------------------\nFREEZE_ENCODER = False\n\ndef _g(name, default):\n    \"\"\"Defensive global getter.\"\"\"\n    return globals().get(name, default)\n\n# Pull globals defensively (fall back to sane defaults)\ntry:\n    _USE_MULTI_GPU = bool(_g(\"USE_MULTI_GPU\", False))\n    _NUM_GPUS = int(_g(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0))\n    _DEVICE = _g(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _BN_LANG = _g(\"BN_LANG\", \"bn\")\n    _EN_LANG = _g(\"EN_LANG\", \"en\")\n    _NUM_SAMPLES = int(_g(\"NUM_SAMPLES\", 30000))\n    _MAX_LENGTH = int(_g(\"MAX_LENGTH\", 48))\n    _BATCH_SIZE = int(_g(\"BATCH_SIZE\", 8))\n    _EPOCHS = int(_g(\"EPOCHS\", 1))\n    _ACCUMULATION_STEPS = int(_g(\"ACCUMULATION_STEPS\", 1))\n    _LR_NMT = float(_g(\"LR_NMT\", 2e-5))\n    _LR_PHI = float(_g(\"LR_PHI\", 1e-5))\n    _ENABLE_ASBN_TRAINING = bool(_g(\"ENABLE_ASBN_TRAINING\", False))\n    _VALIDATION_CHECK_INTERVAL = int(_g(\"VALIDATION_CHECK_INTERVAL\", 0))\n    _DSCD_WARMUP_SAMPLES = int(_g(\"DSCD_WARMUP_SAMPLES\", 8000))\n    _VERBOSE_LOGGING = bool(_g(\"VERBOSE_LOGGING\", False))\n    _HOMOGRAPH_WATCHLIST_BN = set(_g(\"HOMOGRAPH_WATCHLIST_BN\", {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\"}))\nexcept Exception:\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _BN_LANG = \"bn\"\n    _EN_LANG = \"en\"\n    _NUM_SAMPLES = 30000\n    _MAX_LENGTH = 48\n    _BATCH_SIZE = 8\n    _EPOCHS = 1\n    _ACCUMULATION_STEPS = 1\n    _LR_NMT = 2e-5\n    _LR_PHI = 1e-5\n    _ENABLE_ASBN_TRAINING = False\n    _VALIDATION_CHECK_INTERVAL = 0\n    _DSCD_WARMUP_SAMPLES = 8000\n    _VERBOSE_LOGGING = False\n    _HOMOGRAPH_WATCHLIST_BN = {\"কল\", \"কাল\", \"পাতা\", \"ব্যাংক\"}\n\n# DSCD clustering thresholds (defensive)\nDSCD_N_MIN = int(globals().get(\"DSCD_N_MIN\", 5))\nDEFAULT_CLUSTER_MIN_SAMPLES = 20\n_CLUSTER_MIN_SAMPLES = int(globals().get(\"DSCD_MIN_CLUSTER_SAMPLES\", max(DEFAULT_CLUSTER_MIN_SAMPLES, DSCD_N_MIN * 2)))\n\n# -------------------------\n# Helper: Clear GPU caches safely\n# -------------------------\ndef _safe_clear_gpu_caches():\n    try:\n        if \"clear_all_gpu_caches\" in globals():\n            try:\n                clear_all_gpu_caches()\n            except Exception:\n                pass\n            return\n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    with torch.cuda.device(i):\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n# -------------------------\n# Small normalization helpers used for homograph matching\n# -------------------------\ndef _norm_clean_token(tok: Optional[str]) -> str:\n    if tok is None:\n        return \"\"\n    s = str(tok)\n    # remove common subword markers and normalize to NFKC\n    for marker in ('▁', '##', 'Ġ', '@@'):\n        s = s.replace(marker, '')\n    s = s.strip()\n    s = unicodedata.normalize('NFKC', s)\n    return s\n\ndef _token_matches_homograph(token_key: str, homograph: str) -> bool:\n    clean_tok = _norm_clean_token(token_key)\n    clean_h = _norm_clean_token(homograph)\n    if not clean_tok or not clean_h:\n        return False\n    # exact or substring match both ways are considered (conservative)\n    if clean_tok == clean_h:\n        return True\n    if clean_h in clean_tok:\n        return True\n    if clean_tok in clean_h:\n        return True\n    return False\n\n# -------------------------\n# Robust tokenizer loader (lazy imports + helpful errors + fallback)\n# -------------------------\ndef _safe_tokenizer_from_pretrained(model_name: str, local_files_only: bool = False, prefer_fast: bool = True):\n    \"\"\"\n    Robustly load a tokenizer. If transformers missing or unavailable, return a whitespace fallback\n    that implements the key methods used downstream (decode, convert_ids_to_tokens, __len__, vocab_size).\n    \"\"\"\n    try:\n        import transformers as _tf\n        from transformers import AutoTokenizer\n    except Exception as e_tf:\n        # Transformers not importable: return a richer whitespace fallback\n        class _WhitespaceFallback:\n            def __init__(self):\n                self.pad_token = \"<pad>\"\n                self.pad_token_id = 0\n                self.vocab_size = 0\n            def __len__(self):\n                return int(self.vocab_size)\n            def encode(self, text, add_special_tokens=True):\n                if text is None:\n                    return []\n                return text.split()\n            def convert_ids_to_tokens(self, ids):\n                if ids is None:\n                    return []\n                out = []\n                for x in ids:\n                    if isinstance(x, str):\n                        out.append(x)\n                    else:\n                        out.append(str(x))\n                return out\n            def decode(self, ids, skip_special_tokens=True, **kwargs):\n                if ids is None:\n                    return \"\"\n                if isinstance(ids, (list, tuple)):\n                    # join as strings\n                    return \" \".join([str(t) for t in ids])\n                return str(ids)\n            def __call__(self, texts, padding=False, truncation=False, return_tensors=None, max_length=None, add_special_tokens=True):\n                if isinstance(texts, str):\n                    texts = [texts]\n                input_ids = []\n                attention_mask = []\n                for t in texts:\n                    toks = (t or \"\").split()\n                    input_ids.append(toks)\n                    attention_mask.append([1] * len(toks))\n                if return_tensors == \"pt\":\n                    maxlen = max((len(x) for x in input_ids), default=0)\n                    import torch as _torch\n                    ids_t = _torch.zeros((len(input_ids), maxlen), dtype=_torch.long)\n                    mask_t = _torch.zeros((len(input_ids), maxlen), dtype=_torch.long)\n                    for i, row in enumerate(input_ids):\n                        for j, tok in enumerate(row):\n                            ids_t[i, j] = 0\n                            mask_t[i, j] = 1\n                    return {\"input_ids\": ids_t, \"attention_mask\": mask_t}\n                return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n        if _VERBOSE_LOGGING:\n            print(\"WARNING: 'transformers' import failed in _safe_tokenizer_from_pretrained(). Using whitespace fallback.\")\n            print(f\"         Original error: {type(e_tf).__name__}: {e_tf}\")\n        return _WhitespaceFallback()\n\n    tried = []\n    try:\n        from transformers import M2M100TokenizerFast as _M2MFast\n    except Exception:\n        _M2MFast = None\n\n    if _M2MFast is not None:\n        try:\n            return _M2MFast.from_pretrained(model_name, local_files_only=local_files_only)\n        except Exception as e:\n            tried.append((\"M2M100TokenizerFast\", e))\n\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=prefer_fast, local_files_only=local_files_only)\n        return tok\n    except Exception as e_auto:\n        tried.append((\"AutoTokenizer(use_fast=%s)\" % prefer_fast, e_auto))\n        msg = str(e_auto).lower()\n        if \"sentencepiece\" in msg or \"tokenizers\" in msg or \"sacremoses\" in msg:\n            raise RuntimeError(\n                f\"Failed to instantiate tokenizer for '{model_name}'. This often happens because optional deps like 'sentencepiece' or 'tokenizers' are missing.\\n\"\n                \"Please run: pip install transformers sentencepiece tokenizers\\n\"\n                \"Then RESTART the kernel and re-run cells 0→10.\\n\"\n                f\"Original tokenizer error: {e_auto}\"\n            ) from e_auto\n        # try slow tokenizer as fallback\n        try:\n            tok = AutoTokenizer.from_pretrained(model_name, use_fast=False, local_files_only=local_files_only)\n            return tok\n        except Exception as e_slow:\n            tried.append((\"AutoTokenizer(use_fast=False)\", e_slow))\n            summary = \"; \".join([f\"{name}:{type(exc).__name__}\" for name, exc in tried])\n            raise RuntimeError(\n                f\"No usable tokenizer class available for '{model_name}'. Tried: {summary}.\\n\"\n                \"Make sure you have a compatible 'transformers' installed and the optional dependencies (sentencepiece, tokenizers) for the model.\\n\"\n                \"Suggested command:\\n\"\n                \"  pip install transformers sentencepiece tokenizers\\n\"\n                \"Then RESTART the kernel and re-run the notebook.\\n\"\n                f\"Last error: {e_slow}\"\n            ) from e_slow\n\n# -------------------------\n# Minimal fallback dataset if MemoryEfficientDataset is missing\n# -------------------------\nclass _SimpleDataset(Dataset):\n    \"\"\"Minimal dataset used as a safe fallback. Tokenizes on the fly.\"\"\"\n    def __init__(self, pairs: Iterable[Tuple[str, str]], tokenizer, max_length: int = 48):\n        self.pairs = list(pairs)\n        self.tokenizer = tokenizer\n        self.max_length = int(max_length)\n    def __len__(self):\n        return len(self.pairs)\n    def __getitem__(self, idx):\n        src, tgt = self.pairs[idx]\n        try:\n            enc = self.tokenizer(src, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n            tgt_enc = self.tokenizer(tgt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n            input_ids = enc[\"input_ids\"].squeeze(0)\n            attention_mask = enc[\"attention_mask\"].squeeze(0)\n            labels = tgt_enc[\"input_ids\"].squeeze(0)\n        except Exception:\n            # tokenizer fallback (whitespace)\n            toks = (src or \"\").split()\n            L = min(len(toks), self.max_length)\n            import torch as _torch\n            input_ids = _torch.zeros(self.max_length, dtype=_torch.long)\n            attention_mask = _torch.zeros(self.max_length, dtype=_torch.long)\n            for i in range(L):\n                input_ids[i] = 0\n                attention_mask[i] = 1\n            labels = input_ids.clone()\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"src_text\": src,\n            \"token_word_map\": {}\n        }\n\n# -------------------------\n# Main pipeline\n# -------------------------\ndef initialize_environment():\n    print(\"[CELL10] Initializing environment...\")\n    if torch.cuda.is_available():\n        gcnt = torch.cuda.device_count()\n        print(f\"[CELL10] GPUs available: {gcnt}\")\n        for i in range(gcnt):\n            try:\n                name = torch.cuda.get_device_name(i)\n            except Exception:\n                name = \"Unknown GPU\"\n            try:\n                mem = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3\n                print(f\"  - GPU {i}: {name} ({mem:.1f} GB)\")\n            except Exception:\n                print(f\"  - GPU {i}: {name} (mem unknown)\")\n        _safe_clear_gpu_caches()\n        if gcnt > 1:\n            print(\"[CELL10] Multi-GPU detected\")\n    else:\n        print(\"[CELL10] No GPU detected - running on CPU\")\n    return True\n\ndef main_pipeline() -> Tuple[object, object]:\n    \"\"\"\n    End-to-end orchestration. Returns (trained_model, tokenizer).\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"CELL10: TATN MAIN PIPELINE (patched) - Discovery + Homograph verification\")\n    print(\"=\" * 80)\n\n    initialize_environment()\n\n    # -----------------------\n    # Tokenizer\n    # -----------------------\n    print(\"[CELL10] Loading tokenizer...\")\n    tokenizer = _safe_tokenizer_from_pretrained(\"facebook/m2m100_418M\")\n    try:\n        tokenizer.src_lang = _BN_LANG\n    except Exception:\n        pass\n\n    # Ensure pad token exists (best-effort)\n    try:\n        pad_id = getattr(tokenizer, \"pad_token_id\", None)\n        if pad_id is None and hasattr(tokenizer, \"add_special_tokens\"):\n            try:\n                tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    # compute a useful vocab_info for logging\n    vocab_info = \"unknown\"\n    try:\n        if hasattr(tokenizer, \"vocab_size\") and getattr(tokenizer, \"vocab_size\") is not None:\n            vocab_info = int(getattr(tokenizer, \"vocab_size\"))\n        elif hasattr(tokenizer, \"__len__\"):\n            try:\n                vocab_info = int(len(tokenizer))\n            except Exception:\n                vocab_info = \"unknown\"\n        else:\n            vocab_info = \"unknown\"\n    except Exception:\n        vocab_info = \"unknown\"\n    print(f\"[CELL10] Tokenizer loaded (vocab size approx {vocab_info})\")\n\n    # -----------------------\n    # Data loading (fallbacks)\n    # -----------------------\n    print(f\"[CELL10] Loading/preprocessing up to {_NUM_SAMPLES} samples...\")\n    if \"load_and_preprocess_optimized\" in globals():\n        try:\n            pairs = load_and_preprocess_optimized(_NUM_SAMPLES)\n        except Exception:\n            print(\"[CELL10] load_and_preprocess_optimized failed; using fallback single example\")\n            pairs = [(\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\")]\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] Warning: load_and_preprocess_optimized not found; using small fallback dataset\")\n        pairs = [(\"আমি কল বন্ধ করেছি।\", \"i turned off the tap.\")]\n\n    # Dataset: prefer existing MemoryEfficientDataset, else fallback to simple dataset\n    if \"MemoryEfficientDataset\" in globals():\n        DatasetClass = globals()[\"MemoryEfficientDataset\"]\n        try:\n            dataset = DatasetClass(pairs, tokenizer, max_length=_MAX_LENGTH)\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL10] MemoryEfficientDataset constructor failed; using fallback _SimpleDataset\")\n            dataset = _SimpleDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] MemoryEfficientDataset not present - using fallback _SimpleDataset\")\n        dataset = _SimpleDataset(pairs, tokenizer, max_length=_MAX_LENGTH)\n\n    batch_size = int(_BATCH_SIZE)\n    active_device_ids = list(range(_NUM_GPUS)) if (_USE_MULTI_GPU and _NUM_GPUS > 1) else []\n    if active_device_ids and batch_size < len(active_device_ids):\n        usable = max(1, batch_size)\n        active_device_ids = active_device_ids[:usable]\n        print(f\"[CELL10] Adjusting DataParallel devices to {len(active_device_ids)} due to small batch_size\")\n\n    # synchronize global BATCH_SIZE for compatibility with other cells\n    try:\n        global BATCH_SIZE\n        BATCH_SIZE = batch_size\n    except Exception:\n        pass\n\n    # collate function if provided\n    collate_fn = globals().get(\"safe_collate\", None)\n    collate_fn = collate_fn if callable(collate_fn) else None\n\n    # Prefer an optimized dataloader if available, else fallback to vanilla DataLoader\n    try:\n        if \"create_optimized_dataloader\" in globals():\n            train_loader = create_optimized_dataloader(dataset, batch_size=batch_size, shuffle=True)\n        else:\n            train_loader = DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=True,\n                num_workers=0,\n                pin_memory=torch.cuda.is_available(),\n                collate_fn=collate_fn,\n                drop_last=False\n            )\n    except Exception:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] DataLoader construction failed; attempting simple fallback\")\n        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n\n    try:\n        dataset_len = len(dataset)\n    except Exception:\n        dataset_len = \"unknown\"\n    try:\n        batches_count = len(train_loader)\n    except Exception:\n        batches_count = \"unknown\"\n    print(f\"[CELL10] Dataset: {dataset_len} examples, {batches_count} batches (batch_size={batch_size})\")\n\n    # -----------------------\n    # Model init\n    # -----------------------\n    print(\"[CELL10] Initializing model...\")\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        # Do not raise — give a clear message and return a safe no-op\n        print(\"[CELL10] ERROR: Model class MemoryOptimizedTATNWithExplanations not found (Cell 6). Aborting pipeline initialization.\")\n        return None, tokenizer\n    model_core = MemoryOptimizedTATNWithExplanations(tokenizer)\n\n    # Wrap into DataParallel if multiple device ids chosen\n    if active_device_ids and len(active_device_ids) > 1:\n        print(f\"[CELL10] Wrapping model in DataParallel on devices {active_device_ids}\")\n        model = nn.DataParallel(model_core, device_ids=active_device_ids)\n    else:\n        model = model_core\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] Single-GPU / CPU mode (no DataParallel)\")\n\n    # Move to device carefully (avoid .to on DataParallel in some setups)\n    try:\n        model = model.to(_DEVICE)\n    except Exception:\n        try:\n            core = model.module if hasattr(model, \"module\") else model\n            core.to(_DEVICE)\n        except Exception:\n            pass\n\n    core_model = model.module if hasattr(model, \"module\") else model\n\n    # Resize embeddings if tokenizer vocabulary differs from model embedding size\n    try:\n        mb = getattr(core_model, \"mbart\", None)\n        if mb is not None and hasattr(mb, \"get_input_embeddings\"):\n            emb = mb.get_input_embeddings()\n            current_emb = None\n            try:\n                current_emb = getattr(emb, \"num_embeddings\", None) or (emb.weight.shape[0] if hasattr(emb, \"weight\") else None)\n            except Exception:\n                current_emb = None\n            new_size = None\n            try:\n                if hasattr(tokenizer, \"vocab_size\") and getattr(tokenizer, \"vocab_size\") is not None:\n                    new_size = int(getattr(tokenizer, \"vocab_size\"))\n                elif hasattr(tokenizer, \"__len__\"):\n                    new_size = int(len(tokenizer))\n            except Exception:\n                new_size = None\n            if new_size and current_emb and int(current_emb) != int(new_size):\n                try:\n                    mb.resize_token_embeddings(new_size)\n                    print(f\"[CELL10] Resized token embeddings: {current_emb} -> {new_size}\")\n                except Exception:\n                    if _VERBOSE_LOGGING:\n                        print(\"[CELL10] Warning: resize_token_embeddings failed; continuing\")\n    except Exception:\n        pass\n\n    # Optional encoder freeze\n    if FREEZE_ENCODER:\n        try:\n            if hasattr(core_model, \"mbart\") and hasattr(core_model.mbart, \"model\"):\n                for p in core_model.mbart.model.encoder.parameters():\n                    p.requires_grad = False\n                print(\"[CELL10] Encoder frozen for faster training\")\n        except Exception:\n            if _VERBOSE_LOGGING:\n                print(\"[CELL10] Encoder freeze failed; continuing\")\n\n    # -----------------------\n    # Optimizers\n    # -----------------------\n    print(\"[CELL10] Preparing optimizers...\")\n    try:\n        critic_params = list(core_model.asbn.critic_parameters()) if hasattr(core_model, \"asbn\") and hasattr(core_model.asbn, \"critic_parameters\") else []\n    except Exception:\n        critic_params = []\n    critic_ids = {id(p) for p in critic_params}\n    base_params = [p for p in core_model.parameters() if p.requires_grad and id(p) not in critic_ids]\n\n    optimizer = torch.optim.AdamW(base_params, lr=_LR_NMT)\n    phi_optimizer = None\n    if critic_params and _ENABLE_ASBN_TRAINING:\n        try:\n            phi_optimizer = torch.optim.AdamW([p for p in critic_params if p.requires_grad], lr=_LR_PHI)\n            print(f\"[CELL10] ASBN critic optimizer created (params: {len([p for p in critic_params if p.requires_grad])})\")\n        except Exception:\n            phi_optimizer = None\n            print(\"[CELL10] ASBN critic optimizer creation failed; continuing without it\")\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] ASBN critic optimizer disabled\")\n\n    # -----------------------\n    # Training\n    # -----------------------\n    print(\"[CELL10] Starting training phase...\")\n    trained_model = model\n    if \"train_memory_efficient_tatn\" in globals():\n        try:\n            trained_model = train_memory_efficient_tatn(\n                model,\n                tokenizer,\n                train_loader,\n                optimizer,\n                phi_optimizer=phi_optimizer,\n                epochs=_EPOCHS,\n                accumulation_steps=_ACCUMULATION_STEPS,\n                validate_every=_VALIDATION_CHECK_INTERVAL,\n                enable_validation=bool(_VALIDATION_CHECK_INTERVAL > 0)\n            )\n        except Exception as e:\n            print(f\"[CELL10] Training failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            trained_model = model\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] Training function not found (Cell 7). Skipping training.\")\n        trained_model = model\n\n    # ═══════════════════════════════════════════════════════════════════════\n    # Discovery phase: cluster buffered embeddings + verify homographs\n    # ═══════════════════════════════════════════════════════════════\n    print(\"\\n\" + \"=\" * 80)\n    print(\"DISCOVERY PHASE: Clustering DSCD buffers to create prototypes...\")\n    print(\"=\" * 80)\n\n    _safe_clear_gpu_caches()\n\n    try:\n        core_for_discovery = trained_model.module if hasattr(trained_model, 'module') else trained_model\n\n        if not hasattr(core_for_discovery, \"dscd\"):\n            raise RuntimeError(\"Trained model does not have a .dscd attribute (DSCD instance)\")\n\n        dscd = core_for_discovery.dscd\n\n        # Collect clusterable tokens using a conservative threshold (use .buffers safely)\n        buffers_iter = getattr(dscd, \"buffers\", {}) or {}\n        clusterable_tokens: List[Tuple[str, int]] = []\n        for token_type, buffer in buffers_iter.items():\n            try:\n                buf_len = len(buffer)\n            except Exception:\n                buf_len = 0\n            if buf_len >= _CLUSTER_MIN_SAMPLES:\n                clusterable_tokens.append((token_type, buf_len))\n\n        # relax threshold if nothing meets strict threshold\n        if len(clusterable_tokens) == 0:\n            relaxed = []\n            for token_type, buffer in buffers_iter.items():\n                try:\n                    buf_len = len(buffer)\n                except Exception:\n                    buf_len = 0\n                if buf_len >= DSCD_N_MIN:\n                    relaxed.append((token_type, buf_len))\n            if relaxed:\n                print(f\"[DISCOVERY] No tokens >= {_CLUSTER_MIN_SAMPLES}. Relaxing threshold to DSCD_N_MIN={DSCD_N_MIN} (found {len(relaxed)})\")\n                clusterable_tokens = relaxed\n\n        # Sort by buffer size (descending) and limit to top K (do not force minimum)\n        clusterable_tokens.sort(key=lambda x: x[1], reverse=True)\n        MAX_TO_CLUSTER = min(500, max(1, len(clusterable_tokens)))\n        clusterable_tokens = clusterable_tokens[:MAX_TO_CLUSTER]\n\n        print(f\"[DISCOVERY] Found {len(clusterable_tokens)} tokens meeting threshold for clustering (threshold={_CLUSTER_MIN_SAMPLES})\")\n\n        if len(clusterable_tokens) == 0:\n            print(\"[DISCOVERY] WARNING: No tokens with sufficient samples! DSCD will not work reliably.\")\n        else:\n            clustered_count = 0\n            failed_count = 0\n            start_time = time.time()\n\n            for idx, (token_type, buffer_size) in enumerate(clusterable_tokens):\n                try:\n                    success = False\n                    if hasattr(dscd, \"_cluster_buffer_to_prototypes_hierarchical\"):\n                        try:\n                            success = dscd._cluster_buffer_to_prototypes_hierarchical(token_type)\n                        except Exception as e:\n                            if _VERBOSE_LOGGING:\n                                print(f\"  [WARN] Clustering call raised for token '{token_type}': {type(e).__name__}: {str(e)[:200]}\")\n                            success = False\n                    else:\n                        if _VERBOSE_LOGGING:\n                            print(\"  [WARN] DSCD instance has no _cluster_buffer_to_prototypes_hierarchical method; skipping clustering.\")\n                        success = False\n\n                    if success:\n                        clustered_count += 1\n                    else:\n                        failed_count += 1\n\n                    if (idx + 1) % 50 == 0:\n                        elapsed = time.time() - start_time\n                        print(f\"  Progress: {idx + 1}/{len(clusterable_tokens)} tokens processed \"\n                              f\"({clustered_count} successful, {failed_count} failed) \"\n                              f\"[{elapsed:.1f}s elapsed]\")\n\n                except Exception as e:\n                    failed_count += 1\n                    if failed_count <= 10:\n                        token_str = str(token_type)[:40]\n                        print(f\"  [WARN] Clustering failed for token '{token_str}': {type(e).__name__}: {str(e)[:200]}\")\n                    if _VERBOSE_LOGGING:\n                        traceback.print_exc()\n                    continue\n\n            # Final stats: defensive access to prototype_stores\n            prototype_stores = getattr(dscd, \"prototype_stores\", {}) or {}\n            try:\n                total_prototypes = 0\n                for store in prototype_stores.values():\n                    try:\n                        if hasattr(store, \"size\") and callable(store.size):\n                            total_prototypes += int(store.size())\n                        elif hasattr(store, \"__len__\"):\n                            total_prototypes += int(len(store))\n                        else:\n                            total_prototypes += int(getattr(store, \"n_prototypes\", 0))\n                    except Exception:\n                        try:\n                            total_prototypes += int(getattr(store, \"n_prototypes\", 0) or 0)\n                        except Exception:\n                            pass\n            except Exception:\n                total_prototypes = 0\n\n            try:\n                multi_sense_words = sum(1 for store in prototype_stores.values() if ((store.size() if hasattr(store, \"size\") and callable(store.size) else (len(store) if hasattr(store, \"__len__\") else 0)) >= 2))\n            except Exception:\n                multi_sense_words = 0\n\n            elapsed_total = time.time() - start_time\n\n            print(\"=\" * 80)\n            print(\"✓ DISCOVERY PHASE COMPLETE\")\n            print(\"=\" * 80)\n            print(f\"  • Tokens processed: {len(clusterable_tokens)}\")\n            print(f\"  • Successfully clustered: {clustered_count}\")\n            print(f\"  • Failed: {failed_count}\")\n            print(f\"  • Total prototypes created: {total_prototypes}\")\n            print(f\"  • Multi-sense words (≥2 prototypes): {multi_sense_words}\")\n            print(f\"  • Time elapsed: {elapsed_total:.2f}s ({elapsed_total/60:.2f} min)\")\n            print(\"=\" * 80)\n\n            # Homograph verification (normalized matching)\n            print(\"\\n[DISCOVERY] ✅ Verifying homograph words were clustered:\")\n            print(\"-\" * 80)\n            homographs_found = 0\n            homographs_missing = 0\n\n            # Build a normalized map from proto store keys -> (orig_key, store)\n            proto_map = {}\n            for token_key, store in prototype_stores.items():\n                try:\n                    nk = _norm_clean_token(token_key)\n                except Exception:\n                    nk = str(token_key)\n                if nk not in proto_map:\n                    proto_map[nk] = (token_key, store)\n\n            for homograph in (list(_HOMOGRAPH_WATCHLIST_BN) if _HOMOGRAPH_WATCHLIST_BN else []):\n                matched_store = None\n                matched_key = None\n                nh = _norm_clean_token(homograph)\n                if nh and nh in proto_map:\n                    matched_key, matched_store = proto_map[nh]\n                else:\n                    for nk, (orig_k, store) in proto_map.items():\n                        try:\n                            if _token_matches_homograph(orig_k, homograph):\n                                matched_key, matched_store = orig_k, store\n                                break\n                        except Exception:\n                            continue\n\n                try:\n                    store_size = 0\n                    if matched_store is not None:\n                        if hasattr(matched_store, \"size\") and callable(matched_store.size):\n                            store_size = int(matched_store.size())\n                        elif hasattr(matched_store, \"__len__\"):\n                            store_size = int(len(matched_store))\n                        else:\n                            store_size = int(getattr(matched_store, \"n_prototypes\", 0) or 0)\n                    if matched_store is not None and store_size >= 2:\n                        counts = getattr(matched_store, \"counts\", None)\n                        print(f\"  ✓ '{homograph}' → {store_size} prototypes (key='{matched_key}') counts={counts}\")\n                        homographs_found += 1\n                    else:\n                        print(f\"  ✗ WARNING: '{homograph}' has NO multi-sense prototypes\")\n                        print(f\"            This word will NOT be disambiguated in inference!\")\n                        homographs_missing += 1\n                except Exception:\n                    print(f\"  ✗ WARNING: '{homograph}' verification encountered an error\")\n                    homographs_missing += 1\n\n            print(\"-\" * 80)\n            print(f\"Homograph verification: {homographs_found}/{len(list(_HOMOGRAPH_WATCHLIST_BN))} detected\")\n\n            if homographs_missing > 0:\n                print(f\"\\n⚠️ WARNING: {homographs_missing} known homographs were NOT properly clustered!\")\n                print(\"Possible causes:\")\n                print(\"  1. Not enough training samples containing these words\")\n                print(\"  2. Words were filtered out by should_track_token() (Cell 3)\")\n                print(\"  3. Buffer/cluster thresholds too strict\")\n                print(\"  4. Clustering backend unavailable or failed (SciPy/sklearn)\")\n            else:\n                print(\"\\n✅ All homographs successfully clustered! Disambiguation ready.\")\n\n            # Clear buffers only if prototypes were actually created\n            if total_prototypes > 0:\n                if _VERBOSE_LOGGING:\n                    print(\"[DISCOVERY] Clearing DSCD buffers to save memory (prototypes present).\")\n                try:\n                    if hasattr(dscd, \"buffers\") and hasattr(dscd.buffers, \"clear\"):\n                        dscd.buffers.clear()\n                    else:\n                        dscd.buffers = {}\n                except Exception:\n                    try:\n                        dscd.buffers = {}\n                    except Exception:\n                        pass\n                _safe_clear_gpu_caches()\n            else:\n                if _VERBOSE_LOGGING:\n                    print(\"[DISCOVERY] Not clearing DSCD buffers (no prototypes created) to preserve data for debugging/warmup.\")\n\n    except Exception as e:\n        print(f\"[DISCOVERY] CRITICAL ERROR: Discovery phase failed!\")\n        print(f\"  Error type: {type(e).__name__}\")\n        print(f\"  Error message: {str(e)[:300]}\")\n        if _VERBOSE_LOGGING:\n            print(\"\\n[DISCOVERY] Full traceback:\")\n            traceback.print_exc()\n        print(\"\\n[DISCOVERY] WARNING: DSCD homograph detection will NOT work!\")\n        print(\"  The model will function but only at baseline M2M100 quality.\")\n\n    # Optional: Run additional warmup inference (if dscd_discovery_warmup exists)\n    if \"dscd_discovery_warmup\" in globals():\n        try:\n            print(\"\\n[CELL10] Running additional DSCD inference warmup...\")\n            warmup_samples = min(1000, int(_DSCD_WARMUP_SAMPLES))\n            dscd_discovery_warmup(trained_model, tokenizer, num_sents=warmup_samples, max_len=_MAX_LENGTH)\n            print(f\"[CELL10] Inference warmup complete\")\n        except Exception as e:\n            print(f\"[CELL10] Inference warmup failed: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n\n    # -----------------------\n    # Step 8: Post-training evaluation\n    # -----------------------\n    print(\"\\n[CELL10] Step 8: Evaluation\")\n    _safe_clear_gpu_caches()\n    if \"comprehensive_post_training_testing\" in globals():\n        try:\n            eval_results = comprehensive_post_training_testing(trained_model, tokenizer)\n        except Exception as e:\n            print(f\"[CELL10] Evaluation failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n            eval_results = {}\n    else:\n        if _VERBOSE_LOGGING:\n            print(\"[CELL10] comprehensive_post_training_testing not found\")\n        eval_results = {}\n\n    # -----------------------\n    # Save model (core state dict)\n    # -----------------------\n    print(\"[CELL10] Saving model...\")\n    try:\n        core_for_save = trained_model.module if hasattr(trained_model, \"module\") else trained_model\n        save_path = \"tatn_kaggle_final.pt\"\n        # Ensure directory exists\n        sdir = os.path.dirname(save_path)\n        if sdir and not os.path.exists(sdir):\n            try:\n                os.makedirs(sdir, exist_ok=True)\n            except Exception:\n                pass\n        torch.save(core_for_save.state_dict(), save_path)\n        print(f\"[CELL10] Model state saved to {save_path}\")\n    except Exception as e:\n        print(f\"[CELL10] Save failed: {type(e).__name__}: {str(e)[:200]}\")\n        if _VERBOSE_LOGGING:\n            traceback.print_exc()\n\n    # Final report\n    print(\"\\n[CELL10] Final Report Summary:\")\n    if eval_results:\n        try:\n            sr = eval_results.get('success_rate_pct', eval_results.get('success_rate', 0.0))\n            print(f\"  Success Rate: {float(sr):.2f}%\")\n        except Exception:\n            print(f\"  Success Rate: {eval_results.get('success_rate_pct', eval_results.get('success_rate', 'N/A'))}\")\n        print(f\"  DSCD prototype stats: {eval_results.get('dscd_stats', {})}\")\n    else:\n        print(\"  No evaluation metrics available\")\n\n    # Clear caches and return\n    _safe_clear_gpu_caches()\n    return trained_model, tokenizer\n\n# When this cell is executed, the user can call main_pipeline() to execute.\nprint(\"✅ Cell 10 (patched): Discovery phase + homograph verification ready. Call main_pipeline() to execute.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:08.535099Z","iopub.execute_input":"2025-11-22T14:56:08.535450Z","iopub.status.idle":"2025-11-22T14:56:08.600481Z","shell.execute_reply.started":"2025-11-22T14:56:08.535433Z","shell.execute_reply":"2025-11-22T14:56:08.599758Z"},"id":"kEux2BVXH4J5","trusted":true},"outputs":[{"name":"stdout","text":"✅ Cell 10 (patched): Discovery phase + homograph verification ready. Call main_pipeline() to execute.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ==============================================================================\n# CELL 11 (patched): MAIN EXECUTION WRAPPER (MULTI-GPU OPTIMIZED - DEBUGGED)\n# ==============================================================================\n# - Robust globals via globals().get(...) with sensible defaults\n# - Safer invocation of main_pipeline() and tolerant unpacking of return values\n# - Controlled verbose tracebacks via VERBOSE_LOGGING flag\n# - Guarded quick inference check that tolerates many return shapes and call signatures\n# - Improved ceil division helper that accepts numeric-like inputs\n# ==============================================================================\nfrom datetime import datetime, timezone\nimport os\nimport traceback\nimport math\nimport sys\nimport torch\nfrom typing import Any\n\ndef _safe_get(name: str, default: Any):\n    try:\n        return globals().get(name, default)\n    except Exception:\n        return default\n\ndef _safe_div_ceil(a: Any, b: Any) -> int:\n    \"\"\"Return ceil(a/b) for numeric-like inputs, otherwise 0.\"\"\"\n    try:\n        a_f = float(a)\n        b_f = float(b)\n        if b_f == 0:\n            return 0\n        return int(math.ceil(a_f / b_f))\n    except Exception:\n        return 0\n\ndef _is_model_like(obj: Any) -> bool:\n    \"\"\"Heuristic: object that looks like a model (has forward or predict).\"\"\"\n    try:\n        return hasattr(obj, \"forward\") or hasattr(obj, \"generate\") or hasattr(obj, \"state_dict\") or hasattr(obj, \"dscd\")\n    except Exception:\n        return False\n\ndef _is_tokenizer_like(obj: Any) -> bool:\n    \"\"\"Heuristic: object that looks like a tokenizer (has decode or convert_ids_to_tokens).\"\"\"\n    try:\n        return hasattr(obj, \"decode\") or hasattr(obj, \"convert_ids_to_tokens\") or callable(getattr(obj, \"__call__\", None))\n    except Exception:\n        return False\n\n# Entry point guard for script invocation\nif __name__ == \"__main__\":\n    print(\"=\" * 80)\n    print(\"MEMORY-OPTIMIZED TATN FOR KAGGLE T4×2 (Cell 11 - RUNNER)\")\n    print(\"=\" * 80)\n\n    # Read configuration safely from globals (Cell 0 may not have run)\n    _NUM_SAMPLES = _safe_get(\"NUM_SAMPLES\", 30000)\n    _EPOCHS = _safe_get(\"EPOCHS\", 2)\n    _BATCH_SIZE = _safe_get(\"BATCH_SIZE\", 4)\n    _ACCUMULATION_STEPS = _safe_get(\"ACCUMULATION_STEPS\", 16)\n    _DEVICE = _safe_get(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n    _ENABLE_ASBN_TRAINING = _safe_get(\"ENABLE_ASBN_TRAINING\", True)\n    _ENABLE_TRG_INFERENCE = _safe_get(\"ENABLE_TRG_INFERENCE\", True)\n    _PERIODIC_DISCOVERY_FREQUENCY = _safe_get(\"PERIODIC_DISCOVERY_FREQUENCY\", 5000)\n    _VERBOSE_LOGGING = _safe_get(\"VERBOSE_LOGGING\", False)\n    _USE_MULTI_GPU = _safe_get(\"USE_MULTI_GPU\", torch.cuda.is_available() and torch.cuda.device_count() > 1)\n    _NUM_GPUS = _safe_get(\"NUM_GPUS\", torch.cuda.device_count() if torch.cuda.is_available() else 0)\n\n    # user and timestamp\n    user_login = os.getenv(\"KAGGLE_USERNAME\") or os.getenv(\"USER\") or _safe_get(\"CURRENT_USER\", \"manas0003\")\n    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n    print(f\"User: {user_login}\")\n    print(f\"Started: {now_utc}\")\n\n    # Configuration summary\n    print(\"\\nConfiguration:\")\n    print(f\"   • Samples: {_NUM_SAMPLES}\")\n    print(f\"   • Epochs: {_EPOCHS}\")\n    print(f\"   • Batch Size: {_BATCH_SIZE}\")\n    print(f\"   • Accumulation: {_ACCUMULATION_STEPS}\")\n    print(f\"   • Device: {_DEVICE}\")\n    print(f\"   • Multi-GPU: {'ENABLED' if _USE_MULTI_GPU else 'DISABLED'} ({_NUM_GPUS} GPU(s))\")\n    if _USE_MULTI_GPU and _NUM_GPUS > 0:\n        per_gpu = _safe_div_ceil(int(_BATCH_SIZE), int(max(1, _NUM_GPUS)))\n        print(f\"   • Batch per GPU: {per_gpu}\")\n    print(f\"   • ASBN Training: {'Enabled' if _ENABLE_ASBN_TRAINING else 'Disabled'}\")\n    print(f\"   • TRG Inference: {'Enabled' if _ENABLE_TRG_INFERENCE else 'Disabled'}\")\n    print(f\"   • Periodic Discovery: Every {_PERIODIC_DISCOVERY_FREQUENCY} steps\")\n    print(\"=\" * 80)\n\n    trained_model, tokenizer = None, None\n\n    # Ensure main_pipeline exists and is callable\n    mp = globals().get(\"main_pipeline\", None)\n    if mp is None or not callable(mp):\n        print(\"\\nERROR: main_pipeline not found or not callable - please run Cell 10 before executing this cell.\")\n    else:\n        try:\n            print(\"\\nStarting full pipeline (this may take a while)...\")\n            # call main_pipeline; accept several possible return patterns\n            ret = mp()\n\n            # Normalize and unpack the return robustly\n            if isinstance(ret, tuple):\n                # common case: (model, tokenizer) or (model,)\n                if len(ret) >= 2:\n                    trained_model, tokenizer = ret[0], ret[1]\n                elif len(ret) == 1:\n                    trained_model = ret[0]\n                    # attempt to find tokenizer in ret[0] attributes or globals\n                    if hasattr(trained_model, \"tokenizer\") and _is_tokenizer_like(getattr(trained_model, \"tokenizer\")):\n                        tokenizer = trained_model.tokenizer\n                    else:\n                        tokenizer = globals().get(\"tokenizer\", None)\n            elif isinstance(ret, dict):\n                # returned a dict of artifacts\n                trained_model = ret.get(\"model\") or ret.get(\"trained_model\") or ret.get(\"core_model\") or ret.get(\"tatn\")\n                tokenizer = ret.get(\"tokenizer\") or ret.get(\"tok\") or globals().get(\"tokenizer\", None)\n                # if values are not model/tokenizer, try heuristics\n                if trained_model is None:\n                    for v in ret.values():\n                        if _is_model_like(v):\n                            trained_model = v\n                            break\n                if tokenizer is None:\n                    for v in ret.values():\n                        if _is_tokenizer_like(v):\n                            tokenizer = v\n                            break\n            else:\n                # single-object return - try to infer\n                if _is_model_like(ret):\n                    trained_model = ret\n                    tokenizer = globals().get(\"tokenizer\", None)\n                elif _is_tokenizer_like(ret):\n                    tokenizer = ret\n                    trained_model = globals().get(\"trained_model\", None) or globals().get(\"model\", None)\n                else:\n                    # fallback: look into globals for likely objects if pipeline stored them\n                    trained_model = globals().get(\"trained_model\", None) or globals().get(\"model\", None)\n                    tokenizer = globals().get(\"tokenizer\", None)\n\n        except KeyboardInterrupt:\n            print(\"\\nExecution interrupted by user (KeyboardInterrupt).\")\n        except Exception as e:\n            msg = str(e).lower()\n            if isinstance(e, RuntimeError) and (\n                \"no usable tokenizer class available\" in msg\n                or \"failed to instantiate tokenizer\" in msg\n                or \"sentencepiece\" in msg\n                or \"tokenizers\" in msg\n            ):\n                print(f\"\\nPipeline execution failed: {type(e).__name__}: {str(e)[:400]}\")\n                print(\"\\nThis error indicates the tokenizer could not be instantiated. Common causes and fixes:\")\n                print(\"  • Missing or incompatible 'transformers' package.\")\n                print(\"  • Missing optional tokenizer dependencies (sentencepiece, tokenizers, sacremoses).\")\n                print(\"\\nSuggested actions (pick one):\")\n                print(\"  1) Install the recommended packages (in a notebook cell or terminal):\")\n                print(\"       !pip install transformers==4.30.2 sentencepiece tokenizers sacremoses --quiet\")\n                print(\"     Then RESTART the kernel and re-run Cells 0→11 in order.\")\n                print(\"\")\n                print(\"  2) If you are offline but have a cached tokenizer folder, set local_files_only=True in the tokenizer loader or\")\n                print(\"     provide MODEL_LOCAL_TOKENIZER_PATH in your config and re-run.\")\n                print(\"\")\n                print(\"  3) If you want to continue debugging without real tokenization, ensure Cell 10's _safe_tokenizer_from_pretrained\")\n                print(\"     returns the whitespace fallback (it will allow the pipeline to continue but translations will be incorrect).\")\n                if _VERBOSE_LOGGING:\n                    print(\"\\nFull traceback (VERBOSE):\")\n                    traceback.print_exc()\n                else:\n                    print(\"\\nSet VERBOSE_LOGGING = True in Cell 0 to see the full traceback.\")\n            else:\n                print(f\"\\nPipeline execution failed: {type(e).__name__}: {str(e)[:400]}\")\n                if _VERBOSE_LOGGING:\n                    traceback.print_exc()\n                else:\n                    print(\"Set VERBOSE_LOGGING = True in Cell 0 to see full traceback.\")\n\n    # Post-run summary and quick inference check\n    if trained_model is not None and tokenizer is not None:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"SYSTEM INITIALIZATION SUCCEEDED\")\n        print(\"=\" * 80)\n        print(\"\\nCapabilities:\")\n        print(\"   • Bengali → English translation\")\n        print(\"   • Automatic homograph disambiguation (DSCD + TRG)\")\n        print(\"   • Dynamic prototype discovery (hierarchical clustering)\")\n        if _USE_MULTI_GPU:\n            print(f\"   • Multi-GPU acceleration ({_NUM_GPUS} GPUs)\")\n        print(\"=\" * 80)\n\n        # Quick inference validation (best-effort; guarded)\n        print(\"\\nQuick Inference Validation (single sample):\")\n        try:\n            tw = globals().get(\"translate_with_explanations\", None)\n            if callable(tw):\n                sample = \"আমি কল বন্ধ করেছি।\"\n                print(f\"  Testing sentence: {sample}\")\n\n                # Try several plausible call signatures until one works\n                res = None\n                call_attempts = []\n                # define candidate argument permutations\n                arg_permutations = [\n                    (trained_model, tokenizer, sample),\n                    (trained_model, tokenizer, [sample]),\n                    (trained_model, sample, tokenizer),\n                    (tokenizer, trained_model, sample),\n                    (sample, trained_model, tokenizer),\n                    (sample,),  # some wrappers accept only sentence and read model/tokenizer from globals\n                    (trained_model, sample),\n                    (tokenizer, sample),\n                ]\n\n                for args in arg_permutations:\n                    try:\n                        candidate = tw(*args)\n                        res = candidate\n                        call_attempts.append((\"ok\", args))\n                        break\n                    except TypeError as te:\n                        call_attempts.append((\"type_error\", args, str(te)))\n                        continue\n                    except Exception as e:\n                        # record and continue; verbose if requested\n                        call_attempts.append((\"error\", args, f\"{type(e).__name__}: {e}\"))\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        continue\n\n                if res is None:\n                    # try calling with named args as a last resort\n                    try:\n                        res = tw(model=trained_model, tokenizer=tokenizer, input_sentence=sample)\n                    except Exception:\n                        res = None\n\n                # Report results defensively\n                if isinstance(res, dict):\n                    print(f\"  Translation: {res.get('translation', 'N/A')}\")\n                    print(f\"  Ambiguous Words Detected: {res.get('ambiguous_words_detected', 0)}\")\n                    exs = res.get('explanations', []) or []\n                    if exs:\n                        e0 = exs[0]\n                        print(\"  Example explanation (first):\")\n                        print(f\"    Word: {e0.get('ambiguous_word', e0.get('token', 'N/A'))}\")\n                        try:\n                            u = float(e0.get('uncertainty', 0.0))\n                            s = float(e0.get('span', 0.0))\n                            print(f\"    Uncertainty: {u:.3f}\")\n                            print(f\"    Span: {s:.3f}\")\n                        except Exception:\n                            print(f\"    Uncertainty/Span: {e0.get('uncertainty','N/A')} / {e0.get('span','N/A')}\")\n                    else:\n                        print(\"  No explanations returned (high-confidence translation)\")\n                elif res is None:\n                    print(\"  Quick inference returned None (check translate_with_explanations signature or pipeline logs)\")\n                    if _VERBOSE_LOGGING:\n                        print(\"  Call attempts summary:\")\n                        for rec in call_attempts:\n                            print(\"   \", rec)\n                else:\n                    # non-dict result: print repr for debugging\n                    print(\"  translate_with_explanations returned non-dict result; here's its repr:\")\n                    print(\"  \", repr(res)[:1000])\n            else:\n                print(\"  translate_with_explanations not available - ensure Cell 8 is run\")\n        except Exception as e:\n            print(f\"  Quick inference failed: {type(e).__name__}: {str(e)[:200]}\")\n            if _VERBOSE_LOGGING:\n                traceback.print_exc()\n    else:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"SYSTEM INITIALIZATION FAILED\")\n        print(\"=\" * 80)\n        print(\"Troubleshooting tips:\")\n        print(\"  1) Run Cells 0→10 in order to ensure dependencies are loaded.\")\n        print(\"  2) Set VERBOSE_LOGGING = True in Cell 0 to see detailed tracebacks.\")\n        print(\"  3) Ensure GPUs are available and CUDA visible to the process.\")\n        print(\"  4) If warmup/prototype building missed some words, run dscd_discovery_warmup(...) manually.\")\n        print(\"\")\n        print(\"If the failure was tokenizer-related, run the following and then RESTART the kernel:\")\n        print(\"  pip install transformers==4.30.2 sentencepiece tokenizers sacremoses\")\n        print(\"=\" * 80)\n\n    print(\"\\nCELL 11: Execution wrapper finished.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-22T14:56:08.601303Z","iopub.execute_input":"2025-11-22T14:56:08.601878Z","iopub.status.idle":"2025-11-22T18:42:09.214118Z","shell.execute_reply.started":"2025-11-22T14:56:08.601859Z","shell.execute_reply":"2025-11-22T18:42:09.212868Z"},"id":"9n4Hrn1wH4J6","trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nMEMORY-OPTIMIZED TATN FOR KAGGLE T4×2 (Cell 11 - RUNNER)\n================================================================================\nUser: manas0003\nStarted: 2025-11-22 14:56:08 UTC\n\nConfiguration:\n   • Samples: 50000\n   • Epochs: 2\n   • Batch Size: 100\n   • Accumulation: 16\n   • Device: cuda\n   • Multi-GPU: ENABLED (2 GPU(s))\n   • Batch per GPU: 50\n   • ASBN Training: Enabled\n   • TRG Inference: Enabled\n   • Periodic Discovery: Every 100 steps\n================================================================================\n\nStarting full pipeline (this may take a while)...\n================================================================================\nCELL10: TATN MAIN PIPELINE (patched) - Discovery + Homograph verification\n================================================================================\n[CELL10] Initializing environment...\n[CELL10] GPUs available: 2\n  - GPU 0: Tesla T4 (14.7 GB)\n  - GPU 1: Tesla T4 (14.7 GB)\n[CELL10] Multi-GPU detected\n[CELL10] Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84489f89290440d38eb4762b8d5026ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3a8d9509624699ad8f02768f74defb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5f43324ab041e99367ab78196d4154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2450036fc1c34344890680c343d4f4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb77e6b19734ac78465f8d004dceb19"}},"metadata":{}},{"name":"stdout","text":"[CELL10] Tokenizer loaded (vocab size approx 128104)\n[CELL10] Loading/preprocessing up to 50000 samples...\n[CELL2] Loading up to 50000 samples from local CSV: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\n[CELL2] Reading CSV file...\n[CELL2] Processing 50000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 50000/50000 [00:02<00:00, 23195.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 50000 pairs from CSV, skipped 0 rows\n[CELL2] Dataset initialized: 50000 valid pairs, 0 invalid pairs filtered\n[CELL2] DataLoader created: total_batch=100, per_gpu=50, workers=2\n[CELL10] Dataset: 50000 examples, 500 batches (batch_size=100)\n[CELL10] Initializing model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5ffa2faa974ea080c8988c6daae288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0347b5e300354edfb1da7ec1b6d3a1fe"}},"metadata":{}},{"name":"stderr","text":"Using cls_token, but it is not set yet.\nUsing cls_token, but it is not set yet.\nUsing mask_token, but it is not set yet.\nUsing mask_token, but it is not set yet.\n","output_type":"stream"},{"name":"stdout","text":"[CELL10] Wrapping model in DataParallel on devices [0, 1]\n[CELL10] Resized token embeddings: 128112 -> 128104\n[CELL10] Preparing optimizers...\n[CELL10] ASBN critic optimizer created (params: 12)\n[CELL10] Starting training phase...\n[TRAIN] Starting training: epochs=2, batch=100, accum_steps=16\n[TRAIN] Validation: enabled\n[TRAIN] DP enabled: True, GPUs: 2, Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  40%|███████████████████▉                              | 199/500 [44:22<1:07:42, 13.50s/it, fwd_loss=2.0640 bwd_loss=0.129002 rate=100.0% proc=199 skip=0 clusters=12710]","output_type":"stream"},{"name":"stdout","text":"[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.73 resv=12.90\n  GPU 1: alloc=1.30 resv=8.53\n[TRAIN-DEBUG] step=200 loss=2.2224 opt_updates=12 clusters=12741\n\n[CLUSTER] Top 5 clusters (by sample count):\n------------------------------------------------------------------------------------------\nRank  Token             Count       Protos    μ (mean)       τ (dev)     \n------------------------------------------------------------------------------------------\n1     শক্ত              20          4         23.279076      4.302098    \n2     কঠোর              20          4         23.584515      3.812131    \n3     সোনা              19          3         26.127859      4.105221    \n4     জ্ঞান             19          3         23.873760      4.940936    \n5     সুর               19          3         24.755248      4.159158    \n------------------------------------------------------------------------------------------\nTotal clusters: 12741 | Total samples in clusters: 45015\n\n[CLUSTER-STATS] Cluster Statistics:\n  • Total clusters: 12741\n  • Total samples: 45015\n  • Total prototypes: 6733\n  • Avg samples/cluster: 3.5\n  • Avg protos/cluster: 0.5\n  • Max samples/cluster: 20\n  • Min samples/cluster: 0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  41%|████████████████████▋                             | 207/500 [46:10<1:05:35, 13.43s/it, fwd_loss=2.0851 bwd_loss=0.130319 rate=100.0% proc=207 skip=0 clusters=12942]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n[VALIDATION] Quick validation at step 208\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"2025-11-22 15:42:59.652126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763826179.867045      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763826179.921018      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"1. আমি কল বন্ধ করেছি। -> ..\n2. কাল আমি বই কিনব। -> to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n3. পাতা ঝরে পড়েছে। -> ..\n4. আমি ভালো আছি। -> ..\n5. আজ আবহাওয়া ভালো। -> ..\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  42%|████████████████████▊                             | 208/500 [46:39<1:28:39, 18.22s/it, fwd_loss=1.8908 bwd_loss=0.118173 rate=100.0% proc=208 skip=0 clusters=12981]","output_type":"stream"},{"name":"stdout","text":"======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  80%|███████████████████████████████████████▉          | 399/500 [1:29:33<22:48, 13.55s/it, fwd_loss=2.0988 bwd_loss=0.131177 rate=100.0% proc=399 skip=0 clusters=17515]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n[VALIDATION] Quick validation at step 400\n======================================================================\n1. আমি কল বন্ধ করেছি। -> i closed the call.\n2. কাল আমি বই কিনব। -> i will buy this tomorrow.\n3. পাতা ঝরে পড়েছে। -> the page fell.\n4. আমি ভালো আছি। -> i am good.\n5. আজ আবহাওয়া ভালো। -> today is good.\n======================================================================\n[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=5.93 resv=7.14\n  GPU 1: alloc=1.25 resv=3.09\n[TRAIN-DEBUG] step=400 loss=2.3060 opt_updates=25 clusters=17543\n\n[CLUSTER] Top 5 clusters (by sample count):\n------------------------------------------------------------------------------------------\nRank  Token             Count       Protos    μ (mean)       τ (dev)     \n------------------------------------------------------------------------------------------\n1     গত                20          4         20.176730      10.448194   \n2     প্রশস্ত           20          4         23.658534      6.460830    \n3     সমৃ               20          4         21.425780      4.291109    \n4     পুষ্টিকর          20          4         22.383360      4.455706    \n5     মসৃণ              20          4         24.485408      4.359946    \n------------------------------------------------------------------------------------------\nTotal clusters: 17543 | Total samples in clusters: 74812\n\n[CLUSTER-STATS] Cluster Statistics:\n  • Total clusters: 17543\n  • Total samples: 74812\n  • Total prototypes: 11049\n  • Avg samples/cluster: 4.3\n  • Avg protos/cluster: 0.6\n  • Max samples/cluster: 20\n  • Min samples/cluster: 0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2: 100%|██████████████████████████████████████████████████| 500/500 [1:52:11<00:00, 13.46s/it, fwd_loss=1.5028 bwd_loss=0.093923 rate=100.0% proc=500 skip=0 clusters=19278]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 1 summary:\n  duration (min): 112.19\n  optimizer updates: 32\n  batches processed: 500 (processed=500, skipped=0)\n  success rate (updates/expected): 103.2%\n  clustered token types: 19278\n  avg forward loss: 2.808499\n================================================================================\n[CHECKPOINT] Saved tatn_e1_s500_20251122_164843.pt avg_loss=2.808499\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  20%|██████████                                         | 99/500 [22:27<1:29:59, 13.46s/it, fwd_loss=1.4471 bwd_loss=0.090441 rate=102.7% proc=599 skip=0 clusters=19975]","output_type":"stream"},{"name":"stdout","text":"[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.74 resv=13.75\n  GPU 1: alloc=1.33 resv=8.44\n[TRAIN-DEBUG] step=600 loss=1.2463 opt_updates=38 clusters=19978\n\n[CLUSTER] Top 5 clusters (by sample count):\n------------------------------------------------------------------------------------------\nRank  Token             Count       Protos    μ (mean)       τ (dev)     \n------------------------------------------------------------------------------------------\n1     ধান               20          4         21.095281      8.081010    \n2     কৌ                20          4         21.031949      5.118068    \n3     দূর               20          4         25.049261      3.580823    \n4     নাগরিক            20          4         22.141394      4.999902    \n5     ৃতি               20          4         17.211136      3.584237    \n------------------------------------------------------------------------------------------\nTotal clusters: 19978 | Total samples in clusters: 100079\n\n[CLUSTER-STATS] Cluster Statistics:\n  • Total clusters: 19978\n  • Total samples: 100079\n  • Total prototypes: 14431\n  • Avg samples/cluster: 5.0\n  • Avg protos/cluster: 0.7\n  • Max samples/cluster: 20\n  • Min samples/cluster: 0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  22%|███████████                                       | 111/500 [25:09<1:26:15, 13.30s/it, fwd_loss=1.4946 bwd_loss=0.093412 rate=100.0% proc=611 skip=0 clusters=20054]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n[VALIDATION] Quick validation at step 612\n======================================================================\n1. আমি কল বন্ধ করেছি। -> i closed the call.\n2. কাল আমি বই কিনব। -> i will buy it tomorrow.\n3. পাতা ঝরে পড়েছে। -> the page has fallen.\n4. আমি ভালো আছি। -> i am well.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  22%|███████████▏                                      | 112/500 [25:24<1:28:48, 13.73s/it, fwd_loss=1.5706 bwd_loss=0.098166 rate=102.6% proc=612 skip=0 clusters=20059]","output_type":"stream"},{"name":"stdout","text":"5. আজ আবহাওয়া ভালো। -> the weather is good today.\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  60%|█████████████████████████████▉                    | 299/500 [1:07:34<44:51, 13.39s/it, fwd_loss=1.1700 bwd_loss=0.073125 rate=102.0% proc=799 skip=0 clusters=21332]","output_type":"stream"},{"name":"stdout","text":"[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.74 resv=12.90\n  GPU 1: alloc=1.28 resv=8.46\n[TRAIN-DEBUG] step=800 loss=1.3455 opt_updates=50 clusters=21342\n\n[CLUSTER] Top 5 clusters (by sample count):\n------------------------------------------------------------------------------------------\nRank  Token             Count       Protos    μ (mean)       τ (dev)     \n------------------------------------------------------------------------------------------\n1     শিক্ষ             20          4         21.354459      6.465549    \n2     নিষ্ঠ             20          4         23.850510      5.149038    \n3     দায়িত্ব          19          3         22.813807      5.406342    \n4     নিয়ে             19          3         23.675864      5.383530    \n5     খে                19          3         21.088390      4.672610    \n------------------------------------------------------------------------------------------\nTotal clusters: 21342 | Total samples in clusters: 121671\n\n[CLUSTER-STATS] Cluster Statistics:\n  • Total clusters: 21342\n  • Total samples: 121671\n  • Total prototypes: 17252\n  • Avg samples/cluster: 5.7\n  • Avg protos/cluster: 0.8\n  • Max samples/cluster: 20\n  • Min samples/cluster: 0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  61%|██████████████████████████████▎                   | 303/500 [1:08:27<43:41, 13.31s/it, fwd_loss=1.3216 bwd_loss=0.082600 rate=100.0% proc=803 skip=0 clusters=21362]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n[VALIDATION] Quick validation at step 804\n======================================================================\n1. আমি কল বন্ধ করেছি। -> i stopped the call.\n2. কাল আমি বই কিনব। -> i will buy it tomorrow.\n3. পাতা ঝরে পড়েছে। -> the page has fallen.\n4. আমি ভালো আছি। -> i am well.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  61%|██████████████████████████████▍                   | 304/500 [1:08:42<44:48, 13.72s/it, fwd_loss=1.2416 bwd_loss=0.077603 rate=102.0% proc=804 skip=0 clusters=21363]","output_type":"stream"},{"name":"stdout","text":"5. আজ আবহাওয়া ভালো। -> the weather is good today.\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|█████████████████████████████████████████████████▉| 499/500 [1:52:34<00:13, 13.52s/it, fwd_loss=1.2068 bwd_loss=0.075424 rate=101.6% proc=999 skip=0 clusters=22648]","output_type":"stream"},{"name":"stdout","text":"[TRAIN-DEBUG] GPU mem (GB):\n  GPU 0: alloc=7.74 resv=13.88\n  GPU 1: alloc=1.28 resv=8.34\n[TRAIN-DEBUG] step=1000 loss=1.1186 opt_updates=63 clusters=22652\n\n[CLUSTER] Top 5 clusters (by sample count):\n------------------------------------------------------------------------------------------\nRank  Token             Count       Protos    μ (mean)       τ (dev)     \n------------------------------------------------------------------------------------------\n1     সাক্ষাৎ           20          4         23.171738      4.528618    \n2     তত্ত্ব            20          4         21.703555      4.709562    \n3     সুখ               20          4         21.752443      6.131968    \n4     জরুর              20          4         20.615450      5.506017    \n5     বছর               20          4         22.549204      5.459606    \n------------------------------------------------------------------------------------------\nTotal clusters: 22652 | Total samples in clusters: 139289\n\n[CLUSTER-STATS] Cluster Statistics:\n  • Total clusters: 22652\n  • Total samples: 139289\n  • Total prototypes: 19531\n  • Avg samples/cluster: 6.1\n  • Avg protos/cluster: 0.9\n  • Max samples/cluster: 20\n  • Min samples/cluster: 0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|█████████████████████████████████████████████████| 500/500 [1:52:49<00:00, 13.54s/it, fwd_loss=1.1186 bwd_loss=0.069913 rate=101.6% proc=1000 skip=0 clusters=22652]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nEpoch 2 summary:\n  duration (min): 112.83\n  optimizer updates: 64\n  batches processed: 1000 (processed=1000, skipped=0)\n  success rate (updates/expected): 103.2%\n  clustered token types: 22652\n  avg forward loss: 2.060247\n================================================================================\n[CHECKPOINT] Saved tatn_e2_s1000_20251122_184143.pt avg_loss=1.311995\n\n[TRAIN] Training completed\n[TRAIN] Success Rate (updates/expected): 103.2%\n[TRAIN] Batches processed=1000 skipped=0\n[TRAIN] Clustered Token Types: 22652\n\n================================================================================\nDISCOVERY PHASE: Clustering DSCD buffers to create prototypes...\n================================================================================\n[DISCOVERY] Found 500 tokens meeting threshold for clustering (threshold=20)\n  Progress: 50/500 tokens processed (50 successful, 0 failed) [0.0s elapsed]\n  Progress: 100/500 tokens processed (100 successful, 0 failed) [0.1s elapsed]\n  Progress: 150/500 tokens processed (150 successful, 0 failed) [0.1s elapsed]\n  Progress: 200/500 tokens processed (200 successful, 0 failed) [0.1s elapsed]\n  Progress: 250/500 tokens processed (250 successful, 0 failed) [0.2s elapsed]\n  Progress: 300/500 tokens processed (300 successful, 0 failed) [0.2s elapsed]\n  Progress: 350/500 tokens processed (350 successful, 0 failed) [0.3s elapsed]\n  Progress: 400/500 tokens processed (400 successful, 0 failed) [0.3s elapsed]\n  Progress: 450/500 tokens processed (450 successful, 0 failed) [0.3s elapsed]\n  Progress: 500/500 tokens processed (500 successful, 0 failed) [0.4s elapsed]\n================================================================================\n✓ DISCOVERY PHASE COMPLETE\n================================================================================\n  • Tokens processed: 500\n  • Successfully clustered: 500\n  • Failed: 0\n  • Total prototypes created: 19519\n  • Multi-sense words (≥2 prototypes): 6634\n  • Time elapsed: 0.39s (0.01 min)\n================================================================================\n\n[DISCOVERY] ✅ Verifying homograph words were clustered:\n--------------------------------------------------------------------------------\n  ✓ 'কল' → 2 prototypes (key='কল') counts=[5, 8]\n  ✓ 'ব্যাংক' → 2 prototypes (key='ব্যাংক') counts=[10, 7]\n  ✓ 'কাল' → 2 prototypes (key='কাল') counts=[11, 5]\n  ✓ 'মাথা' → 2 prototypes (key='মাথা') counts=[9, 9]\n  ✓ 'ফল' → 2 prototypes (key='ফল') counts=[9, 8]\n  ✓ 'পাতা' → 2 prototypes (key='পাতা') counts=[10, 5]\n--------------------------------------------------------------------------------\nHomograph verification: 6/6 detected\n\n✅ All homographs successfully clustered! Disambiguation ready.\n\n[CELL10] Running additional DSCD inference warmup...\n[WARMUP] Starting DSCD discovery warmup...\n[CELL2] Loading up to 1000 samples from local CSV: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\n[CELL2] Reading CSV file...\n[CELL2] Processing 1000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 1000/1000 [00:00<00:00, 20586.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 1000 pairs from CSV, skipped 0 rows\n[WARMUP] Prototype discovery: word_types=22655, total_protos=18685, multi_sense=6222\n[WARMUP] Restored DSCD configuration\n[CELL10] Inference warmup complete\n\n[CELL10] Step 8: Evaluation\n\n================================================================================\nCOMPREHENSIVE POST-TRAINING EVALUATION (Cell 9)\n================================================================================\n\n[EVAL] Running 5 tests...\n--------------------------------------------------------------------------------\n\nTest 1/5: কল = tap / call\n============================================================\nInput: আমি কল বন্ধ করেছি।\nTranslation: i closed the call.\nAmbiguous Words (real, counted): 0\nNo explanations produced (likely high-confidence translation)\nTranslation successful\n------------------------------------------------------------\n\nTest 2/5: কাল = tomorrow / yesterday\n============================================================\nInput: কাল আমি বই কিনব।\nTranslation: i will buy it tomorrow.\nAmbiguous Words (real, counted): 0\nNo explanations produced (likely high-confidence translation)\nTranslation successful\n------------------------------------------------------------\n\nTest 3/5: পাতা = leaf / page\n============================================================\nInput: পাতা ঝরে পড়েছে।\nTranslation: the page has fallen.\nAmbiguous Words (real, counted): 0\nNo explanations produced (likely high-confidence translation)\nTranslation successful\n------------------------------------------------------------\n\nTest 4/5: ব্যাংক = bank / embankment\n============================================================\nInput: তিনি ব্যাংক গেছেন।\nTranslation: he went to the bank.\nAmbiguous Words (real, counted): 0\nNo explanations produced (likely high-confidence translation)\nTranslation successful\n------------------------------------------------------------\n\nTest 5/5: Simple sentence (no ambiguity expected)\n============================================================\nInput: আজ ভাল আবহাওয়া।\nTranslation: good weather today.\nAmbiguous Words (real, counted): 0\nNo explanations produced (likely high-confidence translation)\nTranslation successful\n------------------------------------------------------------\n\n================================================================================\nEVALUATION SUMMARY\n================================================================================\nTotal tests: 5\nSuccessful translations: 5\nSuccess rate: 100.0%\n\nAmbiguity detection:\n  - Total explanations produced: 0\n  - High-span (S>0.3): 0\n  - Real ambiguous (S>0.3 or U>0.4): 0\n  - Avg explanations/test: 0.00\n  - Avg real ambiguous/test: 0.00\n\nDSCD Prototype Discovery:\n  - Word types tracked: 22655\n  - Multi-sense words (>=2 protos): 6222\n  - Total prototypes: 18686\n  - Avg prototypes/word: 0.82\n================================================================================\n[CELL10] Saving model...\n[CELL10] Model state saved to tatn_kaggle_final.pt\n\n[CELL10] Final Report Summary:\n  Success Rate: 100.00%\n  DSCD prototype stats: {'total_words': 22655, 'multi_sense_words': 6222, 'total_prototypes': 18686}\n\n================================================================================\nSYSTEM INITIALIZATION SUCCEEDED\n================================================================================\n\nCapabilities:\n   • Bengali → English translation\n   • Automatic homograph disambiguation (DSCD + TRG)\n   • Dynamic prototype discovery (hierarchical clustering)\n   • Multi-GPU acceleration (2 GPUs)\n================================================================================\n\nQuick Inference Validation (single sample):\n  Testing sentence: আমি কল বন্ধ করেছি।\n  Translation: i closed the call.\n  Ambiguous Words Detected: 0\n  No explanations returned (high-confidence translation)\n\nCELL 11: Execution wrapper finished.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ==============================================================================\n# CELL 12 (fixed): EXTENDED INFERENCE TESTING & ROBUST CHECKPOINT LOADER\n# - Robust load_state_dict handling across PyTorch versions (namedtuple or tuple)\n# - Clearer error messages and tracebacks controlled by VERBOSE_LOGGING\n# - Safe device mapping and embedding resize before state load\n# - Optional DSCD warm-up when prototypes are empty\n# - Defensive guards for missing globals / helpers\n# ==============================================================================\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Any, Dict, List, Optional, Union\n\nimport torch\n\n# -------------------------\n# Local fallbacks for globals (safe)\n# -------------------------\ntry:\n    _DEVICE = DEVICE  # may be a torch.device or str in the user's globals\n    _USE_MULTI_GPU = USE_MULTI_GPU\n    _NUM_GPUS = NUM_GPUS\n    _VERBOSE_LOGGING = VERBOSE_LOGGING\nexcept Exception:\n    _DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    _USE_MULTI_GPU = _NUM_GPUS > 1\n    _VERBOSE_LOGGING = False\n    print(\"[CELL12] Warning: using fallback device/settings\")\n\n# Determine real-ambiguity uncertainty threshold consistently (use TAU_LOW if present)\n_REAL_AMB_UNCERTAINTY = float(globals().get(\"TAU_LOW\", 0.4))\n\n# Helpers -----------------------------------------------------------------------\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING:\n        traceback.print_exc()\n    else:\n        _safe_print(\"   (set VERBOSE_LOGGING = True for full traceback)\")\n\n\ndef _looks_like_state_dict(d: Any) -> bool:\n    \"\"\"\n    Heuristic to detect whether `d` is a PyTorch state_dict-like mapping:\n      - mapping and keys are strings that contain '.' (module.weight style)\n      - or many values are tensors\n    \"\"\"\n    try:\n        if not isinstance(d, dict):\n            return False\n        if not d:\n            return False\n        # If many keys contain dots, likely state dict\n        dot_keys = sum(1 for k in d.keys() if isinstance(k, str) and \".\" in k)\n        if dot_keys >= max(1, len(d) // 5):\n            return True\n        # If many values are tensors -> also likely state dict\n        sample_vals = list(d.values())[:min(20, len(d))]\n        tensor_count = sum(1 for v in sample_vals if torch.is_tensor(v))\n        if tensor_count >= max(1, len(sample_vals) // 3):\n            return True\n    except Exception:\n        pass\n    return False\n\n# ------------------------------------------------------------------------------\n# try_load_checkpoint: robust loader\n# ------------------------------------------------------------------------------\ndef try_load_checkpoint(checkpoint_path: str, tokenizer: Any) -> Tuple[bool, Union[Any, str]]:\n    \"\"\"\n    Try to load a checkpoint file into a freshly instantiated model.\n    Returns (success, model_instance_or_error_message_or_exception).\n\n    Robust behaviors:\n      - Accepts common checkpoint layouts (dict with 'model_state_dict'|'state_dict', nested 'model':{'state_dict':...}, or direct state-dict)\n      - Attempts embedding resize before loading when tokenizer differs\n      - Retries with stripped 'module.' prefixes if needed\n      - Tries key decoding (bytes -> str) and simple key normalization\n      - Loads to CPU first then moves model to _DEVICE\n      - Sets model.eval() and returns it on success\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        return False, f\"Checkpoint path not found: {checkpoint_path}\"\n\n    if \"MemoryOptimizedTATNWithExplanations\" not in globals():\n        return False, \"Model class MemoryOptimizedTATNWithExplanations not available in current session.\"\n\n    _safe_print(f\"[CELL12] Loading checkpoint from: {checkpoint_path}\")\n    try:\n        ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n    except Exception as e:\n        _safe_print(f\"[CELL12] Failed to load checkpoint file: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        return False, e\n\n    # Locate a plausible state dict inside ckpt\n    state = None\n    try:\n        if isinstance(ckpt, dict):\n            # 1) conventional top-level keys\n            cand_keys = [\n                \"model_state_dict\", \"state_dict\", \"model\", \"model_state\", \"state\", \"net\", \"model_state_dicts\",\n                \"module_state_dict\", \"module\"\n            ]\n            for k in cand_keys:\n                v = ckpt.get(k, None)\n                if v is None:\n                    continue\n                if isinstance(v, dict) and _looks_like_state_dict(v):\n                    state = v\n                    break\n                # nested mapping containing state_dict\n                if isinstance(v, dict) and \"state_dict\" in v and isinstance(v[\"state_dict\"], dict):\n                    state = v[\"state_dict\"]\n                    break\n            # 2) If not found, check if the top-level mapping itself looks like a state_dict\n            if state is None and _looks_like_state_dict(ckpt):\n                state = ckpt\n            # 3) try shallow search: any value that is a dict and looks like state_dict\n            if state is None:\n                for v in ckpt.values():\n                    try:\n                        if isinstance(v, dict) and _looks_like_state_dict(v):\n                            state = v\n                            break\n                    except Exception:\n                        continue\n        else:\n            # ckpt itself might be a state-dict-like mapping\n            if isinstance(ckpt, dict) and _looks_like_state_dict(ckpt):\n                state = ckpt\n    except Exception as e:\n        _safe_print(f\"[CELL12] Error while inspecting checkpoint structure: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        return False, e\n\n    if state is None:\n        return False, \"Could not find a model state-dict inside the checkpoint.\"\n\n    # Instantiate a fresh model\n    try:\n        model_inst = MemoryOptimizedTATNWithExplanations(tokenizer)\n    except Exception as e:\n        _safe_print(f\"[CELL12] Failed to instantiate model class: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        return False, e\n\n    # Try to resize embeddings BEFORE loading to reduce mismatch issues\n    try:\n        mbart = getattr(model_inst, \"mbart\", None)\n        if mbart is not None and hasattr(mbart, \"get_input_embeddings\"):\n            emb = mbart.get_input_embeddings()\n            cur = getattr(emb, \"num_embeddings\", None)\n            tok_len = None\n            try:\n                if tokenizer is None:\n                    tok_len = None\n                elif hasattr(tokenizer, \"vocab_size\") and getattr(tokenizer, \"vocab_size\") is not None:\n                    tok_len = int(getattr(tokenizer, \"vocab_size\"))\n                elif hasattr(tokenizer, \"__len__\"):\n                    tok_len = int(len(tokenizer))\n                else:\n                    tok_len = None\n            except Exception:\n                tok_len = getattr(tokenizer, \"vocab_size\", None) if tokenizer is not None else None\n\n            if cur is not None and tok_len is not None and int(cur) != int(tok_len) and int(tok_len) > 0:\n                _safe_print(f\"[CELL12] Resizing embeddings: {cur} -> {tok_len}\")\n                try:\n                    mbart.resize_token_embeddings(tok_len)\n                except Exception as ex:\n                    _safe_print(f\"[CELL12] Embedding resize attempt failed: {type(ex).__name__}: {str(ex)[:200]}\")\n                    _maybe_traceback(ex)\n    except Exception as e:\n        _safe_print(f\"[CELL12] Embedding resize warning: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n\n    # Helper: attempt to load and return missing/unexpected lists (handles both tuple and NamedTuple results)\n    def _load_and_report(state_dict: Dict[str, Any]) -> Tuple[bool, List[str], List[str], Optional[Exception]]:\n        try:\n            res = model_inst.load_state_dict(state_dict, strict=False)\n            missing: List[str] = []\n            unexpected: List[str] = []\n            # new-style IncompatibleKeys\n            if hasattr(res, \"missing_keys\") or hasattr(res, \"unexpected_keys\"):\n                missing = list(getattr(res, \"missing_keys\", []) or [])\n                unexpected = list(getattr(res, \"unexpected_keys\", []) or [])\n            else:\n                # old-style tuple (missing, unexpected)\n                try:\n                    if isinstance(res, (tuple, list)) and len(res) == 2:\n                        missing = list(res[0]) or []\n                        unexpected = list(res[1]) or []\n                except Exception:\n                    missing, unexpected = [], []\n            return True, missing, unexpected, None\n        except Exception as ex:\n            return False, [str(ex)], [], ex\n\n    # First attempt: direct load\n    try:\n        ok, missing, unexpected, exc = _load_and_report(state)\n        if not ok:\n            raise RuntimeError(f\"Primary load_state_dict failed: {missing[:3]}\")\n        _safe_print(f\"[CELL12] Loaded checkpoint (strict=False). Missing keys: {len(missing)} Unexpected keys: {len(unexpected)}\")\n        if _VERBOSE_LOGGING:\n            if missing:\n                _safe_print(f\"  Missing keys (sample up to 20): {missing[:20]}\")\n            if unexpected:\n                _safe_print(f\"  Unexpected keys (sample up to 20): {unexpected[:20]}\")\n    except Exception as e:\n        _safe_print(f\"[CELL12] load_state_dict(strict=False) raised: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        # Retry 1: strip 'module.' prefixes (DataParallel artifact)\n        try:\n            if isinstance(state, dict):\n                new_state = {}\n                for k, v in state.items():\n                    new_key = k\n                    if isinstance(k, str) and k.startswith(\"module.\"):\n                        new_key = k.replace(\"module.\", \"\", 1)\n                    new_state[new_key] = v\n                ok, missing, unexpected, exc = _load_and_report(new_state)\n                if ok:\n                    _safe_print(\"[CELL12] Retried loading after stripping 'module.' prefixes.\")\n                    if _VERBOSE_LOGGING:\n                        _safe_print(f\"  Missing: {missing[:20]} Unexpected: {unexpected[:20]}\")\n                else:\n                    raise RuntimeError(f\"Retry after strip failed: {missing[:3]}\")\n            else:\n                raise RuntimeError(\"State-dict is not a dict; cannot strip prefixes\")\n        except Exception as e2:\n            _safe_print(f\"[CELL12] Retry after stripping prefixes also failed: {type(e2).__name__}: {str(e2)[:200]}\")\n            _maybe_traceback(e2)\n            # Retry 2: try converting byte keys to str if necessary\n            try:\n                if isinstance(state, dict):\n                    conv_state = {}\n                    changed = False\n                    for k, v in state.items():\n                        if isinstance(k, bytes):\n                            try:\n                                nk = k.decode(\"utf-8\")\n                                conv_state[nk] = v\n                                changed = True\n                            except Exception:\n                                conv_state[k] = v\n                        else:\n                            conv_state[k] = v\n                    if changed:\n                        ok, missing, unexpected, exc = _load_and_report(conv_state)\n                        if ok:\n                            _safe_print(\"[CELL12] Retried loading after decoding byte keys to str.\")\n                        else:\n                            raise RuntimeError(f\"Retry after decode keys failed: {missing[:3]}\")\n                    else:\n                        raise RuntimeError(\"No byte-key conversion possible; load failed previously.\")\n                else:\n                    raise RuntimeError(\"State-dict is not a dict for byte-key conversion\")\n            except Exception as e3:\n                _safe_print(f\"[CELL12] All retry attempts failed: {type(e3).__name__}: {str(e3)[:200]}\")\n                _maybe_traceback(e3)\n                return False, e3\n\n    # Move model to target device and set eval()\n    try:\n        model_inst.to(_DEVICE)\n        model_inst.eval()\n    except Exception as e:\n        try:\n            core = model_inst.module if hasattr(model_inst, \"module\") else model_inst\n            core.to(_DEVICE)\n            core.eval()\n            model_inst = core\n        except Exception:\n            _safe_print(f\"[CELL12] Failed to move model to device: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return False, e\n\n    _safe_print(\"[CELL12] Checkpoint successfully loaded and model prepared on device.\")\n    return True, model_inst\n\n\n# ------------------------------------------------------------------------------\n# If a checkpoint exists, prefer loading it (but fall back to trained_model)\nif os.path.exists(\"tatn_kaggle_final.pt\") and globals().get(\"tokenizer\", None) is not None:\n    succ, model_or_err = try_load_checkpoint(\"tatn_kaggle_final.pt\", globals().get(\"tokenizer\"))\n    if succ:\n        globals()['trained_model'] = model_or_err\n        _safe_print(\"[CELL12] Checkpoint loaded and will be used for inference testing.\")\n    else:\n        _safe_print(\"[CELL12] Checkpoint load failed; falling back to trained_model from runtime (if available).\")\n        if _VERBOSE_LOGGING:\n            _maybe_traceback(model_or_err)\n\n# Warmup helper (useful if prototypes empty)\ndef maybe_run_warmup_if_needed(model, tokenizer, warmup_sents: int = 4000):\n    \"\"\"\n    If DSCD prototype stores are empty, optionally run a short discovery warmup to\n    populate DSCD buffers and allow prototype clustering to run. Uses dscd_discovery_warmup if present.\n    \"\"\"\n    try:\n        core = model.module if hasattr(model, \"module\") else model\n        dscd = getattr(core, \"dscd\", None)\n        if dscd is None:\n            _safe_print(\"[CELL12] No DSCD component on the model; skipping warmup.\")\n            return\n        proto_stores = getattr(dscd, \"prototype_stores\", None)\n        if not proto_stores or len(proto_stores) == 0:\n            if 'dscd_discovery_warmup' in globals():\n                _safe_print(\"[CELL12] No DSCD prototypes detected - running short warmup to build prototypes...\")\n                try:\n                    dscd_discovery_warmup(model, tokenizer, num_sents=warmup_sents, max_len=globals().get(\"MAX_LENGTH\", 48))\n                    _safe_print(\"[CELL12] Warmup complete.\")\n                except Exception as e:\n                    _safe_print(f\"[CELL12] Warmup failed/skipped: {type(e).__name__}: {str(e)[:200]}\")\n                    _maybe_traceback(e)\n            else:\n                _safe_print(\"[CELL12] Warmup helper not available - skipping prototype building.\")\n        else:\n            _safe_print(f\"[CELL12] DSCD prototype stores already contain {len(proto_stores)} types; warmup not needed.\")\n    except Exception as e:\n        _safe_print(f\"[CELL12] Warmup probe failed: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n\n\n# Prepare test sentences -------------------------------------------------------\ntest_sentences: List[Tuple[str, str, str]] = [\n    (\"আমি কল বন্ধ করেছি।\", \"I turned off the tap\", \"কল = tap/call\"),\n    (\"কাল আমি বই কিনব।\", \"Tomorrow I will buy a book\", \"কাল = tomorrow/yesterday\"),\n    (\"পাতা ঝরে পড়েছে।\", \"The leaf has fallen\", \"পাতা = leaf/page\"),\n    (\"তিনি ব্যাংক গেছেন।\", \"He went to the bank\", \"ব্যাংক = bank/embankment\"),\n    (\"আমি ভালো আছি।\", \"I am fine\", \"Simple (no ambiguity)\"),\n    (\"সে খুব মিষ্টি কথা বলে।\", \"She speaks sweetly\", \"Adjective usage\"),\n    (\"এটা আমার বই।\", \"This is my book\", \"Demonstrative pronoun\"),\n    (\"তুমি কি আমাকে সাহায্য করতে পারো?\", \"Can you help me?\", \"Question form\"),\n    (\"আজ আবহাওয়া ভালো।\", \"Weather description\", \"Simple\"),\n    (\"আমরা বাংলাদেশে বাস করি।\", \"We live in Bangladesh\", \"Country name\"),\n    (\"সূর্য পূর্ব দিকে ওঠে।\", \"The sun rises in the east\", \"Directional\"),\n    (\"পাখি আকাশে উড়ে।\", \"Birds fly in the sky\", \"Simple present\"),\n    (\"সে স্কুলে যাচ্ছে।\", \"She is going to school\", \"Present continuous\"),\n]\n\n# Verify prerequisites ---------------------------------------------------------\ntrained_model_present = ('trained_model' in globals() and globals().get('trained_model') is not None)\ntokenizer_available = ('tokenizer' in globals() and globals().get('tokenizer') is not None)\ntranslate_available = ('translate_with_explanations' in globals() and callable(globals().get('translate_with_explanations')))\n\nif not (trained_model_present and tokenizer_available and translate_available):\n    _safe_print(\"\\n❌ Cannot run extended inference tests. Missing one or more of: trained_model, tokenizer, translate_with_explanations.\")\n    _safe_print(\"   Please run the full pipeline (Cells 0-11) or load a model checkpoint and tokenizer.\")\nelse:\n    # Ensure prototypes warmup if needed\n    try:\n        maybe_run_warmup_if_needed(globals().get('trained_model'), globals().get(\"tokenizer\"), warmup_sents=4000)\n    except Exception as e:\n        _safe_print(f\"[CELL12] Warmup invocation failed: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n\n    # Run tests\n    total = len(test_sentences)\n    successes = 0\n    tests_with_explanations = 0\n    total_ambiguous_detected = 0\n\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"CELL 12: EXTENDED INFERENCE TESTING - START\")\n    _safe_print(\"=\" * 80)\n\n    for idx, (sent, expected, note) in enumerate(test_sentences, 1):\n        _safe_print(\"\\n\" + \"-\" * 70)\n        _safe_print(f\"Test {idx}/{total}: {note}\")\n        _safe_print(f\"Input: {sent}\")\n        _safe_print(f\"Expected (informal): {expected}\")\n        try:\n            model_for_infer = globals().get('trained_model')\n            tok = globals().get('tokenizer')\n            if model_for_infer is None or tok is None:\n                raise RuntimeError(\"trained_model or tokenizer missing at inference time\")\n\n            # translate_with_explanations can be DP-wrapped; call whatever is present\n            try:\n                res = translate_with_explanations(model_for_infer, tok, sent)\n            except Exception as e:\n                _safe_print(f\"[CELL12] translate_with_explanations raised: {type(e).__name__}: {str(e)[:200]}\")\n                _maybe_traceback(e)\n                res = None\n\n            if res is None:\n                _safe_print(\"[CELL12] translate_with_explanations returned None or raised; skipping this test.\")\n                continue\n\n            if not isinstance(res, dict):\n                _safe_print(f\"[CELL12] Warning: translate_with_explanations returned non-dict: {type(res)}; coercing to dict\")\n                res = {\"translation\": str(res)}\n\n            translation = str(res.get(\"translation\", \"\") or \"\")\n            amb_count = int(res.get(\"ambiguous_words_detected\", 0) or 0)\n            explanations = res.get(\"explanations\", []) or []\n\n            _safe_print(f\"Translation: {translation}\")\n            _safe_print(f\"Ambiguous words detected (real): {amb_count}\")\n\n            if amb_count > 0:\n                tests_with_explanations += 1\n                total_ambiguous_detected += amb_count\n                _safe_print(\"Explanations (ambiguous tokens):\")\n                for j, e in enumerate(explanations, 1):\n                    try:\n                        word = e.get(\"ambiguous_word\", e.get(\"token\", \"N/A\"))\n                        u = float(e.get(\"uncertainty\", 0.0) or 0.0)\n                        s = float(e.get(\"span\", 0.0) or 0.0)\n                        marker = \"🔥\" if s > 0.3 else \"  \"\n                        _safe_print(f\"  {j}. {marker} '{word}'  U={u:.3f}  S={s:.3f}\")\n                        _safe_print(f\"       {e.get('explanation', '')}\")\n                    except Exception:\n                        if _VERBOSE_LOGGING:\n                            traceback.print_exc()\n                        continue\n            else:\n                _safe_print(\"No real ambiguity detected\")\n\n            if translation and translation.strip():\n                successes += 1\n                _safe_print(\"Translation produced (non-empty) → counted as successful\")\n            else:\n                _safe_print(\"Translation empty or failed → counted as unsuccessful\")\n\n        except Exception as e:\n            _safe_print(f\"Test {idx} failed with exception: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n\n    # Summary\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"CELL 12: EXTENDED INFERENCE TEST SUMMARY\")\n    _safe_print(\"=\" * 80)\n    _safe_print(f\"Total tests: {total}\")\n    if total > 0:\n        _safe_print(f\"Successful translations: {successes} ({successes/total*100:.1f}%)\")\n        _safe_print(f\"Tests with explanations: {tests_with_explanations} ({tests_with_explanations/total*100:.1f}%)\")\n        _safe_print(f\"Total ambiguous words detected (real): {total_ambiguous_detected}\")\n        _safe_print(f\"Avg ambiguous words per sentence: {total_ambiguous_detected/total:.2f}\")\n    else:\n        _safe_print(\"No tests were executed\")\n    _safe_print(\"=\" * 80)\n    _safe_print(f\"Real-ambiguity thresholds used: span > 0.3 OR uncertainty > {_REAL_AMB_UNCERTAINTY:.2f}\")\n    _safe_print(\"Cell 12 testing complete.\")","metadata":{"id":"zWd0uRn7H4J6","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:42:09.215540Z","iopub.execute_input":"2025-11-22T18:42:09.216263Z","iopub.status.idle":"2025-11-22T18:43:11.227059Z","shell.execute_reply.started":"2025-11-22T18:42:09.216238Z","shell.execute_reply":"2025-11-22T18:43:11.226368Z"}},"outputs":[{"name":"stdout","text":"[CELL12] Loading checkpoint from: tatn_kaggle_final.pt\n[CELL12] Resizing embeddings: 128112 -> 128104\n[CELL12] Loaded checkpoint (strict=False). Missing keys: 0 Unexpected keys: 0\n[CELL12] Checkpoint successfully loaded and model prepared on device.\n[CELL12] Checkpoint loaded and will be used for inference testing.\n[CELL12] No DSCD prototypes detected - running short warmup to build prototypes...\n[WARMUP] Starting DSCD discovery warmup...\n[CELL2] Loading up to 4000 samples from local CSV: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\n[CELL2] Reading CSV file...\n[CELL2] Processing 4000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 4000/4000 [00:00<00:00, 21458.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 4000 pairs from CSV, skipped 0 rows\n[WARMUP] Prototype discovery: word_types=750, total_protos=563, multi_sense=0\n[WARMUP] Restored DSCD configuration\n[CELL12] Warmup complete.\n\n================================================================================\nCELL 12: EXTENDED INFERENCE TESTING - START\n================================================================================\n\n----------------------------------------------------------------------\nTest 1/13: কল = tap/call\nInput: আমি কল বন্ধ করেছি।\nExpected (informal): I turned off the tap\nTranslation: i closed the call.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 2/13: কাল = tomorrow/yesterday\nInput: কাল আমি বই কিনব।\nExpected (informal): Tomorrow I will buy a book\nTranslation: i will buy it tomorrow.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 3/13: পাতা = leaf/page\nInput: পাতা ঝরে পড়েছে।\nExpected (informal): The leaf has fallen\nTranslation: the page has fallen.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 4/13: ব্যাংক = bank/embankment\nInput: তিনি ব্যাংক গেছেন।\nExpected (informal): He went to the bank\nTranslation: he went to the bank.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 5/13: Simple (no ambiguity)\nInput: আমি ভালো আছি।\nExpected (informal): I am fine\nTranslation: i am well.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 6/13: Adjective usage\nInput: সে খুব মিষ্টি কথা বলে।\nExpected (informal): She speaks sweetly\nTranslation: he speaks very sweet.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 7/13: Demonstrative pronoun\nInput: এটা আমার বই।\nExpected (informal): This is my book\nTranslation: this is my.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 8/13: Question form\nInput: তুমি কি আমাকে সাহায্য করতে পারো?\nExpected (informal): Can you help me?\nTranslation: can you help me?\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 9/13: Simple\nInput: আজ আবহাওয়া ভালো।\nExpected (informal): Weather description\nTranslation: the weather is good today.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 10/13: Country name\nInput: আমরা বাংলাদেশে বাস করি।\nExpected (informal): We live in Bangladesh\nTranslation: we live in Bangladesh.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 11/13: Directional\nInput: সূর্য পূর্ব দিকে ওঠে।\nExpected (informal): The sun rises in the east\nTranslation: the sun rises to the east.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 12/13: Simple present\nInput: পাখি আকাশে উড়ে।\nExpected (informal): Birds fly in the sky\nTranslation: birds fly in the sky.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n----------------------------------------------------------------------\nTest 13/13: Present continuous\nInput: সে স্কুলে যাচ্ছে।\nExpected (informal): She is going to school\nTranslation: he is going to school.\nAmbiguous words detected (real): 0\nNo real ambiguity detected\nTranslation produced (non-empty) → counted as successful\n\n================================================================================\nCELL 12: EXTENDED INFERENCE TEST SUMMARY\n================================================================================\nTotal tests: 13\nSuccessful translations: 13 (100.0%)\nTests with explanations: 0 (0.0%)\nTotal ambiguous words detected (real): 0\nAvg ambiguous words per sentence: 0.00\n================================================================================\nReal-ambiguity thresholds used: span > 0.3 OR uncertainty > 0.40\nCell 12 testing complete.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==============================================================================\n# CELL 13 (patched): LARGE-SCALE EVALUATION (2000+ SAMPLES) - OPTIMIZED & HARDENED\n# ==============================================================================\n# - Batched generation (faster + VRAM-friendly)\n# - Safe handling of DataParallel / wrapper models\n# - Defensive tokenizer/lang-id handling for forced_bos_token_id\n# - Robust metrics imports and fallbacks\n# - CSV export, progress reporting, and clear error handling\n# - Hardened decoding and many defensive fallbacks for real-world model/tokenizer shapes\n# ==============================================================================\n\nimport os\nimport sys\nimport warnings\nimport time\nimport csv\nimport traceback\nfrom typing import List, Dict, Tuple, Optional, Any\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# ------------------------------\n# Metrics availability detection\n# ------------------------------\nHAS_COMET = False\nHAS_BLEU = False\nHAS_CHRF = False\n\n# Attempt COMET imports (optional)\ntry:\n    # many environments won't have comet; guard carefully\n    from comet import download_model, load_from_checkpoint  # type: ignore\n    HAS_COMET = True\nexcept Exception:\n    HAS_COMET = False\n\n# SacreBLEU (BLEU + CHRF)\ntry:\n    import sacrebleu  # type: ignore\n    # Validate presence of expected API functions\n    if hasattr(sacrebleu, \"corpus_bleu\"):\n        HAS_BLEU = True\n    if hasattr(sacrebleu, \"corpus_chrf\"):\n        HAS_CHRF = True\nexcept Exception:\n    HAS_BLEU = False\n    HAS_CHRF = False\n\n# ------------------------------\n# Local safe global fallbacks\n# ------------------------------\n_DEVICE = globals().get(\"DEVICE\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n_VERBOSE_LOGGING = bool(globals().get(\"VERBOSE_LOGGING\", False))\n\n# ------------------------------\n# Utility helpers\n# ------------------------------\ndef _safe_print(msg: str):\n    try:\n        print(msg)\n    except Exception:\n        pass\n\ndef _maybe_traceback(exc: Exception):\n    if _VERBOSE_LOGGING:\n        traceback.print_exc()\n    else:\n        print(\"   (set VERBOSE_LOGGING = True in Cell 0 for full traceback)\")\n\ndef _unwrap_model(model: torch.nn.Module) -> torch.nn.Module:\n    \"\"\"Return core model (unwrap DataParallel/DistributedDataParallel if needed).\"\"\"\n    return model.module if hasattr(model, \"module\") else model\n\ndef _get_forced_bos_id(tokenizer, core_mbart) -> Optional[int]:\n    \"\"\"Try several tokenizer/model attributes to find an English forced BOS id.\"\"\"\n    forced_id = None\n    try:\n        if hasattr(tokenizer, \"get_lang_id\"):\n            for code in (\"en\", \"en_XX\", \"en-XX\", \"eng\"):\n                try:\n                    lid = tokenizer.get_lang_id(code)\n                    if lid is not None:\n                        forced_id = lid\n                        break\n                except Exception:\n                    continue\n        elif hasattr(tokenizer, \"lang_code_to_id\"):\n            for code in (\"en\", \"en_XX\", \"en-XX\", \"eng\"):\n                try:\n                    candidate = tokenizer.lang_code_to_id.get(code, None)\n                    if candidate is not None:\n                        forced_id = candidate\n                        break\n                except Exception:\n                    continue\n    except Exception:\n        forced_id = None\n    # fallback to mbart config if available\n    try:\n        if forced_id is None and core_mbart is not None and hasattr(core_mbart, \"config\"):\n            forced_id = getattr(core_mbart.config, \"forced_bos_token_id\", None)\n            if forced_id is None:\n                forced_id = getattr(core_mbart.config, \"decoder_start_token_id\", None)\n    except Exception:\n        forced_id = None\n    return forced_id\n\n# ------------------------------\n# Large scale metrics class\n# ------------------------------\nclass LargeScaleEvaluationMetrics:\n    \"\"\"Compute metrics on many samples efficiently, with fallbacks.\"\"\"\n    def __init__(self, device: Optional[torch.device] = None, batch_size: int = 32):\n        self.device = device or _DEVICE\n        self.batch_size = int(batch_size)\n        self.comet_model = None\n        self.metrics_available = {\"comet\": HAS_COMET, \"bleu\": HAS_BLEU, \"chrf\": HAS_CHRF}\n        _safe_print(\"\\n\" + \"=\" * 80)\n        _safe_print(\"INITIALIZING LARGE-SCALE EVALUATION METRICS\")\n        _safe_print(\"=\" * 80)\n        _safe_print(f\"Device: {self.device}\")\n        _safe_print(f\"Batch Size: {self.batch_size}\")\n        _safe_print(f\"Metrics Available: BLEU={HAS_BLEU}, CHRF={HAS_CHRF}, COMET={HAS_COMET}\")\n        _safe_print(\"=\" * 80 + \"\\n\")\n\n        if HAS_COMET:\n            try:\n                _safe_print(\"[EVAL] Loading COMET model (may take time)...\")\n                try:\n                    model_path = download_model(\"Unbabel/wmt22-comet-da\", saving_directory=\".comet_cache\")\n                    self.comet_model = load_from_checkpoint(model_path)\n                    _safe_print(\"[EVAL] ✓ COMET model loaded\")\n                except Exception:\n                    _safe_print(\"[EVAL] COMET automatic load failed; disabling COMET for this run.\")\n                    self.metrics_available[\"comet\"] = False\n                    self.comet_model = None\n            except Exception:\n                self.metrics_available[\"comet\"] = False\n                self.comet_model = None\n\n    def compute_bleu_large(self, references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        if not self.metrics_available[\"bleu\"] or not references or not hypotheses:\n            return {\"bleu\": None, \"error\": \"BLEU unavailable or empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            _safe_print(f\"\\n[BLEU] Computing BLEU on {len(hypotheses)} samples...\")\n            start_time = time.time()\n            import sacrebleu  # type: ignore\n            # sacrebleu expects list-of-reference-lists\n            score = sacrebleu.corpus_bleu(hypotheses, [references])\n            elapsed = time.time() - start_time\n            result = {\"bleu\": float(score.score), \"num_samples\": len(hypotheses), \"computation_time_sec\": elapsed}\n            _safe_print(f\"[BLEU] ✓ {score.score:.2f}/100 computed in {elapsed:.2f}s\")\n            return result\n        except Exception as e:\n            _safe_print(f\"[BLEU] Error computing BLEU: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"bleu\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_chrf_large(self, references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        if not self.metrics_available[\"chrf\"] or not references or not hypotheses:\n            return {\"chrf\": None, \"error\": \"CHRF unavailable or empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            _safe_print(f\"\\n[CHRF++] Computing CHRF++ on {len(hypotheses)} samples...\")\n            start_time = time.time()\n            import sacrebleu  # type: ignore\n            score = sacrebleu.corpus_chrf(hypotheses, [references], beta=3.0)\n            elapsed = time.time() - start_time\n            result = {\"chrf\": float(score.score), \"num_samples\": len(hypotheses), \"computation_time_sec\": elapsed}\n            _safe_print(f\"[CHRF++] ✓ {score.score:.2f}/100 computed in {elapsed:.2f}s\")\n            return result\n        except Exception as e:\n            _safe_print(f\"[CHRF++] Error computing CHRF++: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"chrf\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_comet_large(self, source_texts: List[str], references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        if not self.metrics_available.get(\"comet\") or self.comet_model is None:\n            return {\"comet\": None, \"error\": \"COMET unavailable\", \"num_samples\": len(hypotheses)}\n        if not source_texts or not references or not hypotheses:\n            return {\"comet\": None, \"error\": \"Empty inputs\", \"num_samples\": len(hypotheses)}\n        try:\n            _safe_print(f\"\\n[COMET] Computing COMET on {len(hypotheses)} samples...\")\n            start_time = time.time()\n            data = [{\"src\": s, \"ref\": r, \"mt\": h} for s, r, h in zip(source_texts, references, hypotheses)]\n            try:\n                if torch.cuda.is_available():\n                    self.comet_model.to(self.device)\n            except Exception:\n                pass\n            with torch.no_grad():\n                if hasattr(self.comet_model, \"predict\"):\n                    output = self.comet_model.predict(data, batch_size=self.batch_size, gpus=1 if torch.cuda.is_available() else 0)\n                    scores = np.asarray(getattr(output, \"scores\", []) or [], dtype=np.float32)\n                    system_score = getattr(output, \"system_score\", None)\n                else:\n                    scores = []\n                    for i in range(0, len(data), self.batch_size):\n                        batch = data[i : i + self.batch_size]\n                        try:\n                            out = self.comet_model.predict(batch)\n                            scores.extend(getattr(out, \"scores\", []) or [])\n                        except Exception:\n                            break\n                    scores = np.asarray(scores, dtype=np.float32) if scores else np.array([])\n                    system_score = float(np.mean(scores)) if scores.size else None\n            elapsed = time.time() - start_time\n            result = {\n                \"comet\": float(system_score) if system_score is not None else None,\n                \"comet_mean\": float(np.mean(scores)) if scores.size else None,\n                \"comet_median\": float(np.median(scores)) if scores.size else None,\n                \"comet_std\": float(np.std(scores)) if scores.size else None,\n                \"comet_min\": float(np.min(scores)) if scores.size else None,\n                \"comet_max\": float(np.max(scores)) if scores.size else None,\n                \"comet_scores\": scores.tolist() if scores.size else [],\n                \"num_samples\": len(hypotheses),\n                \"computation_time_sec\": elapsed,\n            }\n            _safe_print(f\"[COMET] ✓ Computed in {elapsed:.2f}s\")\n            return result\n        except Exception as e:\n            _safe_print(f\"[COMET] Error computing COMET: {type(e).__name__}: {str(e)[:200]}\")\n            _maybe_traceback(e)\n            return {\"comet\": None, \"error\": str(e)[:200], \"num_samples\": len(hypotheses)}\n\n    def compute_all_metrics_large(self, source_texts: List[str], references: List[str], hypotheses: List[str]) -> Dict[str, Any]:\n        results: Dict[str, Any] = {\"num_samples\": len(hypotheses), \"metrics\": {}, \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")}\n        if self.metrics_available.get(\"bleu\"):\n            results[\"metrics\"][\"bleu\"] = self.compute_bleu_large(references, hypotheses)\n        if self.metrics_available.get(\"chrf\"):\n            results[\"metrics\"][\"chrf\"] = self.compute_chrf_large(references, hypotheses)\n        if self.metrics_available.get(\"comet\"):\n            results[\"metrics\"][\"comet\"] = self.compute_comet_large(source_texts, references, hypotheses)\n        return results\n\n# ------------------------------\n# Main evaluation function\n# ------------------------------\ndef evaluate_on_large_dataset(\n    model: torch.nn.Module,\n    tokenizer,\n    dataset: Optional[List[Tuple[str, str]]] = None,\n    num_samples: int = 2000,\n    batch_size: int = 32,\n    save_results: bool = True,\n    max_length: int = 512,\n) -> Dict[str, Any]:\n    \"\"\"\n    Evaluate model on a dataset (default 2000 samples) and compute metrics.\n    Uses batched tokenization + generation for efficiency and stability.\n    \"\"\"\n    _safe_print(\"\\n\" + \"=\" * 80)\n    _safe_print(\"LARGE-SCALE EVALUATION ON SAMPLES\")\n    _safe_print(\"=\" * 80 + \"\\n\")\n\n    try:\n        # Prepare dataset\n        _safe_print(f\"[PREP] Preparing dataset (requested {num_samples} samples)...\")\n        if not dataset:\n            if \"load_and_preprocess_optimized\" in globals():\n                try:\n                    pairs = load_and_preprocess_optimized(num_samples)\n                except Exception as e:\n                    _safe_print(f\"[PREP] load_and_preprocess_optimized failed: {type(e).__name__}: {str(e)[:200]}\")\n                    _maybe_traceback(e)\n                    sample_pairs = [(\"আমি কল বন্ধ করেছি।\", \"I stopped the call.\"), (\"কাল আমি বই কিনব।\", \"I will buy a book tomorrow.\")]\n                    pairs = (sample_pairs * ((num_samples // len(sample_pairs)) + 1))[:num_samples]\n            else:\n                _safe_print(\"[PREP] No data loader found; using dummy data\")\n                sample_pairs = [(\"আমি কল বন্ধ করেছি।\", \"I stopped the call.\"), (\"কাল আমি বই কিনব।\", \"I will buy a book tomorrow.\")]\n                pairs = (sample_pairs * ((num_samples // len(sample_pairs)) + 1))[:num_samples]\n        else:\n            pairs = dataset\n\n        pairs = pairs[:num_samples]\n        _safe_print(f\"[PREP] ✓ Loaded {len(pairs)} samples\\n\")\n\n        source_texts = [s for s, _ in pairs]\n        references = [r for _, r in pairs]\n        hypotheses: List[str] = []\n\n        # Unwrap core model and prepare generation function\n        core = _unwrap_model(model)\n        core.eval()\n        try:\n            core.to(_DEVICE)\n        except Exception:\n            pass\n\n        # Determine generation callable\n        gen_callable = None\n        mbart = getattr(core, \"mbart\", None)\n        if mbart is not None and hasattr(mbart, \"generate\"):\n            gen_callable = mbart.generate\n            generation_backend = mbart\n        elif hasattr(core, \"generate\"):\n            gen_callable = core.generate\n            generation_backend = core\n        else:\n            raise RuntimeError(\"No generate() found on model or model.mbart\")\n\n        forced_bos = _get_forced_bos_id(tokenizer, mbart)\n\n        # Batch generation\n        _safe_print(f\"[GEN] Generating predictions in batches (batch_size={batch_size}) ...\")\n        n = len(source_texts)\n        batch_size_gen = max(1, int(batch_size))\n        with torch.no_grad():\n            for start in tqdm(range(0, n, batch_size_gen), desc=\"[GEN] Batches\", unit=\"batch\"):\n                batch_srcs = source_texts[start : start + batch_size_gen]\n                try:\n                    # Tokenize batch\n                    try:\n                        try:\n                            # set source language token if tokenizer supports this\n                            if hasattr(tokenizer, \"src_lang\"):\n                                setattr(tokenizer, \"src_lang\", \"bn\")\n                        except Exception:\n                            pass\n                        enc = tokenizer(batch_srcs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n                        enc = {k: v.to(_DEVICE) for k, v in enc.items() if isinstance(v, torch.Tensor)}\n                    except Exception as e:\n                        _safe_print(f\"[GEN] Batch tokenization failed: {type(e).__name__}: {str(e)[:200]}\")\n                        _maybe_traceback(e)\n                        # fallback per-sentence generation\n                        for src in batch_srcs:\n                            try:\n                                try:\n                                    if hasattr(tokenizer, \"src_lang\"):\n                                        setattr(tokenizer, \"src_lang\", \"bn\")\n                                except Exception:\n                                    pass\n                                enc1 = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=max_length)\n                                enc1 = {k: v.to(_DEVICE) for k, v in enc1.items() if isinstance(v, torch.Tensor)}\n                                gen_kwargs1 = {\"max_length\": 128, \"num_beams\": 1, \"early_stopping\": True}\n                                if forced_bos is not None:\n                                    gen_kwargs1[\"forced_bos_token_id\"] = int(forced_bos)\n                                gen_ids = gen_callable(**enc1, **gen_kwargs1)\n                                # decode\n                                if isinstance(gen_ids, torch.Tensor):\n                                    seqs = gen_ids.cpu().tolist()\n                                    # take first sequence\n                                    seq = seqs[0] if isinstance(seqs, list) and len(seqs) > 0 else seqs\n                                    try:\n                                        hyp = tokenizer.batch_decode([seq], skip_special_tokens=True)[0]\n                                    except Exception:\n                                        hyp = tokenizer.decode(seq, skip_special_tokens=True) if hasattr(tokenizer, \"decode\") else \"\"\n                                else:\n                                    try:\n                                        hyp = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n                                    except Exception:\n                                        try:\n                                            hyp = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n                                        except Exception:\n                                            hyp = \"\"\n                                hypotheses.append(hyp)\n                            except Exception:\n                                hypotheses.append(\"\")\n                        continue\n\n                    # call generate\n                    gen_kwargs: Dict[str, Any] = {\"max_length\": 256, \"num_beams\": 5, \"early_stopping\": True}\n                    if forced_bos is not None:\n                        gen_kwargs[\"forced_bos_token_id\"] = int(forced_bos)\n\n                    # Some generate callables are bound methods; ensure proper device for backend if possible\n                    try:\n                        if torch.cuda.is_available() and hasattr(generation_backend, \"to\"):\n                            generation_backend.to(_DEVICE)\n                    except Exception:\n                        pass\n\n                    generated_ids = gen_callable(**enc, **gen_kwargs)\n\n                    # normalize to list-of-seqs for decoding\n                    gen_ids_tensor = None\n                    if isinstance(generated_ids, torch.Tensor):\n                        gen_ids_tensor = generated_ids\n                    elif isinstance(generated_ids, (list, tuple)):\n                        # sometimes returns list of tensors or list of lists\n                        if len(generated_ids) == 0:\n                            gen_ids_tensor = torch.empty((0, 0), dtype=torch.long)\n                        else:\n                            first = generated_ids[0]\n                            if isinstance(first, torch.Tensor):\n                                try:\n                                    gen_ids_tensor = torch.stack(generated_ids, dim=0)\n                                except Exception:\n                                    # try to convert each to list then pad\n                                    seqs = [g.cpu().tolist() if isinstance(g, torch.Tensor) else list(g) for g in generated_ids]\n                                    # convert to list-of-lists for tokenizer.batch_decode\n                                    gen_ids_tensor = seqs\n                            else:\n                                # list-of-lists\n                                gen_ids_tensor = generated_ids\n                    else:\n                        # unknown type: try to coerce to list\n                        try:\n                            gen_ids_tensor = torch.tensor(generated_ids)\n                        except Exception:\n                            gen_ids_tensor = generated_ids\n\n                    # decode safely\n                    try:\n                        # tokenizer.batch_decode supports list-of-ids or tensor (batch, seq)\n                        if isinstance(gen_ids_tensor, (list, tuple)):\n                            # list-of-lists or list-of-tensors: normalize to list-of-lists of ints\n                            seqs = []\n                            for item in gen_ids_tensor:\n                                if isinstance(item, torch.Tensor):\n                                    seqs.append(item.cpu().tolist())\n                                else:\n                                    seqs.append(list(item))\n                            batch_hyps = tokenizer.batch_decode(seqs, skip_special_tokens=True)\n                        elif isinstance(gen_ids_tensor, torch.Tensor):\n                            batch_hyps = tokenizer.batch_decode(gen_ids_tensor.cpu(), skip_special_tokens=True)\n                        else:\n                            # fallback: try decoding element-wise\n                            batch_hyps = []\n                            try:\n                                for item in gen_ids_tensor:\n                                    try:\n                                        batch_hyps.append(tokenizer.decode(item, skip_special_tokens=True))\n                                    except Exception:\n                                        batch_hyps.append(\"\")\n                            except Exception:\n                                batch_hyps = [\"\" for _ in range(len(batch_srcs))]\n                    except Exception as e:\n                        _safe_print(f\"[GEN] Decoding failed: {type(e).__name__}: {str(e)[:200]}\")\n                        _maybe_traceback(e)\n                        # fallback per-sequence decode\n                        batch_hyps = []\n                        if isinstance(gen_ids_tensor, torch.Tensor):\n                            seqs = gen_ids_tensor.cpu().tolist()\n                        elif isinstance(gen_ids_tensor, (list, tuple)):\n                            seqs = [g.cpu().tolist() if isinstance(g, torch.Tensor) else list(g) for g in gen_ids_tensor]\n                        else:\n                            seqs = []\n                        for seq in seqs:\n                            try:\n                                batch_hyps.append(tokenizer.decode(seq, skip_special_tokens=True))\n                            except Exception:\n                                batch_hyps.append(\"\")\n\n                    hypotheses.extend(batch_hyps)\n\n                except Exception as e:\n                    _safe_print(f\"[GEN] Batch generation failed at start={start}: {type(e).__name__}: {str(e)[:200]}\")\n                    _maybe_traceback(e)\n                    # fallback per-sentence\n                    for src in batch_srcs:\n                        try:\n                            try:\n                                if hasattr(tokenizer, \"src_lang\"):\n                                    setattr(tokenizer, \"src_lang\", \"bn\")\n                            except Exception:\n                                pass\n                            enc1 = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=max_length)\n                            enc1 = {k: v.to(_DEVICE) for k, v in enc1.items() if isinstance(v, torch.Tensor)}\n                            gen_kwargs1 = {\"max_length\": 128, \"num_beams\": 1, \"early_stopping\": True}\n                            if forced_bos is not None:\n                                gen_kwargs1[\"forced_bos_token_id\"] = int(forced_bos)\n                            gen_ids = gen_callable(**enc1, **gen_kwargs1)\n                            seq = gen_ids[0] if isinstance(gen_ids, (list, tuple)) else gen_ids\n                            try:\n                                hyp = tokenizer.decode(seq, skip_special_tokens=True)\n                            except Exception:\n                                try:\n                                    hyp = tokenizer.batch_decode([seq], skip_special_tokens=True)[0]\n                                except Exception:\n                                    hyp = \"\"\n                            hypotheses.append(hyp)\n                        except Exception:\n                            hypotheses.append(\"\")\n\n        # length alignment\n        if len(hypotheses) < len(source_texts):\n            hypotheses.extend([\"\"] * (len(source_texts) - len(hypotheses)))\n\n        _safe_print(f\"[GEN] ✓ Generated {len(hypotheses)} predictions\\n\")\n\n        # Compute metrics\n        metrics_computer = LargeScaleEvaluationMetrics(device=_DEVICE, batch_size=batch_size)\n        metrics_results = metrics_computer.compute_all_metrics_large(source_texts, references, hypotheses)\n\n        # Summary report\n        _safe_print(\"\\n\" + \"=\" * 80)\n        _safe_print(\"FINAL EVALUATION REPORT\")\n        _safe_print(\"=\" * 80 + \"\\n\")\n\n        _safe_print(f\"Dataset: {len(hypotheses)} samples\")\n        _safe_print(f\"Timestamp: {metrics_results.get('timestamp', '')}\\n\")\n\n        _safe_print(\"Metric Scores:\")\n        _safe_print(\"-\" * 80)\n        if \"bleu\" in metrics_results[\"metrics\"]:\n            bleu_data = metrics_results[\"metrics\"][\"bleu\"]\n            if bleu_data.get(\"bleu\") is not None:\n                _safe_print(f\"  BLEU:  {bleu_data['bleu']:>7.2f}/100 (computed in {bleu_data.get('computation_time_sec', 0.0):.1f}s)\")\n            else:\n                _safe_print(f\"  BLEU:  ERROR - {bleu_data.get('error', 'Unknown')}\")\n        if \"chrf\" in metrics_results[\"metrics\"]:\n            chrf_data = metrics_results[\"metrics\"][\"chrf\"]\n            if chrf_data.get(\"chrf\") is not None:\n                _safe_print(f\"  CHRF++: {chrf_data['chrf']:>7.2f}/100 (computed in {chrf_data.get('computation_time_sec', 0.0):.1f}s)\")\n            else:\n                _safe_print(f\"  CHRF++: ERROR - {chrf_data.get('error', 'Unknown')}\")\n        if \"comet\" in metrics_results[\"metrics\"]:\n            comet_data = metrics_results[\"metrics\"][\"comet\"]\n            if comet_data.get(\"comet\") is not None:\n                _safe_print(f\"  COMET:  {comet_data['comet']:>7.4f}/1.0 (computed in {comet_data.get('computation_time_sec', 0.0):.1f}s)\")\n                if comet_data.get(\"comet_mean\") is not None:\n                    _safe_print(f\"         Mean={comet_data['comet_mean']:.4f}, Median={comet_data['comet_median']:.4f}, Std={comet_data['comet_std']:.4f}\")\n            else:\n                _safe_print(f\"  COMET:  ERROR - {comet_data.get('error', 'Unknown')}\")\n        _safe_print(\"-\" * 80)\n\n        # Save results to CSV\n        csv_path = None\n        if save_results:\n            csv_path = \"evaluation_results_2000.csv\"\n            _safe_print(f\"\\n[SAVE] Saving results to {csv_path}...\")\n            try:\n                with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n                    writer = csv.writer(f)\n                    writer.writerow([\"Index\", \"Source\", \"Reference\", \"Hypothesis\"])\n                    for idx, (s, r, h) in enumerate(zip(source_texts, references, hypotheses), 1):\n                        writer.writerow([idx, s, r, h])\n                _safe_print(f\"[SAVE] ✓ Saved {len(hypotheses)} predictions to {csv_path}\")\n            except Exception as e:\n                _safe_print(f\"[SAVE] Error saving CSV: {type(e).__name__}: {str(e)[:200]}\")\n                _maybe_traceback(e)\n\n        # Sample outputs\n        _safe_print(\"\\n\" + \"=\" * 80)\n        _safe_print(\"SAMPLE TRANSLATIONS (first 10)\")\n        _safe_print(\"=\" * 80)\n        for i, (s, r, h) in enumerate(zip(source_texts[:10], references[:10], hypotheses[:10]), 1):\n            _safe_print(f\"\\nSample {i}:\")\n            _safe_print(f\"  Source:      {s}\")\n            _safe_print(f\"  Reference:   {r}\")\n            _safe_print(f\"  Hypothesis:  {h}\")\n        _safe_print(\"\\n\" + \"=\" * 80)\n\n        return {\"metrics\": metrics_results[\"metrics\"], \"num_samples\": len(hypotheses), \"predictions\": list(zip(source_texts, references, hypotheses)), \"csv_file\": csv_path}\n\n    except Exception as e:\n        _safe_print(f\"\\n[ERROR] Evaluation failed: {type(e).__name__}: {str(e)[:200]}\")\n        _maybe_traceback(e)\n        return {\"error\": str(e), \"metrics\": {}}\n\n\n# Example usage (script mode)\nif __name__ == \"__main__\":\n    _safe_print(\n        \"\"\"\n    ╔════════════════════════════════════════════════════════════════════════╗\n    ║          LARGE-SCALE EVALUATION (2000+ SAMPLES) - HOW TO USE           ║\n    ╚════════════════════════════════════════════════════════════════════════╝\n    \"\"\"\n    )\n    eval_results = evaluate_on_large_dataset(model=globals().get(\"trained_model\"), tokenizer=globals().get(\"tokenizer\"), num_samples=2000, batch_size=32, save_results=True)\n    _safe_print(eval_results)\n    _safe_print(\"✅ Cell 13: Large-scale evaluation (2000+ samples) - ready to run\")","metadata":{"id":"hZw0m3uEH4J6","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:43:11.228119Z","iopub.execute_input":"2025-11-22T18:43:11.228381Z","iopub.status.idle":"2025-11-22T18:49:19.206548Z","shell.execute_reply.started":"2025-11-22T18:43:11.228353Z","shell.execute_reply":"2025-11-22T18:49:19.205741Z"}},"outputs":[{"name":"stdout","text":"\n    ╔════════════════════════════════════════════════════════════════════════╗\n    ║          LARGE-SCALE EVALUATION (2000+ SAMPLES) - HOW TO USE           ║\n    ╚════════════════════════════════════════════════════════════════════════╝\n    \n\n================================================================================\nLARGE-SCALE EVALUATION ON SAMPLES\n================================================================================\n\n[PREP] Preparing dataset (requested 2000 samples)...\n[CELL2] Loading up to 2000 samples from local CSV: /kaggle/input/homo-bn-dataset/bn_homograph_complete_dataset.csv\n[CELL2] Reading CSV file...\n[CELL2] Processing 2000 rows from CSV...\n","output_type":"stream"},{"name":"stderr","text":"Loading dataset: 100%|██████████| 2000/2000 [00:00<00:00, 21323.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"[CELL2] Loaded 2000 pairs from CSV, skipped 0 rows\n[PREP] ✓ Loaded 2000 samples\n\n[GEN] Generating predictions in batches (batch_size=32) ...\n","output_type":"stream"},{"name":"stderr","text":"[GEN] Batches: 100%|██████████| 63/63 [06:07<00:00,  5.83s/batch]\n","output_type":"stream"},{"name":"stdout","text":"[GEN] ✓ Generated 2000 predictions\n\n\n================================================================================\nINITIALIZING LARGE-SCALE EVALUATION METRICS\n================================================================================\nDevice: cuda\nBatch Size: 32\nMetrics Available: BLEU=True, CHRF=True, COMET=False\n================================================================================\n\n\n[BLEU] Computing BLEU on 2000 samples...\n[BLEU] ✓ 26.00/100 computed in 0.17s\n\n[CHRF++] Computing CHRF++ on 2000 samples...\n[CHRF++] ✓ 46.54/100 computed in 0.24s\n\n================================================================================\nFINAL EVALUATION REPORT\n================================================================================\n\nDataset: 2000 samples\nTimestamp: 2025-11-22 18:49:18\n\nMetric Scores:\n--------------------------------------------------------------------------------\n  BLEU:    26.00/100 (computed in 0.2s)\n  CHRF++:   46.54/100 (computed in 0.2s)\n--------------------------------------------------------------------------------\n\n[SAVE] Saving results to evaluation_results_2000.csv...\n[SAVE] ✓ Saved 2000 predictions to evaluation_results_2000.csv\n\n================================================================================\nSAMPLE TRANSLATIONS (first 10)\n================================================================================\n\nSample 1:\n  Source:      আমি কল বন্ধ করেছি।\n  Reference:   i have turned off the tap.\n  Hypothesis:  i closed the call.\n\nSample 2:\n  Source:      সে আমাকে পরে কল করবে।\n  Reference:   he will call me later.\n  Hypothesis:  he will call me later.\n\nSample 3:\n  Source:      আমরা প্রতিদিন তাজা ফল খাই।\n  Reference:   we eat fresh fruits every day.\n  Hypothesis:  we eat fresh fruits every day.\n\nSample 4:\n  Source:      তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।\n  Reference:   his hard work has brought good results.\n  Hypothesis:  his hard work resulted well.\n\nSample 5:\n  Source:      গাছে নতুন পাতাগুলো গজিয়েছে।\n  Reference:   new leaves have sprouted on the tree.\n  Hypothesis:  new leaves have grown on the tree.\n\nSample 6:\n  Source:      বইয়ের শেষ পাতা ছিঁড়ে গেছে।\n  Reference:   the last page of the book has been torn.\n  Hypothesis:  the last page of the book has been broken.\n\nSample 7:\n  Source:      আমার মাথা ব্যথা করছে।\n  Reference:   i have a headache.\n  Hypothesis:  my head is hurting.\n\nSample 8:\n  Source:      সে দলের মাথা হিসেবে কাজ করছে।\n  Reference:   he is working as the leader of the team.\n  Hypothesis:  he is working as head of the team.\n\nSample 9:\n  Source:      আমি বাজারে মাছ কিনতে গিয়েছিলাম।\n  Reference:   i went to the market to buy fish.\n  Hypothesis:  i went to buy fish.\n\nSample 10:\n  Source:      সে শেয়ার বাজারে বিনিয়োগ করছে।\n  Reference:   he is investing in the stock market.\n  Hypothesis:  he is investing in the stock market.\n\n================================================================================\n{'metrics': {'bleu': {'bleu': 25.995862705346006, 'num_samples': 2000, 'computation_time_sec': 0.17435073852539062}, 'chrf': {'chrf': 46.53743408458128, 'num_samples': 2000, 'computation_time_sec': 0.24234580993652344}}, 'num_samples': 2000, 'predictions': [('আমি কল বন্ধ করেছি।', 'i have turned off the tap.', 'i closed the call.'), ('সে আমাকে পরে কল করবে।', 'he will call me later.', 'he will call me later.'), ('আমরা প্রতিদিন তাজা ফল খাই।', 'we eat fresh fruits every day.', 'we eat fresh fruits every day.'), ('তার কঠোর পরিশ্রমের ভালো ফল হয়েছে।', 'his hard work has brought good results.', 'his hard work resulted well.'), ('গাছে নতুন পাতাগুলো গজিয়েছে।', 'new leaves have sprouted on the tree.', 'new leaves have grown on the tree.'), ('বইয়ের শেষ পাতা ছিঁড়ে গেছে।', 'the last page of the book has been torn.', 'the last page of the book has been broken.'), ('আমার মাথা ব্যথা করছে।', 'i have a headache.', 'my head is hurting.'), ('সে দলের মাথা হিসেবে কাজ করছে।', 'he is working as the leader of the team.', 'he is working as head of the team.'), ('আমি বাজারে মাছ কিনতে গিয়েছিলাম।', 'i went to the market to buy fish.', 'i went to buy fish.'), ('সে শেয়ার বাজারে বিনিয়োগ করছে।', 'he is investing in the stock market.', 'he is investing in the stock market.'), ('আমার ব্যাগে তালা লাগানো আছে।', 'my bag is locked with a padlock.', 'i have a to put in my bag.'), ('পুকুরের পাশে কয়েকটি তালগাছ আছে।', 'there are several palm trees beside the pond.', 'there are several trees beside the pond.'), ('বাম দিকে মোড় নাও।', 'turn to the left.', 'turn to the left.'), ('সে বামপন্থী রাজনীতির সমর্থক।', 'he supports left-wing politics.', 'he is supportive of leftist politics.'), ('ছেলেটি স্লেটে লিখছে।', 'the boy is writing on the slate.', 'the boy is writing on the slate.'), ('তারা ছাদে পাতলা স্লেট পেতে দিয়েছে।', 'they have laid thin slate sheets on the roof.', 'they found thin slices on the roof.'), ('ছবিটি দেয়ালে ঝুলছে।', 'the picture is hanging on the wall.', 'the picture is hanging on the wall.'), ('তাদের সিদ্ধান্ত এখনো ঝুলে আছে।', 'their decision is still pending.', 'their decision is still hanging.'), ('সে বিছানায় শুয়ে আছে।', 'he is lying on the bed.', 'he is lying in bed.'), ('হাসপাতালে তার জন্য বিছানা খালি আছে।', 'a bed is available for him in the hospital.', 'the bed is empty for him.'), ('গাড়ির চাকা পাংচার হয়ে গেছে।', 'the car’s wheel has got punctured.', 'the wheel has been scattered.'), ('চাকা ধীরে ধীরে ঘুরতে শুরু করল।', 'the wheel started to spin slowly.', 'the wheel began to turn slowly.'), ('তুমি কোথায় যাচ্ছ?', 'where are you going?', 'where do you go?'), ('সে গল্পের কোথা থেকে শুরু করেছিল মনে নেই।', \"i don't remember from which part of the story he started.\", \"i don't remember where she started the story.\"), ('আলোর রঙ খুব উজ্জ্বল।', 'the color of the light is very bright.', 'the color is very brilliant.'), ('তার আলোর খ্যাতি চারিদিকে ছড়িয়ে পড়েছে।', 'his fame has spread everywhere.', 'his reputation spread around.'), ('স্কুলের বেল বেজে উঠেছে।', 'the school bell has rung.', \"the school's ball has risen.\"), ('এখন খাবার খাওয়ার বেল।', 'it’s time to eat now.', 'to eat now.'), ('তারা শহরের কেন্দ্রস্থলে পৌঁছেছে।', 'they have reached the city center.', 'they have reached the city center.'), ('এই বিষয়টি আলোচনার কেন্দ্রবিন্দুতে রয়েছে।', 'this topic is at the center of the discussion.', 'this is at the center of discussion.'), ('সে এক সেট বই কিনেছে।', 'he has bought a set of books.', 'he bought a set of books.'), ('তুমি টেবিলে প্লেটগুলো সেট করে দাও।', 'set the plates properly on the table.', 'set the plate on the table.'), ('তারা নতুন ঘরে উঠেছে।', 'they have moved into a new house.', 'they have come up to a new house.'), ('আমি আমার ঘরে বসে বই পড়ছি।', 'i am reading a book in my room.', 'i am reading in my room.'), ('ছেলেটি বলটি খুব জোরে ছুড়েছে।', 'the boy threw the ball very hard.', 'the boy threw the ball very loudly.'), ('সে সত্য কথা বলেছে।', 'he has spoken the truth.', 'he said the truth.'), ('দেয়ালে সুন্দর ছবি আঁকা আছে।', 'beautiful pictures are painted on the wall.', 'beautiful pictures are painted on the wall.'), ('তার পথে অনেক দেয়াল দাঁড়িয়ে আছে।', 'there are many barriers standing in his way.', 'many walls are standing on his way.'), ('সে রাগে চুপ করে গেছে।', 'he became silent out of anger.', 'he was silent with anger.'), ('ওই কাপড়ে নীল রঙের রাগ দেওয়া হয়েছে।', 'the cloth has been dyed blue.', 'the cloth has been given a blue color.'), ('সে অনেক চাপের মধ্যে আছে।', 'he is under a lot of pressure.', 'he is under a lot of pressure.'), ('তুমি বোতামটা চাপ দাও।', 'press the button.', 'press the button.'), ('টেবিলের ওপর ছোট একটি ডাচা আছে।', 'there is a small bottle on the table.', 'there is a small tower on the table.'), ('সে অতিরিক্ত খেয়ে একটু ডাচা হয়ে গেছে।', 'he got tipsy after eating too much.', 'he became a bit excessive.'), ('সে অতিথিদের সেবা করেছে।', 'he has served the guests.', 'he served guests.'), ('ওয়েটার সবার জন্য চা সেবা করছে।', 'the waiter is serving tea to everyone.', 'the weter serves tea for everyone.'), ('তুমি কী বললে?', 'what did you say?', 'what did you say?'), ('আমি আমার ঘরের চাবি হারিয়ে ফেলেছি।', 'i have lost the key to my room.', 'i lost my key.'), ('সে রান্নায় সরিষার তেল ব্যবহার করে।', 'she uses mustard oil in cooking.', 'she uses oil in cooking.'), ('সে ক্ষতস্থানে ওষুধের তেল লাগিয়েছে।', 'he has applied medicinal ointment on the wound.', 'he placed medicine oil on the wound.'), ('সে আজ সকালের নাস্তা রান্না করেছে।', 'she cooked breakfast this morning.', 'she cooked this morning.'), ('আমি রান্না করতে ভালোবাসি।', 'i love cooking.', 'i love cooking.'), ('সে ফোনে কল পেয়েছে।', 'he has received a phone call.', 'he got a call on the phone.'), ('তুমি কল খুলে পানি দাও।', 'open the tap and pour water.', 'open the tap and give water.'), ('আমার বাগানে অনেক ফল গাছ আছে।', 'there are many fruit trees in my garden.', 'there are many fruit trees in my garden.'), ('তার পরীক্ষার ফল খারাপ হয়েছে।', 'his exam result was poor.', 'his test result bad.'), ('শীতে পাতাগুলো ঝরে পড়ে।', 'the leaves fall off in winter.', 'the leaves fall on winter.'), ('দয়া করে পরের পাতা উল্টে দেখো।', 'please turn to the next page.', 'please turn on the next page.'), ('তার মাথায় টুপি আছে।', 'he is wearing a cap on his head.', 'he has a hat on his head.'), ('সে আমাদের দলের মাথা।', 'he is the head of our team.', 'he is the head of our team.'), ('আমি প্রতিদিন বাজারে যাই।', 'i go to the market every day.', 'i go to the market every day.'), ('তারা নতুন পণ্যের বাজার তৈরি করছে।', 'they are creating a market for the new product.', 'they are building a new product market.'), ('তালা খুলে দাও।', 'open the lock.', 'open the tap.'), ('তালগাছ থেকে ডাব পড়ে গেছে।', 'a green coconut has fallen from the palm tree.', 'fell from the tower.'), ('বাম দিকের রাস্তাটা ধরো।', 'take the road on the left.', 'take the left road.'), ('সে বামপন্থী দলকে ভোট দিয়েছে।', 'he voted for the left-wing party.', 'he voted for the left party.'), ('বাচ্চারা স্লেটে অক্ষর লিখছে।', 'the children are writing letters on the slate.', 'children are writing letters on the slate.'), ('তারা দেয়ালে স্লেট পাথর লাগিয়েছে।', 'they have fixed slate stones on the wall.', 'they placed slate stones on the wall.'), ('ঘড়িটি হুকে ঝুলছে।', 'the clock is hanging on the hook.', 'the clock is hanging.'), ('প্রকল্পের অনুমোদন ঝুলে আছে।', 'the project approval is pending.', 'the approval is hanging.'), ('আমি নতুন বিছানা কিনেছি।', 'i have bought a new bed.', 'i bought a new bed.'), ('তুমি বিশ্রামের জন্য বিছানায় শুয়ে পড়ো।', 'lie down on the bed for some rest.', 'sleep in bed for rest.'), ('চাকার নাট খুলে ফেলো।', 'remove the wheel’s nut.', 'open the wheel.'), ('গাড়ির চাকা ঘুরছে।', 'the car wheel is spinning.', 'the wheel is rolling.'), ('তুমি কোথা থেকে আসছ?', 'where are you coming from?', 'where do you come from?'), ('এই গল্পের কোথা খুব আকর্ষণীয়।', 'this part of the story is very interesting.', 'where this story is very interesting.'), ('আলোর দীপ্তি ঘর ভরিয়ে দিয়েছে।', 'the brightness of the light has filled the room.', 'the light filled the house.'), ('তার আলোর নাম সবাই জানে।', 'everyone knows his famous name.', 'everyone knows his name.'), ('বেল বাজাও, অতিথি এসেছে।', 'ring the bell, the guest has arrived.', 'play the ball, the guest came.'), ('এখন ক্লাস শেষ হওয়ার বেল।', 'it’s time for the class to end.', 'the class ends now.'), ('তারা শহরের কেন্দ্রে দেখা করল।', 'they met at the city center.', 'they met in the city center.'), ('এই বইটির কেন্দ্রবিন্দু হলো ভালোবাসা।', 'the core theme of this book is love.', 'the center of this book is love.'), ('সে একটি সেট কাপ কিনেছে।', 'she bought a set of cups.', 'he bought a set of cups.'), ('তুমি চেয়ারগুলো সেট করে দাও।', 'arrange the chairs properly.', 'set the chairs.'), ('আমার ঘরে দুটি জানালা আছে।', 'there are two windows in my room.', 'i have two windows in my room.'), ('আমাদের ঘরটি গ্রামের পাশে।', 'our house is near the village.', 'our house is next to the village.'), ('ছেলেটি বল কিক করল।', 'the boy kicked the ball.', 'the boy kicked the ball.'), ('তুমি সত্য কথা বলো।', 'speak the truth.', 'say the truth.'), ('দেয়ালটি নীল রঙে রাঙানো।', 'the wall is painted blue.', 'the wall is colored blue.'), ('তার জীবনের পথে অনেক দেয়াল এসেছে।', 'many barriers have come in his life’s path.', 'many walls have come to his life.'), ('সে রাগে চিৎকার করে উঠল।', 'he shouted in anger.', 'he screamed with anger.'), ('তারা কাপড়ে লাল রঙের রাগ দিয়েছে।', 'they dyed the cloth in red color.', 'they gave red color to clothes.'), ('তুমি আজ অনেক চাপের মধ্যে আছো।', 'you are under a lot of pressure today.', 'you are under much pressure today.'), ('সে বোতামটা চাপ দিল।', 'he pressed the button.', 'he pressed the button.'), ('টেবিলের কোণে একটি ছোট ডাচা রাখা আছে।', 'a small bottle is kept at the corner of the table.', 'a small tower is placed on the table.'), ('সে একটু ডাচা হয়ে গান গাইছিল।', 'he was singing slightly drunk.', 'he singed a bit.'), ('তারা বৃদ্ধদের সেবা করছে।', 'they are serving the elderly.', 'they serve the elderly.'), ('ওয়েটার আমাদের জন্য কফি সেবা করল।', 'the waiter served coffee for us.', 'the weter served coffee for us.'), ('তুমি কী খুঁজছ?', 'what are you looking for?', 'what are you looking for?'), ('আমার গাড়ির কীটি ভেঙে গেছে।', 'the key of my car is broken.', 'my car broke.'), ('সে প্রতিদিন সকালে রান্না করে।', 'she cooks every morning.', 'she cookes it every morning.'), ('রান্না করা আমার প্রিয় শখ।', 'cooking is my favorite hobby.', 'cooking is my favorite hobby.'), ('সে আমাকে একটি কল দিয়েছে।', 'he gave me a call.', 'he gave me a call.'), ('কল থেকে পানি পড়ছে।', 'water is dripping from the tap.', 'water is falling from the tap.'), ('ফল খাওয়া স্বাস্থ্যের জন্য ভালো।', 'eating fruits is good for health.', 'eating fruit is good for health.'), ('এই সিদ্ধান্তের ফল মারাত্মক হয়েছে।', 'the result of this decision was serious.', 'the result of this decision was terrible.'), ('গাছের সবুজ পাতাগুলো খুব সুন্দর।', 'the green leaves of the tree are very beautiful.', 'the green leaves are beautiful.'), ('তুমি নোটবুকের প্রথম পাতা খুলে দেখো।', 'open the first page of your notebook.', 'open the first page of the notebook.'), ('আমার মাথায় একটু ব্যথা আছে।', 'i have a slight headache.', 'i have a bit of pain in my head.'), ('সে দলের মাথা হিসেবে নিযুক্ত হয়েছে।', 'he has been appointed as the team leader.', 'he was appointed head of the team.'), ('আজ বাজারে প্রচুর ভিড় ছিল।', 'there was a huge crowd at the market today.', 'there was a lot of crowd in the market today.'), ('সে স্থানীয় বাজারে পণ্য বিক্রি করে।', 'he sells products in the local market.', 'he sells goods in local markets.'), ('আমি দরজায় নতুন তালা লাগিয়েছি।', 'i have installed a new lock on the door.', 'i put a new tap on the door.'), ('বাগানে পুরোনো তালগাছটি কেটে ফেলা হয়েছে।', 'the old palm tree in the garden has been cut down.', 'the old tree has been cut off in the garden.'), ('বাম দিকে বাঁক নাও।', 'take a left turn.', 'turn to the left.'), ('সে বামপন্থী আন্দোলনের নেতা।', 'he is a leader of the leftist movement.', 'he is the leader of the leftist movement.'), ('ছেলেটি স্লেটে নিজের নাম লিখেছে।', 'the boy has written his name on the slate.', 'the boy wrote his name on the slate.'), ('তারা স্লেট পাথর দিয়ে ছাদ তৈরি করেছে।', 'they built the roof using slate stones.', 'they built roof with slate stones.'), ('ঘড়িটি দড়িতে ঝুলছে।', 'the clock is hanging by a rope.', 'the clock is hanging on the rope.'), ('এই মামলাটি এখনও ঝুলে আছে।', 'this case is still pending.', 'this case is still hanging.'), ('আমার বিছানা নরম এবং আরামদায়ক।', 'my bed is soft and comfortable.', 'my bed is soft and comfortable.'), ('রোগীর জন্য নতুন বিছানা তৈরি করা হয়েছে।', 'a new bed has been prepared for the patient.', 'a new bed has been built for the patient.'), ('চাকার ব্যালান্স ঠিক আছে।', 'the wheel is properly balanced.', 'the wheel is well.'), ('চাকা ধীরে ধীরে ঘুরছে।', 'the wheel is spinning slowly.', 'the wheel is moving slowly.'), ('তুমি কোথা থেকে এলে?', 'where did you come from?', 'where did you come from?'), ('গল্পের এই কোথা অনেক আবেগময়।', 'this part of the story is very emotional.', 'this is very emotional.'), ('আলোর প্রতিফলন পানিতে দেখা যাচ্ছে।', 'the reflection of light can be seen in the water.', 'the reflection is visible in the water.'), ('তার আলোর নাম দেশজুড়ে পরিচিত।', 'his famous name is known across the country.', 'his name is known throughout the country.'), ('বেল বাজলেই ক্লাস শুরু হয়।', 'the class starts as soon as the bell rings.', 'as the ball begins the class.'), ('এখন রাতের খাবারের বেল।', 'it’s time for dinner now.', 'the meal is now.'), ('আমরা শহরের কেন্দ্রে একটি পার্কে গিয়েছিলাম।', 'we visited a park in the city center.', 'we went to a park in the city center.'), ('এই গবেষণার কেন্দ্রবিন্দু হলো কৃত্রিম বুদ্ধিমত্তা।', 'the focus of this research is artificial intelligence.', 'the center of this research is artificial intelligence.'), ('সে একটি সেট প্লেট কিনেছে।', 'he has bought a set of plates.', 'he bought a set of plates.'), ('তুমি বইগুলো টেবিলে সেট করে রাখো।', 'arrange the books properly on the table.', 'place the books on the table.'), ('আমাদের ঘরে চারটি ঘর আছে।', 'our house has four rooms.', 'we have four rooms.'), ('আমার ঘরটি খুব পরিষ্কার।', 'my room is very clean.', 'my room is very clean.'), ('ছেলেটি বলটি ধরেছে।', 'the boy has caught the ball.', 'the boy caught the ball.'), ('আমি তাকে কিছু বলতে চেয়েছিলাম।', 'i wanted to tell him something.', 'i wanted to tell him something.'), ('দেয়ালে ফাটল দেখা যাচ্ছে।', 'a crack can be seen on the wall.', 'it is seen on the wall.'), ('সে তার জীবনের দেয়াল ভেঙে ফেলেছে।', 'he has broken the barriers of his life.', 'he broke the wall of his life.'), ('সে রাগে দরজা বন্ধ করে দিয়েছে।', 'he slammed the door in anger.', 'he closed the door with anger.'), ('কারখানায় কাপড়ে নতুন রঙের রাগ দেওয়া হয়েছে।', 'the cloth has been dyed with a new color in the factory.', 'a new color has been given to the factory.'), ('সে প্রচণ্ড মানসিক চাপের মধ্যে আছে।', 'he is under immense mental pressure.', 'he is under heavy mental pressure.'), ('সে কলিং বেলটা চাপ দিল।', 'he pressed the calling bell.', 'he pressed the tap.'), ('টেবিলে একটি খালি ডাচা রাখা আছে।', 'an empty bottle is kept on the table.', 'a vacancy is placed on the table.'), ('সে সামান্য ডাচা হয়ে নাচতে শুরু করল।', 'he started dancing after getting slightly drunk.', 'he started dancing as a bit.'), ('তারা অসুস্থদের সেবা করছে।', 'they are serving the sick.', 'they serve the sick.'), ('ওয়েটার আমাদের জন্য চা সেবা করছে।', 'the waiter is serving tea for us.', 'the weter serves tea for us.'), ('তুমি কী বলবে?', 'what will you say?', 'what will you say?'), ('সে আমার গাড়ির কীটি নিয়ে গেছে।', 'he has taken my car key.', 'he took my car.'), ('সে নদীর তীরে দাঁড়িয়ে ছবি তুলল।', 'he stood by the river and took a photo.', 'he stood on the river and took a picture.'), ('তীর ছুঁড়ে লক্ষ্যভেদ করো।', 'shoot the arrow and hit the target.', 'throw the mark.'), ('সে ব্যাংকে টাকা জমা দিল।', 'he deposited money in the bank.', 'he deposited money to the bank.'), ('আমরা নদীর ব্যাংকে বসে আছি।', 'we are sitting on the river bank.', 'we are sitting in the river bank.'), ('সে সকালবেলায় চা খায়।', 'he drinks tea in the morning.', 'she drinks tea in the morning.'), ('তারা বাগানে চা চাষ করছে।', 'they are cultivating tea in the garden.', 'they are cultivating tea in the garden.'), ('সে একটি বই পড়ছে।', 'he is reading a book.', 'he is reading a book.'), ('তুমি একটু হাওয়ায় বইয়ে দাও।', 'let it move in the air.', 'put it in the air.'), ('সে আমার নাম লিখেছে।', 'he has written my name.', 'he wrote my name.'), ('নৌকাটি নদীতে নামাও।', 'lower the boat into the river.', 'lower the boat to the river.'), ('তুমি টেবিলের পাতা মুছে দাও।', 'wipe the surface of the table.', 'remove the table page.'), ('সে গাছের পাতায় রঙ করেছে।', 'he painted on the leaves of the tree.', 'he painted the tree.'), ('সে স্কুলে দৌড় প্রতিযোগিতায় অংশ নিয়েছে।', 'he has participated in the running competition at school.', 'she participated in running competition at school.'), ('বাচ্চারা মাঠে দৌড়াচ্ছে।', 'the children are running in the field.', 'children are running on the field.'), ('সে অফিসে কাগজ জমা দিল।', 'he submitted the papers to the office.', 'he submitted paper to the office.'), ('বৃষ্টির পর জল জমে গেছে।', 'water has accumulated after the rain.', 'water has accumulated after the rain.'), ('সে ঘর পরিষ্কার করছে।', 'she is cleaning the room.', 'he is cleaning the room.'), ('তারা নতুন ঘর বানাচ্ছে।', 'they are building a new house.', 'they are building a new house.'), ('সে মাটিতে বল ফেলেছে।', 'he dropped the ball on the ground.', 'he has thrown ball on the ground.'), ('সে সত্য কথা বলল।', 'he spoke the truth.', 'he said the truth.'), ('তুমি আলো জ্বালাও।', 'turn on the light.', 'light the light.'), ('এই আলোচনাটি খুব গুরুত্বপূর্ণ।', 'this discussion is very important.', 'this discussion is very important.'), ('তুমি চাবি দিয়ে তালা খোলো।', 'open the lock with the key.', 'open the key.'), ('তালগাছে কচি ডাব ধরেছে।', 'young coconuts have grown on the palm tree.', 'cottle caught.'), ('তুমি বেলটা বাজাও।', 'ring the bell.', 'play the ball.'), ('এখন দুপুরের বেল।', 'it’s lunchtime now.', 'afternoon.'), ('সে এক কাপ দুধ খেয়েছে।', 'he has drunk a cup of milk.', 'he ate a cup of milk.'), ('দুধ গরম হয়ে গেছে।', 'the milk has become hot.', 'the milk has become hot.'), ('সে ঘড়ি পরে স্কুলে যায়।', 'he wears a watch to school.', 'he goes to school with the clock.'), ('ঘড়ির কাঁটা থেমে গেছে।', 'the hands of the clock have stopped.', 'the clock stopped.'), ('সে বইয়ের কভার খুলল।', 'he opened the cover of the book.', 'he opened the cover.'), ('মেঘে সূর্য ঢেকে গেছে।', 'the sun is covered by clouds.', 'the sun is covered in the cloud.'), ('সে অফিসে কাজ করছে।', 'he is working at the office.', 'he is working in the office.'), ('ঘড়িটি ঠিকভাবে কাজ করছে।', 'the clock is working properly.', 'the clock works properly.'), ('সে জলের নিচে ডুব দিয়েছে।', 'he has dived under the water.', 'he dropped under the water.'), ('সূর্য সমুদ্রে ডুব দিচ্ছে।', 'the sun is setting into the sea.', 'the sun is immersing in the sea.'), ('তুমি আলো নিভিয়ে দাও।', 'switch off the light.', 'turn off the light.'), ('এই বিষয়ের ওপর একটি সুন্দর আলো হয়েছে।', 'a nice discussion has been held on this topic.', 'a beautiful light has been made on this subject.'), ('তুমি টেবিলের উপর কলম রাখো।', 'put the pen on the table.', 'place a pen on the table.'), ('সে মাটিতে গাছের কলম লাগিয়েছে।', 'he has planted a sapling in the soil.', 'he placed a pen on the ground.'), ('সে মঞ্চে গান গেয়েছে।', 'she has sung on the stage.', 'he singed on the stage.'), ('তুমি প্রতিদিন সকালে গান শোনো।', 'listen to songs every morning.', 'listen to songs every morning.'), ('সে মাঠে বল খেলছে।', 'he is playing ball in the field.', 'he is playing in the field.'), ('তুমি তোমার কথা স্পষ্টভাবে বলো।', 'speak your words clearly.', 'tell your words clearly.'), ('তারা সিনেমা হলে গেছে।', 'they have gone to the cinema hall.', 'they went to cinema.'), ('সে খুব অবাক হল।', 'he was very surprised.', 'he was very surprised.'), ('তুমি চুল আঁচড়াও।', 'comb your hair.', 'cut your hair.'), ('চুলার আগুন নিভে গেছে।', 'the stove’s fire has gone out.', 'the fire has disappeared.'), ('সে স্কুলে ক্লাস নিয়েছে।', 'she has taken a class at school.', 'he took classes at school.'), ('তারা উচ্চ শ্রেণির ক্লাসে পড়ে।', 'they study in a higher-grade class.', 'they are in high class.'), ('সে রাতের আকাশে তারা দেখছে।', 'he is watching stars in the night sky.', 'he is seeing in the night sky.'), ('তারা স্কুলে একসাথে যায়।', 'they go to school together.', 'they go together to school.'), ('তুমি সময় মতো আসো।', 'come on time.', 'come like time.'), ('এখন পরীক্ষার সময় চলছে।', 'it is exam time now.', 'the time is ongoing.'), ('সে ডান হাতে লিখছে।', 'he is writing with his right hand.', 'he is writing with his right hand.'), ('ডান পাখাটি ভেঙে গেছে।', 'the right wing has broken.', 'the leaf broken.'), ('সে নদীতে সাঁতার কাটছে।', 'he is swimming in the river.', 'he is swimming in the river.'), ('সে সাঁতার প্রতিযোগিতায় প্রথম হয়েছে।', 'he came first in the swimming competition.', 'he was first in the swimming competition.'), ('তুমি দরজাটা বন্ধ করো।', 'close the door.', 'close the door.'), ('তার জীবনের দরজা খুলে গেছে।', 'the doors of his life have opened.', 'the door of his life has been opened.'), ('সে মাঠে খেলা দেখছে।', 'he is watching the game in the field.', 'he is watching the game in the field.'), ('বাচ্চারা খেলছে আনন্দে।', 'the children are playing happily.', 'children are playing with joy.'), ('তুমি পেনসিল দিয়ে লিখো।', 'write with a pencil.', 'write with a pencil.'), ('সে চিঠি লিখে পাঠিয়েছে।', 'he has written and sent a letter.', 'he sent the letter.'), ('সে আজ রোদে বসে আছে।', 'he is sitting in the sunlight today.', 'he is sitting in the sun today.'), ('রোদে কাপড় শুকিয়ে গেছে।', 'the clothes have dried in the sun.', 'clothes have dried.'), ('সে দরজায় কড়া নাড়ল।', 'he knocked on the door.', 'he hit the door.'), ('আজ রোদটা খুব কড়া।', 'the sunlight is very harsh today.', 'the sun is very hard today.'), ('তুমি টেবিলে কাপ রাখো।', 'put the cup on the table.', 'place the cup on the table.'), ('সে বিশ্বকাপ জিতেছে।', 'he has won the world cup.', 'he won the world cup.'), ('সে আজ স্কুলে যাবে।', 'he will go to school today.', 'he will go to school today.'), ('এই পথে অনেক গাড়ি যাবে।', 'many cars will pass through this road.', 'many cars are going on this road.'), ('তুমি গাছের ছায়ায় দাঁড়াও।', 'stand under the shade of the tree.', 'stand in the tree.'), ('সে তার বাবার ছায়ায় বড় হয়েছে।', \"he grew up under his father's care and guidance.\", \"he grew up in his father's shade.\"), ('তুমি বইয়ের লাইন পড়ো।', 'read the line from the book.', 'read the line.'), ('ট্রেনটি লাইনের বাইরে চলে গেছে।', 'the train has gone off the track.', 'the train went out of the line.'), ('সে ফুলের ছবি তুলছে।', 'he is taking a photo of the flower.', 'he is taking a photo of the flowers.'), ('রুটিতে ফুলে উঠেছে।', 'the bread has puffed up.', 'flowers on the bread.'), ('সে পাখির ডাক শুনল।', 'he heard the bird’s call.', 'he heard the bird.'), ('আমি ডাকবিভাগে কাজ করি।', 'i work at the postal department.', 'i am working in the post.'), ('সে বালতি দিয়ে পানি তুলল।', 'he drew water with a bucket.', 'he lifted the water.'), ('সে ছবিটি তুলল।', 'he took the photo.', 'he took the picture.'), ('সে বইটি পড়ে ফেলেছে।', 'he has finished reading the book.', 'he read this.'), ('সে সিঁড়ি থেকে পড়ে গেছে।', 'he has fallen from the stairs.', 'he fell from the stairs.'), ('সে মাটিতে গর্ত খুঁড়ছে।', 'he is digging a hole in the ground.', 'he is throwing hole on the ground.'), ('সে তার পুরোনো গল্প খুঁড়ে বের করেছে।', 'he has dug up his old stories.', 'he broke out his old story.'), ('সে জানালা খুলল।', 'he opened the window.', 'he opened the window.'), ('সে একটি দোকান খুলল।', 'he opened a shop.', 'he opened a shop.'), ('সে গান গাইছে।', 'she is singing a song.', 'he is singing.'), ('এই গানটি খুব জনপ্রিয়।', 'this song is very popular.', 'this song is very popular.'), ('তুমি কথা রাখো।', 'keep your word.', 'keep your speech.'), ('তুমি তার সঙ্গে কথা বলো।', 'talk to him.', 'speak with him.'), ('সে আকাশে পাখি উড়তে দেখল।', 'he saw birds flying in the sky.', 'he saw birds flying in the sky.'), ('তুমি পতাকা উড়তে দাও।', 'let the flag fly.', 'let you fly.'), ('সে রাতে তারা দেখল।', 'he saw stars at night.', 'he saw it at night.'), ('তারা আমাদের সঙ্গে খাবে।', 'they will eat with us.', 'they will eat with us.'), ('তুমি আমার ছবি তুলো।', 'take my photo.', 'take my photo.'), ('তুমি ভারী বাক্সটা তুলো।', 'lift the heavy box.', 'take the heavy box.'), ('সে চোখ খুলল।', 'he opened his eyes.', 'he opened his eyes.'), ('বৃষ্টিতে জানালার কাচে চোখ ঝাপসা হয়ে গেছে।', 'the window glass has fogged up from the rain.', 'the glass of the window has been shattered in the rain.'), ('বাসটি স্টেশনে থেমেছে।', 'the bus has stopped at the station.', 'the bus stopped at the station.'), ('আমি প্রতিদিন বাসে করে অফিসে যাই।', 'i go to the office by bus every day.', 'i am going to the office every day.'), ('বাসে খুব ভিড় ছিল আজ।', 'there was a heavy crowd on the bus today.', 'the bus was very crowded today.'), ('বাসের ভাড়া বেড়ে গেছে।', 'the bus fare has increased.', 'the bus rental increased.'), ('সে ঢাকায় বাস করছে।', 'he lives in dhaka.', 'he is living in Dhaka.'), ('তারা একসাথে এই শহরে বাস করে।', 'they live together in this city.', 'they live together in this city.'), ('আমি এত বছর ধরে এখানে বাস করছি।', 'i have been living here for many years.', 'i have been living here for many years.'), ('সে বিদেশে বাস করার স্বপ্ন দেখে।', 'he dreams of living abroad.', 'she dreams of living abroad.'), ('চাপা পাথরের নিচে একটি ফুল গজিয়েছে।', 'a flower has grown under a hidden stone.', 'a flower has grown under the stone.'), ('সে রাগ চাপা দিয়ে হাসল।', 'he hid his anger behind a smile.', 'he smiled with anger.'), ('চাপা ঘাসের নিচে সাপ ছিল।', 'there was a snake under the hidden grass.', 'there was a serpent under the grass.'), ('চাপা জল ফেটে বের হলো।', 'the covered water burst out.', 'pressing water came out.'), ('সে ফুলগুলো সাবধানে চাপা দিল।', 'she pressed the flowers carefully.', 'she pressed the flowers carefully.'), ('তুমি বোতামটা জোরে চাপা দাও।', 'press the button firmly.', 'press the button strongly.'), ('সে বইয়ের মধ্যে ফুলটা চাপা দিয়েছে।', 'he has pressed the flower between the pages of the book.', 'she pressed the flower into the book.'), ('আমি কাগজগুলো ভালোভাবে চাপা রেখেছি।', 'i have kept the papers tightly pressed.', 'i kept this well.'), ('ধানের ক্ষেত সবুজ হয়ে উঠেছে।', 'the paddy fields have turned green.', 'the field has become green.'), ('কৃষকরা ধান কাটছে।', 'the farmers are harvesting paddy.', 'the farmers are cutting.'), ('এই বছর ধানের ফলন ভালো হয়েছে।', 'this year’s paddy yield has been good.', 'this year has been good.'), ('ধান ভেজানো হয়েছে ধান মাড়াইয়ের আগে।', 'the paddy has been soaked before threshing.', 'the tower was worn before the tower.'), ('চালের গুণ ধানের ওপর নির্ভর করে।', 'the quality of rice depends on the paddy.', 'the quality depends on the rice.'), ('ধান ভাঙলে চাল পাওয়া যায়।', 'rice is obtained when paddy is husked.', 'the rice can be broken.'), ('ধান সিদ্ধ করার পর তা চাল হয়।', 'after boiling, paddy becomes rice.', 'after making it, it becomes rise.'), ('ধানের খোসা ছাড়িয়ে চাল তৈরি করা হয়।', 'rice is made by removing the husk of paddy.', 'the rice is made out of the steep.'), ('নদী আজ খুব শান্ত।', 'the river is very calm today.', 'the river is very calm today.'), ('নদীর ধারে বাতাস ঠান্ডা।', 'the breeze near the river is cool.', 'the wind is cold on the river.'), ('নদীর তীরে নৌকা বাঁধা আছে।', 'boats are tied at the riverbank.', 'a boat is binding on the river.'), ('এই নদী সমুদ্রে গিয়ে মেশে।', 'this river flows into the sea.', 'this river moves to the sea.'), ('সে কথার নদী বইয়ে দিল।', 'he let a river of words flow.', 'he wrote the river.'), ('তার চোখে নদীর মতো জল নেমে এলো।', 'tears flowed down her eyes like a river.', 'water came down like a river in his eyes.'), ('তার হৃদয়ে ভাবনার নদী বয়ে যায়।', 'a river of thoughts flows in his heart.', 'a river of thought flows in his heart.'), ('গানের নদী তার মুখ থেকে বেরিয়ে আসে।', 'a river of songs flows from his mouth.', 'the river comes out from his mouth.'), ('চোখে জল এসে গেছে।', 'tears have come to my eyes.', 'water came to eyes.'), ('বৃষ্টির জল রাস্তায় জমে গেছে।', 'rainwater has collected on the road.', 'rain has accumulated on the street.'), ('নদীর জল খুব ঠান্ডা।', 'the river water is very cold.', 'the river is very cold.'), ('বালতিতে জল ভর্তি করো।', 'fill the bucket with water.', 'fill water in the basin.'), ('তার চোখের জলে কষ্ট ফুটে উঠল।', 'her pain was visible through her tears.', 'he caught pain in his eyes.'), ('তার চোখের জলে ভালোবাসা মিশে ছিল।', 'love was mixed in her tears.', 'there was love in his eyes.'), ('সে হাসল, কিন্তু চোখে জল চকচক করল।', 'she smiled, but her eyes sparkled with tears.', 'he smiled but blinked water in his eyes.'), ('বাবার বিদায়ের সময় তার চোখে জল এসে গেল।', 'tears welled up in his eyes when his father left.', \"when his father's goodbye came to his eyes.\"), ('সে ঘরে বাতি জ্বালাল।', 'he lit the lamp in the room.', 'he burned the light in the room.'), ('বাতি নিভে গেছে।', 'the lamp has gone out.', 'the light has turned off.'), ('টেবিলের উপর দুটি বাতি আছে।', 'there are two lamps on the table.', 'there are two lamps on the table.'), ('সে জানালার পাশে বাতি রেখে পড়ছিল।', 'she was reading by the lamp near the window.', 'he kept the light next to the window.'), ('বাতি মানে জীবন।', 'light means life.', 'light means life.'), ('তার চোখে আশার বাতি জ্বলে উঠল।', 'a light of hope glimmered in her eyes.', 'the light of hope burned in his eyes.'), ('শহরের বাতি নিভে গেলে অন্ধকার নেমে আসে।', 'when the city lights go out, darkness descends.', \"when the city's light disappears, the darkness comes down.\"), ('তার মনে জ্বলে আছে স্বপ্নের বাতি।', 'the lamp of dreams burns in his heart.', 'there is a light of dreams in his mind.'), ('সে রাস্তার ধারে দাঁড়িয়ে আছে।', 'he is standing by the roadside.', 'he stands on the road.'), ('নদীর ধার খুব সুন্দর।', 'the riverbank is very beautiful.', 'the river is beautiful.'), ('সে তলোয়ারের ধার পরীক্ষা করল।', 'he tested the sharpness of the sword.', 'he tested the sword.'), ('ধারের জিনিস সাবধানে ব্যবহার করো।', 'be careful with sharp objects.', 'use this carefully.'), ('আমি তোমার কাছ থেকে টাকা ধার নিয়েছি।', 'i borrowed money from you.', 'i took money from you.'), ('সে বন্ধুর কাছ থেকে বই ধার চেয়েছে।', 'he asked to borrow a book from his friend.', 'he asked to take a book from his friend.'), ('তুমি আমাকে একটা কলম ধার দাও।', 'please lend me a pen.', 'give me a pen.'), ('সে ব্যাংক থেকে ধার তুলেছে।', 'he took a loan from the bank.', 'he took it from the bank.'), ('চা ঠান্ডা হয়ে গেছে।', 'the tea has gone cold.', 'the tea has become cold.'), ('আজ সকালে হালকা ঠান্ডা পড়েছে।', 'it’s a bit cold this morning.', 'i have cold this morning.'), ('ফ্রিজের ঠান্ডা পানি দাও।', 'give me cold water from the fridge.', 'give cold water to the fridge.'), ('সে ঠান্ডায় কাঁপছিল।', 'he was shivering in the cold.', 'he was crying in cold.'), ('তার রাগ এখন ঠান্ডা হয়েছে।', 'his anger has now cooled down.', 'his anger has been cold now.'), ('পরিস্থিতি এখন ঠান্ডা।', 'the situation is calm now.', 'the situation is cold now.'), ('তুমি একটু ঠান্ডা মাথায় ভাবো।', 'think with a cool head.', 'you think a bit cold.'), ('তার মন ঠান্ডা হয়ে গেছে।', 'his mind has become calm.', 'his heart became cold.'), ('পাখি আকাশে উড়ছে।', 'the bird is flying in the sky.', 'birds are flying in the sky.'), ('সে জানালার পাশে বসে পাখি দেখছিল।', 'she was sitting by the window watching birds.', 'he saw birds sitting next to the window.'), ('এই পাখিগুলো শীতে দক্ষিণে চলে যায়।', 'these birds migrate south in winter.', 'these birds move to the south in winter.'), ('বনে অনেক রঙিন পাখি আছে।', 'there are many colorful birds in the forest.', 'there are many colored birds in the forest.'), ('তার মন পাখির মতো উড়ে যায়।', 'her mind flies like a bird.', 'his heart flys like a bird.'), ('স্বপ্নের পাখি দূরে উড়ে গেল।', 'the bird of dreams flew away.', 'the bird of dreams flew away.'), ('তার কথায় পাখির সুর আছে।', 'her words have the melody of a bird.', 'there are birds in his words.'), ('মুক্তির পাখি কারও হাতে ধরা দেয় না।', 'the bird of freedom cannot be caught by anyone.', 'the bird cannot be caught by anyone.'), ('তারা স্কুলে নাটক করছে।', 'they are performing a play at school.', 'they are playing at school.'), ('নাটকটি খুব বাস্তবসম্মত হয়েছে।', 'the play was very realistic.', 'the drama became very realistic.'), ('সে স্থানীয় নাটকে অভিনয় করছে।', 'he is acting in a local play.', 'he is playing in the local show.'), ('এই নাটকটি সামাজিক বিষয় নিয়ে।', 'this play is about social issues.', 'this is about social issues.'), ('তার জীবনে অনেক নাটক ঘটেছে।', 'a lot of drama has happened in his life.', 'many spectacles have happened in his life.'), ('তাদের সম্পর্ক এখন নাটকে ভরা।', 'their relationship is now full of drama.', 'their relationship is filled with play.'), ('সে সবসময় জীবনের নাটক উপভোগ করে।', 'she always enjoys the drama of life.', 'she always enjoys the drama of life.'), ('এই ঘটনার পেছনে বড় একটা নাটক আছে।', 'there’s a big drama behind this event.', 'there is a big drama behind this event.'), ('সে মাঠে বল খেলছে।', 'he is playing football in the field.', 'he is playing in the field.'), ('ছেলেটি বলটা ধরেছে।', 'the boy has caught the ball.', 'the boy caught the ball.'), ('সে বলটি জোরে মারল।', 'he hit the ball hard.', 'he hit this loudly.'), ('খেলার বল মাঠের বাইরে গেছে।', 'the ball has gone out of the field.', 'the ball went out of the field.'), ('সে সত্য কথা বলেছে।', 'he has spoken the truth.', 'he said the truth.'), ('আমি তাকে কিছু বলতে চেয়েছিলাম।', 'i wanted to tell him something.', 'i wanted to tell him something.'), ('তুমি তোমার কথা স্পষ্টভাবে বলো।', 'speak your words clearly.', 'tell your words clearly.'), ('সে হাসিমুখে বলল, ঠিক আছে।', \"he said with a smile, 'okay.'\", 'he said it is fine.'), ('তুমি কাচ ভেঙেছো।', 'you have broken the glass.', 'you broke the glass.'), ('দেয়ালটা ভেঙে পড়েছে।', 'the wall has collapsed.', 'the wall broken.'), ('চাবিটা ভেঙে গেছে।', 'the key has broken.', 'the key broken.'), ('সে গ্লাসটা ফেলে ভেঙে ফেলল।', 'he dropped and broke the glass.', 'he throwed the glass.'), ('তার মন ভেঙে গেছে।', 'his heart is broken.', 'his mind broken.'), ('স্বপ্নগুলো একে একে ভেঙে গেল।', 'the dreams shattered one by one.', 'the dreams broken together.'), ('তার আত্মবিশ্বাস ভেঙে পড়েছে।', 'his confidence has collapsed.', 'his confidence broken.'), ('ভালোবাসার প্রতিশ্রুতি ভেঙে গেছে।', 'the promise of love has been broken.', 'love promise broken.'), ('চাঁদ আজ খুব উজ্জ্বল।', 'the moon is very bright tonight.', 'the moon is brilliant today.'), ('চাঁদ উঠতেই শিশুরা উল্লাস করল।', 'as the moon rose, the children cheered.', 'when the moon rises, the children are glad.'), ('চাঁদের আলো ঘর ভরিয়ে দিল।', 'the moonlight filled the room.', 'the moon filled the room.'), ('চাঁদ রাতকে আরও সুন্দর করে তোলে।', 'the moon makes the night more beautiful.', 'the moon makes it more beautiful.'), ('সে তার মুখের চাঁদ।', 'she is the moon of his heart.', 'he is his moon.'), ('তার মুখটা চাঁদের মতো উজ্জ্বল।', 'her face is as bright as the moon.', 'his face is bright like the moon.'), ('চাঁদের হাসি যেন আকাশে ঝুলছে।', 'the moon’s smile seems to hang in the sky.', 'as the moon is hanging in the sky.'), ('তার নামের সঙ্গে চাঁদের তুলনা করা হয়।', 'she is often compared to the moon.', 'the moon is compared to his name.'), ('সে চাঁদ কোম্পানিতে চাকরি করে।', 'he works at chand company.', 'he works in the moon company.'), ('চাঁদ ফার্মা আজ নতুন ওষুধ প্রকাশ করেছে।', 'chand pharma released a new medicine today.', 'the moon pharmaceum released new medicine today.'), ('চাঁদ নামের এই পণ্যটি খুব জনপ্রিয়।', 'the product named chand is very popular.', 'this product named moon is very popular.'), ('চাঁদ প্রকাশনী নতুন বই প্রকাশ করেছে।', 'chand publications has released a new book.', 'the moon publishing published a new book.'), ('চাঁদের হাটে আজ অনেক ভিড়।', 'there is a huge crowd at chand-er hat today.', 'there is a lot of crowd on the moon.'), ('চাঁদ মেলায় মানুষ নাচছে।', 'people are dancing at the chand fair.', 'people are dancing on the moon.'), ('চাঁদ বাজারটি খুব পুরনো।', 'the chand market is very old.', 'the market is very old.'), ('চাঁদের হাটে মিষ্টি বিক্রি হচ্ছে।', 'sweets are being sold at chand’s fair.', 'sweets are being sold on moon foot.'), ('পাতা গাছ থেকে পড়ে যাচ্ছে।', 'leaves are falling from the tree.', 'leaves are falling from trees.'), ('গাছের পাতাগুলো সবুজ ও নরম।', 'the leaves of the tree are green and soft.', 'the leaves are green and soft.'), ('পাতায় শিশির জমেছে।', 'dew has gathered on the leaves.', 'the page has been gathered.'), ('শীতে সব পাতাই ঝরে পড়ে।', 'all the leaves fall in winter.', 'all leaves fall on winter.'), ('সে বইয়ের পাতা উল্টাচ্ছে।', 'he is turning the pages of the book.', 'he is turning the page of the book.'), ('বইয়ের শেষ পাতায় লেখকের নাম আছে।', \"the author's name is on the last page of the book.\", \"the author's name is on the last page.\"), ('পাতা ছিঁড়ে গেছে।', 'the page has been torn.', 'the page broken.'), ('তুমি নতুন পাতায় লেখো।', 'write on a new page.', 'write on a new page.'), ('সে মাছের পাতায় মসলা মাখল।', 'he marinated spices on the banana leaf.', 'she made it on the fish leaf.'), ('পূজায় ভোগ পাতায় পরিবেশন করা হয়।', 'offerings are served on leaves during puja.', 'the praise is served on the page.'), ('পাতার থালা পরিবেশবান্ধব।', 'plates made of leaves are eco-friendly.', 'the leaf is environmentally friendly.'), ('সে পাতার প্লেটে খাবার পরিবেশন করল।', 'he served food on a leaf plate.', 'he served food on the leaf.'), ('সে স্কুলের পাতায় ভর্তি ফর্ম জমা দিল।', 'he submitted the admission form at the school office.', 'he submitted the form to enter the school page.'), ('ফলাফল পাতায় প্রকাশিত হয়েছে।', 'the result has been published on the record sheet.', 'the results have been published on the page.'), ('প্রতিটি পাতায় ছাত্রের নাম আছে।', \"each sheet contains the student's name.\", 'each page has a student name.'), ('তুমি নতুন পাতায় তালিকা লেখো।', 'write the list on a new sheet.', 'write the list on the new page.'), ('দাগটা পরিষ্কার করো।', 'clean the stain.', 'clean the mark.'), ('শার্টে দাগ পড়ে গেছে।', 'a stain has appeared on the shirt.', 'a mark has fallen on the shirt.'), ('সে কাপড় থেকে রক্তের দাগ তুলছে।', 'she is removing the blood stain from the cloth.', 'he removes blood from clothes.'), ('তুমি জুতার দাগ মুছে ফেলো।', 'wipe the shoe marks off.', 'remove the mark of shoes.'), ('তার মনে গভীর দাগ লেগেছে।', 'a deep scar has formed in his heart.', 'he felt a deep mark.'), ('এই ঘটনাটি মনে চিরস্থায়ী দাগ রেখে গেছে।', 'the event left a permanent mark on the mind.', 'this has left a permanent mark in mind.'), ('তার কথায় দাগ লেগেছিল।', 'her words left a scar.', 'he had a mark in his words.'), ('শিশুর মনে সেই দাগ আজও রয়ে গেছে।', 'that scar remains in the child’s mind to this day.', \"the child's heart has remained.\"), ('চাঁদের দাগ আজ স্পষ্ট দেখা যাচ্ছে।', 'the spots on the moon are clearly visible today.', 'the moon is visible today.'), ('আয়নায় দাগ পড়েছে।', 'there are marks on the mirror.', 'a mark fell in the mirror.'), ('এই পাথরের ওপর ছোট দাগ আছে।', 'there are small spots on this stone.', 'there are small marks on this stone.'), ('দেয়ালের দাগ মুছে গেছে।', 'the marks on the wall have faded away.', 'the wall has been removed.'), ('সে পথে হাঁটছে।', 'he is walking on the road.', 'he is walking on the road.'), ('এই পথটি শহরে যায়।', 'this road leads to the city.', 'this way goes to the city.'), ('পথে অনেক দোকান আছে।', 'there are many shops along the way.', 'there are many shops on the road.'), ('পথের ধারে গাছ লাগানো হয়েছে।', 'trees have been planted along the roadside.', 'tree has been planted on the road.'), ('সত্যের পথ কঠিন।', 'the path of truth is difficult.', 'the path to truth is difficult.'), ('সে জীবনের পথ হারিয়ে ফেলেছে।', 'he has lost his way in life.', 'she lost her way to life.'), ('তুমি সঠিক পথ বেছে নাও।', 'choose the right path.', 'choose the right way.'), ('তার জীবনের পথ এখন স্পষ্ট।', 'the path of his life is now clear.', 'his way to life is clear now.'), ('সে বই পড়ার পথে।', 'he is on the way to reading books.', 'he is on the way to read the book.'), ('তুমি সফলতার পথে এগিয়ে চলেছো।', 'you are moving on the path to success.', 'you are moving forward on the path of success.'), ('সে তার স্বপ্ন পূরণের পথে।', 'he is on the way to fulfilling his dreams.', 'he is on his way to fulfill his dreams.'), ('তুমি তোমার লক্ষ্য অর্জনের পথে আছো।', 'you are on the way to achieving your goal.', 'you are on the way to your goal.'), ('সে মুখ ধুচ্ছে।', 'he is washing his face.', 'he washes his face.'), ('তার মুখে হাসি ফুটে উঠল।', 'a smile appeared on his face.', 'a smile rose in his face.'), ('মুখে ধুলো লেগেছে।', 'dust has stuck to the face.', 'i have dust in my mouth.'), ('সে মুখে জল ছিটাল।', 'he splashed water on his face.', 'he thicked water on his face.'), ('সে মুখ খুলে কিছু বলল না।', 'he didn’t open his mouth to speak.', 'he opened his mouth and said nothing.'), ('মুখ থেকে সত্য বেরিয়ে এলো।', 'the truth slipped out of his mouth.', 'the truth came out from your mouth.'), ('তার মুখে কঠোর কথা শোনা গেল।', 'harsh words were heard from his mouth.', 'he heard hard words in his mouth.'), ('মুখে মিথ্যা বলো না।', 'don’t speak lies.', \"don't lie in your mouth.\"), ('নদীর মুখ সমুদ্রে মিশেছে।', 'the mouth of the river meets the sea.', 'the face of the river is mixed in the sea.'), ('নালার মুখ বন্ধ করে দাও।', 'close the mouth of the drain.', 'close the mouth.'), ('গুহার মুখ সংকীর্ণ।', 'the cave’s opening is narrow.', 'the mouth is narrow.'), ('বন্দুকের মুখ উপরে রাখো।', 'keep the gun’s muzzle facing upward.', 'put the face up of the gun.'), ('সে বলটি ধরেছে।', 'he has caught the ball.', 'he caught the ball.'), ('পুলিশ চোরকে ধরেছে।', 'the police caught the thief.', 'the police caught the thief.'), ('সে পাখি ধরছে।', 'he is catching a bird.', 'he is catching birds.'), ('তুমি মাছ ধরছো নদীতে।', 'you are catching fish in the river.', 'you catch the river.'), ('তার কথা ধরতে পারো না।', 'you can’t grasp his words.', \"can't catch his words.\"), ('আমি তত্ত্বটা ঠিক ধরতে পেরেছি।', 'i have understood the theory correctly.', 'i found this correctly.'), ('সে যুক্তিটা ধরতে পারেনি।', 'he failed to get the logic.', \"he couldn't catch the reason.\"), ('তুমি বিষয়ের মর্ম ধরো।', 'grasp the essence of the topic.', 'catch the matter.'), ('সে রোগে ধরা পড়েছে।', 'he has been diagnosed with a disease.', 'he was caught by illness.'), ('ফলাফল প্রকাশের পর সত্য ধরা পড়ল।', 'the truth came out after the results were announced.', 'after announcing the result, the truth was caught.'), ('ভুলটা এখন ধরা পড়েছে।', 'the mistake has now been caught.', 'the mistake has been caught.'), ('অপরাধী ধরা পড়ে গেছে।', 'the criminal has been caught.', 'the criminal was caught.'), ('সে ছুরি দিয়ে কেক কাটছে।', 'he is cutting the cake with a knife.', 'he is cutting cake with a knife.'), ('সে গাছের ডাল কেটেছে।', 'he has cut a branch of the tree.', 'he cut the tree.'), ('রুটি টুকরো করে কেটে দাও।', 'cut the bread into pieces.', 'cut the bread.'), ('সে আঙুল কেটে ফেলেছে।', 'he has cut his finger.', 'he cut his finger.'), ('বইয়ের কাটা দাম এখন কম।', 'the marked price of the book is now lower.', 'the price is low now.'), ('এই কাপড়ের কাটা দাগ ভালো।', 'the cut pattern of this cloth is nice.', 'this clothes are good.'), ('তার নামের পাশে কাটা দাগ পড়েছে।', 'a cross mark has been drawn next to his name.', 'there was a cut mark next to his name.'), ('কাটা টিকিট ফেরত নেওয়া যায় না।', 'purchased tickets cannot be returned.', 'cutting tickets cannot be returned.'), ('সে কাটা কথায় আহত হয়েছে।', 'he has been hurt by harsh words.', 'he was wounded by speech.'), ('তার মুখে কাটা হাসি।', 'there’s a sharp smile on her face.', 'a smile in his face.'), ('কাটা কথা শোনার পর সে চুপ করে গেল।', 'he fell silent after hearing cutting remarks.', 'after he heard this, he silent.'), ('তার কাটা মন্তব্য সবাইকে কষ্ট দিল।', 'his cutting remark hurt everyone.', 'his cut comment hurt everyone.'), ('সে পান খায়।', 'he eats betel leaf.', 'he is drinking.'), ('বৃদ্ধ পান চিবিয়ে কথা বলছে।', 'the old man is chewing betel while speaking.', 'the elder is speaking.'), ('পান-সুপারি দোকানে বিক্রি হয়।', 'betel and areca nuts are sold in the shop.', 'the beverage is sold in the shop.'), ('সে পান খাওয়া ছেড়ে দিয়েছে।', 'he has stopped chewing betel.', 'he left drinking.'), ('সে নদীর পানি পান করছে।', 'he is drinking water from the river.', 'he is drinking river water.'), ('তুমি ঠান্ডা পানি পান করো।', 'drink cold water.', 'drink cold water.'), ('গরমে বেশি পানি পান করা দরকার।', 'it is necessary to drink more water in summer.', 'more water needs to be drinked.'), ('সে দুধ পান করতে ভালোবাসে।', 'he loves drinking milk.', 'he likes to drink milk.'), ('সে জ্ঞান পান করতে চায়।', 'he seeks to absorb knowledge.', 'he wants to drink knowledge.'), ('শিক্ষক বললেন, ছাত্ররা জ্ঞানের পান করুক।', 'the teacher said, let students drink knowledge.', 'the teacher said to drink knowledge.'), ('সে গুরুজনদের কাছ থেকে বিদ্যার পান করেছে।', 'he has imbibed learning from elders.', 'he drinked from the gentle.'), ('বইপড়া জ্ঞানের পান করার পথ।', 'reading books is a way of drinking knowledge.', 'the way to drink knowledge.'), ('সে ছোঁয়া খেল।', 'he played touch and run.', 'he touched.'), ('বাচ্চারা একে অপরকে ছুঁয়ে পালাচ্ছে।', 'children are touching each other and running.', 'children are touching each other.'), ('সে গাছ ছুঁয়ে প্রতিজ্ঞা করল।', 'he touched the tree and took an oath.', 'he touched the tree.'), ('মূর্তিটাকে হাত দিয়ে ছুঁয়ো না।', 'don’t touch the idol with your hands.', \"don't touch the statue with your hand.\"), ('তার কথায় মনে ছোঁয়া লাগল।', 'his words touched the heart.', 'he touched his words.'), ('সেই গল্পে জীবনের ছোঁয়া আছে।', 'that story carries the touch of life.', 'there is touch of life.'), ('তার সুরে মধুর ছোঁয়া আছে।', 'there is a sweet touch in his tune.', 'he has a sweet touch.'), ('চিত্রটিতে প্রকৃতির ছোঁয়া ফুটে উঠেছে।', 'a touch of nature is reflected in the painting.', 'the touch of nature has emerged on the picture.'), ('সে জ্বালানি জ্বালাচ্ছে।', 'he is lighting the fuel.', 'he is burning fuel.'), ('চুলায় আগুন জ্বলছে।', 'fire is burning in the stove.', 'the fire is burning.'), ('দীপটি জ্বলে উঠল।', 'the lamp lit up.', 'the dust burned up.'), ('ঘরে মোমবাতি জ্বলছে।', 'candles are burning in the room.', 'candles are burning in the room.'), ('তার মনে ঈর্ষা জ্বলছে।', 'jealousy is burning in his heart.', 'he burns in his mind.'), ('রাগে তার চোখ জ্বলছে।', 'his eyes are burning with anger.', 'his eyes are burning in anger.'), ('তার অন্তরে অন্যায়ের আগুন জ্বলছে।', 'a fire of injustice burns within him.', 'the fire is burning in her heart.'), ('বেদনার শিখা তার মনে জ্বলছে।', 'the flame of sorrow burns in his heart.', 'the tears are burning in her mind.'), ('সে রাস্তায় ধীরে চলছে।', 'he is walking slowly on the road.', 'he is walking slowly on the road.'), ('গাড়ি এখন দ্রুত চলছে।', 'the car is running fast now.', 'the car is running fast now.'), ('ট্রেন সময়মতো চলছে।', 'the train is running on time.', 'the train is running timely.'), ('সে প্রতিদিন সকালে হাঁটতে যায়।', 'he goes for a walk every morning.', 'he walks every morning.'), ('ঘড়ি ঠিকভাবে চলছে।', 'the clock is working properly.', 'the clock is running properly.'), ('মেশিনটা চলছে কিন্তু আওয়াজ করছে।', 'the machine is running but making noise.', 'the machine is running but it is sounding.'), ('তোমার পরিকল্পনা ভালোভাবে চলছে।', 'your plan is going well.', 'your plan is going well.'), ('দোকান আজও চলছে।', 'the shop is still operating today.', 'the shop is ongoing.'), ('তার সঙ্গে আমার ভালো চলা নেই।', 'i don’t get along well with him.', \"i don't go well with him.\"), ('তাদের মধ্যে সম্পর্কটা ভালো চলছে।', 'their relationship is going well.', 'the relationship is going well between them.'), ('বন্ধুত্বের চলা অনেক পুরোনো।', 'their friendship has lasted long.', 'the journey of friendship is very old.'), ('সে অফিসে সবার সঙ্গে ভালো চলে।', 'he gets along well with everyone at work.', 'he goes well with everyone in the office.'), ('সে গিটার বাজাচ্ছে।', 'he is playing the guitar.', 'he is playing guitar.'), ('তারা ঢাক বাজাচ্ছে।', 'they are playing drums.', 'they are playing.'), ('সে পিয়ানো বাজাতে জানে।', 'he knows how to play the piano.', 'he knows how to play piano.'), ('ব্যান্ড বাজছে রাস্তায়।', 'a band is playing on the street.', 'the band is playing on the street.'), ('সে দোকানে বাজে জিনিস বিক্রি করে।', 'he sells cheap items in the shop.', 'he sells bad things in the shop.'), ('বাজারে বাজে কথা চলছে।', 'nonsense talk is going around in the market.', 'i am talking badly in the market.'), ('এটা বাজে গুজব।', 'that’s a silly rumor.', 'this is bad.'), ('বাজে আওয়াজ বন্ধ করো।', 'stop the noise.', 'stop bad sounds.'), ('সে বন্ধুকে ছাড়া কোথাও যায় না।', 'he doesn’t go anywhere without his friend.', \"he doesn't go anywhere without his friend.\"), ('আমার তোমাকে ছাড়া আর কেউ নেই।', 'i have no one except you.', 'i have no other than you.'), ('চা ছাড়া সকাল অসম্পূর্ণ।', 'morning is incomplete without tea.', 'the morning is incomplete without tea.'), ('বৃষ্টি ছাড়া এই এলাকা শুষ্ক।', 'without rain, this region is dry.', 'this area is dry without rain.'), ('বন্দিকে জেল থেকে ছাড়া হয়েছে।', 'the prisoner has been released from jail.', 'the prison was released from prison.'), ('পাখিটাকে খাঁচা থেকে ছাড়া দাও।', 'let the bird out of the cage.', 'leave the bird from the cave.'), ('সে চাকরি থেকে ছাড়া পেয়েছে।', 'he has been discharged from his job.', 'he found it out of work.'), ('বন্দীরা অবশেষে ছাড়া পেল।', 'the prisoners were finally freed.', 'the prison finally got free.'), ('সে ধীরে ধীরে ছাড়া পাচ্ছে ব্যথা থেকে।', 'he is slowly getting relief from pain.', 'he gets free from pain slowly.'), ('চিন্তা থেকে ছাড়া পাওয়া যায় না।', 'one cannot escape from worries.', 'cannot be found without thought.'), ('সে দুঃখ থেকে ছাড়া চায়।', 'he seeks freedom from sorrow.', 'he wants to be free from sorrow.'), ('আমরা কষ্ট থেকে ছাড়া পেয়েছি।', 'we have been freed from suffering.', 'we got free from pain.'), ('সে বই নিয়েছে।', 'he has taken a book.', 'he took the book.'), ('তুমি আমার কলম নিয়েছো।', 'you have taken my pen.', 'you took my pen.'), ('সে আমার কথা গুরুত্বের সঙ্গে নিয়েছে।', 'he took my words seriously.', 'he took me seriously.'), ('সে পরামর্শটি গ্রহণ করেছে।', 'he accepted the advice.', 'she accepted the advice.'), ('সে ছবি তুলেছে।', 'he has taken a photograph.', 'he took a photo.'), ('তুমি মোবাইলে ছবি তুলেছো?', 'did you take the photo on your phone?', 'did you take a photo on the mobile?'), ('সে অনুষ্ঠানের ছবি তুলছে।', 'he is taking pictures of the event.', 'he is taking a photo of the event.'), ('ফটোগ্রাফার নতুন ক্যামেরায় ছবি তুলল।', 'the photographer took pictures with a new camera.', 'the photographer took a photo on a new camera.'), ('ঢাকা আজ ব্যস্ত শহর।', 'dhaka is a busy city today.', 'Dhaka is busy today.'), ('সে ঢাকায় বাস করে।', 'he lives in dhaka.', 'he lives in Dhaka.'), ('ঢাকা শহরে যানজট বেড়েছে।', 'traffic has increased in dhaka city.', 'traffic has increased in the city.'), ('ঢাকায় নতুন মেট্রোরেল চালু হয়েছে।', 'a new metro rail has started in dhaka.', 'new metropolitan launched in Dhaka.'), ('মেঘে সূর্য ঢাকা পড়েছে।', 'the sun is covered by clouds.', 'the sun is covered in the cloud.'), ('সে কাপড় দিয়ে টেবিল ঢাকা দিল।', 'he covered the table with cloth.', 'he covered the table with clothes.'), ('বাড়িটা এখন ধুলায় ঢাকা।', 'the house is now covered in dust.', 'the house is dusted.'), ('তার মুখ চুলে ঢাকা।', 'her face is covered by hair.', 'his face is covered.'), ('সে বলটি ফেলেছে।', 'he has dropped the ball.', 'he throwed the ball.'), ('তুমি কাপটা ফেলে দিয়েছো।', 'you have dropped the cup.', 'you have thrown the cup.'), ('সে ব্যাগটা ফেলে দিল।', 'he threw down the bag.', 'he throwed the bag.'), ('বইটা মাটিতে ফেলে দিও না।', 'don’t drop the book on the ground.', \"don't throw the book on the ground.\"), ('সে বন্ধুর কথা ভুলে ফেলেছে।', 'he has forgotten his friend’s words.', 'he forgot his words.'), ('পুরোনো স্মৃতি আমি ফেলে দিতে চাই।', 'i want to forget old memories.', 'i want to remove my old memories.'), ('সে ভুলটা সহজেই ফেলে দিল।', 'he easily forgot the mistake.', 'he removed this easily.'), ('তুমি সব চিন্তা ফেলে রাখো।', 'leave all your worries behind.', 'leave all thoughts.'), ('তারা মাঠে খেলছে।', 'they are playing in the field.', 'they are playing in the field.'), ('ছেলেটি ফুটবল খেলছে।', 'the boy is playing football.', 'the boy is playing football.'), ('তুমি খেলা দেখছো?', 'are you watching the game?', 'you see this?'), ('খেলা শেষ হয়ে গেছে।', 'the game is over.', 'the game ended.'), ('সে ভাগ্যের খেলা বুঝতে পারে না।', 'he cannot understand the game of fate.', 'he cannot understand the game.'), ('সময়ের খেলা অদ্ভুত।', 'the play of time is strange.', 'the time is strange.'), ('তার জীবনে কপালের খেলা চলছে।', 'the play of destiny goes on in his life.', 'the game is ongoing in his life.'), ('প্রকৃতির খেলা দেখো চারপাশে।', 'look around and see the play of nature.', 'see nature around.'), ('সে রঙ দিয়ে ছবি আঁকছে।', 'he is painting with colors.', 'he is painting with color.'), ('তুমি নীল রঙ পছন্দ করো?', 'do you like the color blue?', 'you like this color?'), ('রঙ শুকিয়ে গেছে।', 'the paint has dried.', 'the color has dried.'), ('দেয়ালে নতুন রঙ করা হয়েছে।', 'the wall has been freshly painted.', 'new painted on the wall.'), ('তার মুখে আনন্দের রঙ।', 'her face glows with joy.', 'a color of joy in his face.'), ('জীবনের রঙ বদলে গেছে।', 'the colors of life have changed.', 'the color of life changed.'), ('তার চোখে দুঃখের রঙ স্পষ্ট।', 'the shade of sorrow is clear in his eyes.', 'the color of sorrow is clear in his eyes.'), ('প্রেমের রঙ কারও চোখে লুকানো যায় না।', 'the color of love cannot be hidden in anyone’s eyes.', 'no one can hide the color of love.'), ('রঙ শিল্পী প্রতিযোগিতায় পুরস্কার পেয়েছে।', 'the painter won a prize in the art competition.', 'color artist won prize in competition.'), ('রঙ কোম্পানি নতুন ব্র্যান্ড এনেছে।', 'rong company launched a new brand.', 'the color company brought a new brand.'), ('সে চেনা মানুষদের সঙ্গে দেখা করল।', 'he met familiar people.', 'he met with familiar people.'), ('তুমি এই মুখটা চেনো?', 'do you recognize this face?', 'you know this one?'), ('চেনা জায়গায় ফিরে ভালো লাগে।', 'it feels good to return to familiar places.', 'i like returning to familiar places.'), ('সে পুরোনো চেনা গন্ধে হাসল।', 'she smiled at the familiar smell.', 'he smiled with the old smell.'), ('সে চেনা ধাতব শব্দ শুনল।', 'he heard a recognizable metallic sound.', 'he heard metal sounds.'), ('চেনা সুর কানে ভেসে এলো।', 'a familiar tune reached the ears.', 'known sound came to ear.'), ('বৃষ্টির চেনা গন্ধে মন ভরে গেল।', 'the familiar scent of rain filled the heart.', 'the smell of rain filled my mind.'), ('চেনা আওয়াজ শুনে সে দরজা খুলল।', 'hearing a familiar voice, he opened the door.', 'he opened the door after hearing the sound.'), ('সে ছবি দেখছে।', 'he is watching a movie.', 'he is seeing this.'), ('তুমি জানালার বাইরে দেখো।', 'look outside the window.', 'look outside the window.'), ('আমি নতুন জায়গা দেখতে ভালোবাসি।', 'i love visiting new places.', 'i love seeing new places.'), ('সে আয়নায় নিজেকে দেখছে।', 'she is looking at herself in the mirror.', 'he is seeing himself in the mirror.'), ('তারা একে অপরের দেখা পায়নি।', 'they haven’t met each other.', \"they didn't see each other.\"), ('আমরা অনেকদিন পরে দেখা করেছি।', 'we met after a long time.', 'we found it long later.'), ('সে বন্ধুর সঙ্গে দেখা করতে গেল।', 'he went to meet his friend.', 'he went to meet with his friend.'), ('আমি শিক্ষককে দেখা করেছিলাম।', 'i had met the teacher.', 'i met the teacher.'), ('সে সুন্দর গান শুনছে।', 'he is listening to a beautiful song.', 'he is listening to beautiful songs.'), ('আমি তার কণ্ঠ শুনে চমকে গেলাম।', 'i was startled to hear her voice.', 'i was surprised to hear his voice.'), ('তুমি খবরটা শুনেছো?', 'have you heard the news?', 'you heard the news?'), ('বৃষ্টির শব্দ শুনে মন ভালো লাগে।', 'the sound of rain pleases the mind.', 'i like hearing the sound of rain.'), ('এই গল্পটা আমি আগে শুনেছি।', 'i have heard this story before.', 'i heard this before.'), ('সে বন্ধুর কাছ থেকে খবরটা শুনেছে।', 'he heard the news from a friend.', 'he heard this from his friend.'), ('গুজবটা সবাই শুনেছে।', 'everyone has heard the rumor.', 'everyone heard this rumor.'), ('এই কাহিনি বহুবার শোনা হয়েছে।', 'this tale has been told many times.', 'this story heard several times.'), ('সে উপহার দিয়েছে।', 'he has given a gift.', 'he gave a gift.'), ('তুমি তাকে বই দাও।', 'give him the book.', 'give him a book.'), ('মা সন্তানকে ভালোবাসা দেয়।', 'a mother gives love to her child.', 'mother gives love to children.'), ('সে আমাকে সুযোগ দিয়েছে।', 'he gave me an opportunity.', 'he gave me opportunity.'), ('তুমি আলো দাও।', 'give light.', 'give you light.'), ('এই বাতি আলো দেয়।', 'this lamp gives light.', 'this light gives light.'), ('সূর্য উষ্ণতা দেয়।', 'the sun gives warmth.', 'the sun gives warmth.'), ('গাছ ছায়া দেয়।', 'trees provide shade.', 'the tree gives shade.'), ('সে ঘড়ি রেখে গেছে।', 'he has left the watch behind.', 'he left the clock.'), ('তুমি বইটা টেবিলে রাখো।', 'keep the book on the table.', 'keep the book on the table.'), ('সে টাকা ব্যাগে রেখেছে।', 'he kept the money in the bag.', 'he kept the money in the bag.'), ('আমি ফোনটা পকেটে রেখেছি।', 'i have kept the phone in my pocket.', 'i kept this in my pocket.'), ('সে প্রতিশ্রুতি রেখেছে।', 'he has kept his promise.', 'she held a promise.'), ('তুমি কথা রাখো।', 'keep your word.', 'keep your speech.'), ('সে সময়ের প্রতিশ্রুতি রাখল।', 'he kept the promise of time.', 'he promised time.'), ('আমি আমার বিশ্বাস রেখেছি।', 'i have kept my faith.', 'i kept my trust.'), ('সে আজ খুব খুশি হয়েছে।', 'he has become very happy today.', 'he was very happy today.'), ('আকাশ মেঘলা হয়েছে।', 'the sky has become cloudy.', 'the sky was clouded.'), ('সে অসুস্থ হয়েছে।', 'he has become ill.', 'he was sick.'), ('বৃষ্টি শুরু হয়েছে।', 'the rain has started.', 'the rain started.'), ('সে ছাত্র হয়েছে।', 'he has become a student.', 'he became a student.'), ('তুমি ডাক্তার হয়েছো।', 'you have become a doctor.', 'you became a doctor.'), ('সে শিক্ষক হয়েছে।', 'he has become a teacher.', 'he became a teacher.'), ('তারা বন্ধু হয়েছে।', 'they have become friends.', 'they became friends.'), ('সে নদীতে জাল ফেলেছে।', 'he has cast the net in the river.', 'he threw it into the river.'), ('জালে অনেক মাছ ধরা পড়েছে।', 'many fish were caught in the net.', 'many fish were caught in the net.'), ('মাছ ধরার জালটা পুরোনো।', 'the fishing net is old.', 'the fishing is old.'), ('তারা নদীতে জাল টানছে।', 'they are pulling the net in the river.', 'they are pulling fake into the river.'), ('মাকড়সা জাল বুনছে।', 'a spider is weaving a web.', 'the spider is chewing.'), ('জালে শিশির জমেছে।', 'dew has gathered on the web.', 'the net has been gathered.'), ('জালের নকশা সুন্দর।', 'the pattern of the web is beautiful.', 'the net design is beautiful.'), ('জালটি বাতাসে দুলছে।', 'the web is swaying in the wind.', 'the net is floating in the air.'), ('সে মিথ্যার জালে জড়িয়ে পড়েছে।', 'he is trapped in a web of lies.', 'he has been involved in the net of lies.'), ('চক্রান্তের জাল দিন দিন বাড়ছে।', 'the web of conspiracy is growing day by day.', 'fake days are increasing.'), ('মিথ্যা ও ভয়ের জালে সে আটকা পড়েছে।', 'he is caught in the net of lies and fear.', 'he was stuck in the net of lie and fear.'), ('তার জীবনের চারপাশে মিথ্যার জাল বোনা।', 'a web of lies is woven around his life.', 'fake bells around his life.'), ('বিদ্যুতের জাল পুরো শহরে ছড়িয়ে আছে।', 'the power grid spreads across the city.', 'the net of electricity is spread throughout the city.'), ('ইন্টারনেটের জাল প্রতিটি ঘরে পৌঁছেছে।', 'the web of the internet has reached every home.', 'the net has reached every room.'), ('এই জালটি ডেটা সংযোগের জন্য।', 'this network is for data connectivity.', 'this is for data connection.'), ('তথ্যের জাল খুব বিস্তৃত।', 'the web of information is vast.', 'the net of information is very extensive.'), ('সে তার ধন লুকিয়ে রেখেছে।', 'he has hidden his wealth.', 'he hidden his wealth.'), ('তাদের ধন অনেক বেড়েছে।', 'their wealth has increased greatly.', 'their wealth increased much.'), ('ধনের অহংকার থাকা উচিত নয়।', 'one should not be proud of wealth.', 'we should not be proud.'), ('সে সোনা-রূপোর ধন জমিয়েছে।', 'he has stored gold and silver treasure.', 'he accumulated wealth of gold.'), ('সন্তানই তার ধন।', 'his child is his treasure.', 'the child is his.'), ('বন্ধুত্বই জীবনের সত্যিকারের ধন।', 'friendship is the true wealth of life.', 'friendship is the genuine wealth of life.'), ('মায়ের চোখে সন্তান ধনসম।', 'in a mother’s eyes, the child is like treasure.', 'i have a child in my eyes.'), ('জ্ঞানই প্রকৃত ধন।', 'knowledge is the real wealth.', 'knowledge is genuine.'), ('বেদে বলা হয়েছে, ধর্মই ধন।', 'the vedas say that righteousness itself is wealth.', 'he said it is wealthy.'), ('সাধকের কাছে শান্তিই ধন।', 'for a sage, peace is wealth.', 'peace is wealthy for the saint.'), ('আত্মজ্ঞানই চূড়ান্ত ধন।', 'self-knowledge is the ultimate wealth.', 'self-consciousness is ultimate.'), ('ধন ও ধর্মের সমন্বয়েই জীবন পূর্ণ।', 'life is fulfilled through the union of wealth and virtue.', 'the combination of wealth and religion is full of life.'), ('আমে রস আছে।', 'the mango has juice.', 'i have this.'), ('লেবুর রস চিপে দাও।', 'squeeze the lemon juice.', 'give leaf juice.'), ('ফলের রস পান করা ভালো।', 'drinking fruit juice is healthy.', 'drinking fruit juice is good.'), ('সে আমের রস বিক্রি করছে।', 'he is selling mango juice.', 'he is selling this.'), ('তার কথায় রস নেই।', 'his words have no charm.', 'there is no to say.'), ('গল্পটা রসবিহীন লাগছে।', 'the story feels tasteless.', 'the story feels incomplete.'), ('তার ভাষায় রস আছে।', 'there is a pleasant tone in his language.', 'there is a speech in his.'), ('রসিকতা কথায় ফুটে উঠেছে।', 'wit sparkled in his words.', 'the jokes raised.'), ('এই কবিতায় শৃঙ্গার রস প্রকাশিত।', 'the poem expresses the emotion of love.', 'in this poem, the speech is revealed.'), ('রসতত্ত্ব সাহিত্যের প্রাণ।', 'the theory of rasa is the soul of literature.', 'the literature is life.'), ('ভয় রস নাটকে প্রকাশিত হয়েছে।', 'the emotion of fear is depicted in the play.', 'fear showed.'), ('রসনায় মধুর রস ভরে গেছে।', 'the tongue is filled with sweetness.', 'sweet juice has been filled.'), ('গাছের মূল মাটির গভীরে গেছে।', 'the roots of the tree have gone deep into the soil.', 'the main tree has gone deeper to soil.'), ('মূল শুকিয়ে গেলে গাছ মরে যায়।', 'if the root dries up, the tree dies.', 'when the tree is drying, it dies.'), ('সে গাছের মূল টেনে তুলল।', 'he pulled out the root of the tree.', 'he pulled the tree.'), ('এই গাছের মূল খুব মোটা।', 'the roots of this tree are thick.', 'the origin of this tree is very thick.'), ('বইটির মূল মূল্য একশ টাকা।', 'the original price of the book is one hundred rupees.', 'the original price is a hundred dollars.'), ('মূল দাম এখন বেড়েছে।', 'the base price has increased now.', 'the price has risen now.'), ('তুমি মূল টাকাটা ফেরত দাও।', 'return the principal amount.', 'return the original money.'), ('সে মূল হারে সুদ নিচ্ছে।', 'he is charging interest at the base rate.', 'he is taking interest at the main rate.'), ('সমস্যার মূল বোঝা দরকার।', 'it is necessary to understand the root of the problem.', 'the problem needs to be understood.'), ('বিবাদের মূল তুচ্ছ ব্যাপার।', 'the cause of the quarrel was trivial.', 'the main matter of dispute.'), ('মূল কারণ এখন জানা গেছে।', 'the main reason is now known.', 'the main cause is known now.'), ('প্রতিটি সমস্যার মূল খুঁজে বের করো।', 'find the root of every problem.', 'find the root of each problem.'), ('এটি মূল পাঠের অনুবাদ।', 'this is the translation of the original text.', 'this is the original translation.'), ('মূল গানটি আরও সুন্দর ছিল।', 'the original song was more beautiful.', 'the original song was more beautiful.'), ('মূল কবিতাটি হারিয়ে গেছে।', 'the original poem is lost.', 'the original poem is lost.'), ('মূল নথিটি সংরক্ষণ করো।', 'keep the original document safe.', 'keep the original document.'), ('ধারার পানি ক্রমে নদীতে মিশে যায়।', 'the stream of water gradually merges into the river.', 'the water is mixed into the river.'), ('বৃষ্টির ধারা তীব্র হচ্ছে।', 'the streaks of rain are getting stronger.', 'the rain is intense.'), ('এই ধারা পাহাড় থেকে নেমে এসেছে।', 'this stream has descended from the hills.', 'this line has come down from the mountain.'), ('জলের ধারা মাটিকে ভিজিয়ে দিচ্ছে।', 'the flow of water is wetting the soil.', 'water leaves the soil.'), ('ধারা ৩৭০ বাতিল করা হয়েছে।', 'article 370 has been abolished.', 'the article 370 was canceled.'), ('আইনের ধারা অনুযায়ী ব্যবস্থা নেওয়া হবে।', 'action will be taken according to the section of law.', 'measures will be taken according to the law.'), ('ধারা ১৪ অনুযায়ী সমান অধিকার রয়েছে।', 'equal rights exist under article 14.', 'according to article 14 there are equal rights.'), ('ধারাটি সংবিধানে গুরুত্বপূর্ণ।', 'this clause is important in the constitution.', 'this is important in the constitution.'), ('বাঙালি সাহিত্যের ধারা প্রাচীন।', 'the tradition of bengali literature is ancient.', 'the banana literature is ancient.'), ('ভক্তি সাহিত্যের ধারা মধ্যযুগে গড়ে উঠেছিল।', 'the bhakti literary tradition developed in the medieval period.', 'the line of praise formed in the middle age.'), ('লোকসংস্কৃতির ধারা আজও বেঁচে আছে।', 'the stream of folk culture still lives on.', \"people's culture is still living today.\"), ('এই ধারা বাংলা কবিতায় অনন্য।', 'this school is unique in bengali poetry.', 'this is unique in the poem.'), ('পুরোহিত মন্ত্র পাঠ করছেন।', 'the priest is reciting mantras.', 'the priest is reading.'), ('মন্ত্র উচ্চারণে মন শান্ত হয়।', 'reciting mantras calms the mind.', 'the mind calms with speech.'), ('সে গায়ত্রী মন্ত্র জপছে।', 'he is chanting the gayatri mantra.', 'he is chanting.'), ('মন্ত্র পাঠের সময় নীরবতা রাখা উচিত।', 'silence should be maintained during chanting.', 'should be silent during reading.'), ('মন্ত্রী মন্ত্র ঘোষণা করলেন।', 'the minister announced the policy.', 'the minister announced.'), ('সরকার নতুন মন্ত্র চালু করেছে।', 'the government has introduced a new policy.', 'the government launched a new ministry.'), ('এই মন্ত্র উন্নয়নের প্রতীক।', 'this policy symbolizes progress.', 'this is a symbol of development.'), ('তিনি আর্থিক মন্ত্র পরিবর্তন করেছেন।', 'he has revised the economic policy.', 'he changed the financial department.'), ('সে সফলতার মন্ত্র জানে।', 'he knows the mantra of success.', 'he knows the success.'), ('পরিশ্রমই সফলতার মন্ত্র।', 'hard work is the mantra of success.', 'work is the instrument of success.'), ('সে জীবনের মন্ত্র খুঁজে পেয়েছে।', 'he has found the mantra of life.', 'he found this in life.'), ('সততাই সুখের মন্ত্র।', 'honesty is the mantra of happiness.', 'honesty is the minister of happiness.'), ('তীর নদীর পাশে বালিতে ভরা।', 'the riverbank is filled with sand.', 'filled with sand along the river.'), ('তীরের গাছগুলো দুলছে।', 'the trees on the bank are swaying.', 'the trees are floating.'), ('সে তীরে এসে দাঁড়াল।', 'he stood at the shore.', 'he came to the shore.'), ('তীরের ফুলগুলো ফুটেছে।', 'the flowers by the shore have bloomed.', 'the flowers are floating.'), ('যোদ্ধা ধনুক থেকে তীর ছুঁড়ল।', 'the warrior shot an arrow from the bow.', 'the warrior threw shore from the wealth.'), ('তীর গিয়ে লক্ষ্যভেদ করল।', 'the arrow hit the target.', 'he went to target.'), ('তীরটি কাঠের তৈরি।', 'the arrow is made of wood.', 'this is made of wood.'), ('তীর নিক্ষেপে সে দক্ষ।', 'he is skilled at shooting arrows.', 'he is skilled in throwing.'), ('তীরের মতো কথা তার মুখ থেকে বেরোল।', 'words flew from his mouth like arrows.', 'words come out from his mouth.'), ('তার কথায় তীরের মতো ব্যথা আছে।', 'his words hurt like arrows.', 'he has a pain like a speech.'), ('তীরের মতো সত্য কেউ থামাতে পারে না।', 'the truth, like an arrow, cannot be stopped.', 'no one can stop the truth like a shade.'), ('সে তীর্যক কথায় ব্যথিত হল।', 'he was hurt by her sharp words.', 'he sicked with his speech.'), ('বৃক্ষের ছায়ায় বিশ্রাম নিচ্ছে।', 'he is resting under the shade of a tree.', 'rest in the shade of the tree.'), ('ছায়া পড়ছে দেয়ালে।', 'a shadow is falling on the wall.', 'shadow falls on the wall.'), ('ছায়ার নিচে ঠান্ডা লাগে।', 'it feels cool under the shade.', 'it feels cold under the shade.'), ('সে মায়ের ছায়া হারিয়েছে।', 'he has lost his mother’s protection.', 'she lost her shadow.'), ('মায়ের ছায়া সন্তানকে আশ্রয় দেয়।', 'a mother’s shade shelters her child.', \"mother's shade refers to her child.\"), ('সে নিজের ছায়ার মতো সঙ্গে থাকে।', 'he stays like a shadow beside me.', 'he is with his shadow.'), ('তার ছায়া সর্বদা পাশে থাকে।', 'her shadow is always beside him.', 'his shadow is always on his side.'), ('ছায়ার মতো স্মৃতি ভাসছে মনে।', 'memories float in the mind like shadows.', 'my memory is floating like the shade.'), ('তার দোষ প্রমাণিত হয়েছে।', 'his fault has been proven.', 'his guilty proved.'), ('তোমার দোষ নয়।', 'it’s not your fault.', 'not your fault.'), ('সে নিজের দোষ স্বীকার করেছে।', 'he admitted his mistake.', 'he confessed his guilty.'), ('দোষ করলে শাস্তি পেতে হবে।', 'if you make a mistake, you must face punishment.', 'if you are guilty, you need punishment.'), ('এই ফলের দোষ হলো এটি তাড়াতাড়ি পচে।', 'the flaw of this fruit is that it spoils quickly.', 'this is the fault, it is fast.'), ('যন্ত্রে কোনো দোষ দেখা দিয়েছে।', 'a defect has appeared in the machine.', 'there was a mistake in the machine.'), ('পণ্যের দোষ গ্রাহকের অভিযোগ তুলেছে।', 'the product’s defect has raised customer complaints.', \"the product's fault raised customer's complaints.\"), ('এই খাবারে কোনো দোষ নেই।', 'there is no fault in this food.', 'there is no fault about this food.'), ('আয়ুর্বেদে তিন দোষের কথা বলা আছে।', 'ayurveda speaks of three humors (doshas).', 'there are three words about guilty.'), ('বায়ু দোষ বেড়ে গেলে জ্বর হয়।', 'an excess of vata dosha causes fever.', 'when the air is increased, fever becomes increased.'), ('পিত্ত দোষ শরীরে গরম বাড়ায়।', 'the pitta dosha increases body heat.', 'bitter guilt increases heat in the body.'), ('কফ দোষ সর্দি আনে।', 'the kapha dosha brings cold.', 'the cow brings guilt.'), ('বীজ মাটিতে পুঁতে দাও।', 'plant the seed in the soil.', 'put it on the soil.'), ('এই বীজ থেকে গাছ হবে।', 'a plant will grow from this seed.', 'this will be trees.'), ('ভালো ফলের জন্য মানসম্মত বীজ দরকার।', 'good seeds are needed for quality crops.', 'quality seeds are needed for good fruits.'), ('সে বীজ সংরক্ষণ করছে।', 'he is preserving the seeds.', 'she keeps seeds.'), ('তার মনে সন্দেহের বীজ রোপিত হয়েছে।', 'the seed of doubt has been planted in his mind.', 'he planted seeds of doubt.'), ('দুঃখের বীজ থেকেই ঘৃণা জন্মায়।', 'from the seed of sorrow, hatred grows.', 'hate is born from the seed of sorrow.'), ('প্রতিশোধের বীজ বপন কোরো না।', 'do not sow the seed of revenge.', \"don't plant seeds of revenge.\"), ('আশার বীজ হৃদয়ে রোপিত হওয়া উচিত।', 'the seed of hope should be planted in the heart.', 'the seeds of hope should be planted in heart.'), ('এই গানের বীজ লোকসংগীত থেকে এসেছে।', 'the seed of this song came from folk music.', 'the seed of this song came from popularity.'), ('বিজ্ঞানের বীজ প্রাচীন ভারতে রোপিত হয়েছিল।', 'the seed of science was sown in ancient india.', 'the seeds of science have been planted in ancient India.'), ('তার চিন্তার বীজ ছোটবেলায়ই জন্মেছিল।', 'the seed of his thought was born in childhood.', 'his seeds were born in childhood.'), ('সংস্কারের বীজ পরিবারের মধ্যে থাকে।', 'the seed of culture lies within the family.', 'the seeds of reform remain within the family.'), ('সে দূর থেকে অদ্ভুত ধ্বনি শুনল।', 'he heard a strange sound from afar.', 'he heard strange tune from far.'), ('বজ্রের ধ্বনি আকাশ কাঁপাচ্ছে।', \"the thunder's roar shakes the sky.\", 'the brilliant brilliant sky.'), ('ধ্বনি আস্তে আস্তে মিলিয়ে গেল।', 'the sound slowly faded away.', 'the dust fell slowly.'), ('মন্দিরে ঘণ্টার ধ্বনি শোনা গেল।', 'the temple bell’s sound was heard.', 'the clock was heard in the temple.'), ('ধ্বনি ভাষাবিজ্ঞানের মূল উপাদান।', 'phoneme is a basic unit of linguistics.', 'thin is the main element of linguistics.'), ('বাংলা ভাষায় ৫২টি ধ্বনি রয়েছে।', 'there are 52 phonemes in bengali.', 'there are 52 words in the bengal language.'), ('ধ্বনি ও বর্ণ আলাদা ধারণা।', 'sound and letter are different concepts.', 'weakness and color are different.'), ('ধ্বনি বিশ্লেষণ ভাষাবিজ্ঞানের প্রাথমিক ধাপ।', 'phonetic analysis is the first step in linguistics.', 'dust analysis is the initial step of linguistics.'), ('তোমার কণ্ঠের ধ্বনি খুব মধুর।', 'the sound of your voice is very sweet.', 'your voice is sweet.'), ('তার ধ্বনি ঘর ভরিয়ে দিল।', 'her voice filled the room.', 'he filled his house.'), ('গায়কের ধ্বনি শ্রোতাকে মোহিত করে।', 'the singer’s tone enchants the audience.', \"the singer's tune appears to the audience.\"), ('ধ্বনি রেকর্ড করা হয়েছে স্টুডিওতে।', 'the sound was recorded in the studio.', 'this was recorded in the studio.'), ('তুমি নাম লিপিতে লেখো।', 'write your name in script.', 'write your name in the leaf.'), ('বাংলা লিপি প্রাচীন ব্রাহ্মী লিপি থেকে এসেছে।', 'the bengali script originated from the brahmi script.', 'bengal writes come from ancient brahmi writes.'), ('তামিল লিপি ভিন্ন আকারের।', 'the tamil script has a different shape.', 'the tamil is different.'), ('প্রত্নতাত্ত্বিকেরা প্রাচীন লিপি উদ্ধার করেছেন।', 'archaeologists have recovered ancient inscriptions.', 'the archaeologists rescued ancient scriptures.'), ('পাথরে লিপি খোদাই করা আছে।', 'inscriptions are carved on the stone.', 'the stone is scattered.'), ('এই লিপিতে রাজাদের নাম আছে।', 'this inscription contains the names of kings.', \"there are the king's names.\"), ('প্রাচীন লিপিতে ইতিহাস লেখা আছে।', 'history is written in ancient inscriptions.', 'history is written in the old leaf.'), ('এই লিপি এখন জাদুঘরে রাখা আছে।', 'this inscription is kept in a museum.', 'this is kept in the museum.'), ('তার হাতের লিপি সুন্দর।', 'his handwriting is beautiful.', 'his hand is beautiful.'), ('লিপি স্পষ্ট নয়।', 'the writing is not clear.', 'the letter is not clear.'), ('তোমার লিপি পড়া কঠিন।', 'your handwriting is difficult to read.', 'your writing is difficult to read.'), ('লিপি দেখে শিক্ষক চিনে ফেললেন।', 'the teacher recognized it by the handwriting.', 'the teacher recognized it.'), ('তার হাতে নীল শিরা দেখা যাচ্ছে।', 'blue veins are visible on his hands.', 'he sees a blue shade in his hand.'), ('ডাক্তার শিরায় ইনজেকশন দিলেন।', 'the doctor injected into the vein.', 'the doctor gave injection.'), ('শিরায় রক্ত প্রবাহিত হচ্ছে।', 'blood flows through the veins.', 'blood is flowing in the neck.'), ('এই ওষুধ সরাসরি শিরায় যায়।', 'this medicine goes directly into the vein.', 'this medicine goes straight.'), ('এই শিরায় সোনা আছে।', 'there is gold in this vein.', 'there is gold.'), ('খনিতে নতুন শিরা আবিষ্কৃত হয়েছে।', 'a new vein has been discovered in the mine.', 'new wire has been discovered in the mines.'), ('লোহার শিরা পাহাড়ে দেখা গেছে।', 'iron veins have been seen in the hills.', 'iron shore seen in the mountain.'), ('সোনার শিরা খনন চলছে।', 'gold veins are being mined.', 'gold wire mining ongoing.'), ('শিক্ষার্থীরা দণ্ড পাচ্ছে।', 'the students are receiving punishment.', 'students are punished.'), ('দোষীর দণ্ড ঘোষণা করা হয়েছে।', 'the culprit’s punishment has been announced.', 'the guilty was sentenced.'), ('সে আজীবন দণ্ড পেয়েছে।', 'he has received a life sentence.', 'he was sentenced to life.'), ('বিচারক দণ্ড নির্ধারণ করেছেন।', 'the judge has determined the sentence.', 'the judge determined sentence.'), ('সে হাতে লাঠির মতো দণ্ড ধরেছে।', 'he is holding a stick-like staff in his hand.', 'he held a pen in his hand.'), ('সন্ন্যাসীর হাতে দণ্ড থাকে।', 'a monk carries a staff.', 'the monaster is sentenced.'), ('দণ্ড ধরে গুরু শিষ্যকে আশীর্বাদ দিলেন।', 'the guru blessed the disciple holding a staff.', 'the guru blessed the disciple.'), ('দণ্ড আকারে কাঠ তৈরি হয়েছে।', 'the wood has been shaped into a rod.', 'wood made in form of punishment.'), ('মানুষের প্রাণ অমূল্য।', 'human life is priceless.', 'human life is invaluable.'), ('সে প্রাণ বাঁচাতে দৌড়েছে।', 'he ran to save his life.', 'he walked to save his life.'), ('প্রাণ হারিয়েছে সে দুর্ঘটনায়।', 'he lost his life in the accident.', 'he lost his life in accident.'), ('সে প্রাণ দিয়ে কাজ করে।', 'he works with all his energy.', 'he works with life.'), ('সে আমার প্রাণের বন্ধু।', 'he is my dearest friend.', 'he is my friend.'), ('প্রাণের মানুষ হারানো বড় বেদনা।', 'losing the person you love is deep pain.', 'loss of life is a lot of pain.'), ('তুমি আমার প্রাণ।', 'you are my life.', 'you are my life.'), ('সন্তান তার প্রাণের টুকরো।', 'the child is a piece of her life.', 'the child is a piece of his life.'), ('মাটিতে পানি জমেছে।', 'water has accumulated on the ground.', 'water accumulated in the soil.'), ('মাটি খুঁড়ে ফসল রোপণ করা হয়।', 'crops are planted by digging the soil.', 'crop is planted.'), ('মাটি শুকিয়ে গেছে।', 'the soil has dried up.', 'the soil has dried.'), ('বাগানের মাটিতে সার দেওয়া হয়েছে।', 'fertilizer has been applied to the garden soil.', 'the garden has been fertilized.'), ('শস্য ভালো হচ্ছে এই মাটিতে।', 'crops are thriving in this soil.', 'crop is good on this ground.'), ('এই মাটির গন্ধ ভিন্ন।', 'the smell of this soil is different.', 'this soil smells differently.'), ('তারা মাটিতে বসে খাচ্ছিল।', 'they were sitting on the ground and eating.', 'they were sitting on the ground.'), ('ঘরের মাটিটা মসৃণ করা হয়েছে।', 'the floor of the house has been smoothed.', 'the house was smoothed.'), ('বাড়ির মাটি ধুয়ে নাও।', 'wash the house floor.', 'wash the soil.'), ('মাটিতে ছোপ ছোপ দাগ আছে।', 'there are speckled stains on the ground.', 'there are spinning mark on the soil.'), ('বিছানায় ধূলি জমেছে।', 'dust has gathered on the bed.', 'dust accumulated in bed.'), ('বাইরে ধূলির মাড় দেখা যাচ্ছে।', 'a layer of dust is visible outside.', 'dust is seen outside.'), ('ধূলিতে বইয়ের কোণা কুঁচকে গেছে।', 'the corners of the book are creased with dust.', 'the corner of the book is stuck in the dust.'), ('শীতে ধূলি বাড়ে।', 'dust increases in winter.', 'dust increases in winter.'), ('ধূলি ফুঁকলে উঠল।', 'when you blow, the dust rises.', 'the dust raised up.'), ('ধূলি ফুঁকলে বাচ্চারা কাশি শুরু করে।', 'when dust blows, children start coughing.', 'with dust, children begin to catch.'), ('পুরোনো স্মৃতির ধূলি তার মনে জমে আছে।', 'the dust of old memories has settled in his mind.', 'the dust of old memories is accumulated in his mind.'), ('ধূলি-মলিন কাপড় পরিষ্কার করো।', 'clean the dust-stained cloth.', 'clean dust clothes.'), ('ধূলিতে গাড়ির চাকা ঢাকা।', \"the car's wheels are covered in dust.\", 'the wheel is covered in dust.'), ('ঠাণ্ডা বায়ু ঘর ভরিয়ে দিল।', 'a cold breeze filled the room.', 'cold wind filled the room.'), ('বাতাসে অচেনা বায়ুর গন্ধ আছে।', 'there’s an unfamiliar scent in the air.', 'there is an unknown smell in the air.'), ('বায়ু দিয়ে পাখা ঘুরছে।', 'the fan is spinning by the wind.', 'the birds are moving through the wind.'), ('বাতাসের গতি বাড়ছে।', 'the speed of the wind is increasing.', 'the wind is increasing.'), ('শুদ্ধ বায়ু স্বাস্থ্যবান রাখে।', 'clean air keeps you healthy.', 'clean air keeps healthy.'), ('বায়ুকে আমরা শ্বাস গ্রহণ করি।', 'we breathe in the air.', 'we take breathing.'), ('বায়ু দূষণ শহরকে ক্ষতিগ্রস্ত করে।', 'air pollution harms the city.', 'air pollution damages the city.'), ('তুমি হালকা বায়ু অনুভব করছো?', 'do you feel the gentle breeze?', 'you feel lightly?'), ('বায়ুর চাপ কমে গেলে সমস্যা হয়।', 'when air pressure drops, problems arise.', 'when the air pressure is reduced, there are problems.'), ('যোদ্ধা ধনু টানল।', 'the warrior drew the bow.', 'the warrior pulled wealth.'), ('ধনু থেকে তীর ছুটল।', 'an arrow shot out from the bow.', 'fleet from wealth.'), ('সে দক্ষ ধনুর্ধর।', 'he is a skilled archer.', 'he is skilled.'), ('ধনুতে শক্ত দড়ি বাঁধা।', 'a strong string is tied to the bow.', 'a hard wire is bound to the wealth.'), ('ধনু রাশির জাতকরা ধীর প্রকৃতির।', 'people of the sagittarius sign are calm in nature.', 'wealthy raises are slow.'), ('সে ধনু রাশির জাতক।', 'he belongs to sagittarius.', 'he belongs to rise.'), ('ধনু রাশি আগুন তত্ত্বের অধীন।', 'sagittarius is ruled by the fire element.', 'wealth is under fire theory.'), ('জ্যোতিষে ধনুর ভূমিকা গুরুত্বপূর্ণ।', 'in astrology, sagittarius plays an important role.', 'the role of wealth is important in astronomy.'), ('এই গাছের ফল মিষ্টি।', 'the fruit of this tree is sweet.', 'the fruit of this tree is sweet.'), ('সে ফল খাচ্ছে।', 'he is eating fruit.', 'he is eating fruit.'), ('বাজারে বিভিন্ন ফল বিক্রি হচ্ছে।', 'different fruits are being sold in the market.', 'various fruits are sold in the market.'), ('ফল পাকা হয়েছে।', 'the fruit has ripened.', 'the fruit has been burned.'), ('পরিশ্রমের ফল মিষ্টি।', 'the fruit of labor is sweet.', 'the fruit of work is sweet.'), ('তার ভুলের ফল খারাপ হয়েছে।', 'the result of his mistake was bad.', 'the result of his mistakes was worse.'), ('পরিশ্রম না করলে ফল মেলে না।', 'without effort, there is no result.', \"if you don't work, you don't match the result.\"), ('ভালো কাজের ভালো ফল হয়।', 'good deeds bring good results.', 'good work results.'), ('সে রাগে কথা বলল।', 'he spoke in anger.', 'he spoke with anger.'), ('রাগ নিয়ন্ত্রণ করা উচিত।', 'one should control anger.', 'anger needs to be controlled.'), ('তার মুখে রাগ দেখা যাচ্ছিল।', 'anger was visible on his face.', 'he saw anger in his face.'), ('সে রাগে দরজা বন্ধ করে দিল।', 'he slammed the door in anger.', 'he closed the door with anger.'), ('সঙ্গীতে ভৈরব রাগ গাওয়া হচ্ছে।', 'raga bhairav is being sung in music.', 'anger is singing in music.'), ('সে ইয়ামন রাগ বাজাচ্ছে।', 'he is playing raga yaman.', 'he is playing yaman anger.'), ('রাগদরবারে শিল্পীরা গান গাইছেন।', 'artists are performing at the raga session.', 'the artists sing in anger.'), ('রাগ সংবেদন জাগায়।', 'raga evokes emotion.', 'raise of anger.'), ('গুরু বচন মানা উচিত।', 'one should obey the teacher’s words.', 'the guru should be respected.'), ('তার বচনে গভীর অর্থ আছে।', 'there is deep meaning in his words.', 'he has deep meaning in his speech.'), ('পুরাণে বচন বহু আছে।', 'there are many sayings in the puranas.', 'there are many old words.'), ('বচন পালনে সে বিখ্যাত।', 'he is famous for keeping his word.', 'he is famous for speaking.'), ('বচন রচনা একটি সাহিত্যিক রীতি।', 'composition of sayings is a literary style.', 'speech is a literary practice.'), ('চর্যাপদে বচনের ব্যবহার আছে।', 'sayings are used in the charyapada.', 'there is use of speech.'), ('প্রবচন বচনের এক ধরণ।', 'proverbs are a kind of saying.', 'a form of speech.'), ('গুরু বচন শুনে শিষ্য প্রণাম করল।', 'the disciple bowed after hearing the master’s words.', 'the guru heard the speech.'), ('তার দৃষ্টি মেঘাচ্ছন্ন।', 'his gaze is clouded.', 'his eyes are cloudy.'), ('দৃষ্টির গভীরতায় অনুভূতি লুকানো।', 'emotions are hidden in the depth of gaze.', 'the feeling is hidden in the depth of eyes.'), ('তার দৃষ্টি অনেক দূর পর্যন্ত যায়।', 'his sight reaches far.', 'his eyes go far away.'), ('দৃষ্টি সরিয়ে নাও।', 'look away.', 'remove the eye.'), ('বিজ্ঞানীর দৃষ্টি সমাজে নতুন দিক দেখায়।', 'the scientist’s vision shows a new direction to society.', 'the vision of the scientist shows a new direction in society.'), ('দৃষ্টি উন্নয়নের দিকে নিবদ্ধ।', 'the vision is focused on development.', 'vision focused on development.'), ('তার দৃষ্টিভঙ্গি ইতিবাচক।', 'his perspective is positive.', 'his perspective is positive.'), ('দৃষ্টি নিক্ষেপ করো বইয়ের দিকে।', 'cast your glance toward the book.', 'draw attention to the book.'), ('সূর্যের রশ্মি জানালা দিয়ে পড়ছে।', 'sunrays are falling through the window.', 'the sun is falling through the window.'), ('রশ্মি চোখে লাগছে।', 'the ray is striking the eyes.', 'i am seeing the light.'), ('চাঁদের রশ্মি ঠাণ্ডা।', 'moonlight is cool.', 'the moon is cold.'), ('রশ্মি পৃথিবীতে প্রাণ আনে।', 'rays bring life to earth.', 'light brings life to earth.'), ('রশ্মি নামের মেয়েটি আমার বন্ধু।', 'rashmi is my friend.', 'the girl named rashmi is my friend.'), ('রশ্মি আজ স্কুলে যায়নি।', 'rashmi didn’t go to school today.', \"i didn't go to school today.\"), ('রশ্মি নতুন বই লিখছে।', 'rashmi is writing a new book.', 'i am writing a new book.'), ('রশ্মি আমাকে ফোন করেছে।', 'rashmi called me.', 'the light called me.'), ('নাদের ধ্বনি মন্দিরে প্রতিধ্বনিত।', 'the echo of nada resonates in the temple.', 'the dust of the nail belongs to the temple.'), ('নাদের ধ্বনি মন প্রশান্ত করে।', 'the vibration of nada soothes the mind.', 'the thin calms mind.'), ('নাদ যোগ এক প্রাচীন সাধনা।', 'nada yoga is an ancient practice.', 'to join is an ancient skill.'), ('অন্তর্নাদ ধ্বনিতে মন একাগ্র হয়।', 'the inner sound focuses the mind.', 'inheritance accumulates mind.'), ('রাতের নাদ শহর ভরিয়ে দিল।', 'the night’s hum filled the city.', 'the night filled the city.'), ('কারখানার নাদ বন্ধ হয়েছে।', 'the hum of the factory has stopped.', 'the factory is closed.'), ('পাখির নাদ সকালে শোনা যায়।', 'the sound of birds is heard in the morning.', 'birds are heard in the morning.'), ('নাদের তরঙ্গ শান্ত।', 'the waves of sound are calm.', 'the wave is calm.'), ('সূর্যের জ্যোতি দিগন্ত ভরিয়ে দিল।', 'the sunlight filled the horizon.', 'the sun has filled horizons.'), ('জ্যোতি চোখ ঝলসাচ্ছে।', 'the light is dazzling the eyes.', 'the eye is blinking.'), ('জ্যোতি মানে দীপ্তি।', 'jyoti means brightness.', 'the joke means deception.'), ('জ্যোতি আমার সহপাঠী।', 'jyoti is my classmate.', 'i am my companion.'), ('জ্যোতি গান গাইছে।', 'jyoti is singing.', 'i am singing.'), ('জ্যোতি সাহিত্য ভালোবাসে।', 'jyoti loves literature.', 'i love literature.'), ('কাব্য পড়ে মন শান্ত হয়।', 'reading poetry soothes the mind.', 'the poem calms mind.'), ('কাব্য সাহিত্যের রস আস্বাদন করো।', 'enjoy the essence of poetic literature.', \"taste the poem's juice.\"), ('সে কাব্য রচনা করছে।', 'he is composing poetry.', 'he is making poetry.'), ('কাব্যের ভাষা রুচিশীল।', 'the language of poetry is elegant.', 'the poetry is pure.'), ('কাব্য নামের মেয়েটি সুন্দর গায়িকা।', 'kabya, the girl, is a beautiful singer.', 'the girl named poet is beautiful singer.'), ('কাব্য আজ কলেজে আসেনি।', 'kabya didn’t come to college today.', \"the poem didn't come to school today.\"), ('ছন্দ না থাকলে কবিতা অসম্পূর্ণ।', 'a poem is incomplete without rhythm.', 'if the poem is incomplete.'), ('সে ছন্দে গান গাইছে।', 'he is singing in rhythm.', 'he is singing.'), ('ছন্দ শেখা কঠিন।', 'learning rhythm is difficult.', 'learning is difficult.'), ('ছন্দ ভেঙে গেছে বক্তৃতায়।', 'the rhythm broke in the speech.', 'the speech broke.'), ('জীবনের ছন্দ হারিয়ে গেছে।', 'the rhythm of life is lost.', 'life has lost.'), ('তার জীবনে নতুন ছন্দ এসেছে।', 'a new rhythm has entered his life.', 'a new feeling came to her life.'), ('দৈনন্দিন ছন্দ এখন বদলে গেছে।', 'the daily rhythm has changed now.', 'daily routine has changed now.'), ('ছন্দ ফিরে পেতে চাই।', 'i want to regain my rhythm.', 'i want to get back.'), ('রাজা রথে চড়ে চলে গেল।', 'the king rode away in the chariot.', 'the king walked on the road.'), ('বৈশাখী মিছিলের রথ সাজানো আছে।', 'the chariot for the baishakhi procession is decorated.', 'marks are arranged.'), ('রথ চালক দড়ি টেনে ধরল।', 'the charioteer pulled the reins.', 'the rope pulled rope.'), ('পুজোতে রথযাত্রা দেখা যায়।', 'a chariot procession is seen during the festival.', 'you can see this in the poem.'), ('তার চেহারায় রথের ছাপ রয়ে গেছে।', 'his face bore the imprint of the journey.', 'there has been a speech on his face.'), ('মানবিকতার রথের প্রয়াস চলছেই।', \"the campaign of humanity's chariot continues.\", 'the effort of humanity is ongoing.'), ('রথে চড়া শিশুদের আনন্দ করছে।', 'children riding the chariot are rejoicing.', 'children are enjoying walking on the road.'), ('প্রতিপত্তির রথ জেলার ওপর ছড়ায়।', 'the chariot of authority spreads over the district.', 'the rise spreads across the district.'), ('সমুদ্রে তরঙ্গ উচ্ছ্বাস তৈরি করছে।', 'waves are creating excitement in the sea.', 'the waves are growing in the sea.'), ('তরঙ্গ ভেঙে উপকূলে আঘাত করছে।', 'the waves are crashing on the shore.', 'the wave collapses on the coast.'), ('রিংটোনের তরঙ্গ কানে ভেসে এলো।', \"the ringtone's vibration reached the ear.\", 'the wave of rington came to ears.'), ('রশ্মির তরঙ্গ দূরে পৌঁছে।', 'the waves of the ray reach far.', 'the wave reaches far.'), ('তার কণ্ঠে সঙ্গীতের তরঙ্গ লেগে আছে।', 'there are musical waves in his voice.', 'a wave of music belongs to his voice.'), ('তরঙ্গের ছোঁয়ায় প্রাণ জাগে।', 'life awakens at the touch of the waves.', 'life awakened by touching the wave.'), ('তরঙ্গের গতিবেগ বাড়ছে।', 'the speed of the waves is increasing.', 'the waves are increasing.'), ('রেডিও তরঙ্গ নীরব রাতে বাজে।', 'radio waves play in the silent night.', 'the radio waves are silent at night.'), ('পতাকা ঘরের বাইরে উড়ছে।', 'the flag is flying outside the house.', 'the flag is flying outside the house.'), ('তুমি পতাকা নিয়ে শোভাযাত্রা করো।', 'you march holding the flag.', 'celebrate with the flag.'), ('বৈঠকের পতাকা উত্তোলিত করা হলো।', 'the conference flag was hoisted.', 'the flag of the meeting was lifted.'), ('প্রতিষ্ঠানের পতাকা সশ্রদ্ধে বিদায় নেয়।', \"the organization's flag is lowered with respect.\", \"the organization's flag is admitting.\"), ('তার মুখে পতাকার মতো গর্ব।', 'there is pride on his face like a flag.', 'like a flag in his mouth.'), ('পতাকার রং দেশের গর্ব প্রকাশ করে।', \"the flag's colors express the pride of the nation.\", \"the flag color expresses nation's pride.\"), ('তুমি পতাকার সম্মানে দাঁড়াও।', 'stand in honor of the flag.', 'stand on the flag.'), ('তাঁর কণ্ঠে পতাকার গীতি উচ্চারিত হলো।', 'he chanted the anthem of the flag.', 'the flag is sounded in his voice.'), ('জান্ত্রটি নিখুঁতভাবে কাজ করে।', 'the device operates perfectly.', 'this works perfectly.'), ('প্রাচীন জানতে্র বিদ্যার প্রদর্শনী চলছে।', 'an exhibition of ancient instruments is underway.', 'the exhibition of ancient science is ongoing.'), ('জান্ত্রগণ নিরাপত্তা নিশ্চিত করে।', 'the machines ensure safety.', 'the to ensure safety.'), ('জান্ত্রের চালকমণ্ডলী প্রস্তুত।', \"the instrument's control panel is ready.\", 'the to drive is ready.'), ('তাঁর হাতে জান্ত্রের ছোঁয়া ছিল।', 'there was a mechanical touch to his skill.', 'he had a to touch with him.'), ('জান্ত্রিক কৌশলে তিনি কাজটি সম্পন্ন করলেন।', 'he completed the task with mechanical skill.', 'he completed this with genetic strategy.'), ('এই জান্ত্রটি বিশেষভাবে ডিজাইন করা হয়েছে।', 'this instrument has been specially designed.', 'this to be designed specificly.'), ('জান্ত্র সমস্যায় দ্রুত সারিয়ে ফেলো।', 'fix the device problem quickly.', 'remove this quickly.'), ('তার কথা শুনে মর্ম স্পর্শ করল।', 'hearing him touched the heart.', 'he touched his words.'), ('কথার মর্ম বুঝতে শিখো।', 'learn to understand the essence of words.', 'learn to understand words.'), ('লেখকের মর্ম গভীর।', \"the writer's meaning is profound.\", \"the writer's heart is deep.\"), ('গানটির মর্ম হৃদয় স্পর্শ করে।', \"the song's essence touches the heart.\", 'the speech touches heart.'), ('মারামারি হচ্ছে, দণ্ড হবে মর্মে।', 'a punishment in essence follows the fight.', 'fighting will be punished.'), ('কথার মর্মার্থী কবি ভালো লেখে।', 'a poet skilled in meaning writes well.', 'the poem writes well.'), ('চিকিৎসক মর্ম নির্ণয় করলেন।', 'the doctor diagnosed the core condition.', 'the doctor diagnosed.'), ('রক্ত সঞ্চালনের মর্ম বুঝে ডাক্তার কাজ করেন।', 'doctors work understanding the core of blood circulation.', 'the doctor is working to understand blood circulation.'), ('পানিতে লবণ দ্রবীভূত হয়েছে।', 'salt has dissolved in the water.', 'salt dissolved in the water.'), ('রান্নায় লবণ কম হলে স্বাদ থাকে না।', 'if there is little salt in cooking, there is no taste.', \"if salt is low in cooking, it doesn't taste.\"), ('সমুদ্র থেকে লবণ আহরণ করা হয়।', 'salt is extracted from the sea.', 'salt is caught from the sea.'), ('তার কথায় লবণের মতো সরসতা আছে।', 'there is zest in his words like salt.', 'his words are like salt.'), ('জীবনের লবণ হলো অভিজ্ঞতা।', 'experience is the salt of life.', 'the salt of life is experience.'), ('স্বল্প লবণ খেলে স্বাস্থ্য ভাল থাকে।', 'eating less salt keeps you healthy.', 'eating small salt keeps your health good.'), ('লোণার লবণের চাহিদা বেড়ে গেছে।', 'the demand for rock salt has increased.', 'the salt demand has increased.'), ('খাবারে লবণ পরিমিত করা উচিত।', 'salt should be used in moderation in food.', 'salt should be measured in food.'), ('পাহাড়ের রুদ্র মূর্তি লোকেরা পূজে।', 'people worship the fierce mountain idol.', 'the river statues are praised by people.'), ('তার কণ্ঠে রুদ্র ভাব ছিল।', 'there was a fierce tone in his voice.', 'his voice feels rough.'), ('রুদ্র নামের কবি আজ শহরে এসেছেন।', 'a poet named rudra has come to the city today.', 'the poet named Rudra came to the city today.'), ('রুদ্র সঙ্গীত অদ্ভুত গম্ভীর।', 'rudra music is strangely solemn.', 'gold music is strange.'), ('তার রুদ্র চেহারা সবাইকে ভয় দেখায়।', 'his fierce appearance frightens everyone.', 'his brilliant face scared everyone.'), ('রুদ্র অঙ্গিভাব কাব্যকে প্রভাবিত করে।', 'fierce expressions influence the poetry.', 'the poem affects poetry.'), ('উৎসবে রুদ্র নৃত্য ছিল আয়োজন।', 'a fierce dance was arranged at the festival.', 'the dance was organized at the festival.'), ('রুদ্র চরিত্র রূপকথায় বেশ দেখা যায়।', 'fierce characters are often seen in folklore.', 'this is very visible in the character.'), ('রাত জাগলে ক্ষুধা বাড়ে।', 'staying up at night increases hunger.', 'awakening increases hunger.'), ('ক্ষুধার্ত লোক ভাবছে খাবারের কথা।', 'a hungry person is thinking of food.', 'hungry people are thinking about food.'), ('তার মধ্যে জ্ঞানের ক্ষুধা আছে।', 'he has a hunger for knowledge.', 'there is hunger of knowledge.'), ('প্রতিভার ক্ষুধা তাকে চালিত করে।', 'a hunger for talent drives him.', 'the talent drives him.'), ('রুমে ক্ষুধা নিবারণকারী রুটি ছিল।', 'there was bread in the room to appease hunger.', 'there was hungry bread in the room.'), ('সাহিত্যে তীক্ষ্ণ ক্ষুধা পাঠকের মন জয় করে।', \"sharp appetite in literature wins the reader's heart.\", \"sharp hunger in literature wins the reader's mind.\"), ('ধৈর্য্য ও ক্ষুধা দুটোরই প্রয়োজন।', 'both patience and appetite are needed.', 'both need patience and hunger.'), ('সঠিক সময়ে ক্ষুধা মেটালে সুখ আসে।', 'satisfying hunger at the right time brings happiness.', 'happiness comes with hunger at the right time.'), ('নদীর স্রোত দ্রুত।', \"the river's current is swift.\", 'the river flow is fast.'), ('বৈদ্যুতিক স্রোত পরিবর্তিত হচ্ছে।', 'the electric current is changing.', 'electric current is changing.'), ('তথ্যের স্রোত অবিরাম।', 'the stream of information is continuous.', 'the flow of information is endless.'), ('মানব স্রোত শিবিরে গিয়েছে।', 'the human flow went to the camp.', 'human flow went to camp.'), ('তার চিন্তার স্রোত শান্ত।', 'the flow of his thoughts is calm.', 'his flow of thought is calm.'), ('স্রোত অনুযায়ী নৌকা ডুবল না।', \"the boat didn't sink according to the current.\", 'the boat did not float according to the flow.'), ('ড্যাম স্রোত নিয়ন্ত্রণ করে।', 'the dam controls the current.', 'the dam controls flow.'), ('স্কুলে স্রোত পরিবর্তনের ব্যাখ্যা করল।', 'the school explained the change of current.', 'the flow explained change in school.'), ('সূর্যোদয়ের দ্যুতি আকাশ ভরিয়ে দিল।', 'the brilliance of sunrise filled the sky.', 'the sunlight filled the sky.'), ('তার চোখে দ্যুতি দেখা যায়।', 'a glow is visible in his eyes.', 'the light is visible in his eyes.'), ('পাহাড়ের চূড়ায় দ্যুতি জ্বলে উঠেছে।', 'a glow has lit up on the mountain top.', 'electricity has burned at the top of the mountain.'), ('তার কাজে দ্যুতি স্পষ্ট।', 'a brilliance is evident in his work.', 'the electricity is clear in his work.'), ('কলা-কুশলে দ্যুতি বাড়ে।', 'artistic skill increases brilliance.', 'the electricity increases.'), ('দ্যুতি ছড়িয়ে পড়েছে প্রতিযোগিতায়।', 'brilliance spread in the competition.', 'electricity spread in competition.'), ('তার কণ্ঠে দ্যুতি ছিল।', 'there was a glow in his voice.', 'there was electricity in his voice.'), ('গ্রাম জুড়ে দ্যুতি ছড়িয়ে পড়ল।', 'glow spread across the village.', 'electricity spread throughout the village.'), ('শহরের ভাঙা সেতুতে মেরামত করা হলো।', \"repair was done on the city's broken bridge.\", 'the broken city was repaired.'), ('মেরামতকারী দলে সবাই ব্যস্ত।', 'everyone in the repair crew is busy.', 'everyone is busy in the repair team.'), ('বাড়ির মেরামত শেষ হয়েছে।', 'the house repairs are finished.', 'the house renovation ended.'), ('মেশিনের মেরামত বেশ затрат।', \"the machine's repair is quite costly.\", 'the repair of the machine is very expensive.'), ('মেরামত করে বস্তু নতুন লাগে।', 'after repair, the object feels new.', 'repaired new items.'), ('লোকেরা মেরামতের মান বিচার করছে।', 'people are judging the quality of the repairs.', 'people are judging the quality of repair.'), ('টাকা ছাড়া মেরামত অসম্ভব।', 'repair is impossible without money.', 'repair is impossible without money.'), ('অংশগুলো মেরামত করে পুনরায় স্থাপন করা হয়েছে।', 'parts were repaired and reinstalled.', 'the parts were repaired.'), ('শস্য বুঝে কাটা হয়েছে আগেভাগেই।', 'the crop was harvested early knowing the condition.', 'the grace was cut before.'), ('তারে কেটে রাখা দরকার।', 'you need to cut the wire.', 'need to cut it.'), ('চিঠি কেটে ফেলো।', 'cut the letter off.', 'cut the letter.'), ('তাঁর কাটা কথায় ব্যথা লুকানো ছিল।', 'there was hidden pain in his cutting words.', 'he hidden pain in his words.'), ('ভেজা কাপড় কেটে সেঁকো।', 'wring and dry the wet cloth.', 'cut wet clothes.'), ('কাটা টিকিট ফেরত নেই।', 'cut tickets are non-refundable.', 'not returned tickets.'), ('কাটা ছেঁড়া কাপড় ফেলো না।', \"don't throw away the torn pieces of cloth.\", \"don't throw out clothes.\"), ('তার কাটা হাসিতে কটুতা ছিল।', 'there was bitterness in his cutting smile.', 'his smiling was narrow.'), ('চিত্রশালায় বিভিন্ন বর্ণে ছবি সাজানো আছে।', 'the gallery has paintings arranged in various colors.', 'paintings are decorated in various colors.'), ('তার পোশাকের বর্ণ খুব উজ্জ্বল।', 'the color of his clothes is very bright.', 'his color is very brilliant.'), ('রঙ ও বর্ণ দিয়ে শিল্পী চিত্রটি সাজাল।', 'the artist decorated the painting with colors and hues.', 'the artist decorated the painting with colors and colors.'), ('বর্ণ পরিবর্তন করলে মুছিয়ে ফেলো।', 'if the color changes, wipe it off.', 'if you change the color, remove it.'), (\"বর্ণমালার প্রথম অক্ষর 'অ'।\", \"the first letter of the alphabet is 'অ'.\", 'the first letter is a.'), ('শিশুকে বর্ণ শেখানো হচ্ছে।', 'the child is being taught letters.', 'children are being taught.'), ('পন্ডিতবাহাদুর বর্ণচর্চায় দক্ষ।', 'pandit bahadur is skilled in script practice.', 'the pudding is skilled.'), ('বর্ণ বিন্যাসে ত্রুটি হলে সংশোধন করো।', 'if there is an error in the letter arrangement, correct it.', 'correct if there is errors in the color format.'), ('তাঁর বর্ণ পরিচয় গ্রামে আলোড়ন তোলে।', 'his caste identity caused a stir in the village.', 'his identity brings light in the village.'), ('সমাজে বর্ণবৈষম্য কড়া সমালোচিত।', 'caste discrimination in society is heavily criticized.', 'racism is criticized in society.'), ('পুরো শহর ধ্বংসের মুখে।', 'the entire city is on the brink of destruction.', 'the whole city is destroyed.'), ('বন্যায় শত শত বাড়ি ধ্বংস হয়েছে।', 'hundreds of houses were destroyed in the flood.', 'hundreds of houses were destroyed by the flood.'), ('যুদ্ধ ধ্বংসের ছাপ রেখেছে এখানে।', 'war has left marks of ruin here.', 'there has been a speech of destruction.'), ('প্রকৃতির অপার শক্তি ধ্বংস আনতে পারে।', \"nature's immense power can bring destruction.\", 'nature can destroy power.'), ('তাঁর আত্মার ধ্বংস চাই না।', 'i do not want the ruin of his soul.', \"i don't want to destroy his soul.\"), ('অহংকারে জীবনের ধ্বংস ঘটে।', 'pride leads to the spiritual ruin of life.', 'life is destroyed by pride.'), ('বিবেকহীন নির্বাচনে সমাজের ধ্বংস ঘটে।', \"in an unwise election, society's destruction occurs.\", 'unconscious elections cause destruction of society.'), ('তাঁর কাণ্ডের ফলে পরিবারের ধ্বংস ঘটে।', \"his actions led to the family's ruin.\", 'as a result, his family is destroyed.'), ('অবহেলা ধ্বংসের ইশারা।', 'neglect is a sign of impending ruin.', 'neglect of destruction.'), ('নিয়মিত যত্ন না করলে ধ্বংস অচিরেই আসবে।', 'if not cared for regularly, ruin will come soon.', \"if you don't care regularly, the destruction will come soon.\"), ('তুমি নাভি দিয়ে কাঁধের ব্যথা বলে দিলে।', 'you indicated shoulder pain via the navel area.', 'you tell your shoulder with navi.'), ('স্তনপানকারী শিশুর নাভি ঠিক থাকে না।', 'the navel of a nursing infant may not be intact.', \"breastfeeding babies don't stay well.\"), ('জ্যোতিষশাস্ত্রে নাভিকে কেন্দ্রীয় চিহ্ন বলা হয়।', 'in astrology the navel is considered a central sign.', 'in the astronomy, the navi is called a central mark.'), ('তার আয়োজন গ্রামের নাভি হয়ে উঠেছে।', \"his event has become the village's focal point.\", \"his organization became the village's navi.\"), ('কাপড়ের নাভি টেনে কাঠগড়ায় দাও।', \"tug the fabric's center and hang it on the frame.\", 'draw the clothes to the wood.'), ('রঙের নাভি ঠিক করলে ক্ষত ঢেকে যায়।', 'fixing the center of the dye covers the stain.', 'if the color is fixed, the wound is covered.'), ('গহনা সাজাতে নাভি বিন্যাস গুরুত্বপূর্ণ।', 'arranging the center is important when decorating jewelry.', 'the nave is important to decorate.'), ('প্রতি কেন্দ্রে নাভির গুরুত্ব অনস্বীকার্য।', 'in every center the importance of the navel is undeniable.', 'the importance of navi in every center is negative.'), ('নাভির চারপাশে আভা ছড়ায়।', 'a glow spreads around the navel.', 'eaves spread around the navel.'), ('কবিতায় নাভি বলে ভাবার্থ বর্ণনা করা হয়েছে।', 'in the poem the navel is described metaphorically.', 'in the poem described noise.'), ('অস্থি ভেঙে গেলে ব্যথা তীব্র হয়।', 'when a bone breaks the pain is intense.', 'when the bone is broken, the pain becomes severe.'), ('চিকিৎসক অস্থি পরীক্ষা করলেন।', 'the doctor examined the bone.', 'the doctor examined the bone.'), ('খনিতে অস্থির শিলাগৃহের সন্ধান পাওয়া গেল।', 'in the mine a fossilized bone deposit was found.', 'in the mining found unstable shelter.'), ('পুরাতাত্ত্বিকরা অস্থি বিশ্লেষণ করলেন।', 'archaeologists analyzed the bone remains.', 'the ancient analyzed bone.'), ('অস্থির সন্ধানে মানুষ অতীত খুঁজে ফিরেছে।', 'in search of bones, people have explored the past.', 'looking for instability, people found the past.'), ('তার অস্থি নরম হলে দুর্বলতা দেখা যায়।', 'if his bones are soft, weakness is evident.', 'when his bone is soft, weakness appears.'), ('অস্থিক পেশি সংবলিত কাঠামো শক্ত।', 'a skeletal structure reinforced by bones is strong.', 'the muscular structure is solid.'), ('কবিতায় অস্থি হয়ে বাঁচার ব্যাখ্যা দেওয়া হয়েছে।', 'in the poem survival is explained through bones metaphorically.', 'the poetry explained to survive.'), ('পুরোনো যুগের অস্থির অংশ সংগ্রহে রয়েছে।', 'parts of ancient bones are in the collection.', 'the old part is in collection.'), ('অস্থি ও পেশি সমন্বয়ে দেহ গঠিত।', 'the body is formed by a combination of bones and muscles.', 'bone and muscle formed.'), ('বাণী দিয়ে তিনি শিক্ষা দিলেন।', 'he taught through his words.', 'he taught with words.'), ('ঋষির বাণী মর্মগহ্বর ছোঁয়।', \"the sage's words touch the core of meaning.\", \"the priest's speech touches murderly.\"), ('বাণীপ্রবাহ সামাজিক আন্দোলন চালায়।', 'the flow of messages drives social movement.', 'speech moves social movement.'), ('সংবাদমাধ্যম বাণী পৌঁছে দেয়।', 'the media delivers messages.', 'the media reaches the message.'), ('প্রচারে বাণী বদলাতে চাই না।', 'do not change the message during the campaign.', \"i don't want to change words.\"), ('তিনি শান্তির বাণী শুনালেন।', 'he spoke the message of peace.', 'he heard peace words.'), ('বাণী রেখা নির্ধারণ করে জনগণের পথ।', \"a line of message determines the people's path.\", \"the message determines people's path.\"), ('শৈশবে শেখা বাণী জীবন গড়ে।', 'the messages learned in childhood shape life.', 'learning in childhood builds life.'), ('বাণীর শব্দে আশীর্বাদ লুকিয়ে ছিল।', 'a blessing was hidden in the words of the message.', 'the blessing was hidden in the words.'), ('তার বাণী হৃদয় জয় করে নেয়।', 'his words win the heart.', 'his words beat heart.'), ('তার শীল নম্র ও বিনয়ী।', 'his conduct is humble and modest.', 'his silence is gentle and gentle.'), ('শীলশান্ত মন ভালো কাজ করে।', 'a virtuous and calm mind does good deeds.', 'silent mind works well.'), ('শীলব্রত পালন জীবনে শুভ।', 'observing moral conduct brings goodness in life.', 'fulfillment is happy in life.'), ('তার শীলের কাহিনি গ্রামে জনপ্রিয়।', 'stories of his virtue are popular in the village.', 'his story is popular in the village.'), ('শীলাত্মক ভঙ্গিতে তিনি ভেসে উঠলেন।', 'he rose with a dignified demeanor.', 'he woke up with coolness.'), ('শীলগত ভঙ্গি কর্মে গুরুত্ব দেয়।', 'a dignified style emphasizes the work.', 'the shelter gives importance to action.'), ('তার শীল অত্যন্ত প্রসংশনীয়।', 'his conduct is highly commendable.', 'his shell is very compound.'), ('সমাজে শীল হারালে বিপদ আসে।', 'if virtue is lost in society, danger ensues.', 'when weakness comes in society, there is danger.'), ('শীলসম্মত আচরণ সবাই গ্রহণ করে।', 'everyone accepts conduct that is virtuous.', 'everyone accepts charitable behavior.'), ('শৈশবে শীল শেখা জরুরি।', 'learning good conduct in childhood is essential.', 'learning is urgent in childhood.'), ('তার নয়ন আকাশের মতো স্বচ্ছ।', 'his eyes are as clear as the sky.', 'his nine is clear like the sky.'), ('নয়নে অশ্রু ভরে উঠল।', 'tears welled up in the eyes.', 'the tears were filled.'), ('নয়ন দিয়ে সে দূরদৃষ্টি করল।', 'he gazed far with his eyes.', 'he looked away.'), ('কবিতায় নয়ন রূপক হিসেবে ব্যবহৃত।', 'in poetry the eye is used as a metaphor.', 'in the poem used nine forms.'), ('নয়ন নামের কন্যা আজ স্কুলে গেলো।', 'a girl named nayan went to school today.', 'a daughter named noun went to school today.'), ('নয়নের হাসি সবাইকে মোহিত করে।', \"nayan's smile enchants everyone.\", 'the smile of neon hurt everyone.'), ('চোখ নয়নকে আলোকিত করে।', 'the eyes illuminate the vision.', 'the eye illuminates.'), ('শৈশবে নয়নের ছবি স্মৃতিতে আছে।', 'the image of nayan from childhood remains in memory.', 'in childhood, the picture is in memory.'), ('নয়ন-চোখে কৃতজ্ঞতা স্পষ্ট ছিল।', 'gratitude was evident in the eyes.', 'the gratitude was clear in the eyes.'), ('কবিতার লাইন নয়নকে সাক্ষ্য দিয়েছে।', 'the line of the poem testified through the eye metaphor.', 'the poetry testified to the nine.'), ('কুসুম বাগানে আজ ফুল ফুটেছে।', \"flowers have bloomed in kusum's garden today.\", 'cutting flowers in the garden today.'), ('তার নাম কুসুম, সে গান গায়।', 'her name is kusum; she sings.', 'he is singing.'), ('কুসুমের সৌরভ ঘর ছেয়ে গেছে।', \"kusum's fragrance has filled the house.\", \"the sun's house has gone away.\"), ('কুসুম নাটকের প্রধান চরিত্র।', 'kusum is the main character of the play.', 'cossum is the main character.'), ('কুসুম ফুল তার জীবনের আশা।', 'the blossom is the hope of his life.', \"cossum is his life's hope.\"), ('কাব্যে কুসুমের রূপক ব্যবহার করা হয়েছে।', 'in the poem the blossom is used as a metaphor.', 'the poem has been used.'), ('শীতের সকালে কুসুম কাঁপে।', 'the blossom trembles on a winter morning.', 'cough in the winter morning.'), ('কুসুম নামের মেয়ে পুরস্কার পেল।', 'a girl named kusum received an award.', 'a girl named Kussum received prize.'), ('তার কুসুমী হাসি চোখে পড়ে।', 'her blossomy smile catches the eye.', 'his smiling appears.'), ('কুসুম খেতে মৌমাছি ঘিরে নিয়েছে।', 'bees have surrounded the blossom in the field.', 'apple surrounded to eat.'), ('অমল তার নাম এবং চরিত্র উভয়ই পবিত্র।', 'amal is pure both in name and character.', 'both his name and character are sacred.'), ('অমল নামের ছেলে স্কুলে প্রথম।', 'a boy named amal is first in school.', 'the son named amal was first in school.'), ('তার মন অমল, কেউ তাকে বিকৃত করতে পারে না।', 'his mind is pure; no one can corrupt him.', 'his mind is impaid; no one can disturb him.'), ('অমল পাথরের জিনিস সংগ্রহ করা হয়েছে।', 'objects made of amla (or pure) stone have been collected.', 'stone items collected.'), ('অমলতার স্বভাব তাকে আলাদা করে তোলে।', 'his purity makes him stand out.', 'the nature of impairment distinguishes him.'), ('গ্রাম জুড়ে অমল স্মৃতিগুলো সংরক্ষিত।', 'pure memories are preserved throughout the village.', 'wealthy memories are preserved throughout the village.'), ('অমল আজ মঞ্চে উপস্থাপিত হবেন।', 'amal will perform on stage today.', 'this will be presented on the stage today.'), ('তাঁর আবেগ অমল ও খাঁটি।', 'his emotions are pure and genuine.', 'his emotions are unclean and pure.'), ('অমল রচনা পাঠককে মুগ্ধ করে।', \"amal's composition fascinates the reader.\", 'incomplete creation fascinates the reader.'), ('অমল সত্তায় শান্তি বিরাজ করে।', 'purity of being brings peace.', 'unjustness brings peace.'), ('প্রাচীরের প্রাচী মজবুত করে পুনর্নির্মাণ শুরু হলো।', 'reconstruction began to reinforce the eastern wall.', 'the wall has been strengthened and renovated.'), ('গ্রামের প্রাচী আগে ছিল অনেক উঁচু।', \"the village's eastern wall used to be very high before.\", 'the village was very high before.'), ('প্রাচীরের প্রাচীরের বাইরে প্রাচী শোভিত।', 'beyond the wall, the eastern façade is adorned.', 'the wall is outside the wall.'), ('প্রাচীরের প্রাচী জাগতিক ও ঐতিহাসিক দিক প্রদর্শন করে।', 'the eastern wall displays worldly and historical aspects.', 'the ancient wall shows worldwide and historic aspects.'), ('বনানীর প্রাচী দিক দিয়ে সূর্য উঠে।', 'the sun rises from the eastern side of the grove.', 'the sun rises from the old side of the forest.'), ('তুমি প্রাচী দিকে দেখলে নতুন সূর্য দেখা যাবে।', \"if you look eastward you'll see the new sun.\", 'if you see the old one, you will see a new sun.'), ('প্রাচীন মন্দিরের প্রাচী সমৃদ্ধ অলঙ্করণে ভরা।', 'the eastern façade of the ancient temple is full of rich ornamentation.', 'the ancient temple is filled with rich decorations.'), ('শিশুরা প্রাচীরের কাছে খেলছে।', 'the children are playing near the eastern wall.', 'children are playing near the wall.'), ('প্রাচীরের প্রাচী সংস্কারের তালিকায় আছে।', 'the eastern wall is on the repair list.', 'the wall is on the list of ancient reforms.'), ('সূর্য প্রাচীরের ওপর প্রাচীরের মতো আলো ফেলছে।', 'the sun is casting light over the eastern façade like a wall.', 'the sun is lighting like a wall on the wall.'), ('রাজ্য ক্ষমতায় পৌঁছে সে জনগণকে অভয় দিল।', 'upon reaching power the state granted amnesty to the people.', 'when he reached power, he destroyed people.'), ('অন্তরে অভয় ভরে ছিল; সে আর ভীত নন।', 'he felt a surge of courage inside; he was no longer afraid.', 'he was filled with pain; he was no longer afraid.'), ('প্রাচীন কাহিনীতে দেবতা একজনকে অভয় দেন।', 'in ancient tales a deity grants someone protection.', 'in ancient stories the god belongs to a person.'), ('যোদ্ধারা অভয় পাঠ করেন যুদ্ধের আগে।', 'warriors recite a protection mantra before battle.', 'the warrior leases practice before war.'), ('সে আদালতে জীবনের অভয় চান।', 'he seeks clemency for his life in court.', 'he wants to live in court.'), ('শত্রুর সামনে তার অভয় প্রকাশ পেল।', 'his fearlessness was revealed in front of the enemy.', 'he appeared in front of his enemy.'), ('গাঁয়ের প্রবীণরা নবদম্পতিকে অভয় দান করল।', 'the village elders bestowed their blessing and protection on the newlyweds.', 'the elders donated the newly.'), ('অভয় সনদ পেলে সে নিরাপদে দেশে ফিরবে।', 'if he receives an amnesty certificate, he will return home safely.', 'he will return to the country safely.'), ('পবিত্র স্থলে অভয় পাঠের অনুষ্ঠান হয়।', 'a ceremony of reciting protection prayers takes place at the sacred site.', 'the ceremony is held in the holy place.'), ('তার অভয়পূর্ণ ভাষণে জনসম্মিলন টলমল করল।', 'the crowd stirred at his speech full of bravado.', 'the crowd tossed his unclean speech.'), ('বীণা হাতে নিয়ে তিনি ধীরে সুর তোলেন।', 'holding the vina, he gently drew a melody.', 'he lifted the sound slowly.'), ('পুরনো বীণার সুর আজও কণ্ঠে বাজে।', 'the tune of the old vina still echoes in the voice.', 'the sound of the old bean is still bad.'), ('বীণা নামের সেই মেয়েটি কণ্ঠে দবি আছে।', 'that girl named bina has a compelling voice.', 'the girl named bin has a voice.'), ('মন্দিরে বীণার রাগানুগ সঙ্গিত শুনতে পাওয়া যায়।', 'one can hear raga-based vina music in the temple.', 'in the temple can be heard anger.'), ('তার কাব্যে বীণার রূপক বহুল ব্যবহৃত।', 'his poetry frequently uses the vina as a metaphor.', 'the bean form is widely used in his poem.'), ('শিল্পীর বীণায় চরণ বাঁধে শ্রোতার মন।', \"the listener's heart binds to the artist's vina tune.\", \"the artist's mind is bound to speech.\"), ('বীণা প্রদর্শনীতে একটি বিরল পুরাতন বীণা ছিল।', 'there was a rare antique vina at the exhibition.', 'there was a rare old bean at the winery exhibition.'), ('বীণা-বাদকীর হাতকেই তিনি শ্রেষ্ঠ বললেন।', \"he called the vina player's hands the best.\", 'he said the best.'), ('ছড়ায় বীণা নামের চরিত্র একটি গুরুত্বপূর্ণ বাঁক।', 'in the poem, a character named bina plays an important role.', 'the character named bean is important.'), ('নাচে বীণার সুরে লয় ঢোকে।', 'dance gains rhythm to the tune of the vina.', 'in the dance in the wire.'), ('তল কুঁড়ে বসে সে গল্প শোনাল।', 'sitting at the bottom, he told the story.', 'he sat on the floor and told the story.'), ('তেলের তল অনেক মূল্যবান।', 'the base of the lamp is very valuable.', 'the oil is very valuable.'), ('সমতা ও তল মিলিয়ে হিসেব করা হলো।', 'the calculation was done aligning the levels and base.', 'the equity was calculated.'), ('পাত্রের তলে মাছ আটকে পড়ে।', 'a fish got trapped at the bottom of the vessel.', 'the fish is stuck under the container.'), ('তল থেকে উঠে আসা ধুলো কাপড়ে লেগে গেল।', 'dust rising from the floor stuck to the cloth.', 'the dust clothed up from the floor.'), ('তলটা শক্ত না হলে রকমারি সমস্যা হবে।', 'if the base is not firm, various problems will occur.', 'if the floor is not hard, it will be difficult.'), ('নৌকার তলে জল ঢুকছে।', 'water is entering under the hull of the boat.', 'water is coming under the boat.'), ('মেঝের তল মুছতে হবে।', 'the floor needs to be wiped.', 'the floor needs to be washed.'), ('তল মিলিয়ে সিঁড়ি তৈরি করো।', 'level the base and make the stairs.', 'build up the floor.'), ('তল-তলে কোনো শব্দ করো না।', 'do not make any noise on the floor below.', \"don't say anything on the floor.\"), ('নীরব উঠেই ঘরে নীরবতা নেমে আসে।', 'when nirab enters, a hush falls over the room.', 'when silence comes down, the silence comes down.'), ('রাতটি ছিল নীরব ও স্থির।', 'the night was silent and still.', 'the night was quiet and quiet.'), ('নীরবতার মহিমা সকলে অনুভব করল।', 'everyone felt the glory of silence.', 'everyone felt the glory of silence.'), ('তিনি নীরবভাবে কথাগুলো না বলা ঠিক মনে করেন।', 'he thinks it best to keep the words unspoken silently.', \"he doesn't say the words quietly.\"), ('পাহাড়ের শূন্যে নীরব বায়ু বইছে।', 'a quiet breeze blows in the mountain void.', 'silent wind is in the mountains.'), ('নীরব যত্নে তিনি বাগান সাজালেন।', 'in silent care he tended the garden.', 'he decorated the garden with silent care.'), ('নীরব অবস্থায় মনটি স্থির হয়।', 'in a state of silence the mind becomes steady.', 'the mind is stained in silence.'), ('নীরব নামের মেয়ে আজ বাড়ি নেই।', 'a girl named nirab is not home today.', 'a girl named silent is not home today.'), ('দীর্ঘ সময় নীরব থাকা কখনো স্বাচ্ছন্দ্যের নয়।', 'being silent for a long time is sometimes uncomfortable.', 'stay quiet for a long time is never comfortable.'), ('নীরব কক্ষটি শূন্যতার গান গাইছে।', 'the silent room sings a song of emptiness.', 'the silent room is singing emptiness.'), ('ব্রাহ্মীর অঞ্জল দিয়ে দেবদেবীকে পূজা করা হলো।', 'the priest offered worship to the deities with anjali.', 'the goddess was worshipped by the brahmi angel.'), ('হাত মিলিয়ে অঞ্জল দিলে মন প্রশান্ত হয়।', 'folding hands in anjali calms the mind.', 'when you give an angel, your mind is calmed.'), ('অঞ্জল নামের শিশুটি হাসি ছড়ায়।', 'a child named anjal spreads smiles.', 'the child named angel spreads a smile.'), ('পুণ্যের অঞ্জল সকলের ভাগ্যে বরকত বাড়ায়।', \"an offering of virtue increases blessings in everyone's fate.\", \"the genuine blessings increase everyone's fate.\"), ('অঞ্জল করা আধুনিক জীবনে অনেকেই ভুলে যায়।', 'many forget to fold hands in modern life.', 'many of these are forgotten in modern life.'), ('বয়স্করা শিশুকে অঞ্জল শেখান।', 'elders teach the child to offer anjali.', 'the elder teaches children angels.'), ('তাঁর হাতে অঞ্জল পূর্ণ সূর্য-ছবি আঁকা ছিল।', 'a sun motif was drawn in his palms for the offering.', 'he painted an angel full of sun.'), ('অঞ্জল পরিবেশন শেষে সকলে বিসর্জন দিল।', 'after the offering, everyone received the blessing.', 'after the end of the service, everyone gave it.'), ('অঞ্জল হতে আসে ক্ষমা ও শান্তি।', 'from the offering comes forgiveness and peace.', 'forgiveness and peace come from angels.'), ('শুভ দিনে অঞ্জল পাঠের সাধারণ প্রচলন।', 'on auspicious days, reciting offerings is a common practice.', 'good day is the ordinary practice of reading.'), ('কমল ফুলে ভরা বাগান দেখলেই মন ভরে ওঠে।', 'seeing a garden full of camellias fills the heart.', 'seeing a garden filled with candles, my heart fills up.'), ('তার কমল-কান্তি মুখে সততার ছাপ।', 'his lotus-like radiance bears a mark of sincerity.', 'his speech is a speech of honesty.'), ('কমল নামে একজন শিক্ষক কলেজে আসেন।', 'a teacher named kamal came to the college.', 'a teacher named comel came to college.'), ('পবিত্র জলে কমলের অঙ্কন করা হলো।', 'lotus designs were made in the holy water.', 'painted in sacred water.'), ('কিন্তু কমল নয়নে কাব্যের রূপ দেখা যায়।', 'yet in the lotus-like eyes one sees the form of poetry.', 'but it can be seen in the poem.'), ('কমল তলে বালুকণার সোনালি ঝিলিক।', 'under the lotus, a golden shimmer of sand appears.', 'the golden brick is under the tap.'), ('পুজোতে কমল অর্পণ করা হয়।', 'lotuses are offered during worship.', 'the poem is delivered.'), ('তার ছোট ভাই কমল আজ জন্মদিন পালন করলো।', 'his little brother kamal celebrated his birthday today.', 'his younger brother celebrated birthday today.'), ('কাব্যে কমল বিরহের প্রতীক হিসাবেও আছে।', 'in poetry, the lotus also symbolizes separation.', 'in the poem there is also a symbol of freedom.'), ('বাতাসে কমল সুবাস ছড়িয়ে পড়ল।', 'the scent of lotus spread in the air.', 'smell spread in the air.'), ('চিরন্তন সুর তার কণ্ঠে বাজে।', 'an eternal melody resonates in his voice.', 'the eternal sound becomes bad in his voice.'), ('সুর বাঁধলে গানটি মনোগ্রাহী হয়।', 'when the tune is set, the song becomes captivating.', 'when the sound is hanging the song becomes emotional.'), ('সুরের নাম নব্য-রাগ।', 'the name of the tune is the neo-raga.', 'the name is anger.'), ('সে সুর গেয়ে শ্রোতাকে মাত করে দিল।', 'he sang the tune and enchanted the audience.', 'he spoke to the audience.'), ('সুর নামের ছেলেটি আমার ক্লাসমেট।', 'a boy named sur is my classmate.', 'a boy named sun is my classmate.'), ('তার সুরে সৌন্দর্য আর বেদনা মিশে আছে।', 'beauty and sorrow are mingled in his melody.', 'beauty and pain are mixed in her sound.'), ('তার গানের সুরে গ্রামের রাত জাগে।', \"the village night awakens to his song's tune.\", 'the village awakened by her song.'), ('সুর রাখলে সঙ্গীতের ছন্দ মজবুত হয়।', 'keeping the tune strengthens the rhythm of the music.', 'keeping the sound strengthens the music.'), ('সুর হাড়িয়ে গেলে গান নিঃস্বর হয়ে যায়।', 'if the tune is lost, the song becomes breathless.', 'when the sound disappears, the song becomes unclean.'), ('প্রতিযোগিতায় সুরধারীরা পুরস্কৃত হন।', 'tune masters are awarded in the competition.', 'the competitor is awarded.'), ('নদীর কূল জলে ভিজে গেছে।', 'the river bank is soaked with water.', 'the river is wet in water.'), ('তারা কূল ধরে হাঁটছে সন্ধ্যায়।', 'they are walking along the shore in the evening.', 'they are walking at night.'), ('জাহাজ কূল ছেড়ে চলে গেল।', 'the ship left the coast.', 'the boat left.'), ('কূল নামের গ্রামটি সুন্দরীন।', 'a village named kul is picturesque.', 'the village is beautiful.'), ('তার চরিত্র কূলপানি মতো পরিষ্কার।', 'his character is as clear as coastal water.', 'his character is clear like original.'), ('কূল করে গাছ লাগালে তটরক্ষা হবে।', 'planting trees along the shore will protect the coast.', 'when planting the tree, it will be saved.'), ('নৌকা কূল ধাবিত করে নামল।', 'the boat drifted ashore and landed.', 'the boat dropped.'), ('কূলবর্তী জনসাধারণ উল্লাস করেছে উৎসবে।', 'the coastal public rejoiced at the festival.', 'the main public pleased in the festival.'), ('কূল-রেখা বদলে গেলে মানচিত্র পরিবর্তন হবে।', 'if the coastline changes, maps will be altered.', 'when the main line changes, the map will be changed.'), ('কূলের কদর বাড়াতে পর্যটন উন্নয়ন জরুরি।', \"tourism development is necessary to increase the shore's value.\", 'tourism development is urgent to increase the core.'), ('বাগানের মঞ্জরী আজ ভোরে ঝরে পড়ল।', 'the blossom in the garden fell at dawn today.', 'the garden has fallen in the morning.'), ('তাঁর নাম মঞ্জরী, সে নৃত্যে পারদর্শী।', 'her name is manjari; she is adept at dance.', 'his name is monarchy; he is witness to dance.'), ('কবিতায় মঞ্জরী প্রেমের নীল প্রতীক।', 'in poetry manjari symbolizes the blue of love.', 'the poem is a blue symbol of love.'), ('শীতের সকালে মঞ্জরীর গন্ধ বেশ তেতো।', 'the scent of the blossom is quite sharp on a winter morning.', 'the sun smells very much in the winter.'), ('সংস্কৃতিতে মঞ্জরীকে সৌন্দর্যের প্রতীক বলা হয়।', 'in culture, manjari is called a symbol of beauty.', 'in culture the mantra is called a symbol of beauty.'), ('মঞ্জরীর ছায়ায় পাঠশালা চলছে।', 'a school operates under the shade of the blossom tree.', 'in the shade of the mantra.'), ('তার কণ্ঠে মঞ্জরীর মতো কোমলতা আছে।', 'there is a delicacy like a blossom in her voice.', 'his voice has a gentle.'), ('মঞ্জরী নামের গানের কথা লোকচিত্তে লেগে থাকলো।', \"the lyrics of the song named 'manjari' remained in people's hearts.\", 'the song named mangri appeared in popularity.'), ('বাগানে মঞ্জরীর মল্লিকা বাড়ছে।', \"the garden's cluster of blossoms is increasing.\", 'the crop is growing in the garden.'), ('মঞ্জরী দেখা মানেই বসন্তের আসা।', 'seeing the blossom means the arrival of spring.', 'seeing the moon means coming to spring.'), ('তরী কুশলভাবে উত্তাল জলে চলছে।', 'the small boat sails skillfully on the choppy water.', 'the river is moving on the water.'), ('সে তরী নিয়ে তীরে ফিরে এলো।', 'he returned to shore with the boat.', 'he returned to the shore.'), ('অনোহিত তরীর ডেকে সবাই ভিড় করল।', \"at anohit's call, everyone gathered to the boat.\", 'everybody crowded by calling.'), ('তার জীবনে তরীভাবে পথচলা একাকিত্ব হাল করা।', \"in his life the figurative 'boat-like' journey eased his loneliness.\", 'walking lonely in his life.'), ('তরী তলে নরম কুশল বসানো আছে।', \"a soft cushion is placed under the boat's floor.\", 'there are soft shells placed under the leaf.'), ('অন্বেষণ শুরু করলাম নতুন শহরের গলিগুলোতে।', 'i began exploration in the alleys of the new city.', 'i started exploring new towns.'), ('তাঁর অন্বেষণ ছিল জ্ঞানের প্রতি অনবরত তৃষ্ণা।', 'his quest was an unending thirst for knowledge.', 'his exploration was continuous to thirst for knowledge.'), ('বিজ্ঞানী গবেষণায় অন্বেষণের নতুন দিক উন্মোচন করলেন।', 'the scientist revealed a new aspect of inquiry in research.', 'the scientist revealed a new direction of research.'), ('পুরাতাত্ত্বিকদের অন্বেষণে খণ্ডন মিলেছে।', \"the archaeologists' search uncovered fragments.\", 'in the exploration of the ancient.'), ('মনস্তাত্ত্বিক পরীক্ষায় অন্বেষণের ফল প্রকাশিত হলো।', 'results of the psychological investigation were published.', 'the results of the psychological examination have been revealed.'), ('প্রহরী রাতভর প্রহরায় ছিল।', 'the guard stayed on watch all night.', 'the guard was guarded at night.'), ('প্রহরীর দর্শনে কেমন যেন নিশ্চয়তা জন্ম নেয়।', \"a certain assurance arises at the guard's sight.\", 'how to ensure the guarantee is born.'), ('বৃক্ষপ্রহরীর কাহিনি গ্রামে প্রচলিত।', 'the tale of the tree-ward has been circulated in the village.', \"the tree guard's story is common in the village.\"), ('প্রহরী কাজ থেকে অবসর নিয়ে বাড়ি গেল।', 'the sentinel went home after his shift.', 'the guard went home with retirement from work.'), ('প্রহরীর শব্দ শুনে পশুপাখি ভয় পেল।', \"hearing the watchman's call, the animals were frightened.\", 'after hearing the voice of the guardian, the animal was scared.'), ('পরী বনে এসে শিশুকে পথ দেখাল।', 'a fairy came to the forest and showed the child the way.', 'came to the forest showing the child way.'), ('সে পরী-সুন্দরী; গ্রামের সবাই তাকে শেষ করে।', 'she is like a fairy—beautiful; the entire village adores her.', 'he is beautiful; everyone in the village ends him.'), ('পরীর গান শুনে রাত জাগা লোকেরা মুগ্ধ হল।', \"those who stayed up listening to the fairy's song were enchanted.\", 'people awakened after listening to the aftermath.'), ('ছাতাপাতা আর অন্ধকারে পরীর ছায়া দেখা গেল।', \"among leaves and darkness, a glimpse of a fairy's shadow was seen.\", 'the shadow was seen in the darkness.'), ('শৈশবে গল্পে পরীর আশীর্বাদ পেয়েছিল সে।', \"in childhood tales he had received a fairy's blessing.\", 'he received blessing in his childhood.'), ('তিনি কূট কৌশলে দেশের কুটনীতি বদলে দিলেন।', \"with diplomatic stratagems he changed the country's diplomacy.\", \"he changed country's diplomatic strategy.\"), ('কূটশিল্পে পারদর্শী রাজা শান্তি স্থাপন করল।', 'skilled in diplomacy, the king established peace.', 'the witnessed king established peace in the diplomacy.'), ('তার কূট কথায় সন্দেহ হওয়াটা স্বাভাবিক।', 'it is natural to be suspicious of his crafty words.', 'it is natural to be doubtful in his words.'), ('কূটকৌশল পাল্টাতে লোকেরা চিন্তিত।', 'people are worried about the changing tactics.', 'people are worried about changing policy.'), ('কূটের তন্তু ছেঁড়া পড়ে গেলে যোগাযোগ ভেঙে যায়।', \"if the network's intricate threads are cut, communication breaks.\", 'when the tune falls, the communication is broken.'), ('নীল আকাশে এক গভীর শূন্যতা।', 'a deep void appeared in the blue sky.', 'a deep empty in the blue sky.'), ('নীল নামে মেয়েটি আজ স্কুলে উপস্থিত ছিল।', 'a girl named nil attended school today.', 'the girl named blue appeared at school today.'), ('তার মুখে নীলাভ ছায়া ছিল ক্লান্তির।', 'a bluish tint of fatigue was visible on his face.', 'a blue shade in his face was tired.'), ('নীল জলের ঢেউয়ে সোনালি আলোর ছোঁয়া পড়ল।', 'a golden touch fell on the blue water waves.', 'the waves of blue water touched gold light.'), ('নীল পটভূমিতে পাখিরা উড়ে বেড়ায়।', 'birds fly across the blue backdrop.', 'birds are flying on the blue ground.'), ('তাণ্ডবের মধ্য দিয়ে নগরী কেঁপে উঠল।', 'the city trembled amid the rampage.', 'the city has risen through the tune.'), ('নৃত্যতাণ্ডব দেখাতে শিল্পী তার শক্তি খাটালেন।', 'to depict the dance of fury, the artist exerted his energy.', 'the artist shed his strength to show dance.'), ('জলতাণ্ডব নদীর ধার কেটে দিল।', 'the water onslaught cut into the riverbank.', 'the waterfall cut the river.'), ('রোমহর্ষক তাণ্ডবে দর্শক বিভ্রান্ত হল।', 'the audience was bewildered by the terrifying spectacle.', 'the spectator was confused.'), ('তার তাণ্ডবময় আবেগ গল্পকে জোরালো করেছে।', 'his tempestuous emotion made the story powerful.', 'his intense emotion strengthened the story.'), ('ইল মাছটি নদীতে ছড়িয়ে পড়েছে।', 'the eel has spread into the river.', 'the fish spread to the river.'), ('বৃদ্ধ ইল ধরার শিল্প জানেন।', 'the elder knows the art of catching eels.', 'the elder knows the art of catching.'), ('ইল বলে নদীর গভীরতা বোঝা যায়।', \"the presence of eels indicates the river's depth.\", 'the river depends on the depth.'), ('ইল মরুরূপে গাছের তলে লুকিয়ে রয়েছে—কল্পনার ছবি।', 'an eel in imagination hides under the tree like a mirage.', 'the el is hidden under the tree—image of imagination.'), ('ইল-শৈলী কবিতায় আঞ্চলিক রূপ উপস্থিত।', 'eel-like style appears as a regional motif in poetry.', 'the el-style appears in regional form.'), ('ছট পূর্ণিমায় পল্লী থমথমে হয়ে ওঠে।', 'during the festival, the village hushed under the full moon.', 'the shore becomes full (colloquial).'), ('ছট গানে গ্রামের নারী সক্রিয়ভাবে অংশ নেয়।', 'women of the village actively participate in chhat songs.', \"the village's women participate actively in the short song.\"), ('সে ছটকথায় বার্তা পাঠাল।', 'he sent a message through the traditional rhyme.', 'he sent a message.'), ('ছট সাঁতার প্রতিযোগিতায় স্থানীয়রা অংশ নেয়।', 'locals take part in the chhat swimming competition.', 'local people participate in small swimming competition.'), ('কবিতায় ছটের ইঙ্গিতে প্রকৃতির বন্দনা করা হয়েছে।', 'in the poem, nature is praised with hints of chhat tradition.', 'in the poem, nature has been caught.'), ('অলীক কাহিনায় পুরনো রাজা জীবন্ত।', 'in the unreal tale, the ancient king is alive.', 'the old king is alive.'), ('অলীক প্রত্যাশা বাস্তবতাকে কাঁটায় দেয়।', 'unreal expectations prick reality.', 'uncertainty brings reality.'), ('কবির অলীক বর্ণনা পাঠককে বিভ্রমে ফেলে।', \"the poet's fantastical descriptions bewilder the reader.\", \"the poet's description confuses the reader.\"), ('কথ্যকালের অলীক কল্পনা শিশুদের আকর্ষণ করে।', 'fabulous imaginations of folklore attract children.', 'speaking imagination attract children.'), ('অলীক আশায় হৃদয় ভরিয়ে রাখা ঠিক নয়।', 'it is not right to fill the heart with fanciful hopes.', 'filling heart with hope is not right.'), ('প্রলাপ ঘুরে যায় অচেনা কাহিনি শুনে।', 'rants swirl after hearing the strange tale.', 'turning to hear unknown stories.'), ('ভোরবেলায় তার প্রলাপে পরিবার উদ্বিগ্ন হয়।', \"the family's concerned by his early-morning delirium.\", 'his family is worried about it in the morning.'), ('কথকের প্রলাপে দর্শক উপহাস করলো।', \"the dancer's rambling monologue drew ridicule from the audience.\", 'the spectator laughed on the speech.'), ('বয়সের বাড়ার সঙ্গে প্রলাপ দেখা যায় মাঝে মাঝে।', 'with aging, occasional delirium can be observed.', 'with age, it can be seen sometimes.'), ('প্রলাপময় আবেদনে বোধহীনতা প্রকাশ পেয়েছে।', 'the incoherent plea revealed a lack of sense.', 'uncertainty appeared in the prosecute.'), ('উপনয়ন-অনুষ্ঠানে বাপের হাতে ছেলেটিকে তুলে দিলেন।', 'at the initiation ceremony, the father handed his son over.', \"he lifted the boy to the father's hand.\"), ('উপনয়ন নামক রীতি বহু প্রাচীন।', 'the rite called upanayan is very ancient.', 'the tradition is ancient.'), ('স্কুলে উপনয়ন প্রক্রিয়ার অনুশীলন করানো হলো।', 'the school practiced the process of initiation.', 'the practice was practiced at school.'), ('নতুন পদ্ধতির উপনয়ন করলে ফল ভাল হয়েছে।', 'adopting the new method yielded good results.', 'after making a new approach, the result was good.'), ('প্রকল্পের উপনয়ন সময়সাপেক্ষ ছিল।', 'the adoption of the project was time-consuming.', 'the project was timely.'), ('বিজ্ঞানী নতুন তন্ত্র আবিষ্কার করলেন।', 'the scientist discovered a new technique.', 'the scientist discovered a new mechanism.'), ('রাজনীতিতে তন্ত্রগত কৌশল ব্যবহার করা হয়।', 'tactical system strategies are used in politics.', 'strict strategy is used in politics.'), ('সঙ্গীতের তন্ত্রে তার দক্ষতা অনবদ্য।', 'his skill in musical technique is unparalleled.', 'his skill is incomplete with music.'), ('তন্ত্রভিত্তিক সমাধান সমস্যা নিরসনে কার্যকর।', 'system-based solutions are effective in problem-solving.', 'mechanical solution is effective to solve problems.'), ('তন্ত্র ভাঙলে ব্যবস্থাও ভেঙে যায়।', 'when a system breaks, the arrangement collapses too.', 'when the system is broken, the system is broken.'), ('লুপ্ত গ্রন্থ খুঁজে পেয়ে গবেষক উল্লসিত।', 'finding the lost manuscript thrilled the researcher.', 'finding the hidden book, the researcher is excited.'), ('পুরোনো নগরীর লুপ্ত স্মৃতিগুলো ফিরে পাচ্ছে।', 'the lost memories of the old city are being reclaimed.', \"the old city's hidden memories are returned.\"), ('লুপ্ত ভাষার শব্দ জলচরিতায় বিদ্যমান।', 'words of the extinct language exist in the chronicles.', 'the speech exists in the speech.'), ('লুপ্ত কালের ধ্বনি কবিতায় ফুটে উঠেছে।', 'the echoes of a vanished era emerge in the poem.', 'the thickness has come up in the poem.'), ('আশার লুপ্ত রাশি আবার জাগিয়ে তোলা জরুরি।', 'it is crucial to revive the lost ray of hope.', 'to raise hope is urgent.'), ('পুঞ্জে ভরে ফুল রাখলেন তিনি ভাঁড়ে।', 'he placed the flowers bundled together in the vase.', 'he filled flowers in the pound.'), ('শস্যের পুঞ্জ মাঠে অস্তরাও।', 'sheaves of crops lay strewn across the field.', 'put the grain in the field.'), ('অক্ষরের পুঞ্জ মিলিয়ে কবিতা গঠিত।', 'a poem is formed by clustering letters together.', 'the poem is composed together.'), ('পুঞ্জ ফুলের গন্ধ ঘরে ছড়িয়ে পড়ল।', 'the scent of the bouquet spread throughout the house.', 'the smell of flowers spread into the room.'), ('আশার পুঞ্জে জীবনের প্রত্যাশা জেগে ওঠে।', \"in a cluster of hopes, life's expectations awaken.\", 'expectations of life are awakening.'), ('রহস্য জানলে তার অন্তর কেবল তরল হয় না।', 'if one knows the secret, the heart does not remain still.', 'knowing the mystery, his heart is not just liquid.'), ('রহস্য প্রকাশ পেলে গ্রাম ভেঙে পড়ে।', 'when the secret is exposed, the village is shattered.', 'when the mystery revealed, the village collapses.'), ('কবিতায় রহস্যের ছোঁয়া পাঠককে ধরে রাখে।', 'a touch of mystery in poetry keeps the reader captivated.', 'the touch of mystery keeps the reader.'), ('রহস্যময় সন্ধ্যায় পাখিরা নিশ্চল।', 'in a mysterious evening the birds are quiet.', 'the birds disappeared in the mysterious evening.'), ('তাঁর অস্তিত্ব রহস্যের কনজিও।', 'his existence is wrapped in a shroud of mystery.', 'his existence is mysterious.'), ('বিভ্রান্তি কাটাতে তাকে পুনরায় প্রশিক্ষণ দিলো।', 'to cut through confusion, he was retrained.', 'he trained again to cut confusion.'), ('বিভ্রান্তি ও দ্বন্দ্ব সমাধানে আলোচনাই বড় হাতিয়ার।', 'dialogue is the major tool for resolving confusion and conflict.', 'discussion is a big tool to solve confusion and conflict.'), ('ভাল তথ্য না থাকলে বিভ্রান্তি বহুগুণ বাড়ে।', 'without good information, confusion multiplies.', 'without good information, confusion increases many times.'), ('কবিতায় বিভ্রান্তির রূপক আলোকিত হয়েছে।', 'in the poem, confusion is illuminated as a metaphor.', 'the form of confusion illuminated in the poem.'), ('মানসিক বিভ্রান্তি চিকিৎসার মাধ্যমে হ্রাস পেয়েছে।', 'mental confusion has lessened through treatment.', 'mental confusion decreased through treatment.'), ('তাঁর অভিব্যক্তি ছিল সংক্ষিপ্ত কিন্তু মর্মপূর্ণ।', 'his expression was brief but meaningful.', 'his expression was brief but narrow.'), ('অভিব্যক্তির মাধ্যমে শিল্পী তার মর্ম প্রকাশ করল।', 'through expression, the artist revealed his core meaning.', 'the artist expressed his speech.'), ('কথার অভিব্যক্তি কখনো কঠোর, কখনো কোমল।', 'the tone of speech is sometimes harsh, sometimes gentle.', 'the expression is hard, sometimes gentle.'), ('তার অভিব্যক্তিতে গোপন ব্যথা লুকিয়ে ছিল।', 'a hidden pain was concealed in his expression.', 'he hidden my pain in his expression.'), ('অভিব্যক্তি নিয়ন্ত্রণ শেখা অত্যন্ত জরুরি।', 'learning to control expression is very important.', 'learning to control expression is very important.'), ('তারা সন্ধ্যার কোরলে গান করতে করতেই মিলল।', 'they met while singing in the evening chorus.', 'they got to sing at night.'), ('কোরল গায়করা একসাথে প্রশিক্ষণ করে।', 'chorus singers train together.', 'coral singers train together.'), ('কোরলের সুরে ভোরের আভা ফুটে উঠল।', \"the chorus's tune brought out the dawn's radiance.\", 'the sun of the coral raised.'), ('শিল্পীর কণ্ঠ কোরলে মিশে গিয়েছে।', \"the artist's voice blended into the chorus.\", \"the artist's voice measured.\"), ('কোরলে অংশ নেওয়া শিশুরা আত্মবিশ্বাস পেল।', 'children participating in the chorus gained confidence.', 'children who participated in the coral gained confidence.'), ('বাহিরে এলো দিনে বৃষ্টি বৃষ্টি ধারা।', 'outside, the rains arrived in a steady stream.', 'rain came out on the day.'), ('বৈঠকে ধারাবাহিক ধারা বজায় রাখা জরুরি।', 'maintaining a continuous flow in the meeting is important.', 'it is urgent to keep continuous to meet.'), ('ধারার ধাক্কায় বাঁধ ক্ষয় হয়ে গেছে।', \"the barrage has been eroded by the stream's force.\", 'the band has been broken by pushing.'), ('নতুন ধারায় কবিতা লিখছে সে।', 'he writes poetry in a new style.', 'he is writing a new poem.'), ('ধারার পরিবর্তনে নদীর গতিপথ বদলে গেছে।', \"with the change of current, the river's course has altered.\", \"the river's direction has changed.\"), ('কূচ দিয়ে কৃষক শস্য কাটছে।', 'the farmer is cutting the crop with a sickle.', 'the farmer is cutting grains.'), ('কূচে গাছ শাখা কেটে ফেলো।', 'cut the tree branch with the sickle.', 'cut the tree river.'), ('শৈলীতে কূচের আঘাত স্পষ্ট।', 'the mark of the sickle is clear in the carving.', 'the strike in the style is clear.'), ('কূচ-রীতির উৎস পুরাতন কৃষিকর্ম।', 'the sickle-ritual originates from ancient farming.', 'the source is ancient agriculture.'), ('তার কূচে আত্মবিশ্বাস আছে।', 'he has confidence in his skill with the sickle.', 'he has confidence in his.'), ('বাউলের শিখা রাত জুড়ে জ্বললো।', \"the minstrel's flame burned through the night.\", 'the bow has burned throughout the night.'), ('তিনি শিখায় হাতড়ে জ্বলন্ত মোম রাখলেন।', 'he placed the burning wax near the flame.', 'he placed a burning candle in his hand.'), ('শিখা নামের ব্যক্তি স্কুলে যোগ দিয়েছেন।', 'a person named shikha has joined the school.', 'a person named shaka joined school.'), ('জীবনে শিখা নিয়ে অগ্রসর হও।', 'progress in life with a flame of determination.', 'advance with learning in life.'), ('শিখার আলো গহীন কক্ষে পৌঁছল।', 'the light of the flame reached the deep room.', 'the light reached the hall.'), ('সময় এসেছে বদলের।', 'the time has come for change.', 'time has come for change.'), ('সময়ে কাজ শেষ করো।', 'finish the work on time.', 'finish work in time.'), ('সে সময়ের প্রতি শ্রদ্ধাশীল।', 'he is respectful of time.', 'he is respectful for time.'), ('সময়ের প্রভাব মানুষকে বদলে দেয়।', 'the passage of time changes people.', 'time changes people.'), ('সময়ের মূল্য বোঝা জরুরি।', 'understanding the value of time is essential.', 'understanding the value of time is urgent.'), ('মেরুর পথে উচ্চ তাপমাত্রা দেখা যায়।', 'high temperatures are seen along the polar route.', 'high temperatures are seen on the river.'), ('রশ্মি মেরুতে পড়লে আলোকিত হয়।', 'a ray falling on the pole lights it up.', 'when the light falls into the roof, it is illuminated.'), ('মেরু নামক মেঘলা গ্রামে প্রচলিত কাহিনী আছে।', 'a legend about a place called meru is common in the cloudy village.', 'there is a traditional story in the village named meru.'), ('তার মন মরু-সমান শূন্যতায় ঢেকে গেছে।', 'his mind is covered like a polar void.', 'his mind is covered in the desert.'), ('মেরুতে জীবন কল্পনায় সীমিত।', 'life at the pole is imagined to be limited.', 'life is limited to imagination.'), ('ত্রিশূল নিয়ে দেবতা মূর্তিটি স্থাপন করা হয়েছে।', 'a statue of the deity holding a trident has been installed.', 'the statue of the god was installed with the triangle.'), ('শিবের ত্রিশূল দেবতার প্রতীক।', 'the trident is a symbol of lord shiva.', 'the triangle is the symbol of the god.'), ('ত্রিশূলে লেখা প্রতীক দেখে ভক্তরা শ্রদ্ধা জানায়।', 'devotees pay homage seeing the symbol inscribed on the trident.', 'seeing the symbol written in the triangle, fans give respect.'), ('তীরবন্দী ত্রিশূল প্রাচীন যুদ্ধায় তীক্ষ্ণ অস্ত্র ছিল।', 'the pointed trident was a sharp weapon in ancient warfare.', 'the armed triangle had sharp weapons in ancient battle.'), ('ত্রিশূলের ছায়া মঞ্চে নাটকীয় ভাব এনে দেয়।', \"the trident's shadow gives a dramatic effect on stage.\", 'the shade of the triangle brings a dramatic feeling to the stage.'), ('কল্প কাহিনীর জগত সাজিয়ে রেখেছে কবি।', 'the poet has fashioned a world of imagination.', 'the poet has decorated the world of fiction.'), ('কল্পনা কল্প দিয়ে গল্প তৈরি করো।', 'create stories using imaginative fantasy.', 'create stories with imagination.'), ('কল্প নামের ছেলেটি আজ পুরস্কার পেয়েছে।', 'a boy named kalpa received an award today.', 'the boy named fantasy received prize today.'), ('কল্পতান্ত্রিক ভঙ্গিতে সে সব দেখায়।', 'he shows everything in a fantastical manner.', 'he shows everything in the imagination.'), ('কল্প-উপাখ্যান পাঠককে মুগ্ধ করে।', 'fanciful tales captivate the reader.', 'the imagination fascinates the reader.'), ('নিবিড় বনে পাখির কোলাহল নেই।', 'there is no bird noise in the dense forest.', 'there are no birds in the narrow forest.'), ('নিবিড় মননে তিনি ধ্যান করছেন।', 'he is meditating with focused concentration.', 'he is thinking with intense mind.'), ('নিবিড় বন্ধুত্বে আস্থা গড়ে উঠে।', 'trust builds in deep friendship.', 'deep friendship builds confidence.'), ('নিবিড় গবেষণায় অনেক অজানা বিষয় মিলল।', 'deep research uncovered many unknown aspects.', 'many unknown matters met in intensive research.'), ('নিবিড় ছায়ায় গাছরা বিশ্রাম নিচ্ছে।', 'trees are resting in the dense shade.', 'trees are resting in sharp shade.'), ('সোপান নেমে নদীর কাছে পৌঁছালো।', 'the steps led down to the river.', 'the spoon came down to the river.'), ('জীবনের সোপানে এক ধাপ এগিয়ে চলি।', 'i move one step forward on the ladder of life.', 'i am moving forward a step in life.'), ('মন্দিরের সোপান মার্বেল দিয়ে তৈরী।', 'the temple steps are made of marble.', 'the temple is made with marble.'), ('সুপারিশের সোপানে তাকে পদোন্নতি দিলেন।', 'he was promoted after the recommendation process.', 'he promoted him in the recommendation.'), ('কবিতার সোপানে স্মৃতির আলোক ফোটে।', \"memory's light blossoms on the poem's steps.\", 'the poem brings light to memories.'), ('অভিসার পাঠে গ্রামভিত্তিক শাসন বদলে যায়।', 'through the ritual procession, village governance changed.', 'the village-based rule changes by reading.'), ('অভিসারে রাজস্ব প্রদান করা হয়।', 'tribute is paid during the procession.', 'the revenue is delivered.'), ('কবিতায় অভিসারকে শক্তির প্রতীক হিসেবে দেখানো হয়েছে।', 'the poem depicts the procession as a symbol of power.', 'in the poem the expression is shown as a symbol of strength.'), ('উৎসবের অভিসারে সকলে অংশ নিয়েছিল।', 'everyone participated in the festival procession.', 'everyone participated in the festival.'), ('অভিসার নামের গ্রামের ইতিহাস জটিল।', 'the history of the village named abhisar is complex.', \"the village's history is complex.\"), ('তন্ত্রের নিয়ম মেনে কাজ করলেই সফলতা মিলবে।', \"success comes if you work according to the system's rules.\", 'if you follow the rules, success will be achieved.'), ('তন্ত্র নামে সংগঠনটি শহরে প্রতিষ্ঠিত হয়েছে।', 'an organization named tantra has been established in the city.', 'the organization was established in the city.'), ('চিকিৎসায় তন্ত্রগত কৌশল প্রয়োগ করা হলো।', 'systematic techniques were applied in the treatment.', 'therapeutic methods were applied.'), ('তন্ত্রের ত্রুটিতে পদ্ধতি পুনরায় যাচাই করা হলো।', 'due to a system error, the procedure was re-evaluated.', 'the system was revised due to errors.'), ('নতুন তন্ত্র গ্রহণে আন্দোলন শুরু হয়েছে।', 'a movement has begun to adopt the new system.', 'the movement began with adoption of a new system.'), ('লুপ্ত নগরের বৃত্তান্ত খুঁজে পাওয়া গেল।', 'the chronicles of the lost city were found.', 'the circle of the hidden city was found.'), ('লুপ্ত ভাষার ডিকশনারি পুনরুদ্ধার চলছে।', 'a dictionary of the extinct language is being restored.', 'the dictionary is being recovered.'), ('লুপ্ত গ্রন্থের প্রতিলিপি সংগ্রহে রাখা হলো।', 'a copy of the lost manuscript was placed in the collection.', 'the copy of the hidden book was collected.'), ('কবিতায় লুপ্ত স্মৃতির কথা স্পন্দিত হচ্ছে।', 'the poem echoes the memories of the vanished past.', 'the poetry is spending memories.'), ('লুপ্ত সুর সন্ধানে নবান্নে কাজ চলছে।', 'work is underway at nabanna to find the lost melody.', 'work is ongoing in search for hidden sound.'), ('পুঞ্জ করে ফুল বেঁধে তিনি পথ গেলেন।', 'he tied the flowers into a bundle and went on his way.', 'he walked with flowers.'), ('শস্যের পুঞ্জ মাঠে ছড়িয়ে আছে।', 'sheaves of grain are spread across the field.', 'the crop is spread in the field.'), ('কবিতার পুঞ্জে খুঁজে পাওয়া যায় জীবনের রঙ্গিন দিক।', \"in the poem's clusters one finds the colorful facets of life.\", 'in the poem can be found the colorful direction of life.'), ('পুঞ্জ গাছের তলায় ছায়া করলো।', 'the cluster cast a shadow under the tree.', 'the punch shade under the tree.'), ('পুঞ্জ তৈরির পর মঙ্গলকামনা জানানো হলো।', 'after bundling, well-wishes were declared.', 'after making the punch, good wishes were announced.'), ('রহস্যময় ঘরে আলো নিভে গেল।', 'the light went out in the mysterious room.', 'the light has disappeared in the mysterious room.'), ('রহস্য জানাতে বার্তাটি গোপন রাখা হয়েছে।', 'the message to reveal the secret has been kept confidential.', 'the message was kept secret to reveal mystery.'), ('কবিতায় রহস্যপ্রবণতা পাঠককে আকৃষ্ট করে।', 'a sense of mystery in poetry attracts the reader.', 'mysterious tendencies attract the reader.'), ('রহস্যময় সন্ধ্যার শব্দ কানে খুশি আনে না।', 'the sounds of a mysterious evening do not bring comfort to the ear.', \"mysterious evening sounds don't bring happiness to your ear.\"), ('তার উপস্থিতি রহস্যে মোড়ানো।', 'his presence is wrapped in mystery.', 'his presence turned in mystery.'), ('বিভ্রান্তি কাটাতে ঠিক পথে ফিরে আসা দরকার।', 'to remove confusion, it is necessary to return to the right path.', 'need to come back to the right way to remove confusion.'), ('প্রচারের বিভ্রান্তি নির্মূল করতে সংবাদ সম্মেলন ডাকা হলো।', 'a press conference was called to eliminate campaign confusion.', 'a press conference was called to eliminate confusion.'), ('বিভ্রান্তি ও সন্দেহ মিলিয়ে সিদ্ধান্ত নাও।', 'make decisions after resolving confusion and doubt.', 'make decisions with confusion and doubt.'), ('বিভ্রান্তির সময় সহায়তা চাওয়া উচিত।', 'one should seek help during confusion.', 'need help during confusion.'), ('তার বক্তব্য বিভ্রান্তি বাড়ালো।', 'his statement increased the confusion.', 'his speech increased confusion.'), ('অভিব্যক্তির মাধ্যমে তিনি প্রকৃত ব্যথা প্রকাশ করলেন।', 'through expression he revealed his real pain.', 'he expressed true pain through his expression.'), ('অভিব্যক্তির ছন্দ কাব্যকে প্রাণ দেয়।', 'the rhythm of expression gives life to poetry.', 'the speech gives life to poetry.'), ('শিল্পীর অভিব্যক্তি দর্শককে ছুঁয়েছে।', \"the artist's expression touched the audience.\", \"the artist's expression touched the audience.\"), ('উপস্থাপনার অভিব্যক্তি স্পষ্ট হওয়া দরকার।', 'the expression in the presentation needs to be clear.', 'the expression needs to be clear.'), ('অভিব্যক্তির নিয়ন্ত্রণ শেখানো হচ্ছে কর্মশালায়।', 'control of expression is being taught in the workshop.', 'expression is taught in the workshop.'), ('দল কোরলে গান করে উৎসব উদযাপন করলো।', 'the group sang in chorus to celebrate the festival.', 'the party celebrated the festival by singing.'), ('কোরলের সুর গ্রামের মানুষকে মুগ্ধ করলো।', \"the chorus's melody enchanted the villagers.\", 'the coral sound fascinated people of the village.'), ('স্কুল কোরলে সন্তানেরা অংশ নিলে আত্মবিশ্বাস বাড়ে।', 'children gain confidence when they join the school chorus.', 'when children participate in the school, they increase confidence.'), ('কোরলের লিডার প্রশিক্ষণে গিয়েছেন।', 'the chorus leader has gone for training.', 'the coral leader trained.'), ('কোরল গেয়েছে সন্ধ্যার শান্তি আনতে।', 'the chorus sang to bring the evening calm.', 'the coral went to bring peace at night.'), ('ধারা বদলায় সমুদ্রের লহরও পাল্টায়।', \"when the current changes, the sea's waves also alter.\", 'the sea is changing.'), ('ধারা বজায় রাখলে কাজ সুন্দর হয়।', 'if you maintain continuity, the work is done well.', 'keeping the line becomes beautiful.'), ('আইনের ধারা অনুযায়ী সিদ্ধান্ত নেওয়া হলো।', 'a decision was made according to the legal clause.', 'decision made according to the law.'), ('সাহিত্যের নতুন ধারা যুব সমাজে জনপ্রিয়।', 'a new literary trend is popular among the youth.', 'the new style of literature is popular in youth society.'), ('ধারা পরিবর্তিত হলে প্রকৃতির ভারসাম্য নষ্ট হয়।', \"when the flow changes, nature's balance is disrupted.\", 'when the line changes, the balance of nature is damaged.'), ('তরী-চালক জানে নদীর বয়্যান ভালো।', \"the boatman knows the river's currents well.\", 'the river is well known.'), ('তার তরীতে সবাই আরাম করে যাতায়াত করে।', 'everyone travels comfortably in his boat.', 'everyone travels comfortably.'), ('তরী নামক গ্রামটি তীরের কাছে।', 'a village named tori is near the shore.', 'the village named tiri is near the shore.'), ('তরী-বাহিত স্মৃতি হৃদয়ে স্থায়ী।', 'boat-borne memories remain permanent in the heart.', 'tightening memories are permanent in heart.'), ('তরীর কাঠ মেরামত করা হয়েছে।', \"the boat's wood has been repaired.\", 'the leaf was repaired.'), ('অন্বেষণে নতুন সন্ধান মিলল।', 'a new discovery was found in the exploration.', 'new to find.'), ('গবেষণার অন্বেষণ শিক্ষকদের দৃষ্টি আকর্ষণ করলো।', \"the inquiry of the research attracted the teachers' attention.\", \"research attracted teachers' attention.\"), ('অন্বেষণের পথে তিনি একা চললেন।', 'he walked alone on the path of quest.', 'he walked alone on the way to explore.'), ('অন্বেষণে ত্রুটি থাকলে ফলও ত্রুটিপূর্ণ।', 'if there are errors in the investigation, the results are flawed.', 'if there is a mistake in the search, the result is wrong.'), ('তার অন্বেষণ মানবিক উন্মোচনে সহায়।', 'his exploration helps in unveiling human aspects.', 'his exploration helps in human revelation.'), ('প্রহরী রাতে পাহারা দিল।', 'the guard stood watch at night.', 'the guard guarded at night.'), ('প্রহরীর সতর্ক দৃষ্টি নিরাপত্তা বাড়ায়।', \"the watchman's vigilant gaze increases security.\", \"the warrant's attention increases security.\"), ('প্রহরীর কাজ ক্লান্তিকর কিন্তু জরুরি।', \"the sentinel's work is tiring but essential.\", \"the guard's work is tired but urgent.\"), ('প্রহরী-প্রথা গ্রামে প্রাচীনভাবে বজায় আছে।', 'the sentinel tradition has been preserved in the village since ancient times.', 'the guardians preserved anciently in the village.'), ('প্রহরীর সতর্কতা দুর্ঘটনা রোধ করল।', \"the watchman's vigilance prevented an accident.\", 'the warrior prevented accident.'), ('আসুন আমরা বাতি জ্বালাই।', \"let's light the lamp.\", 'come to light the light.'), ('বাতি নিভে যাওয়ায় ঘর অন্ধকার।', 'with the lamp extinguished the room is dark.', 'the room is dark when the light is turned off.'), ('তার কথায় মানুষের মনে বাতির আলো জ্বলে।', \"his words kindle a lamp in people's hearts.\", 'in his words the light burns in his mind.'), ('বিদ্যুতের বাতি দ্রুত জ্বলে ওঠে।', 'the electric bulb lights up quickly.', 'the light burns quickly.'), ('বাতি-দোকানটি শহরের শেষ প্রান্তে।', 'the lamp shop is at the edge of town.', 'the lamp shop is on the end of the city.'), ('সমুদ্রতীরে নিরবভাবে ঢেউ আঘাত করে।', 'waves strike the seashore silently.', 'silent waves hit the sea.'), ('রেডিও থেকে ঢেউ ধারাবাহিকভাবে শোনা যায়।', 'waves are heard continuously from the radio.', 'the waves can be heard from the radio.'), ('তার গানের ঢেউ হৃদয়ে ধাক্কা দেয়।', 'the waves of his song strike the heart.', 'his wave pushes to heart.'), ('ঠাণ্ডা ঢেউ এসে গ্রাম জুড়ে ছড়ায়।', 'a cold wave comes and spreads across the village.', 'cold waves spread across the village.'), ('ঢেউয়ের গতিবেগ বাড়লে নৌকা সতর্ক হতে হয়।', 'when wave speed increases, boats must be cautious.', 'when the wave moves, the boat needs to be careful.'), ('তাঁরা পাল তোলা শুরু করল।', 'they began to raise the sail.', 'they started to draw.'), ('শিল্পীর পাল-চিত্র বইয়ের প্রচ্ছদে লাগানো ছিল।', \"the artist's sail-like painting was used on the book cover.\", \"the artist's paintings were placed on the edge of the book.\"), ('পালের অভ্যন্তরে হাওয়া প্রবাহ করে।', 'wind flows inside the sail.', 'the air flows inside the palm.'), ('সে পাল পরিবর্তন করে নতুন পথে যাত্রা শুরু করল।', 'he changed course and set out on a new journey.', 'he changed the tap and started his journey on a new way.'), ('পালের দাগে জাহাজের পরিচয় বোঝা যায়।', \"the sail's pattern tells the ship's identity.\", \"the ship's identity is understood.\"), ('তারা জ্ঞান অর্জনে নিরলস কাজ করে।', 'they work tirelessly to acquire knowledge.', 'they work hard to acquire knowledge.'), ('সে নিজের অন্তরের জ্ঞান অনুসরণ করে সিদ্ধান্ত নেয়।', 'he makes decisions following his inner wisdom.', \"he takes decisions following his heart's knowledge.\"), ('বিজ্ঞানের জ্ঞান দ্রুত পরিবর্তিত হয়।', 'scientific knowledge changes rapidly.', 'the knowledge changes quickly.'), ('অসংখ্য শিক্ষার্থী জ্ঞান লাভে উৎসাহী।', 'many students are eager to gain knowledge.', 'many students are enthusiastic to gain knowledge.'), ('জ্ঞানই প্রকৃত সম্পদ।', 'knowledge is the true wealth.', 'knowledge is genuine.'), ('তাঁর বুকে আঘাত লেগে কষ্ট হচ্ছে।', 'he feels pain where he was hit in the chest.', 'he is hurting his chest.'), ('বুক বড় হওয়ায় সে গর্ব বোধ করে।', 'he feels proud because of his big heart (bravery).', 'he feels proud of getting big.'), ('বুক থেকে একটি ছোট ব্যাগ ঝুলছে।', 'a small bag is hanging from his chest.', 'a small bag is hanging from the chest.'), ('বুকের মধ্যখানে হৃদয়ের টোকা স্পষ্ট।', 'the heart beat is clear in the center of the chest.', 'heart tick is clear in midst.'), ('সে বুক ভরে হাসতে শুরু করল।', 'he began to laugh heartily.', 'he started laughing full.'), ('শহরের পুরোনো নিশান এখন সংগ্রহশালায় রয়েছে।', 'the old city landmark is now in the museum.', 'the old destinations of the city are now in the collection.'), ('কবিতায় রক্তের নিশান এক স্মৃতিচিহ্ন।', 'in poetry the stain of blood is a mark of memory.', 'the mark of blood is a memorial.'), ('তিন নম্বর কক্ষে তার নিশান দেখা যায়।', 'his sign is seen in room number three.', 'his goal is seen in the room number three.'), ('নিশান পড়লে চিহ্নটি মুছে ফেলা কঠিন।', 'once the mark is made, it is hard to erase.', 'when the mark falls, it is difficult to remove the mark.'), ('পথে নিদর্শন ও নিশান নির্দেশ করে গন্তব্য।', 'signs and markers on the road indicate the destination.', 'indications and indications on the path.'), ('মাঠে সূত্র লিখে পরীক্ষার প্রস্তুতি নেয়া হলো।', 'they wrote formulas in the field to prepare for the exam.', 'the form was written in the field and prepared for the exam.'), ('সূত্র থেকে খবর নিয়ে রিপোর্ট লেখা হলো।', 'a report was written based on the source.', 'the report was written from the source.'), ('তিনি সূত্র ধরে সমস্যার সমাধান করলেন।', 'he solved the problem by following the formula.', 'he solved the problem with the form.'), ('সংস্কারে সূত্রবদ্ধ তরিকা প্রতিষ্ঠিত হয়েছে।', 'a canonical method has been established in the reform.', 'formed trick established.'), ('সংবাদে অবৈধ সূত্র উদ্ধৃত করা হয়েছে।', 'an illegal source has been cited in the news.', 'illegal sources were cited in the news.'), ('বেত দিয়ে মেঘ ঝাঁঝরা করে বৃষ্টি আনা হয়।', 'they beat the clouds with sticks to scatter and bring rain (folk image).', 'the cloud brings the rain.'), ('গাছের শাখা কেটে বেত বানানো হয়।', 'a stick is made by cutting a tree branch.', 'cutting the tree branch is made.'), ('বেত নামের সেই লোকটি বাজারে থাকেন।', 'a person named bet lives in the market.', 'the person named bet is living in the market.'), ('বেতের ছোঁয়ায় আরাম পেয়েছে বাছুর।', \"the calf felt comfort from the stick's touch (guiding).\", 'the kid touched it.'), ('কাঠের বেত মজবুত হওয়ায় কাজে লাগে।', 'because the wooden staff is strong it is useful for work.', 'trees need to be strong.'), ('সে মেঘে ঢেকে পড়া সূর্য দেখে বিমু on.', 'he looked at the sun hidden by clouds.', 'he sees the sun covered in the clouds.'), ('তার মুখে হাসি ঢেকে ছিল।', 'a smile was hidden on his face.', 'a smile was covered in his face.'), ('কাগজ ঢেকে রাখা হলে তথ্য সুরক্ষিত থাকে।', 'if papers are covered, information remains secure.', 'when the paper is covered, the information is protected.'), ('বর্ষায় শহর ঢেকে যায় ধুরু-পলাশে।', 'in monsoon the town is covered with grass and flowers.', 'the city is covered in the summer.'), ('তারা ঢেকে রেখে ধূলা ছাড়া দিল না।', 'they covered it and did not let dust in.', 'they did not cover the dust.'), ('পথিকের পায়ে চিহ্ন রেখে গেছে বালি।', \"the traveler's steps left marks in the sand.\", 'sand has been placed on the foot.'), ('তার কথায় সিরাজের চিহ্ন স্পষ্ট।', \"siraj's hallmark is clear in his words.\", 'his sign is clear.'), ('চিহ্ন দেখে সিদ্ধান্ত নেওয়া হয়েছে।', 'a decision has been made after seeing the sign.', 'decided to see the mark.'), ('দৈনন্দিন কাজগুলোতে ছোট চিহ্ন রাখতে হবে।', 'you must keep small marks in daily tasks.', 'small mark must be kept in daily work.'), ('শরীরের উপর চিহ্নগুলো তার অতীত বোঝায়।', 'the marks on his body tell of his past.', 'the signs on his body indicate his past.'), ('স্থানীয় ভাষায় রীতি অনেকাংশে ভিন্ন।', 'local customs differ in many ways.', 'the practice in local language is different in many parts.'), ('রীতি অনুযায়ী শোপর্দ করা হলো।', 'it was handed over according to tradition.', 'according to the practice.'), ('শিক্ষকের রীতি প্রজন্মে প্রজন্মে চলে।', \"the teacher's practice passes from generation to generation.\", \"the teacher's practice moves to generations.\"), ('রীতিমতো অনুষ্ঠানে সবাই অংশ নিল।', 'everyone participated in the ritual ceremony as per custom.', 'everyone participated in the event.'), ('আধুনিক রীতিতে বদলেছে জীবনের ছন্দ।', \"life's rhythm has changed with modern customs.\", 'the style of life has changed.'), ('সে বন্যার পরে ফিরিয়ে আনা গাছ লাগাল।', 'he replanted the tree after the flood.', 'he planted the tree after the flood.'), ('অফিসে চিঠি ফিরিয়ে দেওয়া হলো।', 'the letter was returned to the office.', 'the letter was returned to the office.'), ('তুমি টাকা ফিরিয়ে দিলে সমস্যাটা মিটে যাবে।', 'if you return the money, the issue will be resolved.', 'if you return the problem.'), ('ছবিটি পূর্বাবস্থায় ফিরিয়ে দিলাম।', 'i restored the picture to its former state.', 'i returned this to the previous position.'), ('তারা তাকে বাড়িতে ফিরিয়ে দিতে চাচ্ছে।', 'they want to send him back home.', 'they want to bring him home.'), ('ভালো গুণ রয়েছে তার চরিত্রে।', 'he has good qualities in his character.', 'he has good character.'), ('গুণের রীতিতে সে অনন্য।', 'he is unique in his virtues.', 'he is unique in quality.'), ('তার গুন বাজে বলে সবাই মেনে না।', 'not everyone accepts his praise-singing (flattery).', 'everyone does not accept his to say badly.'), ('সংগীতে তার গুণ শিবিরে পরিচিত।', 'his skill in music is known in the camp.', 'his quality is known in the camp.'), ('অভ্যাসে গুণ আনতে প্রতিদিন অনুশীলন করো।', 'practice daily to bring virtues into habit.', 'practice daily to bring quality to habit.'), ('ঝঞ্ঝার আগে আকাশে কালো মেঘ কালে।', 'black clouds gather in the sky before the storm.', 'black clouds are in the sky before the shining.'), ('ঝঞ্ঝায় জাহাজটি উপকূলে আনতে ব্যস্ততা ছিল।', 'there was a rush to bring the ship to shore during the storm.', 'the boat was busy bringing it to the coast.'), ('দুই বন্ধু মধ্যে ঝঞ্ঝা মিটে গেছে—সব ঠিক আছে।', 'the quarrel between two friends has been resolved—everything is fine.', 'twice between two friends — everything is fine.'), ('জীবনে ঝঞ্ঝা এলে ধৈর্য্য ধরো।', 'when storms come in life, keep patience.', 'take patience when it comes to life.'), ('ঝঞ্ঝার পরে সকাল আরো পরিষ্কার হয়।', 'after the storm the morning becomes clearer.', 'after the swing, the morning becomes clearer.'), ('তাঁর কণ্ঠে সুরের অনাবিল প্রশান্তি আছে।', 'there is an inexhaustible calm of melody in his voice.', 'there is unclean calmness in his voice.'), ('প্রশান্তি পাওয়ার জন্য ধ্যান কার্যকর।', 'meditation is effective to gain peace.', 'this is effective for getting peace.'), ('নদীর নীরবতার মাঝে প্রশান্তি আবির্ভূত হয়।', \"peace emerges in the river's silence.\", \"peace appears in the river's silence.\"), ('তার চোখে প্রশান্তির ছাপ দেখা যায়।', 'a trace of peace is visible in his eyes.', 'there is a mark of peace in his eyes.'), ('প্রশান্তি রক্ষা করাই প্রকৃত শেখা।', 'maintaining peace is the true lesson.', 'protecting peace is true learning.'), ('চকের মধ্যে চকচকে বাতি জ্বলছে।', 'a shiny lamp is burning inside the wheel (metaphor).', 'light is burning in the chick.'), ('চক নামের পল্লীটি নদীর পারে।', 'a hamlet named chak is on the riverbank.', 'the pulse named chok can be river.'), ('চকের রেখায় ফুল লাগানো হয়েছে।', \"flowers are arranged along the circle's rim.\", 'flowers have been placed in the chick.'), ('চকের গিটার শিখতে সময় লাগে।', 'it takes time to master the wheel-like guitar (chord) technique.', 'it takes time to learn chick guitar.'), ('না-চলে যাওয়া চকের প্রতিস্থাপন জরুরী।', 'replacing the broken wheel is urgent.', 'no-going chick replacement is urgent.'), ('দরজার তালা শক্ত ছিল।', \"the door's lock was strong.\", 'the door was hard.'), ('তারা তালা বাজিয়ে রেখে গেছেন।', 'they left with the lock fastened.', 'they played.'), ('প্রবেশপথে তালা ভাঙা দেখে পুলিশ ফোন করা হলো।', 'seeing the broken lock at the entrance, the police were called.', 'seeing the tap broken in the entrance, the police called.'), ('সঙ্গীতের তালা বজায় রাখতে সুর দরকার।', 'to keep the rhythm in music, melody is needed.', 'music needs to be kept.'), ('তার তালায় বিশ্বাস হারিয়ে গেছে।', 'trust in his lock (metaphor: security) is lost.', 'his trust has been lost.'), ('রাতের আকাশে তারা ঝকঝক করছে।', 'stars are twinkling in the night sky.', 'they are scattering in the night sky.'), ('তারা নামের বন্ধু আজ অধ্যায় পড়ছে।', 'a friend named tara is studying today.', 'they are reading chapter named today.'), ('কর্মক্ষেত্রে তারা একজন উজ্জ্বল প্রতিনিধি।', 'at work she is a bright representative.', 'they are a brilliant representative in the workplace.'), ('তারা সাজানো সজ্জায় অতিথি আজ আসে।', 'guests arrive today in the arranged decoration.', 'they have decorated guests today.'), ('তারার আলো নদীর জলে প্রতিফলিত হলো।', \"the starlight reflected in the river's water.\", \"the star's light reflected in the river.\"), ('খাতা ভরে লেখা তার জীবনের রেকর্ড।', 'a notebook full of writing records his life.', 'a record of his life is filled with the ledger.'), ('খাতায় নাম লিখে জমা দিন।', 'write your name in the register and submit it.', 'write the name in the ledger.'), ('শিশুদের খাতা দেখা উন্নতির লক্ষণ।', \"checking children's notebooks is a sign of improvement.\", \"seeing children's ledger is a sign of improvement.\"), ('খাতায় আঁকা ছবি স্টোরিতে আপলোড করো।', 'upload the picture drawn in the notebook to the story.', 'upload pictures in the ledger.'), ('পুরোনো খাতা সংরক্ষণ করা জরুরি।', 'it is important to preserve old ledgers.', 'preserving old ledger is urgent.'), ('কলার কাঠ গলায় ঝুলিয়ে রাখা হল।', 'a banana stalk was hung around the neck (ritual).', 'the tap was hanged on the neck.'), ('তুমি কি কলা খেয়েছো আজ?', 'have you eaten a banana today?', 'did you eat this today?'), ('কলার মতন নরম হৃদয় তার আছে।', 'he has a heart soft like a banana.', 'he has a gentle heart.'), ('কলার শিল্পকলা প্রদর্শনীতে উপস্থিত।', 'banana-art is present in the exhibition (folk art).', 'the art is present at the exhibition.'), ('কলার চাষ শুরু হয়েছে পাহাড়ি এলাকা থেকে।', 'banana cultivation has begun from the hilly region.', 'the cultivation began from the mountain area.'), ('নদীর পাড়ে ছোট একটি ঘাট আছে।', 'there is a small landing on the riverbank.', 'there is a small shore on the river.'), ('ঘাটে ভিড় বেশি হয় শনি-রবিবার।', 'the ghat is crowded on saturdays and sundays.', 'there is more crowd on Saturday.'), ('শীতের সকালে ঘাটে মেনেছে শীতের কুয়াশা।', 'in the winter morning mist settled on the ghat.', 'in the winter storms.'), ('ঘাট নামের গ্রামে উৎসব পালিত হয়।', 'a festival is celebrated in a village named ghat.', 'a festival is celebrated in the village named ghat.'), ('ঘাট দিয়ে পয়ঃনিষ্কাশন ব্যবস্থা ঠিক করা জরুরি।', 'it is necessary to fix sanitation through the ghat.', 'it is urgent to fix the to catch.'), ('মিষ্টি খেতে আমি কেক বেশি পছন্দ করি।', 'i prefer cake more when it comes to sweets.', 'i like this more than sweet.'), ('পার্টিতে কেক কাটা হবে সন্ধ্যায়।', 'the cake will be cut at the party in the evening.', 'the cake will be cut at the evening.'), ('রান্না করাটা কেকের মতো সহজ মনে করো না।', \"don't think cooking is as easy as cake.\", \"don't think cooking like cake.\"), ('শিল্পীর কেক ডিজাইন দেখায় সৃজনশীলতা।', \"the artist's cake design shows creativity.\", \"the artist's cake design shows creativity.\"), ('কেক নামের বগলি পাখিটি বাগানে দেখা যায়।', 'a bird named cake is seen in the garden (fictional).', 'the bird named cake is seen in the garden.'), ('প্রাচীরের উপর ছোপ ছোপ ছোপ আঁকা ছিল।', 'there were speckled spots painted on the wall.', 'the tap was painted on the wall.'), ('ছোপ লাগলে কাপড়টি মলিন হয়ে যায়।', 'if it gets a spot the cloth becomes stained.', 'if the cloth becomes soft.'), ('ছোপের নকশা ট্রেন্ডি লুক দেয়।', 'a speckled pattern gives a trendy look.', 'the design hides trends.'), ('সেকেলে খাতা ছোপে ভরা।', 'the old ledger is full of blotches.', 'the seat is filled.'), ('ছোপ মুছলে ধোয়া দরকার।', 'you need to wash to remove the spot.', 'washing is needed.'), ('দারিদ্র্যকে পেছনে ফেলতেই গ্রামে উদ্যোগ চালানো হলো।', 'an initiative was launched in the village to overcome poverty.', 'to remove poverty initiatives in the village.'), ('দারিদ্র্য দূরীকরণে শিক্ষা অপরিহার্য।', 'education is essential to eradicate poverty.', 'education is essential to remove poverty.'), ('তাঁর গল্পে দারিদ্র্যের চিত্র স্পষ্ট।', 'the picture of poverty is clear in his story.', 'the picture of poverty is clear in his story.'), ('দারিদ্র্য দেখা গেল শহরের প্রান্তে।', 'poverty was seen at the outskirts of the city.', 'poorness occurred at the city edge.'), ('দারিদ্র্য হ্রাসে সমাজভিত্তিক উদ্যোগ দরকার।', 'community-based initiatives are needed to reduce poverty.', 'social initiatives are needed to reduce poverty.'), ('শহরের চেয়ার হঠাৎ ভেঙে গেল।', \"the city's chair suddenly broke.\", 'the city chair collapsed suddenly.'), ('চেয়ারম্যান সভায় বসে বক্তব্য দিলেন।', 'the chairman sat and gave a speech at the meeting.', 'the chairman spoke sitting at the meeting.'), ('চেয়ারে বসে কাজ করা আরামদায়ক।', 'working while sitting on a chair is comfortable.', 'sitting on the chair is comfortable.'), ('চেয়ার নামের রেস্তোরাঁ শহরে জনপ্রিয়।', 'a restaurant named chair is popular in the city.', 'the restaurant named chair is popular in the city.'), ('ভারতে চেয়ার প্রতিষ্ঠার ইতিহাস দীর্ঘ।', 'the history of chair establishment in india is long.', 'the history of establishing a chair in India is long.'), ('রঙিন বাতাসে পাখিরা উপড়ে চলছে।', 'birds are flying high in the colorful air.', 'birds are floating in the colorful wind.'), ('তুমি ঠাণ্ডা বাতাসে কাওল করো না।', \"don't catch a cold in the chilly air.\", \"don't cover it with cold air.\"), ('বাতাসে মিষ্টি সুবাস ভেসে আসে।', 'a sweet fragrance drifts in the air.', 'sweet smells are coming in the air.'), ('বাতাসের গতিতে পরীক্ষা স্থগিত করা হয়েছে।', 'the exam was postponed due to wind speed.', 'the test was suspended due to wind speed.'), ('বাতাসে শান্তি এনে দেয় প্রকৃতির শ্বাস।', \"nature's breath brings peace in the air.\", 'bring peace to the air.'), ('তাঁর গলায় সুরেলা সুর আছে।', 'there is a melodious tune in his voice.', 'he has a sound on his throat.'), ('সুর মেনে গাইলে গান ভালো করা যায়।', 'if you sing in tune, the song can be rendered well.', 'with sound can be done well.'), ('সুর নামের মেয়ে ক্লাসে সবার জনপ্রিয়।', 'a girl named sur is popular in the class.', 'a girl named sun is popular in class.'), ('তার সুরের জাদু দর্শককে মোহিত করে।', 'the magic of his tune enchants the audience.', 'his sound fascinates the audience.'), ('সুরের ছন্দে নাচের লয় ঠিক হয়।', \"the dance's rhythm aligns with the tune's beat.\", 'the sound of dance is fine.'), ('বাগানে রা্শি গাছ লাগানো আছে।', 'various trees are planted in the garden.', 'there are trees planted in the garden.'), ('রাশি অনুযায়ী তার ভাগ্য আসে বলে মনে করে।', 'he believes his fate comes according to his zodiac.', 'according to the rush, his fate comes.'), ('তার রাশি মাধুর্যপূর্ণ প্রকৃতি দেখায়।', 'his sign shows a pleasant nature.', 'his rush shows beautiful nature.'), ('রাশি নামের মেয়েটি আজ নবগঠিত দলীয়।', 'a girl named rashi joined the newly formed team today.', 'the girl named rashy is newly formed today.'), ('পায়ের নিচে রাশির রেখা দেখা গেল।', 'lines of the zodiac were seen beneath the feet (metaphor).', 'i saw a line under my feet.'), ('বারান্দায় বসে সূর্যাস্ত দেখা যায়।', 'you can see the sunset sitting on the balcony.', 'you can see sunrise sitting in the bar.'), ('বারান্দায় ফুলের টব রাখা হয়েছে।', 'flower pots are kept on the balcony.', 'flowers are placed in the bar.'), ('বারান্দা নামের স্টলটি মেলায় রয়েছে।', 'a stall named baranda is present at the fair.', 'the stole named baranda is on the fair.'), ('রাত্রে বারান্দায় চাঁদের আলো পড়লে সুন্দর লাগে।', 'it looks beautiful when moonlight falls on the balcony at night.', 'when the moon falls in the bar, it feels beautiful.'), ('তার বারান্দায় শিশুরা খেলছে।', 'children are playing on his balcony.', 'children are playing in his bar.'), ('টেবিলটা পুরোনো কাঠ দিয়ে তৈরি।', 'the table is made of old wood.', 'the table is made of old wood.'), ('টেবিলে নথিগুলো রেখে দাও।', 'leave the documents on the table.', 'keep the documents on the table.'), ('টেবিল নামের রেস্টুরেন্টে খাবার ভালো।', 'food is good at the restaurant named table.', 'food is good in the restaurant named table.'), ('টেবিলের নিচে বই পড়ে গেছে।', 'a book has fallen under the table.', 'a book was read under the table.'), ('টেবিল সাজাতে ফুল লাগালে সুন্দর দেখায়।', 'if you decorate the table with flowers it looks beautiful.', 'when placing flowers to decorate the table, it looks beautiful.'), ('বৃক্ষের ছায়ায় শিশুরা আরাম করে।', \"children rest under the tree's shade.\", 'children rest in the shade of the tree.'), ('ছায়ায় বসে জীবনের কথা মনে পড়ে।', \"sitting in the shade one remembers life's matters.\", 'sitting in the shade remembering life.'), ('ছায়া নামের ছেলেটি আজ অনুপস্থিত।', 'a boy named chhaya is absent today.', 'the boy named shadow is absent today.'), ('সূর্য ঢলে ছায়া কমে যায়।', 'as the sun sets the shadows diminish.', 'when the sun falls the shade.'), ('ছায়ার ওপর আঁকা পিনে ছবিটি সুসজ্জিত।', 'the picture pinned on the shade is beautifully arranged.', 'painted on the shade decorated the picture.'), ('স্কুলে নতুন পাঠ্যবই বিতরণ করা হলো।', 'new textbooks were distributed at school.', 'a new textbook was distributed at the school.'), ('পাঠ্যক্রমে পরিবর্তন আনা প্রয়োজন।', 'there is a need to change the curriculum.', 'need to bring changes to the texture.'), ('পাঠ্য বিষয়ে শিক্ষক প্রশিক্ষণ নেবেন।', 'teachers will receive training in subject matter.', 'teachers will take training on lessons.'), ('কবিতাটি পাঠ্য হিসেবে গ্রহণযোগ্য।', 'the poem is acceptable as a text.', 'the poem is acceptable.'), ('পাঠ্য উপকরণ অনলাইনে আপলোড করা হয়েছে।', 'educational materials have been uploaded online.', 'text material uploaded online.'), ('তারা সন্ধ্যার ভোটে সবাই একত্রে হাসল।', \"they all laughed together at the evening's joke.\", 'they all laughed together at night.'), ('সন্ধ্যা নামের মেয়েটি ক্লাসে এসেছে।', 'a girl named sandhya has come to class.', 'the girl named evening came to class.'), ('সন্ধ্যার আলো পড়ে ঘর আরামদায়ক হয়ে ওঠে।', 'the evening light makes the room cozy.', 'after night light becomes comfortable.'), ('সন্ধ্যায় বাজারে ভিড় বেড়ে যায়।', 'the market gets crowded in the evening.', 'the crowd increases in the evening.'), ('সন্ধ্যার গান শোনার সময় সবাই থমকে যায়।', 'everyone pauses when the evening song is heard.', 'everyone stops while listening to the evening.'), ('গোল্ড ব্যাগে মোদ্দা টাকা রাখা আছে।', 'a large sum of money is kept in the gold bag.', 'money is kept in the gold bag.'), ('মোদ্দা কাজ সবসময় কঠিন নয়।', 'big tasks are not always hard.', 'this is not always difficult.'), ('মোদ্দা কথা বলার সময় সতর্ক হোন।', 'be careful when speaking big words.', 'be careful when speaking.'), ('তার ব্যাগে মোদ্দা বই পড়ে আছে।', 'there are many books in his bag (bulk).', 'she has a book in her bag.'), ('চুক্তিতে মোদ্দা অংশে স্বাক্ষর করা হলো।', 'the main part of the contract was signed.', 'the agreement was signed partly.'), ('পথের ধারে বটগাছের ছায়া পড়েছে।', \"the banyan tree's shade falls by the roadside.\", 'the shade has fallen on the road.'), ('বটগাছের পাদদেশে পুরোনো চিহ্ন আছে।', 'there are old marks at the base of the banyan tree.', 'there are old signs on the bottom of the tree.'), ('বট নামের গ্রামে উৎসব শুরু হয়েছে।', 'a festival has begun in a village named bat.', 'the festival started in the village named but.'), ('কবিতায় বটগাছকে সাথী হিসেবে বর্ণনা করা হয়েছে।', 'in the poem the banyan is described as a companion.', 'the poem described as a companion.'), ('বটের গোড়ায় শিশুরা খেলা করে।', 'children play at the base of the banyan tree.', 'children play at the start.'), ('তাঁর গৃহস্থালি দক্ষতা সহজেই প্রশংসিত।', 'his household skills are easily appreciated.', 'his home skill is appreciated easily.'), ('দক্ষতার অভাবে কাজ ধীর হয়।', 'work slows down due to lack of skill.', 'work is slow due to lack of skill.'), ('কর্মশালায় দক্ষতা বৃদ্ধির প্রশিক্ষণ দেয়া হলো।', 'a workshop was given to increase proficiency.', 'training increased skills in the workshop.'), ('তার দক্ষতায় টিম দ্রুত সফল হলো।', \"the team's success was quick due to his expertise.\", 'his team succeeded quickly.'), ('শিল্পকর্মে দক্ষতা প্রদর্শিত হয়।', 'skill is demonstrated in the artwork.', 'the skill is shown in art.'), ('সিলেটে হাজারো মেলা বসে থাকে।', 'thousands of fairs are held in sylhet.', 'thousands are sitting in the silet.'), ('মেলা দেখতে সবাই আনন্দ করে।', 'everyone enjoys seeing the fair.', 'everyone is happy to see the show.'), ('এই মাছ মেলায় জনপ্রিয়।', 'this fish is popular at the fair.', 'this fish is popular.'), ('কবিতায় মেলা শব্দ দিয়ে সমাবেশ বোঝানো হয়েছে।', \"in poetry the word 'mela' is used to mean assembly.\", 'in the poem, the summit was measured.'), ('উৎসবে মেলা এলাকায় নিরাপত্তা বাড়ানো হয়েছে।', 'security has been increased in the fair area during the festival.', 'security increased in the festival area.'), ('জার্মে হালুয়া দোকানে টোকার সারি লম্বা।', 'at the sweet shop in the market, the queue was long.', 'the tower line is high in the light shop in Germany.'), ('দোকানে নতুন কাপড় এসেছে।', 'new clothes have arrived at the shop.', 'new clothes came to the shop.'), ('দোকানের মালিক খুব বন্ধুত্বপূর্ণ।', 'the shop owner is very friendly.', 'the shop owner is very friendly.'), ('দোকান নামের রেস্টুরেন্টে ইতিমধ্যেই বুকিং আছে।', 'the restaurant named dokan already has bookings.', 'there are already bookings in the restaurant named shop.'), ('দোকান বন্ধ থাকলে বাজার শুন্য মনে হয়।', 'if the shops are closed, the market feels empty.', 'when the shop is closed, the market seems empty.'), ('চাষের জমিতে সুপেয় জল প্রবাহিত করা হয়েছে।', 'drinking-quality water has been channeled into the farmland.', 'water flowed to the land of cultivation.'), ('জল না থাকলে ফসল মরবে।', 'without water the crops will perish.', 'if there is no water, the crops will die.'), ('জল ছোঁয়ায় পাখির ডানা ভেজে যায়।', \"a bird's wings get wet when touched by water.\", 'after touching the air, the birds are wet.'), ('জল নামের গ্রামে উৎসব আজ শুরু।', 'a village named jal starts a festival today.', 'the festival begins today in the village named water.'), ('জল-স্থায়ী পন্থা গড়ে তোলা জরুরি।', 'it is essential to develop water-sustainable methods.', 'building water-permanent approach is urgent.'), ('তুমি ঠিক সময়ে এসে ফেললে সমস্ত কাজ সহজ হবে।', 'if you come at the right time all tasks will be easier.', 'if you come at the right time, everything will be easy.'), ('সময়ের মূল্য জানলে জীবন সুন্দর হয়।', 'life becomes beautiful when you know the value of time.', 'knowing the value of time makes life beautiful.'), ('সময় বলে দেয় সবকিছু বদলে যায়।', 'time tells that everything changes.', 'telling everything changes.'), ('সময়ে কাজ না করলে পরিণতি খারাপ হতে পারে।', \"if you don't act in time, the outcome may be bad.\", \"if you don't work in time, the result can be bad.\"), ('সময়ের সাথে তাল মিলিয়ে জীবনকে সাজাও।', 'arrange life in tune with time.', 'arrange life with time.'), ('সূর্য উঠতেই মাছ ধরার নৌকাটা নৌঘাট ছেড়ে গেল।', 'as soon as the sun rose, the fishing boat left the dock.', 'after the sun rising the boat left the boat.'), ('তার নৌকায় চারজন যাত্রী ছিল।', 'there were four passengers in his boat.', 'there were four passengers on his boat.'), ('জীবনের নৌকাটি সাবধানে চালাতে শেখো।', 'learn to steer the boat of life carefully.', 'learn to drive the boat carefully.'), ('নৌকা নামক গ্রামটি উপকূলীয়।', 'a village called nouka is coastal.', 'the village named boat is coastal.'), ('ঝড়ে নৌকা ভেঙে গিয়েছিল।', 'the boat was broken in the storm.', 'the boat collapsed in storm.'), ('বিধায়ক তার বক্তব্যে বেশ আবেগ বের করলেন।', 'the legislator expressed a lot of emotion in his speech.', 'the judge expressed much emotion in his speech.'), ('বক্তব্য রেকর্ডে রাখা হয়েছে।', 'the statement has been recorded in the archives.', 'the speech was recorded.'), ('তার বক্তৃতার বক্তব্য কয়েকবার পুনরাবৃত্তি হলো।', 'the remarks of his speech were repeated several times.', 'his speech repeated several times.'), ('সভায় সকলের বক্তব্য শোনার সুযোগ ছিল।', 'at the meeting everyone had the opportunity to make remarks.', 'everyone had the opportunity to hear the speech.'), ('বক্তব্যে যুক্তির অভাব খাড়া করা হলো।', 'a lack of logic was pointed out in the statement.', 'lack of reason in speech.'), ('পথে নিয়ে যাওয়ার সময় ঝুলি ভরে ফেলো।', 'fill the bag when taking things along the way.', 'fill it while taking it on the road.'), ('ঝুলিতে খাবার রেখে দিয়েছে মা।', 'mother has put food in the bag.', 'my mother kept food.'), ('সে ঝুলি বয়ে নিয়ে লম্বা পথ টেকে।', 'he carries the bag over long distances.', 'he holds a long way with his hanging.'), ('ঝুলি নামের দোকানটি বাজারে জনপ্রিয়।', 'a shop named jhuli is popular in the market.', 'the shop named hanging is popular in the market.'), ('ঝুলিতে দামি জিনিস রাখা ছিল।', 'valuable items were kept in the sack.', 'precious things were kept in the hang.'), ('শিশু ক্লাসে অঙ্কের সমস্যার সমাধান করল।', 'the child solved the math problem in class.', 'children solved problem in class.'), ('অঙ্কের সূত্র মনে রাখো।', 'remember the formula of arithmetic.', 'remember the form.'), ('কবিতায় অঙ্কের রূপক ব্যাখ্যা করা হয়েছে।', 'in the poem the figure serves as a metaphor.', 'the poem explained the figure.'), ('অঙ্কশাস্ত্রে তার দক্ষতা প্রশংসিত।', 'his skill in mathematics is praised.', 'his skill is praised.'), ('অঙ্ক দিয়ে হিসেব সারো।', 'do the calculation using numbers.', 'count with numbers.'), ('বাড়ির ছাদে পাখারা বাসা বানায়।', 'birds build nests on the house roof.', 'birds build house on the roof.'), ('ছাদে খোলা আলোয় তারা গল্প করছিল।', 'they talked on the open roof under the light.', 'they were telling stories on the roof.'), ('ছাদ নামের রেস্টুরেন্টে খাবার ভালো।', 'food is good at the restaurant named chad.', 'food is good in the roof restaurant.'), ('তারা ছাদ থেকে ঝুঁকে প্রাকৃতিক দৃশ্য দেখল।', 'they leaned from the roof to view the landscape.', 'they saw a natural scene from the roof.'), ('ঘরের ছাদ নষ্ট হলে বৃষ্টিতে ভিজে যায়।', 'if the house roof is damaged, it gets wet in the rain.', 'when the roof is damaged, it is wet in the rain.'), ('তার কথায় কাঁটা ছিল, শুনে বেদন হল।', 'there was a sting in his words; it hurt to hear.', 'he heard it hurting.'), ('কাঁটা থামিয়ে ফুল সংগ্রহ করা দায়িত্বপূর্ণ।', 'it is responsible to remove thorns before collecting flowers.', 'gathering flowers is responsible.'), ('কাঁটার মতো উক্তি কষ্ট করল।', 'a thorn-like remark caused pain.', 'the speech caused pain.'), ('জুতার মাঝে কাঁটা পেয়ে হাঁটা বন্ধ করল।', 'he stopped walking after finding a thorn in his shoe.', 'he stopped walking between shoes.'), ('কাঁটার ভেতর লুকানো সৌন্দর্য খুঁজে পাওয়া যায়।', 'hidden beauty can be found among the thorns.', 'the beauty is hidden inside the tower.'), ('বইয়ের পাতা ছিড়ে ফেলে ময়লা হয়েছে।', \"the book's pages were torn and became messy.\", 'the leaf was broken.'), ('ছিঁড়ে পড়া ফিতাটি বদলে দিতে হবে।', 'the torn ribbon must be replaced.', 'it must be replaced.'), ('চিঠি ছিঁড়ে ফেললে বার্তা মুছে যায়।', 'if you tear the letter it erases the message.', 'if you remove the message, the message is removed.'), ('ছিঁড়ে যাওয়া কাপড় রংয়ে বদলে ফেলো।', 'change the torn cloth by dyeing it.', 'change colored clothes.'), ('ছিঁড়ে ছেঁড়া কাগজ মুছে ফেলো।', 'dispose of the ripped papers.', 'throw off the paper.'), ('বৃক্ষ কিনারা থেকে ছাই পড়ে গেছে।', \"ashes have fallen from the tree's edge (poetic).\", 'the tree has fallen from the stone.'), ('চুলে ছাই লেগে গিয়ে ঠিক করা দরকার।', 'ash in the hair must be cleaned properly.', 'it needs to be fixed.'), ('ছাই নামক গ্রামের মানুষেরা অতিথি স্বাগত জানায়।', 'people of a village named chhai greet guests.', 'people from the village named shai welcome guests.'), ('আগে অগ্নি জ্বলা ছাই ছড়িয়ে দেয়।', 'after the fire burns, ash spreads around.', 'the fire spreads shade.'), ('ছাই-সদৃশ স্মৃতি হৃদয়ে জমে আছে।', 'ash-like memories have settled in the heart.', 'shade-looking memories are accumulated in heart.'), ('ঢাকায় শাড়ি দুটো কিনে নিলাম।', 'i bought two saris in dhaka.', 'i bought two crabs in Dhaka.'), ('তারা ঢাকাকে সবার আগে ছেড়ে চলে গেল।', 'they left dhaka before everyone else.', 'they left Dhaka first.'), ('ঢাকা নামের গ্রামটি শহর থেকে দূরে।', 'a village named dhaka is far from the city.', 'the village named Dhaka is far from the city.'), ('ঘর খোঁজার সময় ঢাকায় যাওয়া প্রয়োজন ছিল।', 'to find the house it was necessary to go into the city center.', 'i needed to go to Dhaka while looking for the house.'), ('ঢাকায় অবস্থান পরিবর্তিত হলে চিঠি পৌঁছায় না।', 'if location changes in dhaka the mail does not reach.', \"if the position changes, the letter doesn't reach it.\"), ('বৃক্ষের শিকড় নিঁড়ে ফেললে মাটি ছিঁড়ে যায়।', \"if you pull out the tree's roots the soil is torn up.\", 'when you cut the tree root, the soil is broken.'), ('শিকড়ের উপর নির্ভর করে গাছের স্থায়িত্ব।', \"a tree's stability depends on its roots.\", \"the plant's stability depends on the root.\"), ('তার ঐতিহ্যের শিকড় গভীর।', 'the roots of his heritage are deep.', 'his roots are deep.'), ('শিকড়ে লুকানো পুষ্টি গাছ খায়।', 'a tree absorbs nutrients hidden in its roots.', 'hidden to feed trees.'), ('পুরোনো প্রথার শিকড় ছোট করা কঠিন।', 'it is difficult to cut the deep roots of old customs.', 'to reduce the root of old tradition is difficult.'), ('বনাঞ্চলে ঝোপঝাড়ে নোনা গন্ধ ভেসে আসে।', 'a salty smell drifts in the bushes of the forest.', 'in the forest the smell flourishes.'), ('ঝোপের ভেতর পাখি ঘর বানিয়েছে।', 'a bird has nested inside the shrub.', 'the bird built a house inside the tower.'), ('ঝোপে লুকিয়ে তারা খেলা করল।', 'they hid in the bushes and played.', 'they played as hidden.'), ('ঝোপের পাশে ফুলের সারি লাগান।', 'plant a row of flowers beside the shrubs.', 'placing flowers on the side of the shape.'), ('ঝোপে লুকিয়ে থাকা বিড়াল দেখা গেল।', 'a cat hiding in the bushes was seen.', 'a cat was seen hidden.'), ('মাসিকী মাটি উর্বর করতে সার দেয়।', 'the monthly tiller applies fertilizer to enrich the soil.', 'monthly soil fertilizes.'), ('মাসিকী নামের মহিলাটি বাজারে বসে আছে।', 'a woman named masiki is sitting in the market.', 'a woman named monthly is sitting in the market.'), ('খামারে মাসিকীর মেশিনটি লোকচানা করে।', 'the monthly machine at the farm operates automatically.', 'the machinery is locked in the farm.'), ('মাসিকীর সাহায্যে জমি খনন করা সহজ হয়েছে।', \"with the tiller's help digging the land became easy.\", 'land mining with the help of the moonist became easy.'), ('খেতে মাসিকী চালানো বন্ধ হয়ে গেছে।', 'the operation of the tiller in the field has stopped.', 'eating has stopped.'), ('দেশে নবান্ন উৎসবে লোকেরা অংশ নিল।', 'people participated in the nabanna festival in the country.', 'people participated in the new festival in the country.'), ('নবান্ন-পর্বে মিষ্টি বিতরণ করা হয়।', 'sweets are distributed during the nabanna period.', 'sweets are distributed.'), ('নবান্ন নামক বাজারে চকচকে মাল বিক্রি হয়।', 'shiny goods are sold at the market called nabanna.', 'goods are sold in the new market.'), ('কবিতায় নবান্ন মানুষের খুশি প্রতিফলিত।', \"in the poem nabanna reflects people's joy.\", \"the poetry reflects people's happiness.\"), ('নবান্ন উৎসবে শিশুদের নাচ প্রদর্শন করা হয়।', 'children perform dances at the nabanna festival.', 'children are showed dance at the festival.'), ('শিক্ষক কক্ষ থেকে ছাত্রদের বারবার ডেকেছে।', 'the teacher called the students repeatedly from the classroom.', 'the teacher called students from the room repeatedly.'), ('কক্ষটি পরিষ্কার রেখে দাও।', 'keep the room clean.', 'keep the room clean.'), ('হাসপাতালের কক্ষগুলো সুষ্ঠুভাবে সাজানো আছে।', 'the hospital rooms are arranged neatly.', 'the hospital rooms are properly arranged.'), ('কক্ষ নামের হোটেলটি শহরে জনপ্রিয়।', 'a hotel named kokkha is popular in the city.', 'the hotel named room is popular in the city.'), ('কক্ষের জানালা দিয়ে সূর্যের আলো পড়ছে।', \"sunlight is falling through the room's window.\", 'the sun is falling through the room window.'), ('মাঠে শিশুরা দৌড়ে কোলাহল সৃষ্টি করেছে।', 'children running in the field created a commotion.', 'children raised on the field.'), ('বাজারে কোলাহল বেশি ছিল আজ।', 'there was a lot of bustle in the market today.', 'there was more of these in the market today.'), ('কোলাহলে নয়ন স্তব্ধ হয়ে যায়।', 'in the uproar, the eyes become stunned.', 'the ninth becomes narrow.'), ('উৎসবে কোলাহল নিয়ে আনন্দ বেড়ে যায়।', \"the celebration's uproar increases the joy.\", 'the joy increases with the tower.'), ('কোলাহলে উদ্ধার কাজ ধীর হয়ে গেল।', 'rescue operations slowed down amidst the chaos.', 'the rescue work slowed.'), ('শিবিরে স্বেচ্ছাসেবীরা তার্তী গ্রুপে কাজ করল।', 'volunteers worked in the tidy group at the camp.', 'the volunteers worked in the camp.'), ('শিবিরের খেতে বুনিয়াদি কাজ সম্পন্ন হলো।', \"basic tasks in the camp's fields were completed.\", 'bunyadi work completed in the camp.'), ('শিবিরে নতুন সদস্য যোগ দিল।', 'a new member joined the camp.', 'new member joined the camp.'), ('শিবির নামক গ্রামে একটি বিদ্যালয় চালু হয়েছে।', 'a school has been opened in a village named shibir.', 'a school has been launched in the village named camp.'), ('জলবায়ুর পরিবর্তনে শিবির সরানোর সিদ্ধান্ত নেয়া হলো।', 'due to climate change, the decision was made to relocate the camp.', 'decision to move the camp was made due to climate change.'), ('বাজারে তাজা মাছের গন্ধ ছড়িয়ে পড়েছে।', 'the smell of fresh fish has spread in the market.', 'fresh fish smell spread in the market.'), ('তাজা ফল বিক্রেতা দোকানে এসেছে।', 'a vendor with fresh fruits has arrived at the shop.', 'fresh fruit vendor came to the shop.'), ('তাজা খবর হাতে পেয়ে সবাই চমকে ওঠল।', 'everyone was surprised to get the latest news.', 'after getting the fresh news, everyone was surprised.'), ('তাজা পাতা দিয়ে রান্না করলে স্বাদ বদলে যায়।', 'cooking with fresh leaves changes the flavor.', 'cooking with fresh leaves changes taste.'), ('তাজা ব্যাক্তি সবসময় কর্মে উদ্যমী থাকে।', 'a lively person is always enthusiastic in work.', 'fresh people are always enthusiastic in action.'), ('জলপথে মালবাহী ট্রাক গমন করল বন্দরের দিকে।', 'a cargo truck moved toward the port along the waterway.', 'carrying truck walked to the port.'), ('মালবাহী যানবাহন শহরের ওজন বহন করে।', \"freight vehicles carry the city's weight.\", \"cargo vehicles carry the city's weight.\"), ('বন্দর থেকে মালবাহী জাহাজ ছেড়ে গেছে।', 'a cargo ship departed from the port.', 'ships have left the port.'), ('মালবাহী ট্রেন লাইন করে ভারি মাল বহন করে।', 'freight trains transport heavy goods in line.', 'cargo train carries heavy goods by lineing.'), ('নতুন মালবাহী নীতি শিল্প বজায় রাখতে জরুরি।', 'new freight policies are essential to sustain industry.', 'new cargo policy is urgent to maintain industry.'), ('সিংহাসনে বসে রাজা জনগণকে দর্শালেন।', 'seated on the throne the king addressed the people.', 'the king visited people sitting on the throne.'), ('সিংহাসনে বসা প্রতীকী ছবি দেওয়ালে ঝুলছে।', 'a symbolic picture of someone on a throne hangs on the wall.', 'a symbolic image is hanging on the wall.'), ('রাজনীতিতে সিংহাসন অর্জন কঠিন কাজ।', 'gaining the throne in politics is a difficult task.', 'achieving throne in politics is difficult.'), ('সিংহাসনে পৌঁছানোর স্বপ্ন তার হৃদয়ে ছিল।', 'the dream of reaching the throne was in his heart.', 'the dream of reaching the throne was in his heart.'), ('চিত্রকাহিনীতে সিংহাসন ধ্বংস হওয়ার চিত্র আছে।', 'the comic depicts the destruction of the throne.', \"there are pictures of the throne's destruction.\"), ('কৃষকের মাঠে বীজ বপন করা শুরু হলো।', \"seeds were sown in the farmer's field.\", 'planting seeds began in the farmer field.'), ('বপন-উৎসবে গ্রামের আনন্দ ছিল বিরাট।', 'there was great joy in the village at the sowing festival.', \"the village's joy was great.\"), ('বাগানে নতুন গাছের বপন করো।', 'plant new trees in the garden.', 'plant new trees in the garden.'), ('বাপনের পর যত্ন নেওয়াই সফল ফল আনে।', 'care after planting yields successful results.', 'taking care after planting brings success.'), ('বপনে ব্যবহৃত বীজ মানসম্মত কিন?', 'is the seed used for sowing of good quality?', 'the seed used in planting is quality?'), ('নতুন সেতু বানাতে প্রকৌশলীরা কাজ শুরু করলেন।', 'engineers began work to build a new bridge.', 'the engineers started work to build a new bridge.'), ('সেতু নামের গ্রামে রাস্তা তৈরি করা হয়েছে।', 'a road has been built in a village named setu.', 'a road built in the village named bridge.'), ('প্রাচীন সেতু এখন পর্যটক আকর্ষণ।', 'the ancient bridge is now a tourist attraction.', 'the old bridge is now tourist attraction.'), ('বন্যায় সেতু ধ্বংস হয়ে যোগাযোগ বিচ্ছিন্ন হল।', 'the bridge was destroyed in the flood and communication was cut off.', 'the bridge collapsed after the flood.'), ('সেতুতে হাঁটার সময় সতর্ক হও।', 'be careful while walking on the bridge.', 'be careful while walking on the bridge.'), ('পিপাসায় সে ঝর্ণার জলে অতি তৃষ্ণা মেটাল।', \"in thirst he quenched his great thirst with the fountain's water.\", 'he filled a lot of thirst in the water.'), ('ঝর্ণার কলিজায় শিশুরা খেলে।', \"children play near the fountain's basin.\", 'children are playing in the shore.'), ('বাগানের ঝর্ণা সন্ধ্যায় সুন্দর শব্দ করে।', \"the garden's fountain makes a beautiful sound in the evening.\", 'the winds of the garden sound beautiful at night.'), ('ঝর্ণার পানি পরীক্ষা করা হয়েছে বিশুদ্ধতা নির্ণয়ে।', 'the fountain water was tested for purity determination.', 'water has been tested to determine purity.'), ('পাহাড়ের ঝর্ণা গ্রামে বন্যা লয় কমায়।', 'the mountain spring reduces flood impact in the village.', 'mountains reduce flood in the village.'), ('চৌকি বসিয়ে নিরাপত্তা বাড়ানো হয়েছে।', 'security has been increased by setting up a checkpoint.', 'the truck has increased security.'), ('সেনাদের চৌকি পাহারা দিচ্ছে সকাল থেকেই।', 'the army post has been on guard since morning.', 'the army has been guarding since morning.'), ('গ্রাম চৌকিতে মিছিলের ব্যবস্থা ছিল।', 'there was a procession arrangement at the village square.', 'there was a march in the village.'), ('চৌকির পাশে ছোট দোকানটি প্রতিদিন খোলা থাকে।', 'the small shop beside the post is open every day.', 'the small shop is open next to the chamber every day.'), ('চৌকির দায়িত্বপ্রাপ্ত অফিসার সময়ে রিপোর্ট দিলেন।', 'the officer in charge of the post submitted the report on time.', 'the chief officer reported at time.'), ('কালিকা পূজায় সবাই অভিষিক্ত হল।', 'everyone was anointed at the kalika worship.', 'everyone has been admitted to praise.'), ('অভিষিক্ত রাজা প্রজাদের কাছে সম্মান পেলেন।', 'the anointed king received respect from his subjects.', 'the precious king received honor from the genuine.'), ('মন্দিরে অভিষিক্ত হওয়ার অনুষ্ঠান শান্তিপূর্ণ ছিল।', 'the consecration ceremony in the temple was peaceful.', 'the ceremony to be celebrated in the temple was peaceful.'), ('অভিষিক্ত নামের গ্রামটি ঐতিহ্যবাহী রাজপথে অবস্থিত।', 'a village named abhishikta is located on a traditional route.', 'the village named narratic is located on the traditional roadside.'), ('নতুন প্রতিমা অভিষিক্ত হবার পরে উৎসব শুরু হল।', 'the festival began after the new idol was consecrated.', 'after the opening of the new statue, the festival began.'), ('বিস্ময়ের সাথে সে বাগানে এল।', 'he came to the garden in wonderment.', 'he came to the garden with wonder.'), ('বিস্ময়ে ভরা চোখে শিশুটি আকাশ দেখচ্ছে।', 'the child looks at the sky with eyes full of wonder.', 'the child is seeing heavily.'), ('অবাক বিস্ময়ে সবাই তাকিয়ে রইল।', 'everyone stood staring in astonished wonder.', 'everyone looked surprised.'), ('বিজ্ঞানী নতুন আবিষ্কারে বিস্ময় প্রকাশ করলেন।', 'the scientist expressed wonder at the new discovery.', 'the scientist expressed surprise with a new discovery.'), ('বিস্ময়ের ভাবনা কবিতায় ফুটে উঠেছে।', 'a sense of wonder is reflected in the poem.', 'the mind of wonder has come up in the poem.'), ('উপহার টুকরো পেয়ে তার মুখ আলোকিত হল।', 'his face lit up on receiving the gift package.', 'after getting a piece of gift, his face was illuminated.'), ('টুকরো কাগজ রেখে নাও।', 'keep the small pieces of paper.', 'keep a piece of paper.'), ('টুকরো টুকরো করে কেটে ফল সাজাও।', 'cut into small pieces and arrange the fruits.', 'cut pieces and decorate the fruit.'), ('টুকরো টুকরো স্মৃতি তার মনে জমে আছে।', 'fragmented memories remain stored in his mind.', 'pieces of memories are accumulated in his mind.'), ('খাবারের টুকরো সবাই ভাগ করে নিল।', 'everyone shared the pieces of food among themselves.', 'everyone shared the piece of food.'), ('ঝরনা ধারার ধারালো কণা গড়ে।', \"the waterfall's stream forms sharp droplets.\", 'the shore forms sharp particles.'), ('মাটির কণা বিশ্লেষণ করে বিজ্ঞানীরা রিপোর্ট দিল।', 'scientists reported after analyzing soil particles.', 'the scientists analyzed soil particles.'), ('কণার সংস্পর্শে আলো নাচতে শুরু করে।', 'light begins to dance on the tiny particles.', 'the light begins to dance in touch with particles.'), ('কণাতে লুকানো সৌন্দর্য অনিন্দ্য।', 'hidden beauty in the particles is exquisite.', 'the beauty is hidden in the neck.'), ('কণার পরিমাণ বাড়লে গুণগতমান পরিবর্তিত হয়।', 'if the particle count increases, the quality changes.', 'when the quantity increases, the quality changes.'), ('বৃন্দাবনে কিশোরেরা গপ্পো করছিল।', 'teenagers were chatting in vrindavan.', 'the younger spoke.'), ('বৃন্দাবনের গানে লোকেরা মর্মস্পর্শী হয়ে ওঠে।', 'people become moved by the songs of vrindavan.', 'people become touched by the tower.'), ('বৃন্দাবনের ছায়ায় শিশুরা খেলে।', 'children play in the shade of vrindavan.', 'children are playing in the shade of the jungle.'), ('বৃন্দাবনের ঐতিহ্য বহু প্রাচীন।', 'the heritage of vrindavan is very ancient.', 'the heritage is ancient.'), ('বৃন্দাবনের রথযাত্রায় ভিড় ছিল বিশাল।', \"there was a huge crowd at vrindavan's chariot procession.\", 'the crowd was vast.'), ('ঘড়ির কাঁটায় সময় দেখে সবাই কাজে নামল।', \"seeing the time on the clock's hand everyone started working.\", 'after seeing the clock, everyone came to work.'), ('কাঁটার তীক্ষ্ণতা পরীক্ষা করো আগে।', 'test the sharpness of the pin first.', 'check the sharpness before.'), ('বিছানায় কাঁটা পড়ে উঠতে ব্যথা লাগল।', 'a thorn in the bed pricked and caused pain when getting up.', 'i felt pain falling up in bed.'), ('কাঁটা নামে শিশুটি আজ স্কুলে অনুপস্থিত।', 'a child named kanta is absent from school today.', 'the child is absent from school today.'), ('কাঁটার মতো সোজাসাপটা বক্তব্য সবাই মেনে নেয় না।', 'a thorn-like blunt statement is not accepted by everyone.', 'everyone does not accept this simple speech.'), ('আশীৰ্বাদ করে দায়িত্ব হাতে দিলেন বৃদ্ধ।', 'the elder blessed and handed over the responsibility.', 'the elder gave responsibility.'), ('আশীর্বাদ পেয়ে নবদম্পতি কৃতজ্ঞ ছিল।', 'the newlyweds were grateful to receive the blessing.', 'the newly blessed was grateful.'), ('বৃহৎ অনুষ্ঠানে আশীর্বাদ সবার কাছে গুরুত্ব পায়।', 'at grand events blessings gain importance among all.', 'the blessing is important for everyone.'), ('বাবার আশীর্বাদ ছিল তার জীবনের শক্তি।', \"his father's blessing was the strength of his life.\", 'his blessing was the strength of his life.'), ('পূজায় দেবতার আশীর্বাদ প্রত্যাশা করা হয়।', \"a deity's blessing is expected during worship.\", 'the blessing of god is expected.'), ('ভূমিতে শস্য বহন করে ট্রাক চলে গেল।', 'a truck carrying crops moved along the land.', 'carrying grains to land, the truck went away.'), ('ট্রাকে মালবাহী বোঝা সঠিকভাবে রাখা হয়নি।', 'the cargo load in the truck was not placed properly.', 'the truck was not placed properly.'), ('ট্রাক চালক রাস্তা সম্পর্কে熟 জানেন।', 'the truck driver knows the road well.', 'truck driver knows about road.'), ('শহরের ট্রাক চলাচলে শৃঙ্খলা আনা জরুরি।', 'it is necessary to bring order to truck traffic in the city.', \"when the city's truck runs, it's urgent to bring rounds.\"), ('ট্রাক নামের দোকানটি লজিস্টিক সেবা দেয়।', 'a business named truck provides logistics services.', 'the shop named truck provides logistical service.'), ('প্রতিথানে আলো নিভে গেলে ভয় লাগল।', 'we were frightened when the light went out in the auditorium.', 'when the light drops down, i am afraid.'), ('প্রতিথানে বক্তৃতা সফলভাবে সম্পন্ন হলো।', 'the speech was successfully completed in the auditorium.', 'the speech completed successfully.'), ('প্রতিথানে রঙিন আলো মঞ্চকে আলোকিত করল।', 'colored lights lit up the stage in the auditorium.', 'colorful light illuminated the scene.'), ('শিক্ষার্থীরা প্রতিথানে পরীক্ষায় বসে।', 'students sit for exams in the auditorium.', 'students are sitting on the exam.'), ('প্রতিথান নামের কফিশপটি ক্যাম্পাসে আছে।', 'a coffee shop named pratithan is on campus.', 'the captain named talent is in the campus.'), ('ফসল কাটা শুরু হল কৃষকের মাঠে।', \"harvesting began in the farmer's field.\", 'cutting begins in the farmer field.'), ('কাঠ কেটে দরজা বানিয়ে দিল।', 'he cut wood and made a door.', 'he cut the wood and made it.'), ('চোখ কাটা বক্তব্য শুনি বিরক্ত হল।', 'i was annoyed by his cutting remark.', 'listening to eye cutting speech is annoying.'), ('কাঠের কাটা অংশ মসৃণ করা হয়েছে।', 'the cut pieces of wood have been smoothed.', 'cutting parts of wood smoothed.'), ('বাধা কাটা ছেঁড়ে ফেলো।', 'cut through and break the obstacle.', 'remove the obstacle.'), ('রাতের আকাশে চাঁদটি উজ্জ্বল দেখানো দিল।', 'the moon shone brightly in the night sky.', 'the moon showed brightly in the night sky.'), ('তার মুখে উজ্জ্বল হাসি ছিল।', 'there was a radiant smile on his face.', 'he had a brilliant smile in his face.'), ('উজ্জ্বল ভবিষ্যৎ দেখতে সে পরিশ্রম করছে।', 'he is working hard to see a bright future.', 'he is working to see brilliant future.'), ('উজ্জ্বল আলোয়ে চিত্রটি স্পষ্ট দেখা যায়।', 'the painting appears clear in the bright light.', 'the picture is visible in brilliant light.'), ('উজ্জ্বল অবস্থানে থাকা তার গৌরব।', 'being in a prominent position is his glory.', 'his glory is in brilliant position.'), ('বাজারে মিষ্টি বিক্রি করে রোজগার হচ্ছে।', 'selling sweets in the market earns a living.', 'sweets are being sold in the market.'), ('রোজগার বাড়াতে নতুন উদ্যোগ নেওয়া হলো।', 'a new initiative was taken to increase income.', 'a new initiative was taken to increase the rise.'), ('শিল্পীর রোজগার ঘরে বসে তৈরি হয়।', \"the artist's earnings are made working from home.\", \"the artist's rest is made sitting in the room.\"), ('রোজগার কমলে পরিবারকে কষ্ট হয়।', 'if earnings decrease the family suffers.', 'the family feels painful.'), ('রোজগারের উৎস খুঁজতে তিনি শহরে গেলেন।', 'he went to the city to search for sources of income.', 'he went to the city to find the source of rosgary.'), ('সামনে বিশাল প্রাচীর দেখে ভয় করল।', 'he was frightened seeing the huge wall ahead.', 'seeing a huge wall in front.'), ('প্রাচীরটি রঙ করে নতুন করে সাজানো হলো।', 'the wall was painted and newly decorated.', 'the wall was painted and decorated again.'), ('প্রাচীরমালার প্রাচীর কাহিনী অনেক পুরানো।', 'the wall story of the compound is very old.', 'the wall story is very old.'), ('প্রাচীরের ওপর ফুলের ছবি আঁকা হয়েছে।', 'flower designs have been painted on the wall.', 'a picture of flowers is painted on the wall.'), ('প্রাচীরের ফাটল দেখে মেরামত জরুরি।', 'seeing cracks in the wall, repair is necessary.', 'it is urgent to repair the wall.'), ('রুই মাছ পুকুরে দ্রুত বৃদ্ধি পাচ্ছে।', 'rui fish is growing rapidly in the pond.', 'roy fish are growing rapidly.'), ('রুই নামের কিশোরটি স্কুলের ফুটবলার।', \"a boy named rui is the school's footballer.\", 'a teenager named roy is a school footballer.'), ('রুই তৈল দিয়ে রান্না করলে স্বাদ বদলে যায়।', 'cooking rui in oil changes its taste.', 'cooking with rice changes taste.'), ('পুকুরে রুই ছাড়লে মাছেরা দ্রুত ছত্রভঙ্গ করে।', 'when rui are released into the pond, the fish disperse quickly.', 'when you leave the rice, the fish breaks quickly.'), ('রুই ধরা একটি সাধারণ কর্ম কৃষকদের কাছে।', 'catching rui is a common task for farmers.', 'catching rois is a common work for farmers.'), ('চালু হয়ে গেছে নতুন প্রকল্পের প্রকৃত কাজ।', 'the real work of the new project has begun.', 'the real work of the new project has been launched.'), ('তার প্রকৃত উদ্দেশ্য কেউ বোঝে না।', 'no one understands his true intention.', 'no one understands his genuine purpose.'), ('প্রকৃত সৌন্দর্য মনুষ্যের অন্তরেই বসে থাকে।', \"true beauty resides in a person's heart.\", 'true beauty is sitting in the heart of man.'), ('প্রকৃত মূল্যায়ন করতে সময় লাগে।', 'it takes time to assess the actual value.', 'it takes time to evaluate.'), ('প্রকৃত বন্ধু খুঁজে পাওয়া সহজ নয়।', 'finding a true friend is not easy.', 'finding a true friend is not easy.'), ('শিল্পকলা প্রদর্শনীতে লোকজন ভিড় করেছে।', 'people crowded at the art exhibition.', 'people crowded in the art exhibition.'), ('শিল্পকলার পাঠ্যক্রম শিক্ষার্থীদের জন্য কার্যকর।', 'the arts curriculum is effective for students.', 'artwork curriculum is effective for students.'), ('তার কাজ শিল্পকলা জগতে পরিচিতি পেয়েছে।', 'his work has gained recognition in the arts world.', 'his work became known in the art world.'), ('শিল্পকলা নামের সংগঠন একটি অনুষ্ঠান করল।', 'an organization named shilpakala held an event.', 'a organization named art performed an event.'), ('শিল্পকলার কারুকাজ খুব মনোগ্রাহী।', 'the craftsmanship of the art is very captivating.', 'the artwork is very emotional.'), ('তাঁর চাহনি আকর্ষণীয় ছিল দর্শকদের কাছে।', 'his gaze was attractive to the audience.', 'his desire was attractive to viewers.'), ('চাহনি বদলে গেল, সংলাপ শুরু হলো।', 'the glance changed and the dialogue began.', 'the desire changed, the dialogue started.'), ('চাহনি নামের ছেলেটি আজ উপস্থাপন করবে কাজ।', 'a boy named chahni will present his work today.', 'the boy named chani will present work today.'), ('কবিতায় তার চাহনি হৃদয় জয় করে নেয়।', 'his glance in the poem wins the heart.', 'his desire conquers his heart.'), ('স্পষ্ট চাহনি দিলে বোঝা যায় সে সতর্ক।', 'a clear glance shows that he is alert.', 'if you want clearly, he is careful.'), ('মাঠে ধান পুঞ্জ করে রাখা আছে।', 'sheaves of paddy are piled up in the field.', 'there is to be putden in the field.'), ('পুঞ্জ করে ফুল বেঁধে উপস্থাপন করো।', 'tie the flowers into a bundle and present them.', 'bring flowers together.'), ('লেখকের বাক্যের পুঞ্জে ভাবের ঘনত্ব আছে।', \"there is a density of meaning in the cluster of the writer's sentences.\", \"the author's speech is density.\"), ('ডাটা পুঞ্জ করে বিশ্লেষণ করা হলো।', 'the data was clustered and analyzed.', 'the data was analyzed.'), ('আশার পুঞ্জে জীবন নতুনভাবে জেগে ওঠে।', 'life awakens anew in a cluster of hopes.', 'life is awakening again.'), ('চালুক ট্র্যাকারে কৃষক গাছ লাগাল।', 'the farmer planted trees using a tractor.', 'the farmer planted trees on the move tracker.'), ('রেসে ট্র্যাক ভাঙাগল বলে প্রতিযোগিতা বাতিল হলো।', 'the competition was canceled because the race track was damaged.', 'the competition was canceled because the track was broken.'), ('অডিও ট্র্যাকটি মিশ্রণে যোগ করা হয়েছে।', 'the audio track has been added in the mix.', 'the audio track added to the mix.'), ('ট্র্যাক নামের গ্রুপটি নতুন গান তুলেছে।', 'a group named track has released a new song.', 'the group named track raised a new song.'), ('তারা ট্র্যাকে দ্রুত দৌড়লো।', 'they ran fast on the track.', 'they run quickly on the track.'), ('বাগানে কচুরিপানা ভরে গেছে।', 'water hyacinths have filled the garden pond.', 'the thief is filled in the garden.'), ('কচুরিপানার পাতায় শিশুরা নৌকা চালায়।', 'children paddle boats on the water hyacinth leaves.', 'children run boat on the rubber page.'), ('তাঁর গলায় কচুরিপানার মতো কোমলতা ছিল।', 'there was a softness to his voice like a water hyacinth.', 'his throat was like a thief.'), ('চাষিরা কচুরিপানা পরিষ্কার করতে ব্যস্ত।', 'farmers are busy clearing the water hyacinth.', 'the farmers are busy cleaning the tower.'), ('পুকুরে কচুরিপানা বেশি হলে মাছের বৃদ্ধি ক্ষতিগ্রস্ত।', 'if the pond has too many water hyacinths, fish growth is harmed.', 'if there is more to steam, the growth of the fish is damaged.'), ('তাঁর চোখে অশ্রু ঝলসে উঠল।', 'tears welled up in his eyes.', 'tears fell in his eyes.'), ('কবিতায় অশ্রুর রূপক ব্যবহার করা হয়েছে।', 'the poem uses tears as a metaphor.', 'tears are used in the poem.'), ('অশ্রু মোছার পর মুখে শান্তি দেখা গেল।', 'after wiping tears, a calm appeared on the face.', 'after removed tears, peace appeared in my face.'), ('অশ্রু মুছতে হাতে রুমাল নাও।', 'take a handkerchief to wipe the tears.', 'wash your tears.'), ('আনন্দের অশ্রু কখনো দুঃখের অশ্রুর চেয়ে ভিন্ন।', 'tears of joy are different from tears of sorrow.', 'the tears of joy are different than the tears of sorrow.'), ('সে বাগানে বেল পাতা লাগাল।', 'he planted clove leaves in the garden.', 'he placed leaves in the garden.'), ('বেল বা ট্যাগোর কাব্যে রূপক বাজে।', \"bel (or tagore's) poetry uses vivid metaphors.\", 'the ball or tag is bad.'), ('বেল নামের গ্রামটি নদীর ধারে।', 'a village named bel is by the riverbank.', 'the village named Bell is on the river.'), ('শীতকালে বেল পাতায় কুঁচি আসে না।', 'clove leaves do not bud in winter.', \"i don't come to the leaf in the winter.\"), ('বেল পাতার গন্ধ রান্নায় ব্যবহার করা হয়।', 'the aroma of clove leaves is used in cooking.', 'ball leaves are used in cooking.'), ('স্কুলে রোটা ভেঙে সবাই খেলায় মেতে উঠল।', 'as the bell rang, everyone got excited in play at school.', 'everyone broke the rope in the school.'), ('রোটা নামের কুকুরটি আজ বাড়ি ফিরেছে।', 'a dog named rota returned home today.', 'the dog named rota returned home today.'), ('রোটা ভাঙলে অনুষ্ঠান শুরু হবে।', 'once the trumpets sound, the ceremony will begin.', 'if the root is broken, the event will begin.'), ('প্রতিযোগিতায় রোটা বাজিয়ে দলের সাফল্য ঘোষণা করা হলো।', \"the team's victory was announced with trumpet calls during the contest.\", 'the team announced success in the competition.'), ('রোটা-শব্দ শুনে সবাই স্তব্ধ হয়ে গেল।', 'everyone fell silent at the sound of the drum.', 'after hearing the sound, everyone got stuck.'), ('জীবনের পথে পাতার মতো ভাসা নয়।', \"life's path is not like a drifting leaf.\", 'not floating like leaves in life.'), ('পাতা-পাতায় বই সাজিয়ে রাখো।', 'arrange the book page by page.', 'place the book on the page.'), ('গাছের পাতায় শিশুরা বসে গান করে।', \"children sit on the tree's leaves and sing (playful image).\", 'children are sitting on the tree leaf.'), ('কাপড়ের পাতায় দাগ আছে—ক্লিনিং দরকার।', \"there is a stain on the fabric's surface—cleaning is needed.\", 'there are mark on the leaf—cleaning is needed.'), ('পত্রিকার পাতায় খবর পড়ে সে চিন্তিত হলো।', 'he became worried reading news in the newspaper pages.', 'he was worried about reading the news on the newspaper.'), ('বনজঙ্গল ঘেরা পাহাড়টি চূড়ায় পৌঁছেছি।', 'we reached the peak of the mountain surrounded by forest.', 'i have reached the top of the forest.'), ('নৌকার চূড়ায় দাঁড়িয়ে সে ছবি তুলল।', 'standing at the prow of the boat he took a picture.', 'he stood at the top of the boat and took a picture.'), ('তার কর্মজীবনের চূড়া এখনই চলছে।', 'the pinnacle of his career is happening right now.', 'his career ends now.'), ('শৈশবের আনন্দের চূড়া স্মৃতিতে রয়ে গেল।', 'the high point of childhood joy remained in memory.', 'the joy of childhood remained in memories.'), ('চূড়ায় হাওয়ার চাপ একটু বেশি ছিল।', 'the wind pressure at the summit was a bit stronger.', 'the air pressure was a bit high.'), ('ফসল উপড়ে গাছের নিচে রাখা হলো।', 'the crop was uprooted and placed under the tree.', 'the crops were placed under the tree.'), ('উপরের তলায় অতিথিরা রাত্রিযাপন করলেন।', 'guests stayed overnight on the upper floor.', 'guests stayed at the top floor.'), ('তুমি উপর থেকে পড়লে সাবধান হবে।', 'be careful when you fall from above.', 'you will be careful when you fall from above.'), ('পড়াশোনায় উপর দৃষ্টি রাখলে উন্নতি হয়।', 'keeping an overview in studies leads to improvement.', 'taking attention to study improves.'), ('দরজার উপর দাগ দেখা গেলে মেরামত করো।', \"if you see a mark on the door's top, repair it.\", 'if you see a mark on the door, repaired it.'), ('তার চিত্রকলা অত্যন্ত সূক্ষ্ম।', 'his painting art is extremely delicate.', 'his painting is very delicate.'), ('চিত্রকলা প্রদর্শনীতে নতুন প্রজন্মের কাজ ছিল।', 'the art exhibition featured works of the new generation.', 'a new generation works at the painting exhibition.'), ('শিক্ষক চিত্রকলায় রঙ ব্যবহারের কৌশল শিখালেন।', 'the teacher taught techniques of color use in painting.', 'the teacher taught methods of painting.'), ('চিত্রকলার জগতে তার নাম পরিচিতি পেয়েছে।', 'his name has gained recognition in the world of painting.', 'his name has been known in the painting world.'), ('চিত্রকলার ক্যানভাসে গল্পটা ফুটে উঠল।', \"the story came alive on the painting's canvas.\", 'the story flows in the canvas.'), ('চালক ব্রেক চাপলে গাড়ি থেমে গেল।', 'when the driver pressed the brake, the car stopped.', 'when the driver pressed the brick, the car stopped.'), ('ব্রেক করার সময় সতর্ক হও।', 'be careful while braking.', 'be careful when breaking.'), ('প্রকল্পের ব্রেক নেওয়া হলো সাময়িকভাবে।', 'the project was temporarily put on hold (brake).', 'the project was taken temporarily.'), ('ব্রেক নামের দোকানে যন্ত্রাংশ পাওয়া যায়।', 'parts are available at a shop named brake.', 'a piece is available in the shop named brick.'), ('ব্রেক লাইন ঠিক না থাকলে বিপদ ঘটতে পারে।', 'if the brake line is faulty, danger may occur.', 'if the brake line is not correct, there can be danger.'), ('বৃক্ষের ছাল ছেঁড়ে খেলনা বানাল ছেলেরা।', 'the boys tore the tree bark and made toys.', 'the boy made toys leaving the tree.'), ('চামড়ার ছাল নরম হলে চামড়া ভালো হবে না।', \"if the skin's surface is soft, the leather will not be good.\", 'if the skin is soft, it will not be good.'), ('ছাল সুন্দর না হলে জিনিসের মান কমে।', 'if the cover is not good, the quality of the item decreases.', 'if the tap is not beautiful, the quality of things is reduced.'), ('ছালের ছাল আঁচে ভাজা হচ্ছিল।', \"the shell's covering was being roasted in the heat.\", 'the tap was painted.'), ('কবিতায় ছাল রূপক হিসেবে ব্যবহৃত হয়েছে।', 'in the poem the shell is used as a metaphor.', 'the poem used as a shape.'), ('বাড়ির সামনে নালা পরিষ্কার করা হলো।', 'the drain in front of the house was cleaned.', 'the net was cleaned before the house.'), ('নালা নামের রাস্তাটি পাহাড়ের দিকে যায়।', 'a road named nala leads toward the hill.', 'the road named naala goes to the mountain.'), ('নালা বন্ধ থাকলে জল জমে যায়।', 'if the drain is blocked water accumulates.', 'when the net is closed, the water is accumulated.'), ('নালা দিয়ে পানি নিষ্কাশন করা হচ্ছে।', 'water is being drained through the channel.', 'water is extinguished through the net.'), ('সার্ভারে নালার মতো লগ ফাইল সংরক্ষণ করা হয়।', 'log files are stored in the server like a channel (metaphor).', 'log files are stored on the server.'), ('তারা মঞ্চে মহিমান্বিত ভাষণ দিলেন।', 'they delivered a grand speech on stage.', 'they gave glorious speech on the stage.'), ('মহিমান্বিত কাজের স্বীকৃতি দিলেন সবাই।', 'everyone acknowledged the magnificent work.', 'everyone recognized honorable work.'), ('মহিমান্বিত অনুষ্ঠানটি ভিড় অঞ্চল আকর্ষণ করল।', 'the magnificent event attracted a large crowd.', 'the prestigious event attracted crowd.'), ('কবিতায় মহিমান্বিত শব্দের ব্যবহার হয়েছে।', \"the word 'magnificent' has been used in the poem.\", 'in the poem used glorious words.'), ('তার মহিমান্বিত ইতিহাস সবাই পড়ে বিস্মিত হয়।', 'everyone is amazed after reading his glorious history.', 'everyone is surprised to read his glorious story.'), ('উপহার পেয়ে শিশুটি আনন্দে মাত্রা হারালো।', 'the child lost all restraint in joy upon receiving the gift.', 'after receiving the gift, the child lost levels of joy.'), ('উপহার প্যাকিং ঠিক করে দিতে ভুলোনা।', \"don't forget to pack the gift properly.\", \"don't forget to fix the gift.\"), ('উপহারের মূল্য নয়, চিন্তা দিয়েই দাও।', \"give with thought, not the gift's value.\", \"don't pay for gift but think about it.\"), ('উপহার নামের দোকানে বই অর্ন্তভুক্ত।', \"books are included at a shop named 'upohar'.\", 'the book is included in the gift shop.'), ('উপহার পেয়ে বৃদ্ধার চোখে জল এসে গেল।', \"tears came to the old woman's eyes on receiving the gift.\", \"getting a gift came to the old woman's eyes.\"), ('খামারে গাভীর দুধ সংগ্রহ করা হলো।', \"cow's milk was collected at the farm.\", 'grain milk was collected in the farm.'), ('দুধ খাবারের পাশাপাশি পুষ্টি জোগায়।', 'milk provides nutrition along with food.', 'along with milk, nutrities.'), ('রোজ সকালে দুধ নিয়ে হাটে আসো।', 'come to the market every morning with milk.', 'walk with milk in the morning.'), ('দুধ নামের শিশুটি ক্লাসে কান্নাকাটি করল।', 'a child named dudh cried in class.', 'a child named milk was crying in class.'), ('দুধ নয়, মিষ্টি দিলে শিশুটি শান্ত হয়।', 'not milk but sweets calm the child.', \"don't give milk, the baby calms.\"), ('তারা পাহাড়ের উপরে সরু পথ ধরে চাইল।', 'they walked along the narrow path on the hilltop.', 'they wanted a straight way above the mountain.'), ('সরু পথটি ভাঙলে কোনো গাড়ি পার পাবে না।', 'if the narrow road breaks, no vehicle can pass.', 'if you break the straight path, no car will pass.'), ('সরু রক সন্ধ্যায় শহরের চিত্র পরিবর্তন করে।', \"the narrow alley changes the city's evening scene.\", \"short rock changes the city's image at night.\"), ('শৈলীতে সরু রেখা দিয়ে চিত্র আঁকা হয়েছে।', 'in the painting, narrow lines were used.', 'painted with narrow lines.'), ('সরু-চোখে সে সব কিছু স্পষ্ট দেখতে পায়।', 'with a focused gaze he can see everything clearly.', 'he can see everything clearly.'), ('চাষী গাছের গোড়ায় সার ঢেলে দিল।', 'the farmer poured fertilizer at the base of the tree.', 'the farmer dropped fertilizer on the tree.'), ('তাহার ভাষায় সার নেই—শুধু সমালোচনা।', 'there is no essence in his speech—only criticism.', 'there is no fertilizer in the language—just criticism.'), ('প্রতিবেদন থেকে সার সরিয়ে নিলে বড় কিছু পাওয়া যাবে না।', \"if you remove the summary from the report, you won't get much.\", \"if you remove fertilizer from the report, you can't find anything big.\"), ('মাছ ধরার পরে সার দিয়ে মিশ্রি করা হয়।', 'after catching fish, they are cured with salt (contextual local process).', 'after catching the fish is mixed with fertilizer.'), ('ঐ সূত্রের সার দাও—মূল কথা লিখ।', 'give the gist of that source—write the main point.', 'give fertilization of that formula—writing the original words.'), ('শীতল সকালে কুল কাঁপিয়ে উঠল।', 'the koel (bird) shivered in the cold morning.', 'a cold morning clothed up.'), ('কুলের পাখা হারিয়ে যাওয়া শোকের বিষয়।', 'losing the clan (family) is a matter of sorrow (alternate sense).', 'the leaf is a matter of sorrow.'), ('গ্রামের কুলে কবর রয়েছে—ঐতিহাসিক স্থান।', \"the village's grove has graves—a historic place.\", 'there is a cave in the village - a historic place.'), ('কুল নামের মিষ্টির দোকান শহরে প্রসিদ্ধ।', 'a sweet shop named kul is famous in the city.', 'the sweet shop named kul is famous in the city.'), ('কুলের ছায়ায় বসে বুড়ো আড্ডা দেয়।', 'sitting in the shade of the grove the elders chat.', 'sitting in the shade of the tap.'), ('সেনা কাঁধে ব্যাগ নিয়ে রওনা দিল।', 'the soldier departed carrying a bag on his shoulder.', 'the army took the bag on his shoulder.'), ('কাঁধে দায়িত্ব নিলে অবশ্যই সতর্ক হও।', 'if you take responsibility on your shoulders, be careful.', 'if you take responsibility on your shoulders, be careful.'), ('কাঁধ নামের দোকানে জুতো পাওয়া যায়।', 'shoes are available at a shop named kadh.', 'shoes are available in the shoes shop.'), ('কাঁধে ব্যথা হলে ডাক্তার দেখাতে হবে।', 'if you have shoulder pain, you should see a doctor.', 'if you have pain in your shoulder, you need to show a doctor.'), ('বস্তা কাঁধে নিলে হাঁটা সহজ হয় না।', 'carrying a sack on the shoulder makes walking hard.', 'taking things on shoulder is not easy to walk.'), ('চিকিৎসকের তত্ত্বাবধানে পরীক্ষা নেওয়া হলো।', \"the test was conducted under the doctor's supervision.\", \"the examination was taken under the doctor's supervision.\"), ('প্রকল্পটি বিশ্ববিদ্যালয়ের তত্ত্বাবধানে চলছে।', \"the project is running under the university's oversight.\", 'the project is under supervision of the university.'), ('তত্ত্বাবধানে কাজ করলে ফল সুবিধাজনক হয়।', 'working under supervision yields favorable results.', 'working with supervision makes the result convenient.'), ('শিশুরা অভিভাবকের তত্ত্বাবধানে খেলছে।', \"children are playing under the guardian's supervision.\", 'children are playing under the supervision of parents.'), ('তত্ত্বাবধানে থাকা মানে দায়িত্ব পালন করা।', 'being under supervision means fulfilling responsibility.', 'being under supervision means fulfilling responsibility.'), ('শহরের প্রবেশ পথে গেইট লাগানো হলো।', \"a gate was installed at the city's entrance.\", 'the gate was placed on the city entrance.'), ('অনলাইন গেইটে লগইন করে কাজ শুরু করো।', 'log in to the online gateway to start work.', 'log in to the online gate and start work.'), ('গেইট নামের গানটি রেডিয়োতে বাজল।', 'a song named gate was played on the radio.', 'the song named gate played on the radio.'), ('কুয়াশায় গেইট দেখা যায় না—সাবধান হও।', 'the gate is not visible in the fog—be cautious.', 'the gate is not visible in the darkness—be careful.'), ('গেইটের লোহার খুঁটির রং বদলে ফেলো।', 'repaint the iron posts of the gate.', 'change the iron color of the gate.'), ('শিশুরা বনের মধ্যে কাঁদোব্রিড় দেখে বিস্মিত।', 'the children were amazed to see a deer in the forest.', 'children are surprised to see crying in the forest.'), ('ব্রিড় নামের কিশোরটি আজ উপস্থিত।', 'a boy named brir is present today.', 'the boy named brid is present today.'), ('ব্রিড়ের ছায়ায় বসে বৃদ্ধরা গল্প করেন।', \"the elders tell stories sitting under the tree's shade (local dialect usage).\", 'the elders are sitting in the shade of the bridge.'), ('বৃদ্ধ ব্রিড় থেকে মধু সংগ্রহ করা হয়।', 'honey is collected from the old grove (metaphorical).', 'honey is collected from old brings.'), ('ব্রিড়ের খাবার খেতে ছাগলেরা আসে।', \"goats come to eat near the grove's food sources.\", 'sheep come to eat rice food.'), ('বইয়ের পাতা থেকে শিক্ষার আলো ছড়ায়।', 'the light of learning spreads from the pages of a book.', 'the light spreads from the leaf.'), ('গানের পাঠ অনুশীলনে সময় লাগে।', 'it takes time to practice the rendition of a song.', 'lessons take time to practice.'), ('পাঠ শুরু হওয়ার আগে বই প্রস্তুত রেখ।', 'keep the books ready before the lesson begins.', 'keep the book ready before reading begins.'), ('পাঠ্যবইতে পাঠ সংযোজন করা হয়েছে।', 'a lesson has been added to the textbook.', 'the lesson was added to the textbook.'), ('কবিতার পাঠ শ্রোতাদের আবেগ ছুঁয়েছে।', \"the recitation of the poem touched the audience's emotions.\", 'the reading of poem touched audiences.'), ('গৃহিণীরা ভাত ভাজা করে দুপুরের জন্য সাজাল।', 'the homemakers fried rice and prepared it for lunch.', 'households cooked meat for lunch.'), ('ভাজা নামের দোকানে ক্রেতারা ভিড় করছে।', 'customers are crowding the shop named bhaja.', 'buyers are crowded in the shop named fried.'), ('ভাজা হওয়ার সময় তেল ঠিক রাখো।', 'keep the oil at the right temperature while frying.', 'keep the oil while cooling.'), ('ভাজা মিষ্টি দাও অতিথিদের।', 'serve fried sweets to the guests.', 'give fresh sweet to guests.'), ('ভাজা হয়ে গেলে সুগন্ধ ঘরভরে ছড়ায়।', 'when the frying is done, a fragrance fills the house.', 'when it becomes fresh, the smell spreads to the house.'), ('কৃষকের ধান কাটা নিয়ে উৎসব চলছে।', \"a festival is ongoing celebrating the farmer's harvest.\", \"the farmer's cutting is ongoing.\"), ('উৎসবে গান ও নাচের আয়োজন ছিল।', 'the festival featured song and dance programs.', 'there were songs and dances at the festival.'), ('উৎসব নামের রেস্তোরাঁয় আজ ভিড় বেশি।', \"a restaurant named 'utsab' is very crowded today.\", 'the restaurant named festival is crowded today.'), ('শিক্ষার উৎসবে ছাত্ররা সঞ্চয় উপহার পেল।', 'students received savings gifts at the education fair.', 'students received preserved gifts at the education festival.'), ('উৎসবের আনন্দে গ্রামজুড়ে ঝকমক হয়েছে।', 'the village is sparkling with joy during the festival.', 'in the village, the joy of the festival.'), ('বইয়ের খসড়া নিয়ে সম্পাদক বাড়তি কাজ করলেন।', \"the editor did extra work on the book's draft.\", 'the editor worked with the book design.'), ('খসড়ায় গুরুত্বপূর্ণ পরিবর্তন করা হয়েছে।', 'important changes have been made in the draft.', 'important changes have been made in the draft.'), ('খসড়া প্রতিবেদন উপস্থাপন করে তারা পরিকল্পনা করল।', 'they planned after presenting the draft report.', 'they presented the draft report.'), ('খসড়া পর্যবেক্ষণ করে চূড়ান্ত পত্র তৈরি করা হয়।', 'after reviewing the draft, the final paper is prepared.', 'the final letter is made by monitoring the draft.'), ('খসড়া থামলে কাজ ধীর হয়ে যায়।', 'if drafting stops, the work slows down.', 'when the project stops, the work becomes slow.'), ('সিগারেট ধোঁয়া বাতাসে ভেসে গেল।', 'cigarette smoke drifted away in the air.', 'cigarette smoke in the air.'), ('হুট করে রান্নাঘর থেকে ধোঁয়া উঠতে লাগল।', 'suddenly smoke began rising from the kitchen.', 'he started to smoke from the kitchen.'), ('প্রকৃতির ধোঁয়ায় ছবি যেন মলিন হয়ে যায়।', \"the scenery seems blurred by nature's haze.\", 'like the to smoke of nature.'), ('ধোঁয়া নামের দোকানে মশলা পাওয়া যায়।', \"spices are available at a shop named 'dhoya'.\", 'spices are available in the smoke shop.'), ('ধোঁয়া-মুক্ত পরিবেশে বসবাস করা ভালো।', 'living in a smoke-free environment is good.', 'living in dust-free environment is good.'), ('বিন্দু কণা জলেতে নাচছে সূর্য আলোর মতো।', 'dew drops dance like sunlight on the water.', 'the point is dancing in the water like the sunlight.'), ('কবিতার প্রতিটি বিন্দু অর্থে পূর্ণ।', 'every point of the poem is full of meaning.', 'every point of the poem is full of meaning.'), ('বিন্দু নামের ছোট শহরটি ভ্রমণ উপযোগী।', 'a small town named bindu is suitable for travel.', 'the small town named point is useful to travel.'), ('उसके চিন্তার বিন্দু গুলো মিলিয়ে গল্প তৈরি করো।', 'combine the points of his thoughts to create a story.', 'combine his points of thinking and create stories.'), ('রাস্তার বিন্দুতে বিশ্রামের ব্যবস্থা করা হয়েছে।', 'a rest facility has been arranged at the road point.', 'rest arranged at the road point.'), ('তাঁর কণ্ঠে ভোলার টান ছিল।', 'there was a plaintive pull in his voice.', 'his voice was pulled.'), ('ভোলার নামের মেঘলা গ্রামে বৃষ্টি পড়েছে।', 'it rained in a cloudy village named vola.', 'rain has fallen in the village named vollar.'), ('ভোলার সময় সবকিছু যেন চুপ করে থাকে।', 'during dawn everything seems silent.', \"everything remains silent while you'll forget.\"), ('ভোলার আভায় হ্রদটির পানিতে আলো পড়ল।', \"the lake's water reflected light in the dawn glow.\", 'light falls in the lake.'), ('ভোলার গান শুনে মন ঠিক হয়ে যায়।', 'listening to the dawn song soothes the mind.', 'listening to the song becomes good.'), ('শীতল বাতাসে শিশুরা কাশে কাশে।', 'children cough in the chilly wind.', 'children are caught in cold air.'), ('রাস্তায় কাশে কাশে কুকুরগুলি ঘুরছে।', 'dogs are wandering about the street, coughing (metaphorical use).', 'dogs are walking on the street.'), ('কাশে নামের পুকুরটি শীতকালে জমে যায়।', 'a pond named kas freezes in winter.', 'the poem named cush is accumulated in winter.'), ('হঠাৎ কাশে করে সবাই চমকে উঠল।', 'everyone was startled by the sudden coughing sound.', 'suddenly everyone surprised.'), ('কষে কষে কাশে হলে ডাক্তার দেখাও।', 'if you cough persistently, see a doctor.', 'if you are caused, see the doctor.'), ('তারা মেয়েকে রূপে সুন্দর বলে আদর করল।', 'they fondly called the girl beautiful in appearance.', 'they showed the girl beautifully.'), ('রূপ নামের কৃষক মাঠে ব্যস্ত।', 'a farmer named rup is busy in the field.', 'the farmer named form is busy in the field.'), ('কবিতায় রূপকে প্রকৃতির অঙ্গ হিসেবে উপস্থাপন করা হয়েছে।', 'the poem presents form as a part of nature.', 'the form is presented in the poem as an organ of nature.'), ('শিল্পীতে রূপ-রং ভালোভাবে মিশে আছে।', 'the artist blends form and color well.', 'the color is mixed well in the art.'), ('রূপ পরিবর্তন করলে বস্তুর মান বোঝা যায়।', \"changing the form shows the object's quality.\", 'if you change the form, you understand the value of the object.'), ('ওয়ালারের দোকানে নতুন জামাকাপড় এসেছে।', \"new clothes have arrived at the tailor's shop.\", 'new clothes have come to the shop.'), ('ওয়ালার কাজ খুব সূক্ষ্ম ও নিখুঁত।', \"the tailor's work is very fine and precise.\", 'the work is very delicate and perfect.'), ('শহরে ওয়ালার নামে কয়েকটি দোকান আছে।', \"there are several shops named 'walar' in the town.\", 'there are several shops named wall in the city.'), ('ওয়ালার ছাঁটা সঠিক হলে পোশাক জমে।', \"if the tailor's cut is accurate, the garment pleases.\", 'if the tap is correct, clothes are accumulated.'), ('ওয়ালারের হাতের ছোঁয়ায় কাপড় নরম লাগে।', \"the fabric feels soft with the tailor's touch.\", 'clothes look soft at the touch of the walker.'), ('তারা আলো নিভিয়ে কুঁড়েঘরে ফিরে গেল।', 'they turned off the light and returned to the hut.', 'they removed the light and returned to the room.'), ('কুঁড়ে নামের গ্রামটি পাহাড়ের পাদদেশে।', 'a village named kure is at the foot of the hill.', 'the village named to the bottom of the mountain.'), ('কুঁড়ে-ছাদে বসে গল্প করা ভালো।', 'sitting on the hut roof to tell stories is pleasant.', 'it is good to sit on the shade.'), ('কুঁড়ে কাঠ বাচিয়ে জ্বালানী হিসেবে ব্যবহার করা হয়।', 'hut timber is conserved and used as fuel.', 'wood is used as fuel.'), ('কুঁড়েতে সাঁতার কাটলে ঠাণ্ডা লাগে।', 'swimming by the hut feels cold.', 'swimming feels cold.'), ('শহরের সব্জির ঠেলায় লোক ভিড় করছে।', \"people are crowding the city's vegetable cart.\", 'people are crowded in the city.'), ('ঠেলায় নামের দোকানে তাজা মাল আসে।', 'fresh goods arrive at a shop named thela.', 'fresh goods come to the shop named.'), ('চাকা ঠেলে গাড়িটি ঠেলায় নেয়া হলো।', 'pushing the wheel, the vehicle was moved by force.', 'the wheel was pulled down.'), ('সময় ঠেলে এগিয়ে নাও—অপচয় করো না।', 'push forward the time—do not waste it (idiomatic advice).', 'take time forward—noun.'), ('ঠেলাকে সামলাতে দুজন দরকার।', 'it takes two people to manage the push.', 'i need two to handle this.'), ('তাঁর কণ্ঠে স্বরলহর শোনা গেল।', 'a melodious note was heard in his voice.', 'he heard his voice.'), ('স্বরের প্রশিক্ষণে নিয়মিত অনুশীলন দরকার।', 'regular practice is needed in voice training.', 'regular exercise is needed.'), ('পবিত্র সংগীতে স্বরের ভাড়া আলাদা।', 'in sacred music the tone has a special value.', 'the saint is distinct.'), ('স্বর নামের কিশোরটা সুরে দক্ষ।', 'a boy named swar is skilled in melody.', 'the boy named sound is skilled.'), ('তার স্বর লয় ঠিক রেখে গান গাইতে সাহায্য করল।', 'his voice helped keep the rhythm steady while singing.', 'keeping his sound correctly helped singing.'), ('ফসলের ঘরে ধান গুছিয়ে রাখা হয়েছে।', 'paddy has been stacked and stored in the granary.', 'crop has been stored in the house.'), ('তার ঘরটি লাল গাড়ির পাশে।', 'his house is beside the red car.', 'his room is next to the red car.'), ('ঘর নামের একটি ছোট বেকার দোকান আছে শহরে।', 'there is a small bakery named ghar in the city.', 'there is a small shop named house in the city.'), ('ঘরের জানালা দিয়ে রোদ পড়ছে।', \"sunlight is falling through the room's window.\", 'sun is falling through the window of the house.'), ('নতুন পরিবারের ঘর সাজাতে সময় লেগেছে।', \"it took time to decorate the new family's home.\", \"it took time to decorate the new family's house.\"), ('সড়কে বড় গর্ত থাকায় গাড়ি থামল।', 'the car stopped because of a big pothole in the road.', 'the car stopped because there was a big hole on the road.'), ('গর্ত নামের গ্রামের প্রবেশ পথটি সংকীর্ণ।', 'the entrance path of a village named gort is narrow.', 'the entrance to the village named hurt is narrow.'), ('গর্তে পড়ে শিশুটি কাঁদতে শুরু করল।', 'the child began to cry after falling into the hole.', 'falling in the hole the child started crying.'), ('পাইপের গর্ত ঢেকে দিলে সমস্যাটা মিটে যায়।', \"covering the pipe's hole fixes the problem.\", 'if you cover the tube hole, the problem dissolves.'), ('গর্তের স্টোরগুলো মেরামত করা জরুরি।', 'repairing the holes in the stores is necessary.', 'it is necessary to repaired the hole.'), ('তারা তিনজন হাত মেলিয়ে ধাত্রী গঠন করল।', 'the three of them formed a pact by joining hands.', 'they formed three hands.'), ('ধাত্রী নামের সংস্থা গ্রামের উন্নয়নে কাজ করে।', 'an organization named dhatri works on village development.', 'the agency named daddy works on the development of the village.'), ('ধাত্রী দেখতে সজীব ও যত্নশীল।', 'the caregiver appears lively and caring.', 'she looks vigilant and careful.'), ('শিশুর হতাশায় ধাত্রী তাকে আশ্বস্ত করল।', 'the guardian consoled the child in despair.', \"the child's disappointment comforted him.\"), ('ধাত্রী-অনুষ্ঠানে অতিথিরা ফুল দিয়ে অভিবাদন জানালেন।', 'guests greeted with flowers at the sponsorship ceremony.', 'the guests greeted with flowers.'), ('নদীর তীরে কাঁকড়া ধরা হয় এখানকার লোকেরা।', 'people here catch crabs on the riverbank.', 'people are caught on the river.'), ('কাঁকড়া নামের রেস্টুরেন্টে সীফুড ভালো তৈরি হয়।', 'seafood is well prepared at a restaurant named kakra.', 'the food is made well in the restaurant named cabbage.'), ('কাঁকড়ার খোলস শক্ত, তাই চিঁড়ে ফেলা কঠিন।', 'crab shells are hard, so breaking them is difficult.', \"the spice is hard, so it's difficult to cut off.\"), ('শীতকালে কাঁকড়া বেশি ধরা পড়ে।', 'more crabs are caught in winter.', 'cough is caught more in winter.'), ('বাজারে কাঁচা কাঁকড়ার দাম বেশি।', 'raw crabs cost more in the market.', 'the price is high in the market.'), ('আমের মৌসুমে বাগানে অনেক আম পাকে।', 'many mangoes ripen in the orchard during mango season.', 'i have many feet in the garden during my season.'), ('আম জনতা পার্টি নির্বাচনে জিতেছে।', 'aam aadmi party won the election.', 'the people won elections.'), ('আম আদমি সবসময় কষ্ট করে।', 'common people always struggle.', 'i am always suffering.'), ('আমের রস খুব মিষ্টি হয়।', 'mango juice is very sweet.', 'my juice is very sweet.'), ('আম জনগণের সমস্যা সমাধান করা দরকার।', 'the problems of ordinary citizens need to be solved.', 'people need to solve problems.'), ('বাতাসে ঘুড়ি উড়ছে আকাশে।', 'a kite is flying in the sky in the wind.', 'the air is flying in the sky.'), ('ঘুড়ি পাখি শিকার ধরছে মাঠে।', 'a kite bird is hunting prey in the field.', 'birds are catching on the field.'), ('সকালে ঘুড়ি ওড়ানো মজার।', 'flying kites in the morning is fun.', 'flying in the morning is fun.'), ('ঘুড়ি শিকারী পাখি হিসেবে পরিচিত।', 'the kite is known as a predatory bird.', 'the hunt is known as bird.'), ('ঘুড়ির সুতো কেটে গেছে।', 'the kite string has been cut.', 'the tower has been cut.'), ('হাতির দাঁত খুব দামি।', 'elephant tusks are very expensive.', 'elephant teeth are very expensive.'), ('হাতি বিল্ডিং এ অফিস আছে।', 'there is an office in hathi building.', 'there is a office in the elephant building.'), ('হাতির শক্তি অসাধারণ।', 'the strength of an elephant is extraordinary.', \"the elephant's strength is extraordinary.\"), ('বনে বুনো হাতি দেখা যায়।', 'wild elephants are seen in the forest.', 'the elephant is seen in the forest.'), ('হাতি চালক হাতিকে নিয়ন্ত্রণ করছে।', 'the mahout is controlling the elephant.', 'the elephant is controlling the elephant.'), ('তার মন খুব ভালো নেই আজ।', 'his mind is not very well today.', 'his mind is not very good today.'), ('এক মন চাল কিনেছি বাজার থেকে।', 'i bought one maund of rice from the market.', 'i bought a heart from the market.'), ('মন দিয়ে পড়াশোনা করো।', 'study attentively.', 'study with mind.'), ('মন বসছে না কাজে।', 'the mind is not settling on work.', 'not sitting in work.'), ('দুই মন গম লাগবে।', 'two maunds of wheat will be needed.', 'i will need two.'), ('কলের জল খাওয়া যায়।', 'tap water can be drunk.', 'water can be eaten.'), ('কলা খেতে ভালো লাগে।', 'i like eating bananas.', 'i like to eat.'), ('কলের জল বন্ধ আছে।', 'the tap water is shut off.', 'the call is closed.'), ('সকালে কল খাই নিয়মিত।', 'i eat bananas regularly in the morning.', 'i am eating regularly in the morning.'), ('কল কারখানায় কাজ করি।', 'i work in a factory.', 'i am working in the call factory.'), ('আলু দিয়ে তরকারি রান্না হয়েছে।', 'curry has been cooked with potatoes.', 'cooked with potatoes.'), ('আলু ভাজা খুব মজার।', 'fried potatoes are very tasty.', 'the potato is very fun.'), ('আলু চাষ করা লাভজনক।', 'potato cultivation is profitable.', 'cultivation is profitable.'), ('আলু বসতে সময় লাগে না।', \"it doesn't take time for potatoes to settle.\", \"it doesn't take time to sit.\"), ('এই জমিতে আলু ভালো হয়।', 'potatoes grow well in this land.', 'potatoes are good in this soil.'), ('পাতা ঝরে যাচ্ছে গাছ থেকে।', 'leaves are falling from the tree.', 'leaves are falling from the tree.'), ('পাতায় লিখে রাখো নামটা।', 'write down the name on a page.', 'write the name on the page.'), ('পাতা হলুদ হয়ে গেছে।', 'the leaves have turned yellow.', 'the page has become yellow.'), ('বইয়ের পাতা ছিঁড়ে গেছে।', 'the book page has been torn.', 'the page is broken.'), ('কলাই পাতা খেতে ভালো।', 'kolai leaves are good to eat.', 'it is good to eat leaves.'), ('ঘরের চাল পুরনো হয়ে গেছে।', 'the roof of the house has become old.', 'the house has become old.'), ('চাল ধুয়ে রান্না করো।', 'wash the rice and cook it.', 'wash the rice.'), ('চালের দাম বেড়েছে।', 'the price of rice has increased.', 'the price increased.'), ('চাল মেরামত করতে হবে।', 'the roof needs to be repaired.', 'need to repaired.'), ('চাল কিনে এনেছি বাজার থেকে।', 'i have bought rice from the market.', 'i bought rice from the market.'), ('মাটির পাত্রে জল রাখা আছে।', 'water is kept in an earthen pot.', 'water is placed in the soil.'), ('মাটি খুঁড়ে চাষ করা হয়।', 'the soil is dug up for cultivation.', 'the soil is cultivated.'), ('মাটির ব্যাংক থেকে টাকা তুলেছি।', 'i withdrew money from a clay bank.', 'i have taken money from the bank.'), ('মাটি উর্বর হলে ফসল ভালো হয়।', 'when soil is fertile, crops grow well.', 'when the soil is fertile, the crops are good.'), ('মাটির কাজ করছে কুমোর।', 'the potter is working with clay.', 'the soil is working.'), ('হাতে কলম ধরে লিখছি।', 'i am writing holding a pen in hand.', 'i have a pen in my hand.'), ('হাত দিয়ে খাবার খাই।', 'i eat food with my hand.', 'i eat with my hand.'), ('হাত ধুয়ে নাও আগে।', 'wash your hands first.', 'wash your hands before.'), ('তার হাত খুব ভালো রান্নায়।', 'her hand is very good at cooking.', 'his hands are cooking very well.'), ('হাত বাড়িয়ে সাহায্য করো।', 'extend your hand to help.', 'lift your hand.')], 'csv_file': 'evaluation_results_2000.csv'}\n✅ Cell 13: Large-scale evaluation (2000+ samples) - ready to run\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Debug & fallback tokenizer with offsets\nimport unicodedata, re\n\ndef safe_tokenize_with_offsets(tokenizer, text):\n    \"\"\"\n    Return (input_ids_tensor, tokens_list, offsets_list).\n    - If tokenizer supports return_offsets_mapping, use it (fast tokenizers).\n    - Else, fall back to slow-tokenizer path: get tokens and reconstruct character offsets.\n    \"\"\"\n    # Try fast tokenizer API first\n    try:\n        enc = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', add_special_tokens=False)\n        ids = enc['input_ids']\n        tokens = tokenizer.convert_ids_to_tokens(ids[0])\n        offsets = [(int(s), int(e)) for (s, e) in enc['offset_mapping'][0].tolist()]\n        return ids, tokens, offsets\n    except NotImplementedError:\n        pass\n    except Exception as e:\n        # Other exceptions: fall back to slow approach\n        pass\n\n    # Slow fallback: get token strings then greedily align with the original text.\n    # This tries to be robust for SentencePiece (▁) and BPE (##) style markers.\n    tokens = tokenizer.tokenize(text, add_special_tokens=False)\n    # Normalize text for matching\n    norm_text = unicodedata.normalize('NFC', text)\n    offsets = []\n    pos = 0\n\n    # Helper to try matching a token string to a substring at or after pos\n    def find_token_span(tok, pos):\n        # Clean token markers to a comparable surface form\n        surface = tok\n        # SentencePiece style: leading '▁' means a word boundary and stands for a space\n        if surface.startswith('▁'):\n            surface = surface.replace('▁', '')\n            # we allow matching at current pos or after a whitespace\n            # attempt to find after the most recent whitespace or at pos\n        # BERT style: '##' is a continuation subword marker\n        if surface.startswith('##'):\n            surface = surface[2:]\n        # Try direct find starting at pos\n        idx = norm_text.find(surface, pos)\n        if idx != -1:\n            return idx, idx + len(surface)\n        # If not found, try skipping whitespace characters forward up to some limit\n        m = re.search(re.escape(surface), norm_text[pos:])\n        if m:\n            start = pos + m.start()\n            return start, start + len(surface)\n        return None\n\n    for tok in tokens:\n        # For empty tokens (rare), skip\n        if len(tok.strip()) == 0:\n            offsets.append((pos, pos))\n            continue\n        res = find_token_span(tok, pos)\n        if res is None:\n            # As a last resort try searching from zero (global)\n            res = find_token_span(tok, 0)\n        if res is None:\n            # Give up for this token — mark a zero-length span to avoid crashes\n            offsets.append((0, 0))\n        else:\n            start, end = res\n            offsets.append((start, end))\n            pos = end  # advance scanning position\n\n    # Build input_ids tensor (without offsets API): use tokenizer.encode to get ids\n    ids_list = tokenizer.encode(text, add_special_tokens=False)\n    import torch\n    ids_tensor = torch.tensor([ids_list], dtype=torch.long)\n    return ids_tensor, tokens, offsets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:49:19.207479Z","iopub.execute_input":"2025-11-22T18:49:19.207869Z","iopub.status.idle":"2025-11-22T18:49:19.217021Z","shell.execute_reply.started":"2025-11-22T18:49:19.207844Z","shell.execute_reply":"2025-11-22T18:49:19.216273Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Quick DSCD/TRG debug cell - paste into your notebook and run\n# Edit `SENT` if you want to test a different failing sentence.\nSENT = \"তিনি ব্যাংক গেছেন।\"  # change to a failing sentence you saw\n\nprint(\"SENTENCE:\", SENT)\n\n# 1) Tokenizer tokens + offsets\ntry:\n    enc = tokenizer(SENT, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n    toks = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n    offs = enc[\"offset_mapping\"][0].tolist()\n    print(\"\\nTOKENIZER (fast) tokens and offsets:\")\n    print(\"tokens =\", toks)\n    print(\"offsets =\", offs)\nexcept Exception as e:\n    print(\"\\nTokenizer fast path failed:\", repr(e))\n    try:\n        # fallback: encode_plus older API\n        enc2 = tokenizer.encode_plus(SENT, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n        toks = tokenizer.convert_ids_to_tokens(enc2[\"input_ids\"][0])\n        offs = enc2[\"offset_mapping\"][0].tolist()\n        print(\"\\nTOKENIZER (fallback) tokens and offsets:\")\n        print(\"tokens =\", toks)\n        print(\"offsets =\", offs)\n    except Exception as e2:\n        print(\"Tokenizer fallback also failed:\", repr(e2))\n        print(\"Please tell me the tokenizer variable name and whether it's a fast tokenizer (use_fast=True).\")\n\n# 2) Check reconstruct_word_spans / safe helpers if present\nfor helper in (\"reconstruct_word_spans\", \"safe_offsets_tokenize\", \"safe_tokenize_with_offsets\"):\n    if helper in globals():\n        try:\n            print(f\"\\nCalling {helper}(...):\")\n            res = globals()[helper](tokenizer, SENT)\n            print(helper, \"->\", type(res), repr(res)[:1000])\n            break\n        except Exception as e:\n            print(helper, \"exists but raised:\", repr(e))\n    else:\n        # not in globals, try to import from likely module if present\n        pass\n\n# 3) Inspect dscd object on model (supports model or model.module)\ndscd = None\ntry:\n    if hasattr(model, \"module\"):\n        dscd = getattr(model.module, \"dscd\", None)\n    else:\n        dscd = getattr(model, \"dscd\", None)\n    print(\"\\nFound dscd on model?:\", dscd is not None)\nexcept Exception as e:\n    print(\"Error checking model.dscd:\", repr(e))\n\n# 4) Ask DSCD if it would track the canonical token for the word(s)\n# We will attempt a couple of likely canonical forms from the token list above.\ncands = []\ntry:\n    # gather candidate tokens from tokenizer output\n    cands = [t for t in toks if t and t not in tokenizer.all_special_tokens]\nexcept Exception:\n    pass\n# include an explicit candidate with the homograph word if not present\nif \"ব্যাংক\" not in cands:\n    cands.append(\"ব্যাংক\")\n\nprint(\"\\nCandidate tokens to test should_track_token (sample):\", cands[:10])\n\nif dscd is None:\n    print(\"\\nNo dscd found on the model instance. If DSCD is a separate object, please provide its variable name.\")\nelse:\n    for w in cands[:10]:\n        try:\n            st = getattr(dscd, \"should_track_token\", None)\n            if st is None:\n                print(\"dscd.should_track_token not found on dscd object.\")\n                break\n            ok = st(w)\n            print(f\"should_track_token('{w}') -> {ok}\")\n        except Exception as e:\n            print(\"should_track_token call failed for\", w, \"->\", repr(e))\n\n# 5) Print buffer and prototype store status for the canonical homograph 'ব্যাংক'\nif dscd is not None:\n    try:\n        buf_keys = list(getattr(dscd, \"buffers\", {}).keys())[:50]\n        proto_keys = list(getattr(dscd, \"prototype_stores\", {}).keys())[:50]\n        print(\"\\nDSCD buffer keys (sample, up to 50):\", buf_keys)\n        print(\"DSCD prototype_stores keys (sample, up to 50):\", proto_keys)\n        print(\"len buffer for 'ব্যাংক':\", len(dscd.buffers.get(\"ব্যাংক\", [])))\n        ps = dscd.prototype_stores.get(\"ব্যাংক\")\n        if ps is None:\n            print(\"No prototype store for 'ব্যাংক' yet.\")\n        else:\n            try:\n                print(\"prototype store for 'ব্যাংক' .size():\", ps.size())\n                print(\"prototype store counts (if present):\", getattr(ps, \"counts\", None))\n            except Exception as e:\n                print(\"Cannot inspect prototype store object:\", repr(e))\n    except Exception as e:\n        print(\"Error inspecting dscd buffers/prototypes:\", repr(e))\n\n# 6) Try a forward with explanations (best-effort). Skip if model or device problems.\ntry:\n    model.eval()\n    device = next(model.parameters()).device\n    ids = enc[\"input_ids\"].to(device)\n    attn = enc[\"attention_mask\"].to(device)\n    out = None\n    # try a few common forward APIs\n    if hasattr(model, \"forward_with_explanations\"):\n        out = model.forward_with_explanations(input_ids=ids, attention_mask=attn)\n    elif hasattr(model, \"forward\") and callable(getattr(model, \"forward\")):\n        out = model(input_ids=ids, attention_mask=attn)\n    else:\n        out = model(ids, attention_mask=attn)\n    print(\"\\nModel forward success. Top-level keys in output (repr):\")\n    try:\n        print(list(out.keys()))\n    except Exception:\n        print(repr(out)[:1000])\n\n    # attempt to extract dscd outputs or proto_probs\n    dscd_out = None\n    if isinstance(out, dict):\n        for k in (\"dscd_outputs\", \"dscd_out\", \"explanations\", \"extra\"):\n            if k in out:\n                print(\"Found key in model output:\", k)\n                dscd_out = out[k]\n                break\n    # also try attribute on model if forward didn't return\n    if dscd_out is None:\n        dscd_out = getattr(model, \"last_dscd_outputs\", None)\n    print(\"dscd_out found?:\", dscd_out is not None)\n    if dscd_out:\n        # try to print proto_probs & uncertainties shapes or repr\n        for name in (\"proto_probs\", \"uncertainties\", \"gates\", \"token_word_map\"):\n            if isinstance(dscd_out, dict) and name in dscd_out:\n                v = dscd_out[name]\n                try:\n                    print(f\"{name}: type={type(v)}, repr slice={repr(v)[:400]}\")\n                except Exception:\n                    print(f\"{name}: type={type(v)} (couldn't repr)\")\nexcept Exception as e:\n    print(\"\\nModel forward/extraction failed (this is OK if your environment can't run the model here):\", repr(e))\n\nprint(\"\\n--- END DEBUG CELL ---\\n\")\nprint(\"Copy the full printed output and paste it here. I will give the exact one-line fix next.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:49:19.217794Z","iopub.execute_input":"2025-11-22T18:49:19.218114Z","iopub.status.idle":"2025-11-22T18:49:19.237468Z","shell.execute_reply.started":"2025-11-22T18:49:19.218096Z","shell.execute_reply":"2025-11-22T18:49:19.236732Z"}},"outputs":[{"name":"stdout","text":"SENTENCE: তিনি ব্যাংক গেছেন।\n\nTokenizer fast path failed: NotImplementedError('return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674')\nTokenizer fallback also failed: NotImplementedError('return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674')\nPlease tell me the tokenizer variable name and whether it's a fast tokenizer (use_fast=True).\n\nCalling reconstruct_word_spans(...):\nreconstruct_word_spans -> <class 'tuple'> ({0: 'তিনি', 1: 'ব্যাং', 2: 'ব্যাংক', 3: 'গ', 4: 'গেছেন', 5: 'গেছেন।'}, ['তিনি', 'ব্যাংক', 'গেছেন।'])\nError checking model.dscd: NameError(\"name 'model' is not defined\")\n\nCandidate tokens to test should_track_token (sample): ['ব্যাংক']\n\nNo dscd found on the model instance. If DSCD is a separate object, please provide its variable name.\n\nModel forward/extraction failed (this is OK if your environment can't run the model here): NameError(\"name 'model' is not defined\")\n\n--- END DEBUG CELL ---\n\nCopy the full printed output and paste it here. I will give the exact one-line fix next.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}